organizations:
  - organization_id: 1
    organization_name: 52°North Spatial Information Research GmbH
    no_of_ideas: 3
    ideas_content: |
                1. KomMonitor
                Angular migration of the KomMonitor Web Client
                Explanation
                KomMonitor is a web based tool that combines methods of GIS (Geographic information System) and statistical data and helps in providing a simpler and easier way to monitor geo-spatial data. Many municipalities have established KomMonitor to address a wide range of challenges in fields such as urban planning, environmental management, and disaster response. The current version of the KomMonitor Web Client has been developed using AngularJS, which has served as a reliable foundation for its functionalities. However, AngularJS has been deprecated for some years now. Therefore, relying on the current code base has several potential drawbacks associated with using AngularJS, such as compatibility issues, limited community support, reduced performance, and version support. To overcome these challenges and take KomMonitor to the next level, it is necessary to adopt the KomMonitor Web Client to the more modern and widely-supported framework Angular. As part of GSoC 2023, essential work has been done by developing a general approach for the Angular migration. The Web Client has been restructured so that it can be deployed as a hybrid web application, which runs both legacy AngularJS components and migrated or new Angular components. This year, the project aims to continue the migration tasks. Hence, the goal of this project is to reimplement several selected components of the KomMonitor Web Client by using the Angular framework.

                Expected Results
                As a result of the project, it is expected that several selected components of the KomMonitor Web Client will have been reimplemented with the Angular framework. The resulting UI of the reimplemented components should be as close as possible to the previous design to preserve the current look&feel. As an additional requirement, the reimplementation should take into account best practices and common design patterns in Angular. This results in also restructuring some of the existing components rather than simply transferring a component from AngularJS to Angular. Finally, the hybrid Web Client, including legacy AngularJS components and new Angular components side-by-side, should run properly without any bugs.

                Code Challenge
                Migrate the kommonitorToastHelperService of the KomMonitor Web Client to Angular and make use of it in a new Angular component as part of the Web Client. Follow the steps below:

                Create a fork of https://github.com/KomMonitor/web-client and checkout the GSoC2025 Branch
                Create a new Angular service as part of the KomMonitor Web Client that provides the same functionality as the existing AngularJS version of the kommonitorToastHelperService
                Create a new Angular component that makes use of the previously implemented kommonitorToastHelperService. Take into account these requirements:
                The component should be opened and closed by clicking on a button on the left sidebar.
                The component should include a text area and a button.
                The required functionality should be to display a message as toast on the screen by filling the text area and clicking on the button. For this purpose the kommonitorToastHelperService should be used.
                Push the code to your fork at GitHub
                Link to the fork within your official GSoC application. Your GSoC application should also include a description of which components you plan to migrate during GSoC as well as an estimation of time required for implementing it.


                Community and Code License
                Apache Software License, Version 2

                Mentors
                Sebastian Drost (s.drost @52north.org), Christoph Wagner (c.wagner @52north.org)

                Project Duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                Chat
                TBD

                ~~~~~~~~~~

                2. LLM and GeoData
                Explanation
                During 52°North’s Student Innovation Challenge in 2024, a first open-source implementation connecting spatial data and Large Language Models (LLM) was developed.

                The ambition was to address the pain points of searchability in Research and Spatial Data Infrastructures (RDI/SDI). Search functionality in such systems is typically limited to a metadata-based approach. However, geospatial data – whether vector or raster based – provides a wealth of interesting data that can currently only be identified by looking at the individual dataset. The challenge of the 2024 Student Innovation Prize was to develop a concept and a possible implementation that allows searching within datasets of/and RDI/SDI, e.g. on the attribute level. There are many interesting aspects related to this challenge: technical solutions, taxonomies and semantics, language/i18n, searching in raster data, and many more such as LLMs.

                The available Proof of Concept (PoC) features a prompt that makes it easier to search and access to spatial data. More user stories are documented in the Innovation Prize project backlog on GitHub: https://github.com/52North/innovation-prize.

                Expected Results
                The PoC should be hardened and developed beyond its current state. For example, less verbose prompts are needed as more sophisticated LLMs emerge. Also, improved software frameworks may provide a better development experience. Various extensions are possible and a selection should be outlined in the proposal. Additional user stories from the backlog in the github project (see above) could be addressed. Another interesting extension could also entail a federated architecture. Furthermore, the use of different LLMs is also a possible option for further development.

                Code Challenge
                Set up the entire working environment based on the existing open source code

                https://github.com/52North/innovation-prize/tree/2024

                and add two more data sets. Share the code and the deployed system.

                Community and Code License
                TBChecked: Apache Software License, Version 2

                Mentors
                Henning Bredel (h.bredel @52north.org), Simeon Wetzel

                Project duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                ~~~~~~~~~~

                3. Weather Routing Tool
                Explanation
                The open-source 52°North Weather Routing Tool (WRT) was initially developed during the MariData project. It provides means to find an optimal ship route that minimizes fuel consumption under varying weather conditions. In the optimization process, several constraints can be integrated, e.g. water depth and traffic separation zones. Currently, there are two algorithms available: an isofuel algorithm and a genetic algorithm. Details of the MariData project and example applications of the Weather Routing Tool can be found in the following publication: https://proceedings.open.tudelft.nl/imdc24/article/view/875.

                Expected Results
                The Weather Routing Tool should be extended by new features and its robustness should be improved. There are three major directions of possible developments:

                Ship speed optimization
                Currently, only the geometry of the route is optimized while the ship speed is assumed to be constant. To cover a broader range of real-world use cases, the Weather Routing Tool should provide the option to optimize ship speed. This could be along a fixed route or simultaneous with the route geometry.
                Genetic algorithm
                The implementation of the genetic algorithm is still very basic. Possible improvements include the generation of the initial population and the strategies for crossover and mutation. Moreover, a multi-objective optimization could be implemented.
                General consumption model
                An important aspect of the Weather Routing Tool is the underlying (fuel) consumption model. The best results can generally be obtained by using a consumption model which is developed specifically for a ship, e.g. based on hydrodynamic modeling or machine learning models. However, developing such specific models is cumbersome and restricts the applicability of the tool. Thus, having a general consumption model which only requires a few characteristics of a ship (e.g. type of vessel, length, breadth, displacement) would be a great improvement. The model should have reasonable accuracy. As this feature includes research aspects and can only be successfully developed with the necessary background knowledge, interested candidates have to provide a clear plan of their approach.
                The features can be implemented in different ways. How they are implemented is up to the candidate and might include deterministic, machine learning or AI methods.

                Code Challenge
                New ship class:

                Implement a new ship class
                It should inherit from the Boat base class
                The get_ship_parameters method has to be implemented; it should return a “synthetic” fuel rate which depends on at least one environmental parameter (e.g. wave height)
                Make sure the fuel rates (kg per second) are within a reasonable value range. Besides the weather conditions, typical fuel rates also depend on the ship size, type (e.g. container ship, tanker, fishing vessel) and speed.
                The choice of the considered environmental parameters and the type of the function is free
                You can take the ConstantFuelBoat class as an example
                Prepare weather conditions
                Options:
                Create your own synthetic weather conditions
                Download actual historical or forecast data from public portals (Copernicus, NOAA, …). You can use the Python package maridatadownloader directly or indirectly by setting “DATA_MODE” to “automatic“.
                Run the Weather Routing Tool with your new ship class and a route of your free choice
                Hint: because the Python package mariPower is not publicly available, you need to comment or delete the corresponding lines in ship.py.
                Configuration:
                Set “ALGORITHM_TYPE” to “isofuel”
                Provide the expected results for review
                Mandatory:
                Final route as GeoJSON file
                Python code of new ship class
                Optional:
                Log file (info.log)
                Snapshots of routing steps (WRT_FIGURE_PATH)
                Used weather data
                Community and Code License
                MIT License

                Mentors
                Martin Pontius (m.pontius @52north.org), Katharina Demmich (k.demmich @52north.org)

                Project Duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                TBD

                Cloud Native OGC SensorThings API 2
                enviroCar
    totalCharacters_of_ideas_content_parent: 10019
    totalwords_of_ideas_content_parent: 1397
    totalTokenCount_of_ideas_content_parent: 2059
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/52north-spatial-information-research-gmbh/
    idea_list_url: https://52north.org/outreach-dissemination/google-summer-of-code/project-ideas/


  - organization_id: 2
    organization_name: AFLplusplus
    no_of_ideas: 4
    ideas_content: |
      Proposal 1: Tool for Automated generic/bounds simplification
        Create a (general, not LibAFL-specific) rust tool to simplify/minimze bounds

        Description
        As commented by many users and maintainers of LibAFL, our codebase is absolutely full of complicated generics. We use these to allow for structured and statically-checked compatibility between various components provided in our codebase, and is a critical part of how LibAFL is structured.

        Unfortunately, these can be very difficult to maintain. Our goal is to develop a tool capable of assisting developers in this maintenance process.

        Please check out issue #2868 for more details.

        Expected Outcomes
        A tool that works on any rust code, tries to minimize the used bounds, and fixes the code

        Skills Expected
        Rust
        A good understanding of Generics and the Rust Type system
        Possible Mentors
        @addisoncrump
        @tokatoka
        Expected size of the project
        The project is expected to take either 175 or 350 hours.

        Difficulty Rating
        The overall difficulty of this project is expected to be medium.
        ~~~~~~~~~~
        Proposal 2: Adapt qemuafl Frontend to LibAFL QEMU
        The project consists of adapting the frontend of qemuafl, the AFL++'s QEMU fork, with LibAFL QEMU.

        Description
        The end goal of this project would be to run fuzzers built for qemuafl while using LibAFL QEMU as the backend, in a retrocompatible way.
        A draft PR is already available and can be used as a starting point by the student.
        Ideally, the student would measure the performance (in terms of exec/s and coverage) of the new qemuafl adaptation with some fuzzers to evaluate how the final implementation compares with the reference.

        Expected Outcomes
        In short, we expect the student to make the new frontend work for most fuzzers developed for qemuafl while maintaining (at least) similar performance.

        See #1983 for an initial implementation that still lacks features.

        The main tasks the student would have to perform are the following:

        Speak the AFL++ forkserver protocol (check the draft PR).
        Add TCG caching to the LibAFL QEMU forkserver
        Use LibAFL QEMU snapshots where possible
        Add as many env variable features as possible
        Skills Expected
        We expect the student to:

        have a strong background in the Rust and C languages.
        be familiar with fuzzing.
        ideally, have some experience using AFL++ and / or LibAFL.
        ideally, have prior experience with the QEMU project.
        Possible Mentors
        The possible mentors for this project are:

        @domenukk
        @rmalmain
        Expected size of the project
        The project is expected to take either 175 or 350 hours.

        Difficulty Rating
        The overall difficulty of this project is expected to be medium.

        Original post
        This proposition is mostly an adaptation of issue #2964.
        ~~~~~~~~~~
        Proposal 3: Network Emulation for LibAFL_QEMU
        Implement syscall emulation for filesystem and network in libafl_qemu.

        Description
        The student must implement something similar to preeny and FitM to hook the network API and an emulator filesystem that can be snapshot-restored always hooking the syscall in libafl_qemu user mode

        Expected Outcomes
        A working network emulation layer for LibAFL_QEMU

        Required Skills
        Good understanding of Rust, C, system programming
        Ideally: prior knowledge in emulators and fuzzing
        Difficulty Rating
        The overall difficulty of this project is expected to be medium.

        Possible mentors
        @domenukk
        @rmalmain
        Expected size of the project
        The project is expected to take either 175 or 350 hours, depending on details
        ~~~~~~~~~~
        Proposal 4: Remote Worker Stage
        Mutations and execution of a Stage is always on the machine LibAFL runs at. For very slow targets it may be beneficial to offload the actual executions to stateless worker.

        Description
        We could add a RemoteWorkerLauncherStage that builds n work packages, each including a next scheduled corpus entry, all metadata for this Testcase, the current feedback state, as well as additional random corpus entries for splicing.
        The work package should then be posted to Redis or some other queue db (very much like celery, whatever a rust alternative is).
        After the execution, the results should be collected in an extra stage

        Expected Outcome:
        The implementation and a set of working examples, including:
        LibAFL Workers / RemoteWorkerLauncherStage + RemoteWorkerCollectorStage

        Required Skills
        Rust
        Prior knowledge in distributed computing and/or fuzzing are a plus
        Difficulty Rating
        easy to medium

        Possible mentors
        @domenukk
        @tokatoka
        @addisoncrump
        Length
        175 hours
    totalCharacters_of_ideas_content_parent: 4418
    totalwords_of_ideas_content_parent: 600
    totalTokenCount_of_ideas_content_parent: 1003
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aflplusplus/
    idea_list_url: https://github.com/AFLplusplus/LibAFL/issues/2992

  - organization_id: 3
    organization_name: AOSSIE
    no_of_ideas: 13
    ideas_content: |
      Agora Blockchain
        Project Type: Medium
        Description:
        Agora Blockchain is a decentralized voting platform designed to enhance electoral integrity and accessibility. It enables transparent, tamper-proof voting through smart contracts, leveraging Chainlink CCIP for cross-chain functionality. Agora ensures fair participation and trust in election results by eliminating centralized control and providing a verifiable, immutable ledger of votes.

        Key features include:

        Multi-algorithm voting: Supports different voting mechanisms like ranked choice, quadratic voting, and stake-based voting.
        Cross-chain voting: Uses Chainlink CCIP to enable voting across multiple blockchains.
        Gas-efficient smart contracts: Optimized Solidity contracts reduce transaction costs.
        Decentralized governance: Community-driven elections and decision-making.
        User-friendly interface: Built with Next.js, Wagmi, and MetaMask for seamless interaction.
        Expected Outcomes:
        Smart Contract Enhancements:

        Implement private elections for confidential voting.
        Further optimize election factory contracts for gas efficiency.
        Cross-Chain Expansion:

        Extend Chainlink CCIP integration to support multiple blockchains.
        Frontend & dApp Integration:

        Build an intuitive UI using Next.js and Wagmi.
        Ensure smooth wallet connectivity and real-time vote updates.
        Analytics & Insights:

        Develop a real-time dashboard for election statistics.
        Track voter participation and engagement metrics.
        Required Skills:
        Solidity
        Hardhat
        Chainlink CCIP
        Next.js
        MetaMask
        Wagmi
        TailwindCSS
        Zustand
        Mentors:
        Ronnie

        ~~~~~~~~~~

        BabyNest
        Project Type: Large
        Description:
        Pregnancy is a life-changing journey filled with crucial medical appointments, tests, and healthcare decisions. However, expecting parents often struggle to keep track of these milestones, which can lead to missed appointments and added stress. Studies show that adherence to prenatal checkups directly impacts pregnancy outcomes, yet there is no universally accessible tool to assist parents in navigating healthcare requirements based on their country and trimester.

        BabyNest is designed to solve this problem through a minimalist React Native app integrated with an AI-powered assistant. This intelligent assistant acts as a personal pregnancy planner and guide, ensuring that expecting parents stay informed, organized, and stress-free.

        Users can benefit from features such as:

        Automated tracking of trimester-specific medical appointments
        Country-specific healthcare requirement notifications
        Offline access to pregnancy care guidelines
        AI-powered personalized recommendations and reminders
        Expected Outcomes:

        Mobile application built with React Native for cross-platform support
        AI agent integration for intelligent pregnancy milestone scheduling, reminders and tracking
        Offline-first architecture with local storage of healthcare guidelines
        Required Skills:

        Frontend Development (React Native)
        Backend Development (Node.js, FastAPI)
        AI and NLP (Python, LangChain)
        Database Management (SQLite, Pinecone)
        Mentors: Bhavik Mangla

        ~~~~~~~~~~
        DebateAI
        Project Type: Large
        Description:
        DebateAI is an interactive, AI-enhanced debate game platform designed to improve users communication skills through structured competitive debates. Users can engage in real-time debates against both human opponents and AI-driven challengers on a wide range of real-world topics. The platform mimics formal debate competition structures, making it an effective practice and competitive tool.

        Expected Outcomes:
        User vs. User Debates:

        Real-time interaction using WebSockets and WebRTC for audio, video, and text communication.
        Structured debate formats with timed rounds, including opening statements, rebuttals, cross-examinations, and closing arguments.
        User vs. AI Debates:

        AI-driven opponents using LLMs to generate realistic counterarguments and adapt to user inputs.
        User Management and Profiles:

        Secure authentication and access control.
        Personal dashboards to track debate history and manage settings.
        Elo rating system for matchmaking and ranking users.
        Custom Debate Spaces:

        Users can create private rooms to debate on topics of their choice.
        Platform Enhancement & Codebase Refactoring:

        Refactor the existing codebase for better maintainability, scalability, and performance.
        Improve real-time communication efficiency and backend services.
        Required Skills:
        ReactJS
        TypeScript
        GoLang
        Python
        Databases
        LLMs
        Mentors:
        Bruno Keshav


        ~~~~~~~~~~
        Devr.AI
        Project Type: Large
        Description:
        Devr.AI is an AI-powered Developer Relations (DevRel) assistant designed to seamlessly integrate with open-source communities across platforms like Discord, Slack, GitHub, and Discourse. It acts as a virtual DevRel advocate, helping maintainers engage with contributors, onboard new developers, and provide real-time project updates.

        By leveraging LLMs, knowledge retrieval, and workflow automation, the assistant enhances community engagement, simplifies contributor onboarding, and ensures open-source projects remain active and well-supported.

        Expected Outcomes:
        AI-Driven Contributor Engagement

        Automates interactions, welcomes new contributors, and guides them through onboarding.
        Automated Issue Triage & PR Assistance

        Helps maintainers prioritize issues and assists contributors in resolving them efficiently.
        Knowledge Base & FAQ Automation

        Provides instant answers to common queries, reducing repetitive maintainer workload.
        AI-Powered Community Analytics

        Tracks engagement metrics, identifies active contributors, and generates insights.
        Required Skills:
        GenAI
        Supabase
        FastAPI
        Integrations:
        Discord
        Slack
        GitHub
        Mentors:
        Chandan


        ~~~~~~~~~~
        DocPilot
        Project Type: Large
        Description:
        Build a new age EMR application using conversational AI at its best. Existing EMR solutioning is Age-old! Doctors resist the overwhelming software which is high on costs and difficult to operate. Last innovation was made in 1990's. DocPilot listens to the whole consultation conversation between a doctor and patient, and generates a prescription for the doctor to just sign, print and save digitally.

        The app should be able to separate out things like symptoms, diagnosis, medications and tests from the conversation it listens to. These are just the basic requirements. Research more on OPD appointments and include them in our solutioning.

        Expected Outcomes:
        Conversational AI-powered EMR that listens and auto-generates prescriptions.
        Eliminates outdated, complex, and costly software for doctors.
        Affordable and easy to use, reducing resistance from medical professionals.
        Extracts symptoms, diagnosis, medications, and tests from conversations.
        Allows doctors to review, sign, print, and save prescriptions digitally.
        Integrates OPD appointment management for a seamless experience.
        A modern solution replacing decades-old EMR systems.
        Required Skills:
        Flutter
        AI
        Appwrite
        Mentors:
        Jaideep

        ~~~~~~~~~~

        EduAid
        Project Type: Medium
        Description:
        EduAid is an AI-driven tool designed to enhance online learning by generating quizzes from educational content, helping students improve retention and engagement. Currently available as a browser extension, we aim to expand it into a full-fledged platform with a website, optimized model pipelines, and better system performance.

        Our current model supports difficulty-controlled quizzes for short-answer and multiple-choice questions (MCQs). We plan to extend this functionality to other formats, including fill-in-the-blanks, boolean, and match-the-following, by improving our models for diverse question generation. Additionally, we seek to integrate EduAid with other educational platforms to make it a seamless part of the learning ecosystem.

        Expected Outcomes:
        Fully deploy the EduAid browser extension and website.
        Optimize model pipelines for better accuracy and response time.
        Improve system performance for a smoother user experience.
        Expand difficulty-controlled question generation to new formats.
        Enhance UI/UX for better usability.
        Integrate with other educational platforms for wider adoption.
        Required Skills:
        Frontend Development
        Backend Development
        PyTorch & NLP
        System Design & Architecture
        Mentors:
        Aditya Dubey


        ~~~~~~~~~~
        Ell-ena
        Project Type: Large
        Description:
        Ell-ena is an AI-powered product manager that automates task management by creating to-do items, tickets, and transcribing meetings while maintaining full work context. It is input-agnostic and features a chat interface where users can interact naturally.

        Users can ask Ell-ena to perform tasks such as:

        Create a ticket to work on the dark mode feature.
        Add a to-do list item for my math assignment.
        The AI understands the context and adds relevant details automatically. Advanced algorithms like Graph RAG can be leveraged for efficient context retrieval and decision-making.

        Expected Outcomes:
        AI-powered system that generates tasks, tickets, and meeting transcriptions.
        Seamless chat-based interface for intuitive user interactions.
        Context-aware automation to enrich task details automatically.
        Implementation of Graph RAG or similar techniques for intelligent processing.
        Scalable backend to support real-time task creation and management.
        Required Skills:
        ReactJS / NextJS
        NodeJS / Any backend tech stack
        AI / NLP
        Graph RAG
        Mentors:
        Jaideep


        ~~~~~~~~~~

        Inpact
        Project Type: Large
        Description:
        Inpact is an AI-powered creator collaboration and sponsorship matchmaking platform designed to connect content creators, brands, and agencies through data-driven insights. This open-source platform enables influencers to discover relevant sponsorship deals, collaborate with like-minded creators, and optimize brand partnerships.

        By leveraging GenAI, audience analytics, and engagement metrics, Inpact ensures highly relevant sponsorship opportunities for creators while maximizing ROI for brands investing in influencer marketing.

        Expected Outcomes:
        AI-Driven Sponsorship Matchmaking

        Automatically connects creators with brands based on audience demographics, engagement rates, and content style.
        AI-Powered Creator Collaboration Hub

        Facilitates partnerships between creators with complementary audiences and content niches.
        AI-Based Pricing & Deal Optimization

        Provides fair sponsorship pricing recommendations based on engagement, market trends, and historical data.
        AI-Powered Negotiation & Contract Assistant

        Assists in structuring deals, generating contracts, and optimizing terms using AI insights.
        Performance Analytics & ROI Tracking

        Enables brands and creators to track sponsorship performance, audience engagement, and campaign success.
        Required Skills:
        ReactJS
        GenAI
        Supabase
        FastAPI
        Mentors:
        Chandan

        ~~~~~~~~~~
        Monumento
        Project Type: Large
        Description:
        Monumento is an AR-integrated social app that transforms how you connect with the world’s most iconic landmarks. Through Monumento, you can check in to popular monuments, explore famous sites, and discover new people, all within a social platform dedicated to cultural and historical experiences. Whether you're a traveler or a history enthusiast, Monumento offers an immersive way to engage with the world’s most treasured locations.

        Expected Outcomes:
        Improved UI responsiveness

        Improve the app's responsiveness across different devices and screen sizes.
        Ensure a seamless user experience on various platforms.
        Better social system

        Improve the social aspect of the app by improving the feed and user profiles and the ability to interact with other users.
        Introduce new features like events, communities to keep users engaged
        Make Popular Monumnets Dynamic

        Introduce a dynamic system where popular monuments can be updated with new information and images by the users.
        Allow users to add new monuments to the app and make them available for users to check in to.
        Itineray

        Introduce a itinerary feature to help users plan their trips and discover new places.
        Allow users to save their favorite monuments and create personalized itineraries.
        Required Skills:
        Flutter
        Appwrite/Pocketbase/Supabase
        Generative AI
        ARCore/ARKit
        UI/UX Design
        Mentor:
        Mohammed Mohsin

        ~~~~~~~~~~

        Neurotrack
        Project Type: Medium
        Description:
        Neurotrack is an AI-powered platform designed for schools and therapy centers to detect, assess, and manage neurodevelopmental conditions like Autism, ADHD, and learning difficulties. By automating assessments, personalized education plans, and therapy tracking, it empowers educators, therapists, and parents to provide more effective, data-driven support.

        Expected Outcomes:
        AI-Powered Student Grouping

        Identifies patterns and groups students with similar needs for tailored interventions.
        Automated Individualized Education Plans (IEPs)

        Creates personalized learning strategies with AI-driven recommendations.
        Digital Assessments

        Conducts efficient, research-backed evaluations to track progress.
        Real-Time Reports & Insights

        Provides actionable data for educators, therapists, and parents.
        Comprehensive Therapy Tracking

        Logs sessions, progress, and improvements over time.
        Parent Support Assistant

        AI-driven chat support for guidance and resource recommendations.
        Seamless Scheduling

        Simplifies session planning for educators and therapists.
        Required Skills:
        GenAI
        Supabase/Appwrite
        Flutter
        Mentors:
        Mohsin
        ~~~~~~~~~~
        Perspective
        Project Type: Large
        Description:
        In today's digital landscape, personalized content algorithms and social media feeds often create echo chambers of various news and different perspectives and narratives. Users are repeatedly exposed to viewpoints confirming their beliefs. This reinforcement of confirmation bias leads to increased polarization and limits critical thinking.

        The Perspective app tackles the issue of echo chambers and confirmation bias by actively presenting users with well-researched, alternative viewpoints alongside their regularly consumed content. It analyzes the current narrative of a news article, social media post, or online discussion, then curates counterarguments from credible sources. This exposure encourages critical evaluation and helps users see beyond the single perspective they might be constantly fed, ultimately fostering a more balanced and nuanced understanding of complex facts. You don't need to rely on truncated news, get complete facts.

        Users can benefit from features such as:

        Counter-perspective: Instantly see counterarguments and narration of why other perspective.
        Reasoned Thinking: The tool will provide a counter-narrative of the same fact with strongly connected facts.
        Updated Facts: With the help of context-aware LLMs, we will provide the latest facts and counter-facts.
        Seamless Integration: Works with news, blogs, and social media applications.
        Real-Time Analysis: You don't need to wait for any author, make Perspective your companion for immediate insights as you browse.
        Expected Outcomes:

        Less Bias in narratives: Break out of echo chambers.
        Wider Perspectives: Broaden your understanding of the news you are watching.
        Better Discourse: More balanced discussions.
        Sharper Analysis: Improved critical thinking and decreased your mind's polarisation.
        Required Skills:

        Frontend Development (ReactJS)
        Backend Development (Python, FastAPI)
        AI and NLP (Python, LangChain, Langgraph, Prompt Engineering)
        Database Management (Any VectorDB)
        Mentors: Manav Sarkar

        ~~~~~~~~~~

        Pictopy
        Project Type: Medium
        Description:
        Pictopy is currently built using Tauri, relying on Rust, but it comes with platform-specific dependencies that make it difficult to containerize and ship. Electron has been considered as an alternative, but issues with rendering local machine photos and bypassing security have caused challenges in the past. This has led to difficulty in onboarding new contributors as many give up during the setup process, resulting in fewer active contributors.

        The backend has been stable but stagnant and could use refactoring and design enhancements to improve its growth and functionality. While the backend is working without issues, there is potential for improvement and future scaling.

        Expected Outcomes:
        Rework the frontend to explore other options that can simplify setup and containerization.
        Address issues related to Electron, including photo rendering and security bypassing.
        Increase contributions from new developers by simplifying the setup process.
        Refactor and enhance the backend for better growth and scalability.
        Provide design improvements to the backend for smoother development and future expansions.
        Required Skills:
        Rust
        Electron
        Backend Development
        Frontend Development
        Mentors:
        Pranav Aggarwal
        ~~~~~~~~~~
        Resonate
        Project Type: Medium
        Description:
        Resonate is an open-source social voice platform designed to enable real-time audio interactions, storytelling, and voice-based social networking. The project is built with a strong focus on open collaboration, accessibility, and innovation in voice communication. Whether it's live discussions, pair chats, or immersive story experiences, Resonate is designed to put voice at the center of social engagement.

        Expected Outcomes:
        Expanded Audio Story Marketplace

        Develop a fully-fledged marketplace for audio stories, allowing users to create, browse, and follow creators.
        Implement profile pages with a follower system, showcasing user content and social interactions.
        User & Creator Search Functionality

        Enhance the explore page by adding user search functionality.
        Enable users to follow creators, view their profiles, and stay updated on their latest audio stories.
        Friend System for Personal Communication

        Implement a friend request and acceptance system.
        Enable direct personal chats and voice calls between friends.
        Improved Pair Chat Experience

        Introduce a lobby system where users can see the number of people waiting before joining a pair chat.
        Improve UI/UX to enhance user engagement and interaction.
        Required Skills:
        Flutter
        Appwrite
        LiveKit
        WebRTC
        UI/UX Design
        Mentor:
        Aarush Acharya
    totalCharacters_of_ideas_content_parent: 17886
    totalwords_of_ideas_content_parent: 2039
    totalTokenCount_of_ideas_content_parent: 3492
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aossie/
    idea_list_url: https://aossie.org/ideas

  
  - organization_id: 4
    organization_name: API Dash
    no_of_ideas: 10
    ideas_content: |

          1. DashBot
          Related Issue - #621

          Develop DashBot - the AI assistant for API Dash which supercharges developer productivity by helping developers automate tedious tasks, follow best practices, interact & obtain contextual suggestions, all via natural-language input. DashBot must be designed in a modular and extensible manner and provide the following list of features (suggestive, not exhaustive):

          Explain responses & identify any discrepancy
          Debug requests based on Status codes & Error messages
          Generate API documentation
          Understand API and generate tests
          Generate plots & visualizations for API responses along with ability to customize
          Generate API integration frontend code for frontend frameworks like React, Flutter, etc.
          For each of the tasks you are also required to prepare benchmark evaluations so that it is easier for end users to choose the right backend LLM.

          Skills: AI, Agent, LLM Evaluation, Testing, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          2. AI Agent for API Testing & Tool Generation
          Related Issue - #620

          Develop an AI Agent which leverages the power of Large Language Models (LLMs) to automate and enhance the process of testing APIs. Also, simplify the process of converting APIs into structured tool definitions to enable seamless integration with popular AI agent frameworks like crewAI, smolagents, pydantic-ai, langgraph, etc.

          Traditional API testing involves manually crafting requests, validating responses, and writing test cases. However, AI Agents can significantly streamline this process by generating test cases, validating API responses against expected outputs, and even suggesting improvements based on API documentation. Developers can describe test scenarios in natural language, and the agent can automatically generates API requests, parameter variations, and edge cases. It can also interpret API responses, checking for correctness, consistency, and performance benchmarks. This reduces manual effort while increasing coverage and efficiency, making API testing smarter and more efficient.

          You are also required to prepare benchmark dataset & evaluations so that the right backend LLM can be selected for the end user.

          Skills: AI, Agent, LLM Evaluation, Testing, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          3. API Explorer
          Related Issue - #619

          This project is designed to enhance the API Dash user experience by integrating a curated library of popular and publicly available APIs. This feature allows users to discover, browse, search, and directly import API endpoints into their workspace for seamless testing and exploration. Developers can access pre-configured API request templates, complete with authentication details, sample payloads, and expected responses. This eliminates the need to manually set up API requests, reducing onboarding time and improving efficiency. APIs spanning various domains—such as AI, finance, weather, and social media—are organized into categories, making it easy for users to find relevant services. You are required to develop the entire process backend in the form of an automation pipeline which parses OpenAPI/HTML files, auto-tag it to relevant category, enrich the data, create templates. You can also add features such as user ratings, reviews, and community contributions (via GitHub) to ensure accurate and up-to-date resources.

          Skills: UX Design, OpenAPI, Automation, Dart, Flutter
          Difficulty: Low-Medium
          Length: 175 hours

          ~~~~~~~~~~

          4. AI API Eval Framework
          Related Issue - #618

          Develop an end-to-end AI API eval framework and integrate it in API Dash. This framework should (list is suggestive, not exhaustive):

          Provide an intuitive interface for configuring API requests, where users can input test datasets, configure request parameters, and send queries to various AI API services
          Support evaluation AI APIs (text, multimedia, etc) across various industry task benchmarks
          Allow users to add custom dataset/benchmark & criteria for evaluation. This custom scoring mechanisms allow tailored evaluations based on specific project needs
          Visualize the results of API eval via tables, charts, and graphs, making it easy to identify trends, outliers, and performance variations
          Allow execution of batch evaluations
          Work with both offline & online models and datasets
          Skills: AI, Evaluations, Dart, Python, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          5. API Testing Support for - WebSocket, SSE, MQTT & gRPC
          Related Issue - #15 #115 #116 #14

          Testing WebSocket, MQTT (Message Queuing Telemetry Transport), and SSE (Server-Sent Events) protocols is crucial for ensuring the reliability, scalability, and security of real-time communication systems. Whereas, gRPC (Remote Procedure Call) facilitates efficient communication between distributed systems using Protocol Buffers (protobuf) as its interface definition language (IDL) and offers features such as bi-directional streaming, authentication, and built-in support for load balancing and health checking. Each of these API protocols/styles serves different purposes and is utilized in various applications ranging from finance to web applications to IoT (Internet of Things) devices. The objective of this project is to design the architecture of the core library, understand the specs & implement the support for testing, visualization & integration code generation of these APIs in API Dash.

          Skills: Understanding Specs/Protocols, UX Design, Dart, Flutter
          Difficulty: Medium-High
          Length: 350 hours

          ~~~~~~~~~~

          6. AI UI Designer for APIs
          Related Issue - #617

          Develop an AI Agent which transforms API responses into dynamic, user-friendly UI components, enabling developers to visualize and interact with data effortlessly. By analyzing API response structures—such as JSON or XML—the agent automatically generates UI elements like tables, charts, forms, and cards, eliminating the need for manual UI development. One can connect an API endpoint, receive real-time responses, and instantly generate UI components that adapt to the data format. It must also support customization options, allowing developers to configure layouts, styles, and interactive elements such as filters, pagination, and sorting. Finally, users must be able to easily export the generated UI and integrate it in their Flutter or Web apps.

          Skills: AI, UX, Parsing, XML, JSON, Python, Dart, Flutter
          Difficulty: Easy-Medium
          Length: 90 hours

          ~~~~~~~~~~

          7. API Testing Suite, Workflow Builder, Collection Runner & Monitor
          Related Issues - #96 #100 #120

          The objective of this project to design and implement an API testing & workflow builder suite which allows various types of API testing:

          Validation Testing: Verify that the API meets functional and business requirements. Automate the testing & validation of responses received from an API against predefined expectations (assertions), Schema validations, etc.
          Integration Testing: Checks proper interaction between different APIs
          Security Testing: Identifies vulnerabilities and safeguards data
          Performance Testing: Measures speed, responsiveness, and stability under varying loads
          Scalability Testing: Evaluates the system's ability to grow with demand
          Users should be able to easily create collections of APIs for testing. It will also be useful to provide a API workflow builder (a drag and drop environment) to create API workflows and chain requests. The UI must allow users to execute this collection of API requests and test it in a systematic and automated manner (Collection Runner) and finally monitor the results.

          Skills: UI/UX Design, Automation, Testing, Dart, Flutter
          Difficulty: Medium-High
          Length: 350 hours

          ~~~~~~~~~~

          8. Adding Support for API Authentication Methods
          Issue - #609

          Add support for various API authentication methods:

          Basic authentication: Sending a verified username and password with API request Add API Auth: Basic authentication #610
          API key: Sending a key-value pair to the API either in the request headers or query parameters Add API Auth: API key #611
          Bearer token: Authenticate using an access key, such as a JSON Web Token (JWT) Add API Auth: Bearer token #612
          JWT Bearer: Generate JWT bearer tokens to authorize requests Add API Auth: JWT Bearer #613
          Digest Auth: Client must send two requests. First request sent to the server receives a nonce value, which is then used to produce a one-time-use hash key to authenticate the request Add API Auth: Digest Auth #614
          OAuth 1.0 Add API Auth: OAuth 1.0 #615
          OAuth 2.0 Implement OAuth 2.0 authentication #481
          Skills: Authentication, Dart, Flutter
          Difficulty: Low-Medium
          Length: 90 hours

          ~~~~~~~~~~

          9. mem0 for Dart
          mem0 is the goto memory layer for developing personalized AI Agents in Python. It offers comprehensive memory management, self-improving memory capabilities, cross-platform consistency, and centralized memory control. It leverages advanced LLMs and algorithms to detect, store, and retrieve memories from conversations and interactions. It identifies key information such as facts, user preferences, and other contextual information, smartly updates memories over time by resolving contradictions, and supports the development of an AI Agent that evolves with the user interactions. When needed, mem0 employs a smart search system to find memories, ranking them based on relevance, importance, and recency to ensure only the most useful information is presented.

          Currently, we lack this memory layer in Flutter AI applications and your task is to port mem0 to Dart.

          Skills: AI, Database, Data Structures, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~    

          10. API Dash Feature Improvements
          We always believe in improving our core features to help the end user. A suggestive list of features that can be improved are:

          Adding pre-request script/post request script Pre-request and post-request for api collections #557
          Importing from/Exporting to OpenAPI/Swagger specification Importing Requests from OpenAPI Specification file #121
          Adding support for more content types in request Support for application/x-www-form-urlencoded Content-Type as body type formdata currently only supports multipart/form-data #337 Support File as Request Body #352
          JSON body syntax highlighting, beautification, validation - Enhance Request Body Editor: JSON formatting, syntax highlighting, validation and other features #22 Add option to automatically/manually beautify JSON request body #581 Add syntax highlighting for JSON request body #582 Add validation for JSON request body #583 Add environment variable support in request body #590 Env. Variable Support for Text request body #591 Env. Variable Support for JSON request body #592 Env. Variable Support for Form request body #593
          Support for comments in JSON body Support comments in JSON request body #599
          Reading environment variables from OS environment Reading environment variables directly from OS environment #600
          Adding color support for environments (like RED for prod, GREEN for dev) Adding color support for environments #601
          Tab & whitespace settings
          Notification when new app updates are available [feat] in-app update check #373
          Better GraphQL editor
          Beautify and expand/collapse feature for GraphQL query
          Allow inspecting GraphQL schema
          Support for GraphQL variables, fragments, mutation, subscription, etc.
          More widget & integration tests
          More code coverage
          Skills: UX Design, Dart, Flutter
          Difficulty: Easy-Medium
          Length: 175 hours
    totalCharacters_of_ideas_content_parent: 12602
    totalwords_of_ideas_content_parent: 2654
    totalTokenCount_of_ideas_content_parent: 2473
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/api-dash/
    idea_list_url: https://github.com/foss42/apidash/discussions/565


  
  - organization_id: 5
    organization_name: AboutCode
    no_of_ideas: 10
    ideas_content: |
        PURLdb - DeadCode: track End-Of-Life code
        Code Repositories: https://github.com/aboutcode-org/purldb

        Description:

        Eventually old code goes unmaintained and dies. The goal of this project are:

        To add data structures, models, and APIs in purldb to track end-of-life code and in general package and projects activities
        To improve purl coverage at endoflife.date, see https://github.com/endoflife-date/endoflife.date/issues/763
        To import and sync data from projects such as https://github.com/endoflife-date/endoflife.date
        To design a module that can detect when
        a project is turning end-of-life (using the above)
        a project is unmaintained (use metrics from scorecard/other tools)
        Note that on the endoflife.date side, we need to help improve PURL coverage of the database there, as this would be key to integrate with purldb. There

        Priority: High

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        EOL
        End of life
        Mentors:

        @pombredanne
        @JonoYang
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/purldb/issues/42
        https://github.com/aboutcode-org/vulnerablecode/issues/722
        https://github.com/endoflife-date/endoflife.date/issues/763

        ~~~~~~~~~~
        PURLdb - PopularCode - Find and track actually used and most popular open source code
        Code Repositories: https://github.com/aboutcode-org/purldb

        Description:

        There are between 100 and 200 million open source projects and repos out there. Not all of them are equal. Some are much more useful than others, and some could be safely ignored. For instance, the linux kernel is more important, used and popular than a 1st year computer student school assignment project. The goal of this project is to determine when a project is popular and what are the most popular projects. If we do not know what code is used, we can spend a lot of resources to index less used code.

        There are some simple approaches to this, using available statistics for downloads or Github stars, but that is not satisfying alone.

        An idea would be to consider multiple factors to rank popularity and usage.

        For instance: create a (current and updated) graph of dependencies and compute something like a pagerank but for packages
        Then create with a metric on the freshness of the code like when last release and how much downloaded or based on git activity (excluding bots). This would grow for used code and decay for declining packages
        Then combine this with the dependencies "connectedness"
        Or, just a use the graph connections and no download stats, just a giant graph on top of purldb

        Or something like this:

        Finding strongly connected components
        Relate packages ignoring versions
        Find most connected
        Discount distant connections, boost closest
        Apply decay based on version freshness or git activity
        The approach would be to start small with a single ecosystem as PoC and then extend this to all packages types.

        Ideally, this should be exposed in PurlDB API and integrated in data collection operations.

        Priority: High

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        Popularity
        Mentors:

        @pombredanne
        @JonoYang
        @AyanSinhaMahapatra

        ~~~~~~~~~~
        VulnerableCode project ideas
        There are two main categories of projects for VulnerableCode:

        A. COLLECTION: this category is to mine and collect or infer more new and improved data. This includes collecting new data sources, inferring and improving existing data or collecting new primary data (such as finding a fix commit of a vulnerability)

        B. USAGE: this category is about using and consuming the vulnerability database and includes the API proper, the GUI, the integrations, and data sharing, feedback and curation.

        VulnerableCode: Process unstructured data sources for vulnerabilities (Category A)
        Code Repositories:

        https://github.com/aboutcode-org/vulnerablecode
        Description:

        The project would be to provide a way to effectively mine unstructured data sources for possible unreported vulnerabilities.

        For a start this should be focused on a few prominent repos. This project could also find Fix Commits.

        Some sources are:

        mailing lists
        changelogs
        reflogs of commit
        bug and issue trackers
        This requires systems to "understand" vulnerability descriptions: as often security advisories do not provide structured information on which package and package versions are vulnerable. The end goal is creating a system which would infer vulnerable package name and version(s) by parsing the vulnerability description using specialized techniques and heuristics.

        There is no need to train a model from scratch, we can use AI models pre-trained on code repositories (maybe https://github.com/bigcode-project/starcoder?) and then fine-tune on some prepared datasets of CVEs in code.

        We can either use NLP/machine Learning and automate it all, potentially training data masking algorithms to find these specific data (this also involved creating a dataset) but that's going to be super difficult.

        We could also start to craft a curation queue and parse as much as we can to make it easy to curate by humans and progressively also improve some mini NLP models and classification to help further automate the work.

        References: https://github.com/aboutcode-org/vulnerablecode/issues/251

        Priority: Medium

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        Security
        Vulnerability
        NLP
        AI/ML
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        @Hritik14
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues/251

        ~~~~~~~~~~
        VulnerableCode: Add more data sources and mine the graph to find correlations between vulnerabilities (Category A)
        Code Repositories:

        https://github.com/aboutcode-org/vulnerablecode
        Description:

        See https://github.com/aboutcode-org/vulnerablecode#how for background info. We want to search for more vulnerability data sources and consume them.

        There is a large number of pending tickets for data sources. See https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        Also see tutorials for adding new importers and improvers:

        https://vulnerablecode.readthedocs.io/en/latest/tutorial_add_new_importer.html
        https://vulnerablecode.readthedocs.io/en/latest/tutorial_add_new_improver.html
        More reference documentation in improvers and importers:

        https://vulnerablecode.readthedocs.io/en/latest/reference_importer_overview.html
        https://vulnerablecode.readthedocs.io/en/latest/reference_improver_overview.html
        Note that this is similar to this GSoC 2022 project (a continuation):

        https://summerofcode.withgoogle.com/organizations/aboutcode/projects/details/7d7Sxtqo
        References: https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        Priority: High

        Size: Medium/Large

        Difficulty Level: Intermediate

        Tags:

        Django
        PostgreSQL
        Security
        Vulnerability
        API
        Scraping
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        @Hritik14
        @jmhoran
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        ~~~~~~~~~~
        VulnerableCode: On demand live evaluation of packages (Category A)
        Code Repositories: https://github.com/aboutcode-org/vulnerablecode

        Description:

        Currently VulnerableCode runs importers in bulk where all the data from advisories are imported (and reimported) at once and stored to be displayed and queried.

        The objective of this project is to have another endpoint and API where we can dynamically import available advisories for a single PURL at a time.

        At a high level this would mean:

        Support querying a specific package by PURL. This is not for an approximate search but only an exact PURL lookup.

        Visit advisories/package ecosystem-specific vulnerability data sources and query for this specific package. For instance, for PyPi, the vulnerabilities may be available when querying the main API. An example is https://pypi.org/pypi/lxml/4.1.0/json that lists vulnerabilities. In some other cases, we may need to fetch larger datasets, like when doing this in batch.

        This is irrespective of whether data related to this package being present in the db (i.e. both for new packages and refreshing old packages).

        A good test case would be to start with a completely empty database. Then we call the new API endpoint for one PURL, and the vulnerability data is fetched, imported/stored on the fly and the API results are returned live to the caller. After that API call, the database should now have vulnerability data for that one PURL.

        This would likely imply to modify or update importers to support querying by purl to get advisory data for a specific package. The actual low level fetching should likely be done in FetchCode.

        This is not straightforward as many advisories data source do not store data keyed by package, as they are not package-first, but they are stored by security issue. See specific issues/discussions on these importers for more info. See also how things are done in vulntotal.

        Priority: Medium

        Size: Medium/Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        PostgreSQL
        Security
        web
        Vulnerability
        API
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues/1046
        https://github.com/aboutcode-org/vulnerablecode/issues/1008

        ~~~~~~~~~~
        ScanCode.io project ideas
        ScanCode.io: Create file-system tree view for project scans
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        Description:

        When large packages/containers are scanned in scancode.io it is useful to have a tree-view to explore thorugh the file-tree for that package/container to look into scan data for a particular subset of the file-tree/directory or to research more into detections and detection issues.

        This would be something similar to what we have at scancode-workbench for example: https://scancode-workbench.readthedocs.io/en/develop/ui-reference/directory-tree.html

        I.e. we need the following features:

        To be able to toggle showing the directory contents from the directory icon
        Show nested directory contents in a tree like structure
        Have this view ideally in a pane left to the table-view of resources
        Show only info from the selected directory in the table-view of resources
        Note that we do have a ProjectCodebaseView in the projects page currently in scancode.io but this is fairly limited as it only lets you browse through the codebase one directory at a time (only shows the files/directories in one directory), and lets you navigate to directories in the current directory or the parent directory from there.

        Priority: High

        Size: Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        UI/UX
        File-system
        Navigation
        Mentors:

        @tdruez
        @pombredanne
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/697

        ~~~~~~~~~~
        ScanCode.io: Add ability to store/query downloaded packages
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        Description:

        Packages which are downloaded and scanned in SCIO can be optionally stored and accessed to have a copy of the packages which are being used for a specific product for reference and future use, and could be used to meet source redistribution obligations.

        The specific tasks would be:

        Store all packages/archives which are downloaded and scanned in SCIO
        Create an API and index by URL/checksum to get these packages on-demand
        Create models to store metadata/history and logs for these downloaded/stored packages
        Additionally support and design external storage/fetch options
        There should be configuration variable to turn this on to enable these features, and connect external databases/storage.

        Priority: Low

        Size: Medium

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        CI
        Security
        Vulnerability
        SBOM
        Mentors:

        @tdruez
        @keshav-space
        @jyang
        @pombredanne
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/1063


        ~~~~~~~~~~

        ScanCode.io: Update SCIO/SCTK for use in CI/CD:
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        https://github.com/aboutcode-org/scancode-action
        Description:

        Enhance SCIO/SCTK to be integrated into CI/CD pipelines such as Github Actions, Azure Piplines, Gitlab, Jenkins. We can start with any one CI/CD provider like GitHub Actions and later support others.

        These should be enabled and configured as required by scancode configuration files to enable specific functions to be carried out in the pipeline.

        There are several types of CI/CD pipelines to choose from potentially:

        Generate SBOM/VDRs/VEX with scan results:

        Scan the repo to get all purls: packages, dependencies/requirements
        Scan repository for package, license and copyrights
        Query public.vulnerablecode.io for Vulnerabilities by PackageURL
        Generate SPDX/CycloneDX SBOMs from them with scan and vulnerability data
        License/other Compliance CI/CD pipelines

        Scan repo for licenses and check for detection accuracy
        Scan repo for licenses and check for license clarity score
        Scan repo for licenses and check compliance with specified license policy
        Check for OpenSSF scorecard data and specified policy on community health metrics
        The jobs should pass/fail based on the scan results of these specific cases, so we can have:
        a special mode to fail with error codes
        description of issues and failure reasons, and docs on how to fix these
        ways to configure and set up for these cases with configuration files
        Dependency checkers/linters:

        download and scan all package dependencies, get scan results/SBOM/SBOMs
        check for vulnerable packages and do non-vulnerable dependency resolutuion
        check for test failures after dependency upgrades and add PR only if passes
        Jobs which checks and fixes for misc other errors:

        Replaces standard license notices with SPDX license declarations
        checks and adds ABOUT files for vendored code
        We have an initial CI runner at https://github.com/nexB/scancode-action but we need to improve this with more functions, specially checking against predefined policies and failing/successful CI based on that.

        References:

        https://github.com/aboutcode-org/scancode.io/issues/599
        https://github.com/aboutcode-org/scancode.io/issues/1582
        Priority: High

        Size: Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        CI
        Security
        License
        SBOM
        Compliance
        Mentors:

        @pombredanne
        @tdruez
        @keshav-space
        @tg1999
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/599

        ~~~~~~~~~~
        ScanCode Toolkit project ideas
        Have variable license sections in license rules:
        Code Repositories:

        https://github.com/aboutcode-org/scancode-toolkit
        Description:

        There are lots of variability in license notices and declarations in practice, and one example of modeling this is the SPDX matching guidelines. Note that this was also one of the major ways scancode used to detect licenses earlier.

        Support grammar for variability in license rules (brackets, no of words)
        Do a massive analysis on license rules and check for similarity and variable sections This can be used to add variable sections (for copyright/names/companies) and reduce rules.
        Support variability in license detection post-processing for extra-words case
        Add scripts to add variable sections to rules from detection issues (like bsd detections)
        Priority: Medium

        Size: Medium

        Difficulty Level: Intermediate

        Tags:

        Python
        Licenses
        LicenseDetection
        SPDX
        Matching
        Mentors:

        @AyanSinhaMahapatra
        @pombredanne
        @jyang
        @DennisClark
        Related Issues:

        https://github.com/aboutcode-org/scancode-toolkit/issues/3601

        ~~~~~~~~~~

        Mark required phrases for rules automatically using NLP/AI:
        Code Repositories:

        https://github.com/aboutcode-org/scancode-toolkit
        Description:

        Required phrases are present in rules to make sure the rule is not matched to text in a case where the required phrase is not present in the text, which would be a false-positive detection.

        We are marking required phrases automatically based on what is present in other rules and license attributes, but this still leaves a lot of rules without them. See https://github.com/aboutcode-org/scancode-toolkit/pull/3924 where we are also adding a script to add required phrases as individual rules if applicable and also adding required phrases added to other rules.

        research and choose a model pre-trained on code (StarCoder?)
        use the dataset of current SCTK rules to train a model
        Mark required phrases in licenses automatically with the model
        Test required phrase additions, improve and iterate
        Bonus: Create a minimal UI to review rule updates massively
        Priority: Medium

        Size: Medium

        Difficulty Level: Advanced

        Tags:

        Python
        ML/AI
        Licenses
        Mentors:

        @AyanSinhaMahapatra
        @tg1999
        @pombredanne
        Related Issues:

        https://github.com/aboutcode-org/scancode-toolkit/issues/2878

          
    totalCharacters_of_ideas_content_parent: 19350
    totalwords_of_ideas_content_parent: 4455
    totalTokenCount_of_ideas_content_parent: 4378
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aboutcode/
    idea_list_url: https://github.com/aboutcode-org/aboutcode/wiki/GSOC-2025-Project-Ideas




  - organization_id: 6
    organization_name: Accord Project
    no_of_ideas: 7
    ideas_content: |
        1. Linter for Concerto
        Write a linter in TypeScript for Concerto Source files. It should make use of existing functionality to validate the Concerto DSL syntax and JSON AST of Concerto model against a set of rules. Rules should be defined in Typescript and which rules are run should be configurable. You may be able to make use of a tool like Spectral as the framework for defining our own rules over the Concerto AST (JSON).

        Expected Outcomes:
        A tool that allow users to:

        Specify the naming of declarations. E.g. all names of scalars should be in camel case.
        Specify the naming of properties, enum cases e.t.c
        Specify which language features can be used. E.g. disallow maps, disallow forward references in regex validators.
        Enforce the use of certain features. E.g. all string properties should have a length validator.
        Enforce the use of @Term decorators on all declarations and properties e.t.c
        All concepts in a namespace should extend a given concept
        All concepts in a namespace must have unique names across multiple namespaces
        Skills required/preferred:
        Algorithms, Functional programming, Back end development, NodeJS, TypeScript

        Possible Mentors:
        Jamie Shorten, Sanket Shevkar

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        2. Decorator Command Set JSON<->YAML Convertor
        Design and implement a convertor that would convert Decorator Command Sets JSON Objects to a much more human readable YAML format and vice-versa. Currently DCS JSON objects are very verbose to read, write and edit. With the new custom YAML format we aim to make DCS objects much more easier to read, write and edit.

        Expected Outcomes:
        A utility/method in DecoratorManager to convert DCS JSON to YAML and from YAML to JSON.
        1:1 conversion is not expected. YAML should have a custom format that is less verbose and more readable.
        Skills required/preferred:
        NodeJS, Typescript, Javascript, Basic understanding of Data Formats like JSON and YAML

        Possible Mentors:
        Sanket Shevkar

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        3. Accord Project Agreement Protocol
        The Accord Project Agreement Protocol (APAP) defines the protocol used between a document generation engine or contract management platform and an agreement server that provides agreement features like template management, document generation, format conversion etc.

        Expected Outcomes:
        Updated Open API specification
        Updated reference implementation for the specification
        Address (some of) open issues
        Skills required/preferred:
        NodeJS, Typescript, Javascript, REST API design

        Possible Mentors:
        Dan Selman, Niall Roche

        Expected size of project:
        350 hours (large)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        4. Specification Conformance Tests
        Our specification conformance testing is in need of an overhaul! We'd like to migrate to a robust, proven testing framework like Vitest which would support ESM, and be performant and have a new dedicated concerto package used for Concerto conformance testing. An AI tool may be useful in helping with the migration, so feel free to mention how AI could help you with this project! The goal is to have a set of tests that can be run against any Concerto implementation to assess whether it is conformant with the specification.

        Expected Outcomes:
        Migration to Vitest (or other appropriate framework)
        Consolidation of testing methodology and tooling
        New concerto package for tests, focused on conformance
        Build a set of tests for the Concerto validation rules
        Skills required/preferred:
        Node / Javascript
        Unit testing (Mocha / Jest for example)
        Behaviour driven testing (optional, Cucumber, for example)
        Possible Mentors:
        Dan Selman, Ertugrul Karademir

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        5. Incorporating AI into Template Playground
        Our Template Playground web application is used to help onboard users to our technologies. We'd love to make this even easier by adding AI features to make it easier to create, edit, and preview contract templates. This project will build upon the work that was carried out last year in the context of VS Code.

        Expected Outcomes:
        Allow users to upload a file and we'd use AI to convert it to an Accord Project template
        Possibly incorporate auto-complete suggestions when editing using the code editors built into the web app
        Skills required/preferred:
        ReactJS, AI tooling

        Possible Mentors:
        Diana Lease

        Expected size of project:
        350 hours (large)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        6. Testing for Code Generation Targets
        We have tools that allow users to generate code from their Concerto models, supporting several languages. We would like to introduce a way of testing this code generation that compiles code for each language we are generating.

        Expected Outcomes:
        Set of Docker images for each code generation target
        Run code gen tests within the correct image using GitHub actions, for example, generate Java code and then compile and run it using javac to ensure the generated code is correct
        Skills required/preferred:
        Systems engineering, CI/CD
        Docker, Docker compose
        GitHub actions
        Possible Mentors:
        Dan Selman, Ertugrul Karademir

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        7. Migration of Template Playground to use Tailwind CSS
        As mentioned previously, our Template Playground web application is used to help onboard users to our technologies. By using a popular, well-maintained CSS framework like Tailwind, we could improve performance and code maintainability.

        Expected Outcomes:
        Template Playground updated to use Tailwind CSS
        Existing UI tests updated
        Possibly other UI changes to make user experience better, more performant, and/or optimized for multiple screen sizes
        Skills required/preferred:
        ReactJS, Tailwind CSS

        Possible Mentors:
        Diana Lease

        Expected size of project:
        175 hours (medium) - 350 hours (large)

        Expected difficulty:
        Medium


          
    totalCharacters_of_ideas_content_parent: 6850
    totalwords_of_ideas_content_parent: 1663
    totalTokenCount_of_ideas_content_parent: 1391
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/accord-project/
    idea_list_url: https://github.com/accordproject/techdocs/wiki/Google-Summer-of-Code-2025-Ideas-List



  - organization_id: 7
    organization_name: Alaska 
    no_of_ideas: 15
    ideas_content: |
        [1] Automated coastline extraction for erosion modeling in Alaska.

        Mentors: Frank Witmer (fwitmer -at- alaska.edu) and Rawan Elframawy (rawann.elframawy -at- gmail.com)

        Overview: The rapidly warming Arctic is leading to increased rates of coastal erosion, placing hundreds of Alaska communities at the frontline of climate change. Understanding current rates of coastline change and accurately forecasting future changes is critical for communities to mitigate and adapt to these changes. Current modeling approaches typically use a simple linear model based solely on historical coastline positions to measure rates of change and extrapolate them into the future. In doing so, these models fail to capture the dynamic effects associated with decreasing sea ice, increasing annual wave energy, and increasing temperatures. To improve the quality of these coastal models, we need to increase the quantity of digitized coastlines, but manual photointerpretation is slow and laborious.

        Current Status: An initial model and pipeline have been developed to automatically extract coastlines from PlanetLabs imagery. An auto-download script is available to retrieve PlanetLabs imagery (3-5m spatial resolution) by specifying any timeframe, cloud coverage percentage, and geometry. Additionally, NDWI with a majority sliding window has been introduced, allowing a specific threshold for each window to improve water detection accuracy. The DeepWaterMap algorithm was originally trained with the Global Surface Water (GSW) dataset at 30 m resolution from Landsat imagery, but the model did not not work well applied to PlanetLabs imagery. We are working to re-train the model using PlanetLabs imagery automatically labeled using the NDWI thresholding method. This project extends and expands on the progress made in 2024.

        Potential areas of improvement:

        Data Expansion (Deering 2017–2019 and Beyond): Currently using data from 2017 to 2019 for Deering; we plan to include more recent data to extend the time series.
        Improved Cliff Area Segmentation: Enhance segmentation performance specifically in steep or cliff-like coastal areas.
        Handling Challenging Conditions: Improve segmentation in regions with water shadows, buildings, satellite artifacts, and other data quality issues.
        SWIR and Elevation Data Integration: Investigate combining short-wave infrared (SWIR) data and elevation data (e.g., DEMs) to further refine segmentation accuracy.
        Expected Outcomes: A finished model with high accuracy that automatically extracts a vectorized coastline representation from PlanetLabs satellite imagery. Then, the model can be applied to large amounts of imagery to model coastline changes over time.

        Required Skills: Python

        Code Challenge: Experience with multi-band satellite imagery, geospatial data processing, and machine learning.

        Source Code: https://github.com/fwitmer/CoastlineExtraction

        Discussion Forum: https://github.com/fwitmer/CoastlineExtraction/discussions

        Effort: 350 Hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [2] Support for Logarithmic Number Systems in a Deep-Learning Framework.

        Mentors: Mark Arnold (markgarnold -at- yahoo.com), Ed Chester (ed.chester -at- gmail.com), and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: The Logarithmic Number System (LNS) is an alternative to built-in Floating Point (FP), which makes multiplication and division easy at the expense of more difficult addition. Using overloaded operators, xlns provides an open-source Python library for LNS arithmetic. Interest in fabricating LNS hardware has grown since it may reduce power consumption for applications that tolerate approximate results, such as deep learning (see [1]-[5]). The problem is deep learning often relies on open-source Python frameworks (like Tensorflow or Pytorch) that are hardcoded to use FP hardware. A key feature of these frameworks is the ability to automatically compute gradients (based on the chain rule) by recording extra information about the computation stored in FP format. Such gradients are used during backpropagation training to update network weights.

        Current Status: xlns, Tensorflow and Pytorch are all interoperable with the widely-used open-source Numpy library, but xlns is not interoperable with the Tensorflow and Pytorch frameworks because both frameworks are hard coded to use built-in int or FP data internally instead of LNS.

        Expected Outcomes: The goal of this project is to provide support for a deep learning framework that uses xlns instead of FP internally (including network specification, automatic gradient, forward inference, and back-propagation training) while keeping high-level compatibility with the framework. This might be as part of xlns, or as a forked version of the chosen framework, or both. The contributor may choose either Pytorch or Tensorflow. The contributor should justify these decisions as part of the proposed design.

        Required Skills: Calculus, Python, Numpy, and either Pytorch or Tensorflow

        Code Challenge: The following three challenges illustrate the breath of issues involved. Each involves only a few lines of Python. Each involves working with both xlns and the framework. Doing all three in both Tensorflow and Pytorch might give evidence for which framework is more likely to lead to the expected outcome.

        Currently, when the data starts in xlns format, Pytorch/Tensorflow converts to FP. As part of the code challenge, we expect the contributor to provide short Python code snippets that demonstrate that if the data starts in xlns format, the computation cannot be carried out in the xlns format.

        xlns/examples/arn_generic.py is a hard-coded illustration of training a fully connected MLP with 28*28 input nodes, 100 hidden nodes and 10 output nodes using MNIST digit set. The hidden layer uses RELU and the output layer uses softmax. The FP weights for this are initialized as:

        W1 = np.array((list(np.random.normal(0, 0.1, (785, 100)))))                    
        W2 = np.array((list(np.random.normal(0, 0.1, (101, 10)))))
        Because there is an extra weight for a constant 1.0 input in each layer, the number of rows is one larger than the inputs to the layer. The example can be run with various data types, for example with xlnsnp (LNS internally implemented with int64 Numpy ufuncs):

        python3 arn_generic.py --type xlnsnp --num_epoch 7
        or more conventionally

        python3 arn_generic.py --type float --num_epoch 7
        The code challenge is to implement a similar size fully connected network (in FP) using the provided features of Pytorch or Tensorflow and compare its convergence with arn_generic.py (Note: arn_generic.py uses manual differentiation, ie, the derivative of RELU is a constant, which depends on the sign of the argument, and elementary backpropagation implements the chain rule).

        Consider LNS addition (1+2=3 and 3-1=2). The following illustrates the overloaded operator and xlnsnp internal representation (sign is LSB of the int64 value; the log portion is the rest):
        >>> import xlns as xl
        >>> x=xl.xlnsnp([2.0, 3.0])
        >>> x
        xlnsnp([xlns(1.9999999986889088) xlns(2.9999999688096786)])
        >>> x.nd
        array([16777216, 26591258])
        By default, the log portion here is given with 23 bits of precision (see help for xl.xlnssetF for details on how to lower the precision as would be useful in machine learning), which is why the log(2.0) is given as 16777216.

        >>> 2*np.int64(np.log2([2.0, 3.0])*2**23)
        array([16777216, 26591258])
        The expression with log2 double checks the answer for x in 23-bit format (with the additional *2 to make room for the sign bit). Had the +2.0 been -2.0, the representation would have been 16777217.

        >>> y=xl.xlnsnp([1.,-1.])
        >>> y
        xlnsnp([xlns(1.0) xlns(-1.0)])
        >>> y.nd
        array([0, 1])
        The above illustrates that the log(1.0)=0, and that the sign bit is one for negative values.

        >>> x+y
        xlnsnp([xlns(2.9999999688096786) xlns(1.9999999986889088)])
        >>> (x+y).nd
        array([26591258, 16777216])
        Although the Pytorch/Tensorflow frameworks don’t support LNS, LNS can be constructed from int64 and float operations (which is how xlnsnp works). In xlns/src/xlns.py, there is a function sbdb_ufunc_ideal(x,y). If you call this with the following code:

        >>> import numpy as np
        >>> def myadd(x,y):  
                  return np.maximum(x,y)+xl.sbdb_ufunc_ideal(-np.abs(x//2-y//2), (x^y)&1) ))
        it performs the same operation internally on int64 values as the overloaded operator:

        >>> myadd(x.nd,y.nd)
        array([26591258, 16777216])
        Such operations are supported by the frameworks (rather than here from np). This code challenge is to do a similar toy example within the tensor types provided by the framework, which gives a small taste of the difficulty involved in this project. (The code above for myadd is a slight oversimplification of xl.xlnsnp.__add__; see this for details on the treatment of 0.0.)

        References:

        [1] G. Alsuhli, et al., “Number Systems for Deep Neural Network Architectures: A Survey,” https://arxiv.org/abs/2307.05035, 2023.

        [2] M. Arnold, E. Chester, et al., “Training neural nets using only an approximate tableless LNS ALU”. 31st International Conference on Application-specific Systems, Architectures and Processors. IEEE. 2020, pp. 69–72. https://doi.org/10.1109/ASAP49362.2020.00020

        [3] O. Kosheleva, et al., “Logarithmic Number System Is Optimal for AI Computations: Theoretical Explanation of Empirical Success”, https://www.cs.utep.edu/vladik/2024/tr24-55.pdf

        [4] D. Miyashita, et al., “Convolutional Neural Networks using Logarithmic Data Representation,” https://arxiv.org/abs/1603.01025, Mar 2016.

        [5] J. Zhao et al., “LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update,” IEEE Trans. Computers, vol. 71, no. 12, pp.3179–3190, Dec. 2022, https://doi.org/10.1109/TC.2022.3202747

        Source Code: https://github.com/xlnsresearch/xlns

        Discussion Forum: https://github.com/xlnsresearch/xlns/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [3] Developing Distributed Algorithm for Metagenomic Error Correction and Assembly.

        Mentors: Arghya Kusum Das (akdas -at- alaska.edu) and Yali Wang (ywang35 -at- alaska.edu)

        Overview: A metagenomics study of Alaska would explore the diverse microbial communities in its unique environments, including the Arctic, marine, and terrestrial ecosystems. Such research could uncover insights into microbial adaptation to extreme conditions and contribute to understanding environmental and climate-related changes in the region. Metagenomic study has an immense impact on multiple science and engineering projects in Alaska such as, arctic healthcare, arctic water pollution, bio leaching on rare earth elements, arctic environmental sustainability and resilience, understanding boreal forest dynamics, wildfire mitigation, and so on. The list is never ending. Shotgun metagenomics, which involves sequencing DNA from a mixed sample of genomes within a community, offers a high-throughput approach to examine the genomic diversity of microbial populations. A key step in metagenomic analysis is assembling the shotgun reads into longer contiguous sequences, or contigs. However, genome assemblies from short reads are often highly fragmented, potentially generating millions of contigs per sample, especially in diverse communities. This challenge arises due to issues like sequence repeats within and between genomes, low coverage of certain species, and strain variability.

        Current Status: Because of the variability in abundance in multiple species in the mixed sample of genomes, it is hard to design a theoretically solid algorithm to rectify the error in the sample and assemble it accurately. The low abundance species in the mixed sample are often wrongly classified as error if we use a traditional/existing algorithms that can rectify the error in a single species’ whole genome sequence. For the similar reason, the existing metagenomic assemblers are perform sub-optimally. Further, the existing software are limited in terms of their data handling capability. Most of them are capable to operate in a single node only. So, their data nailing is severely limited by the RAM available in one node. Also the time consumed for large datasets are often unreasonable.

        Expected Outcomes: In this project, we will address the first two steps in metagenomic analysis i.e., error correction and assembly which are paramount for any downstream project. Metagenomic data is often large in size spanning to hundreds of gigabytes to terabyte scale. Our motivation is to develop distributed, HPC compatible solution for metagenomic error correction and assembly

        (1) We are looking for working solutions (a solid algorithm and its implementation) for metagenomic error correction and assembly. The solutions should be theoretically justifiable and/or biologically meaningful. (2) The algorithm and the software implementation for both error correction and assembly should be distributed in nature. (3) We are open for AI/ML-enabled solutions but that is not a requirement. (4) GPU-enabled solutions are also encouraged but, it’s also not a requirement.

        Required Skills: Python and experience with Deep Neural Networks

        Code Challenge: Prior experience creating deep learning models is expected.

        Source Code: https://github.com/akdasUAF/Metagenome

        Discussion Forum: https://github.com/akdasUAF/Metagenome/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium/Hard

        ~~~~~~~~~~

        [4] Telehealth over L4S.

        Mentors: Kolawole Daramola (koladaramola -at- icloud.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service 1, 2, 3 has shown promising performance, by rethinking congestion control. Can we have a telehealth deployment with pairs of L4S nodes? Perhaps starting with something simple, such as two DICOM endpoints to send radiographic images in between? Linux kernel with L4S patches can be a good point to start for the endpoints. How L4S, with telehealth and other applications, as well as classic non-L4S traffic, share the network will be an interesting test.

        Current Status: A prototype has been built as part of the GSoC 2024. As rural Alaska is largely unconnected by the road network, people often need to fly into larger towns such as Fairbanks and Anchorage for their healthcare needs. This state of affairs has steered the telehealth initiatives in Alaska much more than elsewhere in the US. Our research partners from healthcare organizations such as Alaska Native Tribal Health Consortium (ANTHC) utilize telehealth in their daily operations. Improved telehealth access and performance can significantly benefit the patients and providers in terms of patient satisfaction and comfort.

        Expected Outcomes: This project will review the latest advances from the research, deployment, and testing perspectives with using L4S in telehealth. The contributor will look into how this can be deployed in practice for various telehealth applications – sending DICOM images for diagnostics (high volume of data but tolerance for high latency), telemonitoring via wearable devices (low volume of data but demand for low latency), televisits (a video call through apps such as Zoom – high volume of data and demand for high latency). As a result of this project, we will understand whether we need any optimizations for L4S to use for telehealth applications and potential alternative approaches.

        Required Skills: Python

        Code Challenge: Experience with network protocols and installing Linux servers is a plus. Coding experience demonstrating such experiences is considered positive.

        Source Code: https://github.com/KathiraveluLab/L4SBOA

        Discussion Forum: https://github.com/KathiraveluLab/L4SBOA/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [5] Creating shareable "albums" from locally stored DICOM images

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM data sets downloaded from PACS environments typically remain in the local environments, such as a research server or a cluster where the DICOM retriever (C-MOVE) is run. To use this data, researchers must identify certain subsets of data. This can be achieved by querying the retrieved data. DICOM images consist of textual metadata. By querying the metadata, subsets of images can be identified. However, currently, creating "albums" from locally stored DICOM images is not seamless.

        Current Status: This feature does not exist in our open-source frameworks. We share images through other orthogonal approaches (via rclone, for example). This project will implement a stand-alone utility to effectively create albums from locally stored DICOM images.

        Expected Outcomes: Several approaches to implementing such album features exist. One approach is to use Kheops to provide an interface to create and view the albums. MEDIator can be extended to create subsets and share the images via a unique URL as well. The proposed feature will make the images accessible to more researchers for their experiments by replacing the current manual data sharing efforts. Moreover, Kheops natively integrates with OHIF Viewer. As such, images retrieved locally can be viewed through OHIF Viewer by creating albums with Kheops. Contributors are encouraged to use Kheops or alternatives rather than reinventing the wheel (unless there is a convincing reason).

        Required Skills: Python and Java.

        Code Challenge: Experience working with DICOM images from previous projects or through a sample dummy project will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Easy

        ~~~~~~~~~~

        [6] Beehive: Integrated Community Health Metrics Framework for Behavioral Health to Supplement Healthcare Practice in Alaska.

        Mentors: David Moxley (dpmoxley -at- alaska.edu) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: This project, a collaboration between the University of Alaska Anchorage Departments of Computer Science and Human Services, seeks to create a digital approach to translating the digitalization of art and photographic images into a digital database that stores in retrievable formats those images for use in advancing the delivery of human services and health care to people who experience considerable vulnerability and marginalization within the community. One of the project goals is to create a digital repository of these images, many of which reflect Outsider Art since the people who produce them are not formally trained as artists and experience considerable discrimination. The repository can be used to support research on Outsider art and Outsider Artists, education of health and human services practitioners about the impact of negative stereotypes on the health and well-being of people who are highly vulnerable, and arts programs devoted to advancing the health of vulnerable people.

        This project aims to develop Beehive, a prototype implementation as an open-source data federation framework that can be used in research environments in Alaska and elsewhere.

        Current Status: A prototype has been built as part of Alaska Season of Code. We are researching the approach for its use with our community partners in Anchorage, aiming to support marginalized folks such as the unhoused.

        Expected Outcomes: In this project, the contributor will develop the Beehive platform for (1) translating digital images into the database, (2) developing the database to support user interactions with content, and (3) facilitating retrieval of images. The contributor will obtain an orientation to the project, instruction in how the arts and photography can represent health and well-being, and insight into using digital representations as an advocacy tool for improving the well-being of highly vulnerable people.

        Required Skills: Database (MySQL or Mongo) and Python or Java. A build management tool such as Apache Maven is recommended if using Java.

        Code Challenge: Prior experience with database management through established coding examples.

        Source Code: https://github.com/kathiraveluLab/beehive.

        Discussion Forum: https://github.com/KathiraveluLab/Beehive/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [7] DICOM Image Retrieval and Processing in Matlab.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM (Digital Imaging and Communications in Medicine) is a radiographic imaging standard for how various modalities of scanners, PACS (Picture archiving and communication system) and other imaging systems communicate. As a storage protocol, it defines how images are stored in a standard way. It also functions as a messaging protocol, an extension to TCP.

        Many DICOM processing tools exist. They support receiving images from the scanners and PACS to a research cluster in real-time as an imaging stream or on-demand selectively. They also provide means to anonymize the DICOM images to preserve patient privacy, export the DICOM images into a format such as PNG or JPEG, and extract the textual metadata from DICOM files to store it in a CSV file format or a database. Machine learning pipelines cannot be executed in clinical systems such as scanners and PACS. Therefore, the DICOM images and their metadata in the research clusters can be used to run machine learning pipelines.

        Matlab has some out-of-the-box support for certain DICOM functions, and it could make our job easy in certain projects. This facilitates processing the files from the file system 2. Region-of-Interest is natively supported for DICOM-RT files in Matlab 3. It also supports deep learning on DICOM and NifTi files 4. Matlab currently does not support receiving images from DICOM systems such as PACS and Scanners over the network. Matlab used to have functions that utilize the Dicom toolkit to pull images from another server. It was available through Matlab's file exchange at one point called "dicom server connection." This is not publicly available anymore. However, we have the implementation available locally. The code was not recently tested, and therefore, its usability with the latest Matlab versions needs to be confirmed.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This project aims to create an easy-to-use open-source Matlab DICOM processing framework. We start with processing DICOM images since the current status of the DICOM networking in Matlab is unknown. But we will explore it, if possible and time permitting. Since this is a research project, we should study the existing projects first to avoid re-inventing the wheel. From Google Scholar, we see many processing and pipelines (ROI, deep learning, ...) on DICOM/DICOM-RT have been implemented using Matlab. Regardless of the scientific novelty, we can get an open-source solution to help with further ML stuff using Matlab on the DICOM files. However, we should also observe how this could be a scientific contribution and its merits beyond what is already available. We can use readily available public DICOM data sources to test our implementations, such as the Cancer Imaging Archive (TCIA), as that avoids having to deal with sensitive patient data with PHI. We will narrow down on a specific research use case to highlight the framework's usage in research.

        Required Skills: Matlab

        Code Challenge: Experience working with DICOM images from previous projects and prior experience with Matlab, as demonstrated through code examples, will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Hard


        ~~~~~~~~~~

        [8] Making ZeroMQ a first-class feature of concore.

        Mentors: Shivang vijay (shivangvijay -at- gmail.com), Rahul Jagwani (rahuljagwani1012 -at- gmail.com), and Mayuresh Kothare (mvk2 -at- lehigh.edu)

        Overview: concore is a lightweight framework for closed-loop peripheral neuromodulation control systems. concore consists of a file-sharing based concore protocol to communicate between the programs in a study. concore also allows a shared-memory based communication between programs. This project will implement a ZeroMQ-based communication between programs, as an alternative to the file-sharing based and shared-memory based communications. ZeroMQ is a message-oriented middleware implemented in multiple languages, which natively supports communications across computing nodes. Such an implementation will improve the usability of concore in distributed environments.

        The study with 0MQ

        Current Status: We experimented with an osparc-control based communication as an alternative to this default file-sharing based concore protocol. osparc-control is an extension of ZeroMQ. Our experimental osparc-control based implementation replaces the file-sharing mechanism restricted to one local machine with message queues that can be transmitted between locally networked machines. The contributor will use this osparc-control based communication as an inspiration for the proposed ZeroMQ-based implementation, which will function as a first-class approach to implement the edges of concore without using osparc-control. In our current experimental osparc-control based implementation, these ZeroMQ edges are not visible in the concore editor, the browser-based visual editor for concore. Consequently, studies with osparc-control are represented as forests instead of directed hypergraphs due to the "invisible" ZeroMQ communication. This also means to run a concore study with ZeroMQ communication, we have to run each hypergraph in the forest separately.

        Expected Outcomes: We need to promote a unified experience in concore, whether the edges are implemented via the default file-sharing approach, shared-memory approach, or through this ZeroMQ message-based approach. In the concore file-sharing approach, we label the edges with alphabetical characters. In the concore shared-memory approach, we label the edges starting with positive decimal integers (specifying the memory channels used for the sharing). Therefore, to denote the concore ZeroMQ-based edges, the contributor should assume that all the ZeroMQ-edges must start with "0" in their labels, followed by a hexadecimal port, followed by an underscore (_). For example, edge 0x1234_Y assigns the logical Y to port 1234 and edge 0xabcd_U assigns the logical U to port abcd. Once such a graph with ZeroMQ-edges is made (a single directed hypergraph, rather than a forest with disjoint two or more directed hypergraphs), we should be able to seamlessly build and run the study regardless of the underlying communication mechanism. Thus, we aim to demonstrate the possibility of a seamless local vs. distributed execution in a cluster through ZeroMQ.

        As the expected outcome of this project, we propose a ZeroMQ-based communication for concore with Python. In addition, the contributor may also implement the ZeroMQ-based communication with other programming languages supported by concore such as Matlab and C++. The contributor may also get inspiration from how the shared-memory based communication is implemented in concore.

        Required Skills: Python

        Code Challenge: Prior experience in Python must be demonstrated. Prior experience with message-oriented middleware frameworks such as ZeroMQ can be a plus, although not mandatory.

        Source Code: https://github.com/ControlCore-Project/concore

        Discussion Forum: https://github.com/ControlCore-Project/concore/discussions

        Effort: 350 Hours

        Difficulty Level: Medium


        ~~~~~~~~~~

        [9] Dynamic DICOM Endpoints.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM (Digital Imaging and Communications in Medicine) is a radiographic imaging standard for how various modalities of scanners, PACS (Picture archiving and communication system), and other imaging systems communicate. As a storage protocol, it defines how images are stored in a standard way. It also functions as a messaging protocol, an extension to TCP. DICOM implementations often have a queue to hold the images sent from the source. Since this is a networking communication, a queue may degrade the performance or introduce data loss. DICOM communications are defined by static source, query, and destination endpoints. Each endpoint is defined by hostname/IP address, port, and AE (Application Entity) Title. A DICOM endpoint such as a PACS or a scanner usually has these endpoints statically configured to ensure security and patient privacy.

        This project attempts to send data from a source to dynamic destinations based on the queue and the performance. This can be a use case for teleradiology with multiple remote healthcare/radiologist sites present or a potential framework to enable federated learning on radiographic images. Orthanc can be set up as a DICOM endpoint that mimics a PACS 1. With multiple Orthanc servers configured, such a federated deployment can be prototyped. Ultimately, this project aims to study the possibilities and opportunities of supporting dynamic DICOM endpoints in practice.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: A prototype implementation that supports dynamic DICOM endpoints.

        Required Skills: Python

        Code Challenge: Experience working with DICOM images from previous projects or through a sample dummy project will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [10] Bio-Block: A Blockchain-based Data Repository and Payment Portal.

        Mentors: Chalinda Weerasinghe (chalindaweerasinghe -at- gmail.com), Erik Zvaigzne (erik.zvaigzne-at-gmail.com), and Forrester Kane Manis (Forrester-at-headword.co)

        Overview: Most biological, genomic, genetic, medical, and behavioral data are currently collected, stored, and sold by vendors who initially offer products and services to clients in order to accumulate this data. The data, once given to companies, remains the property of the company, with very little compensation and autonomy offered to customers who provided the data in the first place. Can we create a secure, decentralized, and scalable data repository of such information for humans and animals, a true bio-block available to all and open-sourced, whereby the data owners get directly compensated? This project offers a response in the affirmative and leverages blockchains for data distribution, archiving, recording, and payments using a dual-chain structure on the Ethereum blockchain.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This overall project will be one of the first offerings of an open-source platform for all biological/medical/genomic/behavioral data that leverages the advantages of blockchains. While proprietary dual-chain blockchain architectures are used by companies in this space, our endeavor, through its sub-projects, aims to proof up the architecture that can be scaled and extended to all forms of client-submitted data and multiple retrieval and payment options. A proof of concept of the architecture will be tested using multivariate, heterogenous synthetic data.

        Required Skills: Python is proposed as the programming language. However, students can also propose their preferred alternative programming language and frameworks. Prior experience developing on Ethereum is a plus.

        Code Challenge: Prior experience in Python (or the proposed alternative language) and, preferably, Ethereum blockchain through established coding examples. Students are expected to establish their experience with Blockchain technologies and architecting and programming them through previous projects - ideally through their respective GitHub repository (or similar code repositories).

        Source Code: https://github.com/bio-block/healthy (New Project).

        Discussion Forum: https://github.com/bio-block/healthy/discussions

        Effort: 350 hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [11] Adopting Nunaliit for Alaska Native Healthcare Practices.

        Mentors: Jessica Ross (jmross2 -at- alaska.edu) and Maria Williams (mdwilliams6 -at- alaska.edu)

        Overview: Nughejagh is an Alaska Native holistic healthcare application. It uses Nunaliit as its map-based interface to store its data. The data is curated from various sources in the form of images, stories, and videos - which are stored using the Nunaliit map-based interface, supported by its CouchDB database. However, currently, Nunaliit lacks several desired features for Nughejagh. This project aims to fill the gap by implementing those features and developing scripts to automate the installation, configuration, and data loading process.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: The expected goal is to have Nunaliit fine-tuned and configured to run Nughejagh with all its requirements. The project outcome might be a new stand-alone repository that uses Nunaliit, a forked version of Nunaliit, or more likely both. The contributor should justify their design decisions as part of the proposed design.

        Required Skills: Prior experience in Javascript, Java, and Python.

        Code Challenge: Deploy and configure Nunaliit locally and share a screenshot of a locally-running Nunaliit instance. Nunaliit runs well on Ubuntu 24.04.

        Source Code: https://github.com/Nughejagh/nughejagh (New Project).

        Discussion Forum: https://github.com/Nughejagh/nughejagh/discussions

        Effort: 350 hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [12] AWANTA: A Virtual Router based on RIPE Atlas Internet Measurements.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: RIPE Atlas is an Internet Measurement network composed of small network devices, known as RIPE Atlas Probes and Anchors, connected to the participating volunteers' routers. Using RIPE Atlas, we can measure the Internet latency and routing path through ping and traceroute measurements. This project aims to develop a software router that dynamically uses RIPE Atlas measurements to change the scheduling path. Before the implementation of the project, we should study the existing works on using RIPE Atlas probe for such network optimization tasks at the Internet scale to quickly understand the state-of-the-art and ensure scientific novelty in our approach.

        Current Status: A prototype has been built as part of the GSoC 2024. We observe the use of such a framework in the Circumpolar North. Such an approach can provide significant benefits, especially in Alaska and the Canadian North, where Internet connectivity can be spotty.

        Expected Outcomes: This project extends the RIPE Atlas client to use the measurements in network scheduling decisions. First, the measurements should be streamlined to perform periodically across several probes set as sources and destinations. The measurements across several probes in a single city can provide a more generalized measurement for a city rather than restricting to individual changes of any given probe when multiple such probes are available to a given city. Second, we will build a virtual router to use these measurements to dynamically influence the network scheduling decisions across several nodes. As the network performance changes with time, we can observe how the network path changes with time. We have more than 60 million RIPE Atlas credits that we accumulated by hosting a RIPE Atlas probe for the past five years. So, we have sufficient resources for these Internet measurement experiments.

        Required Skills: Python.

        Code Challenge: Prior experience in Python through established coding examples.

        Source Code: https://github.com/KathiraveluLab/AWANTA

        Discussion Forum: https://github.com/KathiraveluLab/AWANTA/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium


        ~~~~~~~~~~

        [13] Alaska Wildfire Prediction Using Satellite Imagery.

        Mentors: Yali Wang (ywang35 -at- alaska.edu) and Arghya Kusum Das (akdas -at- alaska.edu)

        Overview: Given Alaska’s unique wildfire patterns, where large-scale fires occur annually in boreal forests, tundra, and remote wilderness, predicting fire-prone areas can help mitigate disasters and optimize resource allocation. The presence of vegetation (fuel) is necessary for a fire, but the determining factors are weather conditions (humidity, wind speed, temperature) and an ignition source (lightning, human activity, etc.). This project aims to develop a hybrid deep learning model to predict wildfire risk in Alaska by integrating optical, thermal, and synthetic aperture radar (SAR) satellite imagery with ground-based weather data. Traditional wildfire prediction relies on weather data, historical fire records, and human observations, which can be delayed or inaccurate in remote areas like Alaska. In contrast, satellite imagery provides real-time, high-resolution insights into vegetation health, thermal anomalies, burn severity mapping, soil moisture, fuel dryness, and even cloud-penetrating fire detection.

        Satellite choices:

        Satellite	Resolution	Revisit Frequency	Why Use It?
        Landsat 8 & 9 (NASA/USGS)	30m (multispectral), 100m (thermal)	16 days	Tracks pre/post-fire vegetation and burn severity with great detail.
        Sentinel-2 (ESA)	10m (RGB, NIR), 20m (SWIR)	5 days	High-resolution images for fire risk classification and early warnings.
        MODIS (Terra/Aqua, NASA)	250m (fire detection), 1km (thermal)	Daily	Provides historical fire perimeters and active fire locations.
        VIIRS (Suomi NPP & NOAA-20)	375m (fire detection), 750m (thermal)	Daily	Real-time fire monitoring, capturing active hotspots.
        Sentinel-1 (ESA)	5m - 20m	6-12 days	SAR imaging for vegetation moisture & burned area mapping.
        ALOS-2 (JAXA)	10m - 100m	14 days	L-band SAR for detecting dry fuel and terrain changes.
        Additional ground data sources:

        1). ERA5 Climate Reanalysis (ECMWF): Provides historical & real-time temperature, wind, and humidity data.

        2). NOAA NWS Weather Data: Near real-time humidity, wind, and temperature.

        3). Alaska Fire Service (AFS) Wildfire Data: Historical ignition source data (lightning, human activity).

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This project aims to develop a deep-learning model that predicts wildfire risk in Alaska using a combination of satellite and ground-based weather data. The expected outcome of this project would involve both the dataset preprocessing pipeline and the performance of the developed model. Especially, the dataset preprocessing would include how to process the pre-fire and post-fire images efficiently and integrate the ground-based data with satellite imagery. Expected outcomes include:

        Minimum viable product (MVP):

        Fire risk classification: Given pre-fire satellite images, the model predicts the probability of a fire occurring within a defined time frame like 1 month, 3 months, or 6 months. The classifications should be "High Fire Risk," "Moderate Risk," or "No Risk."

        1). Data pipeline development:

        Preprocessing satellite images: Band selection, geospatial cropping, cloud removal (For this step, we are mostly interested in analyzing Sentinel-2 data);

        Synthetic Aperture Radar (SAR) analysis: Extracting fuel moisture & terrain features (For this step, we are mostly interested in extracting information like vegetation density and soil moisture from Sentinel-1 SAR data);

        Time-series weather data integration: Incorporating temperature, wind, and humidity. We have access to past decades of weather data for almost the past 30 years for multiple different places in Alaska.

        2). Model training and prediction:

        A hybrid model such as CNN-LSTM that analyzes satellite data and time-series weather trends (CNN-LSTM is just an example. We are open to multiple different types of analysis methodology);

        A web-based GIS dashboard to visualize fire-prone regions in Alaska;

        A report on model performance and fire risk metrics.

        Required Skills: Python. Experience with deep learning and machine learning.

        Code Challenge: Experience with multi-band satellite imagery, geospatial data processing (like ArcGIS Pro), and remote sensing.

        Source Code: https://github.com/YaliWang2019/AK-Satellite-Imagery-Wildfire-Prediction (New Project)

        Discussion Forum: https://github.com/YaliWang2019/AK-Satellite-Imagery-Wildfire-Prediction/discussions

        Effort: 350 Hours

        Difficulty Level: Medium/Hard

        You are welcome to propose new open-source project ideas, especially those that serve the state of Alaska and its people. Please use the below template to create new project ideas. However, if you are proposing a new project idea as a contributor, make sure they are relevant to Alaska specifically and the circumpolar north in general. Also, contact potential mentors from the above-listed mentors and confirm their interest in your project idea before drafting an entire proposal based on your own idea.

        ~~~~~~~~~~
        
        [14] Support for Logarithmic Number Systems in Large Language Models.

        Mentors: Mark Arnold (markgarnold -at- yahoo.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: The Logarithmic Number System (LNS) is an alternative to built-in Floating Point (FP), which makes multiplication and division easy at the expense of more difficult addition. Using overloaded operators, xlnscpp provides an open-source C++ library for both 16- and 32-bit LNS arithmetic. Interest in fabricating LNS hardware has grown since it may reduce power consumption for applications that tolerate approximate results, such as deep learning (see [1]-[5]). Although LNS has been studied extensively for feed-forward networks, only recently [6] has LNS been considered for Large Language Models (LLMs).

        LLMs consist of two main computations: a) feed-forward neural networks for which LNS has been shown to be useful, and b) an operation known as attention. The training of an LLM produces weights for both of these computations, which are often quantized to reduce data storage requirements. These quantized weights are reconstructed (usually in either 16- or 32-bit FP) and operated on by vectors of tokens (usually in similar FP format).

        Existing LLM engines, such as the open-source llama.cpp, perform vector/matrix/tensor operations (mostly matrix multiply) between the FP tokens and the weights (in a variety of formats, not including LNS).

        llama.cpp uses a library called ggml to do the actual math. The design of ggml supports a variety of FP hardware, such as CPUs and GPUs.

        Current Status: xlnscpp is not supported by llama.cpp or ggml. Weights can be stored in a variety of built-in int or FP formats instead of LNS. Matrix operations are carried out in FP.

        Expected Outcomes: The goal of this project is to provide support for xlnscpp instead of FP in ggml (and indirectly) llama.com. At a minimum, this involves modifying ggml to support a "virtual" LNS "machine" using xlnscpp to perform the actual LNS computation, but which appears to the calling llama.cpp like another hardware platform, like a GPU. The storage format of the quantized weights would still be the same, but they would be converted to LNS for computations like attention on LNS-format tokens. It is not expected that the speed would be as fast as if hardware FP were used, although a design that minimizes the slowdown is desirable (for instance, converting to LNS once, and reusing LNS many times, much as data is transferred to GPU memory and reused many times there). The purpose is a proof of concept that LNS yields valid output from an LLM. The design needs to implement enough ggml features to support an actual LLM, like Deepseek.

        Required Skills: C++ and some familarity with LLMs

        Code Challenge:

        Run the xlns16test.cpp and xlns32test.cpp examples.

        Go through the ggml example for 32-bit FP matrix multiplication on CPU ( https://huggingface.co/blog/introduction-to-ggml) which illustrates concepts like: ggml_backend (the code that does the computation on a GPU or CPU), ggml_context (a "container" that holds data), ggml_cgraph: (what computation the backend performs), ggml_backend_buffer: (hold the data of multiple tensors), and ggml_backend_buffer_type: (a "memory allocator" connected to each ggml_backend). This is quite involved because of the ggml_backend concept. Such experience will help you design a new ggml_backend for LNS (which your design proposal will describe as running on CPU using xlnscpp).

        Write a standalone C++ program that has a function to do 32-bit FP matrix multiply with a main program that prints the FP result. Test it with the same matrix data as the previous ggml example. (Hint: use nested for loops to compute the sum of products that form the matrix product).

        Modify this program to include xlns32.cpp (define xlns_ideal first) and perform the internal computation in LNS format. The main program and the signature of the function it calls remain the same (32-bit FP), which requires that the function convert to/from LNS before and after the matrix multiply. (Hint: if you do it properly, the overloaded xlnscpp assignment operator takes care of this automatically.) The sum of products should be computed entirely in LNS (not FP). Notice the numeric results are close to what FP produces.

        Modify the program to include xlns16.cpp instead. Notice the numeric results are slightly less accurate (the 16-bit LNS product is stored in the 32-bit FP result). This illustrates the tradeoff of using reduced precision LNS, which is what we want to experiment with in this project.

        These code challenges provide possible insight as to how the LNS-CPU backend your design proposal will describe can "look like" an FP backend to llama.cpp. When data would be transferred to the backend, it is converted to LNS. When data is transfered back to llama.cpp, it is converted back to 32-bit FP. This is one idea for this project. You may incorporate improvements to this concept in your design proposal that considers the features of ggml.

        References:

        [1] G. Alsuhli, et al., “Number Systems for Deep Neural Network Architectures: A Survey,” https://arxiv.org/abs/2307.05035, 2023.

        [2] M. Arnold, E. Chester, et al., “Training neural nets using only an approximate tableless LNS ALU”. 31st International Conference on Application-specific Systems, Architectures and Processors. IEEE. 2020, pp. 69–72. https://doi.org/10.1109/ASAP49362.2020.00020

        [3] O. Kosheleva, et al., “Logarithmic Number System Is Optimal for AI Computations: Theoretical Explanation of Empirical Success”, https://www.cs.utep.edu/vladik/2024/tr24-55.pdf

        [4] D. Miyashita, et al., “Convolutional Neural Networks using Logarithmic Data Representation,” https://arxiv.org/abs/1603.01025, Mar 2016.

        [5] J. Zhao et al., “LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update,” IEEE Trans. Computers, vol. 71, no. 12, pp.3179–3190, Dec. 2022, https://doi.org/10.1109/TC.2022.3202747

        [6] P. Haghi, C. Wu, Z. Azad, Y. Li, A. Gui, Y. Hao, A. Li, and T. T. Geng, “Bridging the Gap Between LLMs and LNS with Dynamic Data Format and Architecture Codesign ,” in 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO). Los Alamitos, CA, USA: IEEE Computer Society, Nov. 2024, pp. 1617–1631. https://doi.ieeecomputersociety.org/10.1109/MICRO61859.2024.00118

        Source Code: https://github.com/xlnsresearch/xlnscpp

        Discussion Forum: https://github.com/xlnsresearch/xlnscpp/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [15] Time and Ordering in Beehive.

        Mentors: Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu) and David Moxley (dpmoxley -at- alaska.edu)

        Overview: Beehive was initiated as a collaboration between the University of Alaska Anchorage Departments of Computer Science and Human Services, but grew largely into a software platform through open-source contributions. Beehive seeks to create a digital approach to translating the digitalization of art and photographic images into a digital database that stores in retrievable formats those images for use in advancing the delivery of human services and health care to people who experience considerable vulnerability and marginalization within the community. One of the project goals is to extend the current Beehive software as a repository of photomemories (a 2D projection of 3D spaces) X time. This project aims to extend Beehive with these additional capacities and develop data mining algorithms to support this use case of photos as frozen snapshots in an individual's life.

        Current Status: The current Beehive prototype does not consider the complexities of time and ordering in the use of behavioral patterns and narratives in the journey to recovery.

        Expected Outcomes: In this project, the contributor will (1) extend the Beehive platform to support time and ordering as attributes across images, (2) develop algorithms to understand the impact of past events through the series of images and their narratives, and (3) implement data mining algorithms that could fetch and understannd evolving narratives around photomemories. We see spaces as 3D or 2D if we are referring to geolocations. Photos are 2D projections of a 3D space. There is one dimension that we omit in most of these projections. That is time. Time as a 4th dimension is not entirely new in research and applications. A search on spatiotemporal data and space-time continuum will give you plenty of examples, from climate change to science novels. Time, or more specifically, ordering, is an essential variable in behavior. Don't you wonder how you see places differently just because you have seen the same or something similar before? Where it gets more interesting or challenging (depending on how you see it) is how the time affects the exact location and even those "near" it. When we say "near," it is in terms of data, not necessarily in terms of geographical proximity. Sometimes, it is just a minor change, and the location is the same! In data mining, we call this "near duplicates." A change in the name of a place (can be a city or a restaurant!). Other times, these are two entirely different places. Perhaps, Kivalina has moved over time due to the Arctic Erosion (sadly). But that is still geographical proximity. For instance, your visit to Portugal will influence your visits to other Portuguese-speaking nations (such as Angola and Brazil) because they share a language and culture, although they are oceans apart. On a smaller scale, your experience in a library will impact how you perceive another library in a different location. How do we consider time (or in a more accurate sense, "relative time" or "ordering") in our analysis/perception? This is intertwined as the 4th dimension (or 3rd dimension, if you are already projecting the 3D world into a 2D map/photo). This project aims to understand these complexities in a prototype version over simulated/synthetic data.

        Required Skills: Database (MySQL or Mongo) and Python or Java. Experience and interest in data mining is a plus.

        Code Challenge: Prior experience with database management through established coding examples.

        Source Code: https://github.com/kathiraveluLab/beehive.

        Discussion Forum: https://github.com/KathiraveluLab/Beehive/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium

        [N] PROJECT TITLE.

        Mentors: FIRSTNAME1 LASTNAME1 (email-address) and FIRSTNAME2 LASTNAME2 (email-address)

        Overview:

        Current Status:

        Expected Outcomes:

        Required Skills:

        Code Challenge:

        Source Code:

        Discussion Forum:

        Effort: 90/175/350 Hours

        Difficulty Level: Easy/Medium/Hard


          
    totalCharacters_of_ideas_content_parent: 54320
    totalwords_of_ideas_content_parent: 9380
    totalTokenCount_of_ideas_content_parent: 12130
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/alaska/
    idea_list_url: https://github.com/uaanchorage/GSoC




  - organization_id: 8
    organization_name: AnkiDroid
    no_of_ideas: 5
    ideas_content: |
    
          Multiple Profiles (175 hours)
          Problem
          Currently, AnkiDroid only allows a single profile as opposed to Anki (Desktop) and AnkiMobile (iphone) which allow multiple profiles. In particular, a family using a shared device will be able to have one profile per user, instead of sharing data. If you have multiple ankiweb accounts, you can sync all of them on your android. You can see discussion about this topic on https://github.com/ankidroid/Anki-Android/issues/2545.


          Expected Outcomes
          A user should be able to add a profile, sync this profile with an ankiweb account, switch between those accounts, and update the preference on a profile-by-profile level.
          Each profile should have its own collection, as in Anki. 

          There are at least three big questions that you need to consider. The UI, the backend, and the data structure. 
          The UI
          Firstly, you should design the UI. Your proposal should show us what would be the path the user will take to add a second profile, and switch to another profile. 
          The data structure
          AnkiDroid follows anki data structure. We have a folder, called Ankidroid, which contains all the files used to store user data. We should figure out a way to change the data structure in order to allow us to store data of multiple profiles.
          When the device does not use scoped storage, the AnkiDroid folder is in the top level general purpose folder of the AndroidDevice. We expect and hope most users understand this folder is used by ankidroid and leave it alone. We can’t just add other top level folders as this may clutter the user storage system and increase the risk a user delete folder, hence losing their data.
          The backend
          We need to figure out everything that will need to be changed. At the very least:
          We need a way to determine which profile is currently being used, and remember this information when ankidroid is restarted
          Each access to the data structure needs to take the profile into account, in order to access the right collection and media folder. This information should be available to the backend; so we should determine whether any change to the backend is required (probably not)
          We need to determine which preferences are profile based and which are used by the whole app. We need to update all preferences access to ensure we use the profile ones. We probably will need a lint rule that ensure that most preferences are profile-dependent unless there is a good reason not to. When creating an account, we need to determine whether to use default preferences or copy existing ones.


          Stretch goals
          If you have remaining time after the main project is done, there are two extensions that could be worth considering.

          Advertise this feature

          On the first update where multiple accounts are available, show a message to the user inviting them to add other accounts. This message should be similar to the message to new users.
          Saving space by avoiding to duplicate the media

          If multiple accounts have the same media (let’s say, the device is used by multiple users, who all should learn Ultimate Geography or Anking deck), ensure that the media are not duplicated in order to save storage on the device. This may require collaboration with the backend, because this optimization is not done on desktop. 
          It is probably worth doing it because storage is very precious on mobile.
          Language:
          Kotlin & XML
          Potentially some rust if we need to touch the backend for the stretch goal
          Difficulty: Medium
          Mentor(s):
          Arthur Milchior
          Shridhar Goel



          ~~~~~~~~~~



          Review Reminder (175 hours)
          Problem
          The user can request AnkiDroid to send them a daily notification in Android to remind them to review their cards if there are cards to review today.
          At least in theory. In practice those notifications have been broken for a long time. We tried years ago to solve the issue. Our solution was to remove most features, but what remains is still far from ideal.  It’s now clear that we should just scratch the current notification system and recreate one from scratch (and automatically migrate users from the previous feature to the new one). 
          Expected Outcomes
          In your proposal, you should tell us what the notification system will look like.
          We need to know what user interface you plan to implement. Every journey the user can take.

          The most basic idea is to have a notification shown if any card is due. This is what we currently have. Also let the user decide at which hour the notification should be shown. Maybe the notification should have a snooze button, to remind the user later (when?)
          We could also consider adding notification for specific decks.
          If so, there should probably be a way to see all notifications currently planned, in order to easily remove them, or edit them together.
          We should also find a way to test those notifications, manually and with automated tests. Ensuring they only trigger once a day in order not to overwhelm the user.
          You may consider reaching users over reddit and the forum in order to gather feedback 

          Language:
          EITHER:
          Kotlin & XML
          Difficulty: Hard
          Mentor(s):
          Arthur Milchior
          criticalAY


          ~~~~~~~~~~

          Note Editor: Note Type Preview (175 hours)
          Problem
          AnkiDroid is a flashcard app with a complex HTML/field-based templating engine. We currently have difficulties explaining a number of concepts to new users, both while onboarding, and for intermediate users:
          The unexpected fact that the user adds ‘notes’ to the app, not ‘cards’
          One note can generate multiple cards
          Various ‘Note Types’ have unique properties
          A user can create or download additional note types
          Currently, the user is provided with a text-based selection screen:

          Expected Outcomes
          In order to resolve the above issues, we want to modify this screen to provide a preview of each note type available in the system, showing
          The number of cards which will be produced when the note type is used
          A visual preview of how each of the cards will look
          Each card has a separate HTML template, so the designs may vary
          Taking into account some special features: 
          A note type may request that the user types in the answer
          Cloze deletions: 1 input produces 1…n cards
          Image occlusion: 1 input
          The ability to open our Card Template Editor
          The screen should allow a user to open up our Note Type Management screen and our manual. We should aim for the screen to prefer graphical elements over text
          Language:
          EITHER:
          Kotlin & XML
          If the screen is Android-specific
          Svelte (Typescript + HTML)
          If the screen is to be integrated into all Anki clients
          Difficulty: Medium
          Mentor(s):
          David Allison
          criticalAY

          ~~~~~~~~~~



          Tablet & Chromebook UI (175/350 hours)
          Problem
          AnkiDroid was initially designed for Android mobile phones. Over the years, Android has come to tablets and Chromebooks, but our UI has continued to be designed around the mobile phone.

          We currently have ~10% of our users on Tablets or Chromebooks, and we want to improve their user experience using the app, both with the aim to improve the user experience for our existing users, and increasing the number of users who can effectively use our app on larger devices

          Sanjay Sargam greatly improved the user experience on table and chromebook through GSoC 24’, and I invite you to read his report. Still, much remains to do, and what was done can certainly be polished.
          Expected Outcomes
          This primarily depends on your proposal. 
          Any screen in the app is open for your suggestions. 

          Suggestions
          Show NoteEditor and Previewer Side by Side
          Currently, the NoteEditor and Previewer are separate screens in AnkiDroid. On mobile devices, this makes sense due to limited scree n space. However,on tablets and Chromebooks, users have larger displays, and constantly switching between editing and previewing can feel cumbersome.

          Resizable Layout in DeckPicker and CardTemplateEditor
          The goal is to introduce a draggable slider that lets users dynamically adjust the size of two sections.

          Language: Kotlin, XML
          Difficulty: Medium
          Mentor(s):
          David Allison
          Arthur Milchior
          Sanjay Sargam


          ~~~~~~~~~~

          Additional Widgets (175/350 hours)
          Problem
          Widgets were introduced to AnkiDroid in 2010. These provide significant benefit to our power users and we started using them through GSoC 24. I invite you to read last year’s contributor’s report to see what was done.

          
          Expected Outcomes
          Android 12 Widget-based functionality is evaluated and integrated with the widgets when appropriate


          The GSoC proposal is expected to propose additional widgets that would be useful to our users
          Language: Kotlin, XML, UI & UX
          Difficulty: Medium
          Mentor(s):
          David Allison
          criticalAY





          
    totalCharacters_of_ideas_content_parent: 9691
    totalwords_of_ideas_content_parent: 2448
    totalTokenCount_of_ideas_content_parent: 1986
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ankidroid/
    idea_list_url: https://docs.google.com/document/d/1Va6IWEYcWTkK4KDtyoFxtOKzpdcYe54GdrVpuMcFvlI/edit?pli=1&tab=t.0



  - organization_id: 9
    organization_name: Apache DataFusion
    no_of_ideas: 11
    ideas_content: |
        
        Implement Continuous Monitoring of DataFusion Performance
        Description and Outcomes: DataFusion lacks continuous monitoring of how performance evolves over time – we do this somewhat manually today. Even though performance has been one of our top priorities for a while now, we didn’t build a continuous monitoring system yet. This linked issue contains a summary of all the previous efforts that made us inch closer to having such a system, but a functioning system needs to built on top of that progress. A student successfully completing this project would gain experience in building an end-to-end monitoring system that integrates with GitHub, scheduling/running benchmarks on some sort of a cloud infrastructure, and building a versatile web UI to expose the results. The outcome of this project will benefit Apache DataFusion on an ongoing basis in its quest for ever-more performance.

        Category: Tooling

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and mertak-synnada

        Skills: DevOps, Cloud Computing, Web Development, Integrations

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Supporting Correlated Subqueries
        Description and Outcomes: Correlated subqueries are an important SQL feature that enables some users to express their business logic more intuitively without thinking about “joins”. Even though DataFusion has decent join support, it doesn’t fully support correlated subqueries. The linked epic contains bite-size pieces of the steps necessary to achieve full support. For students interested in internals of data systems and databases, this project is a good opportunity to apply and/or improve their computer science knowledge. The experience of adding such a feature to a widely-used foundational query engine can also serve as a good opportunity to kickstart a career in the area of databases and data systems.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): jayzhan-synnada and xudong963

        Skills: Databases, Algorithms, Data Structures, Testing Techniques

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Improving DataFusion DX (e.g. 1 and 2)
        Description and Outcomes: While performance, extensibility and customizability is DataFusion’s strong aspects, we have much work to do in terms of user-friendliness and ease of debug-ability. This project aims to make strides in these areas by improving terminal visualizations of query plans and increasing the “deployment” of the newly-added diagnostics framework. This project is a potential high-impact project with high output visibility, and reduce the barrier to entry to new users.

        Category: DX

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): eliaperantoni and mkarbo

        Skills: Software Engineering, Terminal Visualizations

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Robust WASM Support
        Description and Outcomes: DataFusion can be compiled today to WASM with some care. However, it is somewhat tricky and brittle. Having robust WASM support improves the embeddability aspect of DataFusion, and can enable many practical use cases. A good conclusion of this project would be the addition of a live demo sub-page to the DataFusion homepage.

        Category: Build

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and waynexia

        Skills: WASM, Advanced Rust, Web Development, Software Engineering

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        High Performance Aggregations
        Description and Outcomes: An aggregation is one of the most fundamental operations within a query engine. Practical performance in many use cases, and results in many well-known benchmarks (e.g. ClickBench), depend heavily on aggregation performance. DataFusion community has been working on improving aggregation performance for a while now, but there is still work to do. A student working on this project will get the chance to hone their skills on high-performance, low(ish) level coding, intricacies of measuring performance, data structures and others.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): jayzhan-synnada and Rachelint

        Skills: Algorithms, Data Structures, Advanced Rust, Databases, Benchmarking Techniques

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Improving Python Bindings
        Description and Outcomes: DataFusion offers Python bindings that enable users to build data systems using Python. However, the Python bindings are still relatively low-level, and do not expose all APIs libraries like Pandas and Polars with a end-user focus offer. This project aims to improve DataFusion’s Python bindings to make progress towards moving it closer to such libraries in terms of built-in APIs and functionality.

        Category: Python Bindings

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): timsaucer

        Skills: APIs, FFIs, DataFrame Libraries

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Optimizing DataFusion Binary Size
        Description and Outcomes: DataFusion is a foundational library with a large feature set. Even though we try to avoid adding too many dependencies and implement many low-level functionalities inside the codebase, the fast moving nature of the project results in an accumulation of dependencies over time. This inflates DataFusion’s binary size over time, which reduces portability and embeddability. This project involves a study of the codebase, using compiler tooling, to understand where code bloat comes from, simplifying/reducing the number of dependencies by efficient in-house implementations, and avoiding code duplications.

        Category: Core/Build

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): comphead and alamb

        Skills: Software Engineering, Refactoring, Dependency Management, Compilers

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Ergonomic SQL Features
        Description and Outcomes: DuckDB has many innovative features that significantly improve the SQL UX. Even though some of those features are already implemented in DataFusion, there are many others we can implement (and get inspiration from). This page contains a good summary of such features. Each such feature will serve as a bite-size, achievable milestone for a cool GSoC project that will have user-facing impact improving the UX on a broad basis. The project will start with a survey of what is already implemented, what is missing, and kick off with a prioritization proposal/implementation plan.

        Category: SQL FE

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): berkaysynnada

        Skills: SQL, Planning, Parsing, Software Engineering

        Expected Project Size: 350 hours


        ~~~~~~~~~~

        Advanced Interval Analysis
        Description and Outcomes: DataFusion implements interval arithmetic and utilizes it for range estimations, which enables use cases in data pruning, optimizations and statistics. However, the current implementation only works efficiently for forward evaluation; i.e. calculating the output range of an expression given input ranges (ranges of columns). When propagating constraints using the same graph, the current approach requires multiple bottom-up and top-down traversals to narrow column bounds fully. This project aims to fix this deficiency by utilizing a better algorithmic approach. Note that this is a very advanced project for students with a deep interest in computational methods, expression graphs, and constraint solvers.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): ozankabak and berkaysynnada

        Skills: Algorithms, Data Structures, Applied Mathematics, Software Engineering

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Spark-Compatible Functions Crate
        Description and Outcomes: In general, DataFusion aims to be compatible with PostgreSQL in terms of functions and behaviors. However, there are many users (and downstream projects, such as DataFusion Comet) that desire compatibility with Apache Spark. This project aims to collect Spark-compatible functions into a separate crate to help such users and/or projects. The project will be an exercise in creating the right APIs, explaining how to use them, and then telling the world about them (e.g. via creating a compatibility-tracking page cataloging such functions, writing blog posts etc.).

        Category: Extensions

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and andygrove

        Skills: SQL, Spark, Software Engineering

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        SQL Fuzzing Framework in Rust
        Description and Outcomes: Fuzz testing is a very important technique we utilize often in DataFusion. Having SQL-level fuzz testing enables us to battle-test DataFusion in an end-to-end fashion. Initial version of our fuzzing framework is Java-based, but the time has come to migrate to Rust-native solution. This will simplify the overall implementation (by avoiding things like JDBC), enable us to implement more advanced algorithms for query generation, and attract more contributors over time. This project is a good blend of software engineering, algorithms and testing techniques (i.e. fuzzing techniques).

        Category: Extensions

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): 2010YOUY01

        Skills: SQL, Testing Techniques, Advanced Rust, Software Engineering

        Expected Project Size: 175 to 350 hours*

        *There is enough material to make this a 350-hour project, but it is granular enough to make it a 175-hour project as well. The student can choose the size of the project based on their availability and interest.

          
    totalCharacters_of_ideas_content_parent: 10170
    totalwords_of_ideas_content_parent: 1940
    totalTokenCount_of_ideas_content_parent: 2086
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/apache-datafusion/
    idea_list_url: https://datafusion.apache.org/contributor-guide/gsoc_project_ideas



  - organization_id: 10
    organization_name: ArduPilot
    no_of_ideas: 6 
    ideas_content: |
          Non-GPS Position Estimation Using 3D Camera and Pre-Generated Map¶
          Skills required: Python, C++

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Hard

          Expected Outcome: Copter with low-cost 3D camera estimates its local position by comparing the camera point cloud to a pre-generated 3D map

          The goal of this project is to allow a Copter to estimate its local position using a low-cost 3D camera (e.g. Intel D465) by comparing the camera’s point cloud to a pre-generated 3D map. The steps involved include:

          Create a tool to capture a 3D map of the flight area. The resulting map should be loaded onto the vehicle’s companion computer (e.g. RPI5)

          Mount a low-cost 3D camera (e.g. Intel D465) onto an ArduPilot copter (e.g. EDU650 or similar) equipped with a companion computer

          Write localisation software (e.g. python code) to compare the output of the 3D camera to the pre-generated 3D map and send the estimated position to the vehicle’s EKF (see Non-GPS Position Estimation)

          Implement a simulator of the system (e.g. gazebo)

          Document the setup and operation for future developers and users

          Funding will be provided for hardware including a copter (e.g. Hexsoon EDU650), companion computer and 3D camera (e.g. Intel D465) if necessary

          ~~~~~~~~~~

          AI Chat WebTool for use with MP and/or QGC
          Skills required: JavaScript, OpenAI, Google Gemini

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Web tool capable following a pilot’s verbal commands and converting them to MAVLink in order to control an ArduPilot multicopter

          This project involves re-implementing the MAVProxy’s AI chat module (see blog here) to run as a WebTool

          Once complete the WebTool should be capable of:

          Connecting to the vehicle via Mission Planner or QGC

          Responding to verbal or written questions and commands from the pilot

          Arming the vehicle

          Issuing takeoff commands and flying the vehicle a specified distance in any direction

          Changing the vehicle’s flight mode

          Most of the development can be completed using the SITL simulator and any OpenAI or Google Gemini usage costs will be covered

          ~~~~~~~~~~
           
          AI Chat Integration with all WebTools¶
          Skills required: JavaScript, OpenAI, Google Gemini

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: All WebTools include AI chat to help users understand and use the tool

          This project involves adding an OpenAI or Google Gemini chat window into some or all of the ArduPilot Webtools

          Once complete some or all of the WebTools should:

          Include a new chat widget allowing users to ask an AI assistant questions about the tool using text or voice

          Allow the AI assistant to operate the tool based on user input (e.g. push buttons, change zoom of graphs, etc)

          The top priority WebTool is the “UAV Log viewer” although simpler tools like the “Hardware Report” could be a good starting point

          Most of the development can be completed using the SITL simulator and any OpenAI or Google Gemini usage costs will be covered
          ~~~~~~~~~~


          Gazebo Plug-in Model of a Motor¶
          Skills required: Gazebo, C++

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: ArduPilot Gazebo plugin simulates a Motor

          As part of the ArduPilot_Gazebo plugin, we ask a student to model the electromechanical properties of a motor (no thrust/aero, just the motor angular acceleration/power itself)
          ~~~~~~~~~~

          SITL AI Reinforcement Learning Concept Script¶
          Skills required: Gaazebo, Lua, AI

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Lua script that uses re-inforcement learning to automate changing some parameters

          An AP-SITL reinforcement learning script concept, focuses on using Lua applets or some python to automate parameter changes according to some basic implementation of online reinforcement learning (actor-critic/SARSA/Q-learning)
          ~~~~~~~~~~

          SITL Test Script for Controls Testing¶
          Skills required: Gaazebo, Python

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Python code that allows easily setting up an AP vehicle in SITL for controls testing

          A safe “for education/rookies” SITL test script that strips away the majority of complexity in set-up and gives a Copter (and Plane if time permits) that requires some basic tuning and gives hints/pointers in a UI (this could lower the threshold for earlier year mech/electrical engineers to get their hands dirty on some software and try out basic controls testing)

          
    totalCharacters_of_ideas_content_parent: 5201
    totalwords_of_ideas_content_parent: 1290
    totalTokenCount_of_ideas_content_parent: 1143
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ardupilot/
    idea_list_url: https://ardupilot.org/dev/docs/gsoc-ideas-list.html


  - organization_id: 11
    organization_name: AsyncAPI
    no_of_ideas: 9
    ideas_content: |
          1) Enhancing Performance and Reliability of AsyncAPI CLI
          Improve the AsyncAPI CLI by optimizing performance, enhancing test reliability, and introducing long-requested features such as publishing and syncing AsyncAPI files with remote repositories.

          🎯 Outcome: Achieve a faster CLI execution, stable tests, file sync/publish support, and enhanced validation.
          🛠️ Skills Required: JavaScript/TypeScript, Node.js, Testing Frameworks, API, and testing automation.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AayushSaini101 | @Souvikns
          ⏳ Length: 175 Hours


          ~~~~~~~~~~
          2) AI-Powered Assistant for AsyncAPI
          Build an AI-powered assistant fine-tuned on AsyncAPI to provide accurate answers, generate code snippets, debug specifications, and recommend best practices.

          🎯 Outcome: A fine-tuned LLM-powered chatbot integrated with AsyncAPI’s ecosystem for enhanced developer support.
          🛠️ Skills Required: Javascript/Typescript, Machine Learning (LLMs), NLP, OpenAI/Llama, Chatbot Integration.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AceTheCreator
          ⏳ Length: 175 Hours

          ~~~~~~~~~~
          3) AsyncAPI Generator Maintainership
          This initiative aims to guide you from contributing to maintaining the project. You'll gain insight into the responsibilities of a maintainer, which involve tasks beyond mere coding.

          🎯 Outcome: Responsible for the project's future and continuous improvement.
          🛠️ Skills: JavaScript/TypeScript, testing libraries, Docker, virtualization, and test automation.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @derberg
          ⏳ Length: 350 Hours

          ~~~~~~~~~~
          4) AsyncAPI Conference Website UI Kit Development
          Develop a comprehensive UI Kit to enhance design consistency, modularity, and maintainability of the AsyncAPI Conference website.

          🎯 Outcome: A structured UI Kit with reusable components, Storybook integration, and improved design consistency.
          🛠️ Skills Required: React, TypeScript, Storybook, UI/UX Design, Component Development.
          🧩 Difficulty: Medium
          👩🏿‍🏫 Mentor(s): @AceTheCreator
          ⏳ Length: 175 Hours

          ~~~~~~~~~~
          5) VS Code Extension Maintainership
          This initiative will guide you from contributing to becoming a maintainer of the VS Code AsyncAPI Preview extension. You'll learn the responsibilities of a maintainer, including code contributions, issue triaging, release management, and community engagement.

          🎯 Outcome: Taking ownership of the VS Code extension to ensure its long-term stability and improvement.
          🛠️ Skills Required: TypeScript/JavaScript, VS Code Extensions, Spectral Linting, Testing, and Open Source Contribution.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @ivangsa
          ⏳ Length: 350 Hours

          ~~~~~~~~~~
          6) Java + Quarkus Template for AsyncAPI Generator
          Develop a new AsyncAPI Generator template for Java with Quarkus, leveraging its growing adoption in cloud-native development.

          🎯 Outcome: A fully functional Java + Quarkus template for generating AsyncAPI-based applications.
          🛠️ Skills Required: Java, Quarkus, Templating Engines (Nunjucks/Handlebars), AsyncAPI Generator.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AayushSaini101, @Souvikns
          ⏳ Length: 350 Hours
          ~~~~~~~~~~
          7) Refactor the Scripts inside the website and add Integration tests
          Add the script execution to a new folder inside the website, and add integration tests for those scripts.

          🎯 Outcome: A full Unit + Integration tests setup will be added for the scripts to fully test the functionalities
          🛠️ Skills Required: Typescript, Node js, Jest, Github actions
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @akshatnema
          ⏳ Length: 350 Hours
          ~~~~~~~~~~
          8) Add E2E tests for the Website critical flows
          Add E2E tests for the website where some of the critical flows (that are centered around user experience are tested thoroughly).

          🎯 Outcome: This project will ensure that we are not breaking any critical flows where user experience is our topmost priority
          🛠️ Skills Required: Typescript, Node js, E2E Testing, Github actions
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @sambhavgupta0705
          ⏳ Length: 175 hours
          ~~~~~~~~~~
          9) Redesign of website and addition of Dark theme
          Create new designs for the website pages based on the theme chosen by @Mayaleeeee and replicate those designs inside the website, along with the Dark mode theme.

          🎯 Outcome: This project will ensure that we are not breaking any critical flows where user experience is our topmost priority
          🛠️ Skills Required: Typescript, Node js, Figma, TailwindCSS
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @Mayaleeeee, @devilkiller-ag
          ⏳ Length: 350 hours

          
    totalCharacters_of_ideas_content_parent: 5231
    totalwords_of_ideas_content_parent: 1242
    totalTokenCount_of_ideas_content_parent: 1147
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/asyncapi/
    idea_list_url: https://github.com/asyncapi/community/blob/master/mentorship/summerofcode/2025/asyncapi-gsoc-ideas-page.md



  - organization_id: 12 
    organization_name: BRL-CAD 
    no_of_ideas: 25
    ideas_content: |
          Improving the k-File to BRL-CAD Converter
          Outline
          In the past years, we put some effort in the development of a LS-DYNA keyword file to BRL-CAD converter. Although we made great progress there, we still can't convert every k-file to g, i.e. the native BRL-CAD format. The goal of this project is to increase the number of covertable k-files.

          Details
          The sources of the current k-file to BRL-CAD converter can be found in the brlcad repository at src/conv/g. You have to compile BRL-CAD from its sources to work on this project and see the effects of your changes.

          Examples of k-files, which cannot be converted with k-g, can be found here: THUMS You can however use your own examples.

          Expected Outcome
          We expect an improved k-g LS-DYNA keyword file to BRL-CAD converting program as the outcome from this project.

          Project Properties
          Skills
          C/C++
          LS-DYNA or a similar FE solver software, which can read k-files (needed as reference for how the geometry should look like)
          Difficulty
          medium

          Size
          This project could have any size, short (90h), medium (175h) or long (350h), depending on the amount of functionality you want to add.

          Additional Information
          Potential mentor(s):
          Ali Haydar
          Daniel Rossberg
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Development and Build Support for Native Windows
          Outline
          OpenSCAD is multi-platform desktop application, with official support for Windows, macOS and Linux, and unoffical support for various other Unix-like OSes like FreeBSD.

          While OpenSCAD does support Windows, Windows development and build setup is a bit suboptimal:

          Windows binaries are built on Linux using the MXE cross compilation environment (https://mxe.cc). Build setup
          Windows native builds, including tests are done on msys2 (https://www.msys2.org) Build setup
          Goal of this project is to improve Windows support for native Windows development.

          Details
          Adjust OpenSCAD source code as needed to build OpenSCAD using MSVC
          Adjust the CMake build system as needed to support MSVC
          Find a way of managing building (and packaging if needed) 3rd party dependencies. OpenSCAD depends on a number of 3rd party packages (e.g. Qt, QScintilla2, CGAL, Manifold, GMP, MPFR, boost, OpenCSG, GLEW, Eigen, glib2, fontconfig, freetype2, harfbuzz, libzip, Bison, Flex, double-conversion). Not all of these have great Windows build support.
          Integrate library building/packaging into our Continuous Integration framework (e.g. GitHub Actions or CircleCI).
          Build OpenSCAD on MSVC + run tests natively for every PR, as we do for other build environments. This to make sure contributions don't fall out of maintenance.
          Establish support for debugging OpenSCAD in the MSVC debugger, and documentation on how to set that up, if necessary
          Write/update documentation on how to establish a native WIndows development environment
          Consider switching the official OpenSCAD Windows build to use MSVC.
          Prior work: openscad/openscad#4976

          Expected Outcome
          A native MSVC build environment for OpenSCAD is reasonable easy to set up, and such an environment is continuously integrated.

          Project Properties
          Skills
          Good understanding of the Windows OS and native components (DLLs, executables) on a low enough level to be able to debug odd behaviors.
          Experience using MSVC and native Windows development for C++ projects
          Good understanding of how 3rd party libraries are built and distributed.
          Experience with or interest in acquiring skills using CMake and writing custom CMake configs and macros
          Experience with GitHub CI or CircleCI is a bonus
          Difficulty
          medium

          Size
          long (350h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Note: None of the mentors have relevant Windows skills, but have excellent understanding on how all the relevant technologies work and how they are integrated with OpenSCAD. It's important that the candidate is able to acquire any necessary Windows-specific skills independently.
          Organization website: https://www.openscad.org/

          ~~~~~~~~~~

          Integrated language help feature in OpenSCAD #100
          Outline
          Add more interactive help features for built-in functions and modules. Right now there's already a nice summary of parameters linked as cheat sheet. Scope of this projects would be to use this information in extended form and make it available in a more direct way in the editor.

          Details
          Convert the cheat sheet information into machine readable format
          Find a way to generate the existing HTML format based on the core data
          Add context help to editor giving help for built-in functions and modules, e.g. by adding formatted help output to the console window, including the links to further documentation like the language manual on Wikibooks
          Expected Outcome
          Cheat sheet is integrated into the application and additional context help for built-in functions and modules is available.

          Project Properties
          Skills
          Programming language is C++
          GUI programming with the Qt framework
          Difficulty
          Easy

          Size
          Medium (175h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Organization website: https://www.openscad.org/


          ~~~~~~~~~~

          Create a compelling interface and functionality for the IfcOpenShell WASM / pyodide module #99

          Outline
          http://wasm.ifcopenshell.org/

          Details
          Expected Outcome
          Future Possibilities
          Project Properties
          Skills
          Difficulty
          Size
          Additional Information
          Potential mentor(s): NAME
          Organization website: https://
          Communication channels: https://******.zulipchat.com


          ~~~~~~~~~~

          Authoring interface for IFC4.3 alignment geometry in Bonsai #98
          Outline
          Industry Foundation Classes (IFC) offer the ability for rich information exchanges between modeling, analysis, planning, and other software tools in the Architecture, Engineering, and Construction (AEC) industry. Specifically, the latest release of IFC (version 4.3, also referred to as IFC4X3) adds linear referencing via alignment modeling, which is core to describing the construction and maintenance of infrastructure assets such as roads, bridges, and railways.

          Details
          Alignment import (read) capabilities have been added to IfcOpenShell and the Bonsai add-in for Blender. They have reached a state of maturity such that the next logic step is to enable alignment authoring (write) capabilities.

          Expected Outcome
          Alignment authoring will take place in Blender via the Bonsai add-in. A user-focused workflow has been developed and documented, along with preliminary user interface mockups. This project would add alignment authoring capabilities via new panels and other items within Blender. The ifcopenshell.api namespace will also need to be enhanced incrementally to support the new user interface tools.

          Project Properties
          Skills
          Understanding and general working knowledge of python.

          Difficulty
          Medium

          Size
          Medium (175 h)
          The participant focuses on authoring horizontal alignments via the PI method. This could be via interactive icons or primarily through a table-based interface. The user would need to be able to add, edit, and remove PI (point of intersection) points. Additionally the user would need to be able to adjust the radius that corresponds to each PI point. Though not strictly required for this project, the authoring tool would also enable definition and editing of entry and exit transition curve type (clothoid, sine spiral, polynomial spiral, etc.) and length.

          Long (350 h)
          PI-based alignment would be added for vertical and cant as well. A basic corridor modeling UI tools would be implemented to allow for sweeping geometry (open or closed profile) along an alignment curve to generate 3D linear geometry via IfcSectionedSolidHorizontal and related IFC entities.

          Additional information
          Mentors: Rick Brice @RickBrice & Scott Lecher @civilx64

          Organization website: https://ifcopenshell.org

          Communication channels: https://github.com/IfcOpenShell/IfcOpenShell/discussions

          Technical resources:

          https://docs.bonsaibim.org/guides/development/index.html

          Blender 4.3: Precise Modeling for Architecture, Engineering, and 3D Printing

          Python Scripting in Blender


          ~~~~~~~~~~

          Manifoldness repair 
          Outline
          Repair triangle soup that are not manifold.

          Details
          Basically, a valid solid mesh should be both manifold and has no self-intersection. However, models from the internet may contain defects. This project is about coming up with an algorithm that converts and repair a triangle soup into a manifold mesh.

          This will contain a lot of heuristics, basically what we need is:

          Stitching faces together, and maybe join faces that are close enough.
          Fill holes.
          Duplicate vertices and edges such that the result is a manifold in terms of connectivity.
          Expected Outcome
          Implementation of said algorithm.

          Future Possibilities
          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): @elalish @pca006132
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions

          ~~~~~~~~~~

          Overlap removal
          Outline
          Remove overlaps in meshes that contain self-intersection, assuming the mesh is a manifold.

          Details
          Basically, a valid solid mesh should be both manifold and has no self-intersection. However, models from the internet may contain defects. This project is about coming up with an algorithm that removes self-intersections.

          See elalish/manifold#289 for details about ideas for the algorithm.

          Expected Outcome
          Implementation of said algorithm.

          Future Possibilities
          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): @elalish
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions

          ~~~~~~~~~~

          Creation of an IFC geometry library in IfcOpenShell that uses Manifold

          Outline
          For the past 10 years, IfcOpenShell has had a tight coupling with OpenCASCADE as its only geometry library and OCCT providing the datatypes in the IfcOpenShell C++ APIs.

          In IfcOpenShell v0.8 an additional abstraction is introduced over the geometric concepts in IFC (taxonomy.h) and the evaluation of such concepts using pre-existing geometry libraries (AbstractKernel).

          Also in v0.8, CGAL is introduced as an additional runtime selectable choice besides OpenCASCADE, because of (a) it's extensive set of modules for analysis (e.g convex decomposition, skeleton, ...) and (b) it's arbitrarily robust (and precise) implementation of boolean operations using Nef polyhedra on a number type represents a binary tree of operands taking part in the construction of that number.

          Both OpenCASCADE and CGAL are high quality efforts, but quite complex and resulting in fairly large compiled object sizes. This project proposal aims at introducing Manifold as a 3rd geometry library implementation. Manifold is modern, efficient and robust.

          https://github.com/elalish/manifold

          cc @elalish just fyi.

          Expected Outcome
          Another AbstractKernel implementation that uses Manifold to evaluate a small set of geometrical concepts (boolean, extrusion, brep for example) in IFC. Expecting reasonable outcomes on a small building model (such as the Duplex A model) without necessarily resolving all complexities and corner cases encountered in that model.

          Future Possibilities
          Comparison between implementations and development of a hybrid composition of these libraries that based on prior inspection picks the most suitable implementation for a specific IfcProduct or representation item. For example, OpenCASCADE will likely still excel at curved surfaces (e.g nurbs), but suffers a monumental performance overhead when ingesting detailed triangular meshes (that are also prevalent in IFC) due the overheads of it BRep data model.

          Additional Information
          Potential mentor(s): Thomas Krijnen (aothms)



          ~~~~~~~~~~

          Turn BlenderBIM into a client for remote BIM-collaboration on existing OpenCDE-API-server with a graph backend

          Outline
          The project aim is to turn BlenderBIM into a client for remote BIM-collaboration and a client for remote BIM-model-sharing through a Common Data Environment (a CDE working as BIM/IFC-server) using the already developed OpenCDE API server and the OpenCDE API specifications provided by buildingSMART: BCF API and Documents API.

          OpenCDE API:s are open standards. This project will hence enable usage of BlenderBIM as a client on other BIM-servers that implements the OpenCDE API:s.

          Details
          An OpenCDE API server that implements all buildingSMART OpenCDE API:s (BCF API, Documents API and Foundation API) has been developed in python and the FastAPI framework. Solibri Office was used as a client for testing this server software during development.

          The code of the OpenCDE server is located in the IfcOpenShell repository here: https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/opencdeserver

          The OpenCDE API:s is a set of open API-specifications provided by buildingSMART. https://github.com/buildingSMART/OpenCDE-API

          BIM Collaboration Format (BCF) API is used for collaboration on shared BIM models through a remote BCF-server. BCF API has the same purpose as BCF XML (which is a file format) but the difference is that the data is communicated as JSON through a BCF-server, instead of sending XML-files. https://github.com/buildingSMART/BCF-API
          Documents API is used communication between a client and a CDE (acc. ISO 19650-1). The purpose is a common data environement for sharing models, documents et.c. https://github.com/buildingSMART/documents-API
          Foundation API is used for authentication et.c. and must be implemented by any client or server that implements anyone of the other two OpenCDE API:s. https://github.com/buildingSMART/foundation-API
          To summarize: The model (the IFC data) will normally be shared to the server using Documents API, and downloaded form the server using Documents API. BCF API can be used for remote collaboration on the models located on the server et.c.

          The purpose of the open API specification is to enable independent development of clients and servers that can communication with eachother. A server has already been developed and shared as open source on IfcOpenShell. However, at the moment there is no open source client with a graphical user interface for the OpenCDE API:s. A python library for BCF API communication is available: https://pypi.org/project/bcf-client/.

          The aim of this project is to turn BlenderBIM into a OpenCDE-client (a client that already have BIM-capabilities) that can communicate remotely with (and make use of) the existing open source OpenCDE-server on: https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/opencdeserver

          An add-on for GIT-collaboration have already been developed:

          https://blenderbim.org/docs/users/git_support.html
          https://www.youtube.com/watch?v=cJZhSCSSWdA
          Collaboration using Documents API and BCF API is just another way of collaboration. This way of collaboration might be more suitable for AEC-professionals who does not have experience of GIT. Som features of GIT are not possible. But other features that are possible when using the OpenCDE API:s are not possible using GIT.

          Expected Outcome
          The BlenderBIM can connect as a client to the OpenCDE API server using the Foundation API
          User management functionality is added to OpenCDE API server: Register, invite, delete users
          BlenderBIM is a BCF API client - can collaborate on BIM-models remotely using the already developed OpenCDE-server
          BlenderBIM is a Documents API client - can share/download BIM-models remotely with the already developed OpenCDE-server
          User interface in BlenderBIM for setting up OpenCDE-server on localhost and user management (inviting collegues, adding/deleting users et.c.)
          User interface in BlenderBIM for remote BIM collaboration using BCF API
          User interface in BlenderBIM for remote model download and sharing using Documents API
          Simplify the process of turning your computer into an OpenCDE-server and inviting colleages to collaborate on your BIM model
          Simplify the process of deploying the OpenCDE-server as a BIM-server to the cloud for remote collaboration with BlenderBIM as a client
          Extra: Implement som of the routes/endpoints of the OpenCDE API specifications that Solibri Office does not support. I.e. implement the full official buildingSMART open specifications using the open source server (OpenCDE API server) and open source client (BlenderBIM).

          Future Possibilities
          IFC-server capabilities: Round-tripping of IFC data between IFC STEP (or python or C++ objects in IfcOpenShell) and the OpenCDE API server graph DB. More info on storing IFC data as label property graph (LPG) here: https://www.sciencedirect.com/science/article/pii/S0926580523000389
          Potential synergies with the other GSoC project "Web-based UI integration with Blender" Web-based UI integration with Blender #87 because the Web-based UI could be hosted by the same OpenCDE API server as in this project.
          Visualization of IFC data as graph in that Web-based UI. For example using pyviz or similar tools.
          Project Properties
          Skills
          Python, including how to setup a minimal basic server on localhost using FastAPI. https://fastapi.tiangolo.com/
          Blender Python API to develop user interfaces in BlenderBIM.
          Cypher query language and any graph DB that implements Cypher (such as Neo4j or MemGraph). https://neo4j.com/docs/cypher-manual/current/introduction/
          API development.
          Difficulty
          Hard

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): [Martin Wiss]
          Organization website: https://blenderbim.org/ http://ifcopenshell.org/
          Communication channels: OSArch

          ~~~~~~~~~~

          Geometry Verification and Validation GUI in Qt (AI Project)

          Outline
          Help develop a new GUI application that checks geometry for common issues and/or helps fix them.

          Details
          A new GUI is in prototype development (built on Arbalest) that checks geometry files for common verification and validation (V&V) issues such as topology errors, solidity errors, and more. It's very much an experimental work in progress and we'd like your help to make it complete. The overarching goal of this effort is to extend our prototype in a significant way, either improving usability, checking for more issues, improving the Qt GUI infrastructure integration, integrating workflow(s) for review and repair, or leveraging AI to identify and/or fix issues.

          Expected Outcome
          You will propose a complete project description that identifies the specific objectives you'll aim to achieve. It's expected that you'll leverage the previous work (talk with us to get access to those materials). The proposal should identify 3-10 primary objectives that are researched and specific, starting with our previous effort.

          We essentially want a tool that "compiles" geometry reporting warnings and errors for issues encountered, akin to compiling source code in Visual Studio or Eclipse. There are questions of application architecture to resolve (e.g., whether to extend 'arbalest', integrate 'qged', integrate 'gist', etc). We want the tool to be graphical and interactive. We want it to have the ability to generate reports for auditing. Some of those capabilities exist in isolation, but none exist as a tool tailor-made for 3D geometry V&V.

          Future Possibilities
          This is a long term priority project with future possibilities in:

          GUI infrastructure
          AI integration
          geometry healing and repair workflows
          geometry auditing
          geometry standards development
          Skills
          Qt, C/C++

          Difficulty
          Easy or Medium depending on the objectives

          Size
          long (350h) preferred, but medium (175h) also possible

          Additional Information
          Potential mentor(s): Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com
          https://brlcad.org/design/v&v/


          ~~~~~~~~~~

          Blender UI / integration with voxelisation toolkit software

          Outline
          There is software known as the Voxelisation Toolkit (pip install voxec). It converts the 3D model into voxels (e.g. 3D cubes that represent geometry), analyses and transforms those voxels, and outputs statistics (e.g. distance between voxels, etc).

          Image

          Image

          Voxels are super cool and can be used to calculate head heights, resolve complex non-manifold geometry, egress distances, or concrete formwork areas and strutting distances, and air volume for mechanical calculations. All of this stuff is useful for engineers and construction professionals.

          This project is to add a UI in Blender to start making this general purpose analysis tool available to non-programmers.

          Details
          You will be expected to design an interface for the voxelisation toolkit, prepack some simple recipes, and write scripts that take the output (currently visualised as images or plots) and instead visualise the results in 3D by generating 3D coloured meshes that represent the output.

          Expected Outcome
          Bundle the voxelisation toolkit software with Blender.
          A UI to execute the voxelisation toolkit.
          Simple presets to run the toolkit.
          Visualise the output of the voxelisation analysis as a 3D coloured mesh.
          Future Possibilities
          Bundle scripts for common usecases, like formwork calculation, air volume calculation, or external / internal metadata addition.

          Project Properties
          Skills
          Python
          Difficulty
          Medium

          Size
          Medium to Long

          Additional Information
          Potential mentor(s): Dion Moult, Thomas Krijnen
          Organization website: https://blenderbim.org http://ifcopenshell.org
          Communication channels: https://osarch.org/chat


          ~~~~~~~~~~

          Features for CG artists to visualise beautiful IFC models in Blender 
          Outline
          The architecture, engineering, and construction industry creates 3D models of buildings. These models are generally quite poor and do not contain any textures, lighting, or high quality objects that are suitable for 3D rendering. They often hire artists to help create beautiful renders of their designs.

          This project will build utility functions and workflows to easily get beautiful pictures of 3D models.

          Details
          3D artists typically do the following steps to make a 3D model look beautiful. They:

          Set camera angles with specific camera settings, with "clay" (e.g. all white colours) materials.
          Add lights and sun / sky settings.
          Add simple colours and textures.
          Remodel low quality geometry
          Add new objects (e.g. entourage) to decorate the scene, like trees, grass, people, extra furniture, walruses, shrimp, etc.
          Set common compositing and post processing rules
          You will use the Blender Python API to set simple presets for most of these steps to allow less skilled artists to quickly setup renders. You will also setup a workflow to guide artists on how to organise their files relative to the IFC model and keep the IFC model separate so that when the IFC model is changed, the artists doesn't need to start from scratch or play spot the difference.

          You do not need to be an expert in 3D modeling or CG visualisation or rendering. You will be taught what type of settings and options are appropriate for presets and the details of the workflow. However, you will be expected to automate that detail (every aspect of the Blender settings can be set using Python trivially).

          You will also be expected to create a Blender interface to interact with the settings, e.g. a button to add camera, a button to set a preset sky, etc.

          Expected Outcome
          Note: scope is flexible and you may achieve less or more or different to the below:

          A graphical interface in Blender that relate to the 6 steps above
          Buttons to add cameras, set common camera aspect ratios and settings. Buttons to add common types of lights, set sun angles and sky settings with bundled HDRI textures.
          Buttons to add simple material presets.
          Buttons to mark an object to be replaced by another
          A few preset assets using Blender's built in asset tools to drag and drop in entourage.
          Future Possibilities
          Project Properties
          Skills
          Python (definitely required!)
          Artistic sense (do you like 3D graphics? rendering?) If you have ever rendered a 3D scene before, this is the project for you!
          Difficulty
          Easy to Medium

          Size
          Medium to Long

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://blenderbim.org http://ifcopenshell.org
          Communication channels: https://osarch.org/chat

          ~~~~~~~~~~

          Implement 3D mesh offset
          Outline
          Implement efficient 3D mesh offset, instead of using minkowski sum with high resolution spheres. (elalish/manifold#192)

          Details
          3D mesh offset is a useful feature that many users asked for, but is difficult to implement efficiently. Many users use minkowski sum with sphere to perform positive offset, but this can be very slow due to the need for exact convex decomposition.

          Our approach will only work for positive offset, negative offset can be implemented by performing additional mesh boolean operations, so this is not an issue. The approach has four phases:

          Figure out all pairs of faces that do not share any vertex and may overlap after offsetting. (let's call them conflict pairs)
          Cut the mesh in a way such that for each part, no two faces are in the same conflict pair. (decomposition step, requires monte carlo tree search)
          Perform the positive offset on each part, using a modified algorithm from Offset Triangular Mesh Using the Multiple Normal Vectors of a Vertex. Note that we need to figure out how to blend the surfaces for smooth results.
          Union the parts.
          Expected Outcome
          A fast 3D mesh decomposition algorithm!

          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.
          Size
          Long.
          Additional Information
          Potential mentor(s): @elalish @pca006132 @zalo
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions


          ~~~~~~~~~~

          Add fuzzing tests

          Outline
          Add more fuzzing tests for both 2D and 3D operations.

          Details
          Fuzzing is an effective technique to expose bugs in software. Fuzzing tests randomly generate structured inputs (according to specification), and test if the program crashes/failed assertions.

          This project aims to test 2D and 3D CSG operations on geometrically valid polygons/meshes. To do this, we will define a very simple AST for our CSG operations, and use the recursive domain feature of fuzztest for the tests.

          We will also randomly apply slight perturbation to make the valid geometry only epsilon-valid, to test for robustness of the algorithm.

          Expected Outcome
          Fuzz tests that test for union, intersection, difference, 2D extrude/revolve, etc.

          Project Properties
          Skills
          C++
          Basic understanding of graph data structure.
          Difficulty
          Medium
          Size
          Medium
          Additional Information
          Potential mentor(s): @pca006132
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions


          ~~~~~~~~~~

          Physically-Based Rendering (PBR) advanced shaders
          Outline
          Get BRL-CAD physically-based rendering working with advanced shaders.

          Details
          BRL-CAD recently integrated with Appleseed which provides physically-based rendering. It's presently a command-line renderer called 'art'. For art rendering to work, a shader and colors are specified on geometry. BRL-CAD has preliminary support for material objects including OSL shaders and MaterialX shaders in art, however their support has only been tested with basic shaders such as the Disney Principled Shader. It's hard-wired to single-file shaders.

          Expected Outcome
          The goal of this task will be to make art read and work with any OSL or MaterialX shader networks, including ones using texturing, emission, subsurface scattering, etc. applied to BRL-CAD geometry.

          Project Properties
          Skills
          Decent C/C++ skills
          Some basic familiarity with PBR.
          Basic familiarity with shaders.
          Difficulty
          medium

          Size
          long

          Additional Information
          Potential mentor(s): Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          Improve FreeCAD Hidden Line Removal
          Outline
          FreeCAD's Technical Drawing module (TechDraw) relies heavily on the OpenCascade Hidden Line Removal algorithms. These algorithms can be very slow, do not provide progress reporting and do not provide any linkage between the input shape and the output.

          Details
          The TechDraw module provides projections, section views and detail views of 3D model components and assemblies developed in FreeCAD modules such as Part, PartDesign and Draft.

          Expected Outcome
          a) develop new code for projecting shapes and creating the geometry for technical drawings.
          -or-
          b) modify the existing OpenCascade code as an enhancement.

          Project Properties
          Both OpenCascade and TechDraw are written in C++.

          Skills
          The student should have a good knowledge of C++ and be familar with graphics topics such as the painters algorithm, face detection and hidden line removal.
          Knowledge of technical drawing standards and previous exposure to Qt will be helpful. Familiarity with OpenCascade is a definite plus.

          Difficulty
          Hard

          Size
          long

          Additional Information
          Potential mentor(s): wandererfan
          Organization website: https://freecadweb.org
          Communication channels: https://forum.freecadweb.org


          ~~~~~~~~~~

          Continuation of a prior BRL-CAD GSoC effort
          Outline
          BRL-CAD has been participating in GSoC for over 10 years with nearly 100 students! Any past accepted projects can be submitted as a continuation project.

          Details
          You can find all past participants documented on BRL-CAD's wiki by selecting a given year (e.g., 2018). Even the most successful and completely integrated projects have room for improvement! If any of those past efforts for any prior year sound very interesting to you, you can propose a continuation effort for it.

          Of course, you will need to research the prior effort to determine the status of the work, whether code was integrated or is sitting pending integration in a patch, whether it's functional or was in an intermediate state, etc. You'll also want to come chat with us on Zulip to make sure there is mentoring support for it, but there usually is if you're passionate and independently productive.

          For your proposal, note that it's a continuation effort. Explain what you are doing and how it relates to the prior effort. It's strongly recommended that your development plan focus on production-quality integration aspects such as making sure there are no usability or user experience (UX) issues, no build integration issues, that testing is covered adequately, and with focus on UX.

          Expected Outcome
          The expected outcome of a continuation effort is new capability and features that are "complete", integrated, bug-free, and issue-free, in the hands of users. This means your project covers all vertical integration aspects of development integration including build system and usability / UX concerns. Not prototyped. Not simply rewritten or re-attempted.

          If the prior effort was integrated, your outcome will be specific polish, adaptiveness, and robustness improvements.

          If the prior effort was not integrated, your outcome will be issue-free integration that addresses prior issues preventing integration (which will require research and understanding on your part).

          Project Properties
          Skills
          This varies greatly by continuation. There are continuation projects for C/C++, Python, Javascript/Node.js, Tcl/Tk, OpenCL, OpenGL, Qt, GPGPU, and more.

          Difficulty
          Varies.

          Size
          You are welcome to scope your project medium (175h) or long (350h) depending on the objectives and development scope.

          Additional Information
          Potential mentor(s): Morrison (contact devs@brlcad.org)
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Implement AP242 STEP geometry importer for BRL-CAD
          Outline
          Implement a geometry importer for the ISO 10303 STEP AP242 standard.

          Details
          BRL-CAD has geometry import support for STEP AP203 (v1), but AP242 has emerged as its industry replacement. This project entails implementing as comprehensive import support as possible in BRL-CAD.

          In order to track implementation progress and manage development risk, you will need to track implementation coverage by setting up a dashboard similar to what is used by the CAx-IF -- it can be a simple text file or web page.

          Existing conversion support can be examined for AP203 and other formats in BRL-CAD's repository under src/conv/step

          Expected Outcome
          New AP242 importer that converts STEP entities into BRL-CAD's .g geometry file format.

          Future Possibilities
          AP242 export support...

          Project Properties
          Skills
          C/C++
          STEPcode

          Difficulty
          Hard.

          Size
          This project can be scoped medium (175h) or long (350h) depending on your familiarity and expertise, or you can propose a subset of entities in a shorter timeframe (note though that advanced boundary representation entities should be prioritized).

          Additional Information
          Potential mentor(s): Morrison (contact devs@brlcad.org)
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          BRL-CAD Python bindings
          Outline
          Implement bindings for the BRL-CAD functionality to Python programming language

          Details
          There are long time on-going efforts to wrap BRL-CAD functionality with python code, e.g.

          https://github.com/kanzure/python-brlcad
          https://github.com/nmz787/python-brlcad-tcl
          These projects are however still in early development stages.

          Expected Outcome
          A Python module which can read and write BRL-CAD databases, and provide access to their contents to read, create, and modify the objects.

          Project Properties
          Skills
          C/C++
          Python
          Difficulty
          This project may be of easy or medium difficulty, depending on your familiarity and expertise.

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Daniel Rossberg
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          Webapp to create and check BIM project exchange requirements for IfcOpenShell
          Outline
          When projects exchange data, we often need to set contractual requirements about what data we expect to see in their CAD or Building information data. The is an international standard for describing project requirements in XML called the Information Delivery Specifications (IDS).

          There is a half-built webapp which allows viewing and minor editing of IDS files here: https://blenderbim.org/ifctester/

          Your job would be to finish this web app, building features for more editing, drag and drop from a library of specifications, adding and removing requirements, etc.

          Expected Outcome
          A working example of the web application.

          Project Properties
          Skills
          HTML, CSS, and vanilla Javascript (i.e. no frameworks).
          Difficulty
          Easy

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://ifcopenshell.org
          Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues


          ~~~~~~~~~~

          Scripts for generating simple animations (e.g. appear / disappear, bounce, appear left to right, fade in from above, etc)

          Outline
          Often, construction firms need to visualise animations of construction sequencing. A project timeline will be created, and related to individual model elements. For example, when a concrete slab is poured, it is linked to a 3D object called a slab. We need the ability to automatically generate animations from Blender where objects appear / disappear in various different ways when they start / end their task in the project timeline. The systems for describing project timelines is already in place, so now we need a little animation generator!

          Details
          Expected Outcome
          A series of small scripts that take objects and can automatically animate the visibility, locations, or staggered appearances of building elements, as well as sub elements, and basic scripts that correlate real world time to animation frames, and frames per second, and generate an animated timeline bar in various styles.

          Future Possibilities
          This animation system can be then used from BIM models either in Blender, FreeCAD, or via other software altogether, so it has quite a large impact on the ecosystem.

          Skills
          Basic knowledge of the principles of animation (keyframing)
          Basic Blender animation (you can do some tutorials and get up to speed pretty quick)
          Python
          Artistic sense! We should offer beautiful and elegant animations!
          Difficulty
          Easy

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://ifcopenshell.org
          Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues


          ~~~~~~~~~~

          NURBS Editing Support in BRL-CAD

          Outline
          Implement the prerequisites for NURBS editing in BRL-CAD's GUIs

          Details
          BRL-CAD has support for raytracing of NURBS surfaces implemented, but they are handed over as BLOBs to the openNURBS library. Beyond basic operations such as rotation and translation, the BRL-CAD core has no ability to edit them. This project would implement support for editing NURBS curves and surfaces in the BRL-CAD core, thus creating the prerequisites to handle them with higher level (i.e. GUI) tools.

          See this task's description in former GSoCs for some more information: https://brlcad.org/wiki/NURBS_Editing_Support

          The key-feature would be to have ged command(s) that lets you build NURBS objects from scratch. This could be done by having a declarative ASCII description of these entities and/or wrapping the openNURBS library by a scripting language.

          Describe in your proposal which approach you want to use and why. You may let inspire you by solutions in other programs:

          NURBS-Python: https://github.com/orbingol/NURBS-Python
          Blender: https://blender.stackexchange.com/questions/7020/create-nurbs-surface-with-python
          Web3D: https://www.web3d.org/x3d/content/examples/Basic/NURBS/
          3DSMax/Maya: https://help.autodesk.com/view/3DSMAX/2016/ENU/?guid=__files_GUID_75CD4DE9_8024_4E25_B147_0A0EC8B10031_htm
          Ayam: http://ayam.sourceforge.net/docsdraft/ayam-6.html
          Expected Outcome
          Implementing the necessary logic for NURBS handling in librt, libbrep, and libged

          Future Possibilities
          Implementing a visual NURBS editor in a BRL-CAD GUI (mged, Archer, Arbalest)

          Project Properties
          Skills
          C/C++
          Difficulty
          medium

          Size
          long (350h)

          Additional Information
          Potential mentor(s): Daniel Rossberg, Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          New BRL-CAD GUI

          Outline
          Develop further the new GUI for BRL-CAD!

          Details
          BRL-CAD has two main graphical applications called 'mged' and 'archer' which look like they were developed in the 80's and 90's respectively (because they were). We need a modern GUI, ideally using Qt.

          This new GUI will need to leverage our existing libraries in a big way. This includes the C++ coreInterface ( see https://brlcad.org/wiki/Object-oriented_interfaces) or its successor MOOSE (see https://github.com/BRL-CAD/MOOSE) and LIBGED (see src/libged). The latter is basically all commands available to both mged and archer.

          During past GSoCs an amazing start was made with arbalest. Based on this, the development of a GUI called 'qged' (see src/qged) was started, which you should include in your considerations too. This program implements the traditional BRL-CAD workflow under a modern Qt-based user interface.

          You may propose a complete different approach, but we recommend to use arbalest as starting point for your work. Which additions would you like to program in this years GSoC? You can use the results of the former prototype CAD GUI Google Code-in tasks (http://brlcad.org/gci/data/uncategorized/, search for CAD_GUI there) for inspiration.

          Keep your proposal lean and simple. The main emphasis should be on adding features and/or improvements to our next generation GUI.

          Expected Outcome
          An improved BRL-CAD GUI.

          Project Properties
          Skills
          C/C++
          Qt
          Difficulty
          medium

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Daniel Rossberg
          Himanshu Sekhar Nayak
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Online Geometry Viewer (OGV)

          Outline
          Write a proposal that leverages rewrite of the existing application in the latest tech stack for frontend and backend.

          Details
          We have been working on OGV for over many years. It started with PHP and then was revamped to meteor.js. We want to focus on the backend of OGV, making sure it works properly, converts the models properly, and basically finish a 1.0 version of OGV so we can launch it for the masses! For that, we are planning to change the legacy backend to be rewritten along with the frontend.

          You can use any tech stack (react, vue) for frontend and node, C/C++ for the backend. We faced some problems like removing certain deprecated dependencies and adding new features with Meteor. We are planning to port the application and add all specified features.

          Possible New Features
          Integrating BRL-CAD GCV(Geometry Conversion Vocabulary) to add support for more file formats like .stl, .obj, and .3dm.
          Automated Conversion for Web Display. Convert uploaded files automatically into polygonal formats for web visualization. Ensure smooth rendering and compatibility with web-based 3D viewers.
          Implementing a Model Repository based project architecture for storing and downloading 3d models.
          Conduct a full STIG compliance audit (Security Technical Implementation Guide), which involves ~200+ security checks.
          Run security scans using OWASP and Dependency-Check, addressing any reported vulnerabilities.
          However, you don't have to limit yourself to those ideas.

          Checklist to write proposal for OGV
          Download and clone OGV from https://github.com/BRL-CAD/OGV-meteor
          Setup and Run OGV on your local machine.
          Fork OGV repo
          Understand the flow of existing application
          Talk to mentors
          Choose list of issues that you would like to solve this summer
          Make a detailed weekly implementation plan
          Share your proposal with your mentors
          Submit it to the GSoC website
          Expected Outcome
          You're expected to propose an outcome useful to end-users. That is a broad range of possibilities that will depend on your interests and experience level. For example, you might propose focusing on the backend conversion to triangles for display (C/C++/Node.js). Or you might propose changing the backend to NURBS surfaces (C/C++) and using verbnurb or three.js (Javascript) to display them instead of triangles. Or you might propose keeping the backend the way it is and focus on front-end robustness (Vue, React), website features, or deployment infrastructure. You hopefully get the idea.

          Project Properties
          Skills
          JavaScript (Vue, React)
          Node.js (required)
          C/C++ (optional)
          Verbnurb (optional)
          Three.js (optional)
          Difficulty
          Hard

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Amanjot Singh
          Daniel Rossberg
          Divyanshu Garg
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Add OpenSCAD support for exporting models in STEP format
          Outline
          The STEP format is widely used in the industry to transfer CAD data between different systems. Currently OpenSCAD does not support STEP import or export. Adding STEP export would open up a number of new usecases or simplify the workflow as no external conversion tools are needed to convert to STEP. This includes the design of 3D models for other CAD tools, e.g. for KiCAD where STEP models are used to render 3D representations of PCBs. Other use cases are for manufacturing where sometimes only STEP files are accepted as input, e.g. for CNC milling services.

          Details
          The main focus of this project is to get the ground work done for exporting more detailed models, as opposed to just exporting the fully rendered single mesh which is the normal case right now.

          Topics that need to be solved

          Research options of usable libraries
          Investigate what type of STEP files are accepted as input by various tools
          Select library and integrate into OpenSCAD
          Implement base functionality to export single meshes
          Add test cases to verify the new export functionality
          Update build system to include the new library into installers
          Prototype how more advanced models can be exported
          Expected Outcome
          OpenSCAD supports exporting single meshes as STEP
          (optional) Understanding/Plan of how to support additional features supported by STEP
          Project Properties
          Skills
          Programming language is C++
          Understand and use APIs from external libraries
          Integrate new libraries into the build system for the 3 supported platforms
          Add test cases with files using the new features to allow regression testing
          Difficulty
          Hard

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Organization website: https://www.openscad.org/
          Known libraries:

          StepCode - http://stepcode.org/ (https://github.com/stepcode/stepcode)
          OpenCASCADE - https://www.opencascade.com/


          
    totalCharacters_of_ideas_content_parent: 51942
    totalwords_of_ideas_content_parent: 12320
    totalTokenCount_of_ideas_content_parent: 10905
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/brl-cad/
    idea_list_url: https://github.com/opencax/GSoC/issues?q=is%3Aissue+is%3Aopen+label%3A%22GSoC+2025%22



  - organization_id: 13
    organization_name: BeagleBoard.org
    no_of_ideas: 5
    ideas_content: |
        Deep Learning 
        Medium complexity 175 hours

        A Conversational AI Assistant for BeagleBoard using RAG and Fine-tuning
        BeagleBoard currently lacks an AI-powered assistant to help users troubleshoot errors. This project aims to address that need while also streamlining the onboarding process for new contributors, enabling them to get started more quickly.

        Goal: Develop a domain-specific chatbot for BeagleBoard using a combination of RAG and fine-tuning of an open-source LLM (like Llama 3, Mixtral, or Gemma). This chatbot will assist users with troubleshooting, provide information about BeagleBoard products, and streamline the onboarding process for new contributors.
        Hardware Skills: Ability to test applications on BeagleBone AI-64/BeagleY-AI and optimize for performance using quantization techniques.
        Software Skills: Python, RAG, Scraping techniques, Fine tuning LLMs, Gradio, Hugging Face Inference Endpoints, NLTK/spaCy, Git
        Possible Mentors: Aryan Nanda

        ~~~~~~~~~~



        Linux kernel improvements
         Medium complexity 350 hours

        Update beagle-tester for mainline testing
        Utilize the beagle-tester application and Buildroot along with device-tree and udev symlink concepts within the OpenBeagle continuous integration server context to create a regression test suite for the Linux kernel and device-tree overlays on various Beagle computers.

        Goal: Execution on Beagle test farm with over 30 mikroBUS boards testing all mikroBUS enabled cape interfaces (PWM, ADC, UART, I2C, SPI, GPIO and interrupt) performing weekly mainline Linux regression verification
        Hardware Skills: basic wiring, embedded serial interfaces
        Software Skills: device-tree, Linux, C, OpenBeagle CI, Buildroot
        Possible Mentors: Deepak Khatri, Anuj Deshpande, Dhruva Gole

        ~~~~~~~~~~

        Linux kernel improvements
         Medium complexity 175 hours

        Upstream wpanusb and bcfserial
        These are the drivers that are used to enable Linux to use a BeagleConnect Freedom as a SubGHz IEEE802.15.4 radio (gateway). They need to be part of upstream Linux to simplify on-going support. There are several gaps that are known before they are acceptable upstream.

        Goal: Add functional gaps, submit upstream patches for these drivers and respond to feedback
        Hardware Skills: wireless communications
        Software Skills: C, Linux
        Possible Mentors: Ayush Singh, Jason Kridner

        ~~~~~~~~~~

        Automation and industrial I/O Medium complexity 175 hours

        librobotcontrol support for newer boards
        Preliminary librobotcontrol support for BeagleBone AI, BeagleBone AI-64 and BeagleV-Fire has been drafted, but it needs to be cleaned up. We can also work on support for Raspberry Pi if UCSD releases their Hat for it.

        Goal: Update librobotcontrol for Robotics Cape on BeagleBone AI, BeagleBone AI-64 and BeagleV-Fire
        Hardware Skills: basic wiring, motors
        Software Skills: C, Linux
        Possible Mentors: Deepak Khatri, Jason Kridner

        ~~~~~~~~~~

        RTOS/microkernel imporvements
        Medium complexity 350 hours

        Upstream Zephyr Support on BBAI-64 R5
        Incorporating Zephyr RTOS support onto the Cortex-R5 cores of the TDA4VM SoC along with Linux operation on the A72 core. The objective is to harness the combined capabilities of both systems to support BeagleBone AI-64.

        Goal: submit upstream patches to support BeagleBone AI-64 and respond to feedback
        Hardware Skills: Familiarity with ARM Cortex R5
        Software Skills: C, RTOS
        Possible Mentors: Dhruva Gole, Nishanth Menon
        Upstream Repository: The primary repository for Zephyr Project

          
    totalCharacters_of_ideas_content_parent: 3803
    totalwords_of_ideas_content_parent: 786
    totalTokenCount_of_ideas_content_parent: 816
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/beagleboard.org/
    idea_list_url:  https://gsoc.beagleboard.io/ideas/


  - organization_id: 14
    organization_name: Blender
    no_of_ideas: 15
    ideas_content: |
        Flamenco¶
        Flamenco is Blender's render farm management system. For all the ideas below, a requirement is that you've set up Flamenco for yourself and used it to render things. Of course also some experience with Blender itself is needed.

        Statistics¶
        Description: Design & build a system for Flamenco to collect and display statistics.
        The goal is to show per-job, per-task, and per-worker statistics.
        This should make it possible for users to predict how long a render job will be, as they can look up things like render times of similar render jobs.
        The underlying design should be generic enough to store all kinds of statistics and events, not specific to Blender render times only.
        This project would include the technical design of this feature, the frontend / UI / UX design, and the implementation of both front- and back-end.
        Optional: record more statistics, such as per-frame render time, memory usage, etc.
        Optional: show progress of jobs in the jobs list.
        Expected outcomes: A way for users to get a better understanding of how a new render job will behave, as they can look up information about past & currently running jobs.
        Skills required: Familiarity with Go and unit testing. Familiarity with web languages (HTML/CSS/JavaScript, VueJS).
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        RNA Overrides¶
        Description: Render jobs should be able to specify "RNA overrides" (#101569). In other words, the job definition should be able to include some Python code that sets certain properties in Blender to certain values.
        Design a way to include this in a job definition, and how it affects tasks & commands.
        This could include the ability to add new RNA overrides to existing jobs.
        This should include the ability to update those override values via the web interface.
        Design how such additions / changes affect already-created tasks/commands in the database. Or a way to make this work without changing things in the database?
        This can be used both when a user wants to change something (like re-rendering with increased sample count), or for Flamenco itself to adjust things in the blend file without having to save those values in the blend file itself (#104264)
        Expected outcomes: Give users a simpler way to configure Flamenco for their needs.
        Skills required: Familiarity with Blender (for making the RNA overrides themselves work). Familiarity with Go and unit testing for adjusting Flamenco. Familiarity with web languages (HTML/CSS/JavaScript, VueJS) for the web frontend.
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        Configuration Web Interface¶
        Description: Flamenco's configuration file can be created via its Setup Assistant, but that's only for the initial configuration. For managing more complex things, like the two-way variables for cross-platform support, users still have to manually edit the YAML file. This project is about introducing a configuration editor in the web frontend, and potentially new backend API functions to support that.
        New tab in the frontend for managing the configuration.
        A way to retrieve and visualise the configuration.
        New front-end widgets to represent these, including more complex cases like one-way and two-way variables.
        A way to save the edited configuration.
        Optional: A validator for the configuration options, so that changes can be checked before they take hold.
        Optional: A way for Flamenco Manager to load and apply the configuration without restarting the process.
        Optional: A custom job type for validating configuration paths, so that, for example, a macOS path can be actually checked on a Worker running macOS.
        Expected outcomes: Give users a simpler way to configure Flamenco for their needs.
        Skills required: Familiarity with web languages (HTML/CSS/JavaScript, VueJS, OpenAPI). Potentially also familiarity with Go in case of backend work.
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        Polish & Shine¶
        Description: Fix various issues & implement missing pieces to solve common bumps in the road.
        Reconsider some design aspects of the web frontend, so that it works better on narrower / smaller screens.
        Allow finishing setup assistant without Blender on Manager (#100195).
        Introduce per-Worker logs on the Manager, for introspection and debugging.
        Fix issues with two-way variables (#104336, #104293)
        Add a web interface for the mass deletion of jobs. There is already an API call for this, which deletes all jobs older than a certain timestamp.
        Other issues from the tracker.
        Expected outcomes: Improve the overall experience people have when working with Flamenco.
        Skills required: Familiarity with web languages (HTML/CSS/JavaScript, VueJS) for front-end work. Familiarity with Go and OpenAPI for backend work.
        Possible mentors: Sybren Stüvel.
        Expected project size: 175 hours
        Difficulty: medium

        ~~~~~~~~~~
        Geometry Nodes¶
        Regression Testing¶
        Description: As people build more assets on top of Geometry Nodes, it becomes more and more important to ensure good backwards compatibility. This project focuses on improving our regression tests to cover more issues as early as possible. This involves:
        Adding new tests in our existing test framework.
        Extending the test framework to cover node tools, baking and maybe other areas we still have to find.
        Preparing more complex production files for use in regression tests.
        Expected outcomes: Improved stability of Geometry Nodes.
        Skills required: Good in C/C++ and Python.
        Possible mentors: Jacques Lucke, Hans Goudey
        Expected project size: 90 or 175 hours depending on how many of the mentioned areas are covered
        Difficulty: Easy (using existing framework) and Medium (extending framework)

        ~~~~~~~~~~
        Modeling¶
        Improve Edit-Mesh Mirror¶
        Description: Blender's mesh mirroring in mesh edit-mode works for basic transformations, but does not work for most other operations such as sliding, smoothing, marking seams, etc. In practice, this makes the edit-mode mirror only useful in very specific circumstances and not for general modeling.

        While supporting every operation isn't practical, enabling it for a subset of operators such as those that only adjust existing geometry (rather than adding or removing geometry) would be immediately useful for artists.

        Expected outcomes: Improved edit-mesh mirror support for existing tools.
        Skills required: Proficient in C/C++.
        Possible mentors: Campbell Barton
        Expected project size: 175 or 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        Sculpt & Paint¶
        Mesh Sculpting Performance Improvements¶
        Description: Last year's sculpting rewrite project gave a large improvement in performance, but the team didn't have the time to pursue everything. This task lists possible future improvements. This GSoC project would explore one or more of those ideas with in depth performance testing and experimentation.
        Expected outcomes: More interactive sculping with large meshes
        Skills required: Proficient in C++, familiarity with data-oriented-design.
        Possible mentors: Hans Goudey
        Expected project size: 175 or 350 hours
        Difficulty: medium or hard
        
        ~~~~~~~~~~
        VFX & Video¶
        Hardware accelerated video encoding/decoding¶
        Description: Currently Blender encodes and decodes video though ffmpeg C libraries, on the CPU. ffmpeg also has support for hardware video processing (various kinds depending on platform), this project would enable usage of that.
        Build ffmpeg with hardware video processing support included.
        Note: Blender can't include "non-free" ffmpeg libraries (which means cuda_nvcc, cuda_sdk, libnpp can't be used).
        On Blender's video decoding and encoding side, implement code that would use any relevant ffmpeg C libraries parts for hardware video processing, when supported.
        Decide which additional UI settings need to be exposed to the user, to control hardware video processing.
        Implement code needed to transfer video frames between hardware memory and CPU memory as needed (the rest of VSE processing pipeline is purely on CPU currently).
        Expected outcomes: Video encoding or decoding is more efficient by using dedicated hardware.
        Skills required: Proficient in C/C++, familiarity with ffmpeg.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        High Dynamic Range (HDR) support for video¶
        Description: This project has several partially dependent parts that are all about HDR support within VSE:
        Make Sequencer preview window be able to display HDR content on a capable display (like 3D viewport or Image window can).
        Make blender movie reading code be able to decode HDR videos into proper scene-linear or sequencer color space as needed. HDR video data might be PQ or HLG encoded, and this might need special decoding into destination color space.
        Make blender movie writing code be able to encode HDR videos. Blender already can encode 10/12 bit videos, but only for regular LDR. Additional PQ or HLG data encoding and necessary video metadata is not currently done.
        Expected outcomes: HDR video handling is improved within Blender.
        Skills required: Proficient in C/C++, familiarity with ffmpeg, knowledge of color spaces and color science.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: OpenTimelineIO support¶
        Description: built-in support for OpenTimelineIO import/export within Blender VSE. Blender Studio has experimented with it in 2021, by using and extending a 3rd party addon vse_io. It might be useful to have built-in support for this.
        Expected outcomes: Blender VSE can import and export .otio files.
        Skills required: Proficient in C/C++, familiarity with video editing workflows.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Pitch correction for sound playback¶
        Description: Currently when audio is retimed, the pitch changes, would be nice to have an option to preserve pitch. Different approaches could be researched and implemented (e.g. pitch correction for mostly human speech might be different from pitch correction of music). Might need integration of some 3rd party library if it is suitable for the task, or implementing the correction algorithms manually.
        Expected outcomes: Retimed sound playback has options to preserve original pitch.
        Skills required: Proficient in C/C++, sound processing algorithms.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Animation retiming¶
        Description: Modify animation of strips, when changing their playback speed. Retiming allows changing playback speed of strips, but when strips are animated, the animation keys are fixed in position. These could be moved, such that animation is seemingly mapped to frames of the content.
        Expected outcomes: Animation proportionally is scaled with strip when retiming.
        Skills required: Proficient in C++.
        Possible mentors: Richard Antalik
        Expected project size: 175 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Keyframing in preview¶
        Description: Open workflow quick animation in VSE preview region. In 3D viewport it is possible to transform object and press I key to add key for its position. This also could be done in sequencer preview. This feature should follow same rules and preferences.
        Expected outcomes: Possibility of quick and easy animation in VSE preview
        Skills required: Proficient in C++.
        Possible mentors: Richard Antalik
        Expected project size: 90 hours
        Difficulty: medium

        ~~~~~~~~~~
        Compositor: Implement new nodes¶
        Description: The compositor has been rewritten to be more efficient and future proof. Moving forward, it would be nice to implement new nodes to make the compositor as powerful as it can be. Interested students are encouraged to propose their own ideas. Some example nodes include:
        Define low / high points
        Expected outcomes: Implemented one or more nodes for both CPU and GPU backends.
        Skills required: Good in C/C++, image processing algorithms, familiarity with shaders.
        Possible mentors: Habib Gahbiche / Omar Emara
        Expected project size: 90 or 175 hours depending on the node
        Difficulty: Easy or Medium depending on the node

        ~~~~~~~~~~
        Compositor: UI improvements¶
        Description: The compositor has been rewritten to be more efficient and future proof. It would be nice to improve the UI of some nodes as well as the workflow overall. Interested students are encouraged to suggest ideas. Some examples include:
        Implement 2D gizmos for exisiting nodes.
        Re-design the UI of exisiting nodes
        Expected outcomes: Improved UI within node editors.
        Skills required: Proficient in C/C++, familiarity with design patterns.
        Possible mentors: Habib Gahbiche / Omar Emara
        Expected project size: 90 or 175 hours depending on the scope of the project
        Difficulty: Easy or Medium depending on the scope

          
    totalCharacters_of_ideas_content_parent: 14181
    totalwords_of_ideas_content_parent: 3076
    totalTokenCount_of_ideas_content_parent: 2963
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/blender-foundation/
    idea_list_url: https://developer.blender.org/docs/programs/gsoc/ideas/

  - organization_id: 15
    organization_name: CCExtractor Development
    no_of_ideas: 13
    ideas_content: |
      
        CCExtractor Release 1.00	This is our ambitious project for the summer - work directly with the core team to prepare 1.00 - our first major version bump ever, by getting our PR's from last year vetted, tested and integrated	Some of these: Rust, C, Flutter, Docker, GitHub actions	The rest from the previous list.	Hard	350 hours
        ~~~~~~~~~~
        Ultimate Alarm Clock III	The ultimate alarm clock, with features no other one has. And free!	Flutter	Good application design	Medium	350 hours
        ~~~~~~~~~~
        Beacon Watch Companion	Beacon was started in 2021 and it got a great push also during 2022 and 2024. It aims to ease the group travelling (or hiking). This project is intended to be a companion for the beacon project for smart watches.	Flutter	Scalability	Medium	175 hours
        ~~~~~~~~~~
        Ultimate Alarm Clock Watch Companion	Ultimate Alarm Clock launched in 2023 and gained significant momentum in 2024. It aims to offer unique features that set it apart from other alarm clock apps—all for free!. This project is intended to be a companion for the ultimate alarm clock project for smart watches.	Flutter	Scalability	Medium	175 hours
        ~~~~~~~~~~
        Smart Health Reminder	A fun and interactive health tracking app with smart reminders, challenges, and gamification. Stay healthy effortlessly!	Flutter	Gamification & UX design	Medium	350 hours
        ~~~~~~~~~~
        support more torrent clients	We'd like to add support for other clients to our ruTorrent mobile interface (which of course will get a new name): Flood and Deluge.	Flutter	API, Teamwork	Medium	Discuss
        ~~~~~~~~~~
        URL shortener, with a twist	A URL shortener converts a long URL into a shorter one. There are many use cases. Some times it's just the shortening itself we want, for example to share it on twitter. Other times it's about obfuscation. We want to create our own, but with some unique features.	Any language you want	Internet infrastructure	Medium	175 hours
        ~~~~~~~~~~
        COSMIC Session For Regolith	COSMIC is a wayland based desktop environment written from scratch in rust, with modularity in mind. We're interested in swapping the GNOME components of Regolith DE with COSMIC.	Rust	Wayland, Iced, DBus, etc	Medium	350 hours
        ~~~~~~~~~~
        Add complex layouts to sway	Sway is a drop-in replacement for i3, a popular windows manager for Linux that finally gets rid of the ancient X11 protocol. It's fantastic, but it's still missing support for complex scenarios. We'd like you to work on that support.	C	Sway	Hard	350 hours
        ~~~~~~~~~~
        Expose ectool functionality as a library	ectool is a CLI that lets you interact with an embedded controller for laptops. Expose its functionality as a library so it's possible to use it without spawning the CLI.	C, Python	Interlanguage connectivity	Medium	350 hours
        ~~~~~~~~~~
        CCSync	This project aims to develop a comprehensive platform that can be used sync tasks with taskserver.A hosted solution for syncing your TaskWarrior client.Setting up your own TaskServer takes some effort.And platforms like inthe.am,freecinc have shut down their services.So we want to create a platform similar to inthe.am , freecinc and wingtask.	Any language you want	Internet infrastructure	Medium	175 hours
        ~~~~~~~~~~
        Mouseless for Linux v2 - i3 edition	Mouseless is a nice tool to practice keyboard shortcuts for a few popular apps. Unfortunately it's only available for Mac. Last year we created an open source one that runs on Linux. Using that work or not (this is your choice) we want to create one that helps use i3vm (the fantastic windows manager) using keys only.	Your choice	??	Unknown	175 hours
        ~~~~~~~~~~
        Desktop Actions in Ilia	Desktop Actions defined in .desktop files are used by app launcher to provide access to additional functionalities, typically via context menus. Ilia is an app launcher that currently doesn't support for Desktop Actions due to its keyboard based approach.	Vala, GTK	GTK	Medium	175 hours




          
    totalCharacters_of_ideas_content_parent: 4077
    totalwords_of_ideas_content_parent: 734
    totalTokenCount_of_ideas_content_parent: 964
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ccextractor-development/
    idea_list_url: https://ccextractor.org/docs/ideas_page_for_summer_of_code_2025/

  - organization_id: 16
    organization_name: CERN-HSF
    no_of_ideas: 37
    ideas_content: |
          Precision Recovery in Lossy-Compressed Floating Point Data for High Energy Physics
          Description
          ATLAS is one of the particle physics experiments at the Large Hadron Collider (LHC) at CERN. With the planned upgrade of the LHC (the so-called High Luminosity phase), allowing for even more detailed exploration of fundamental particles and forces of nature, it is expected that the recorded data rate will be up to ten times greater than today. One of the methods of addressing this storage challenge is data compression. The traditional approach involves lossless compression algorithms such as zstd and zlib. To further reduce storage footprint, methods involving lossy compression are being investigated. One of the solutions in High Energy Physics is the reduction of floating point precision, as stored precision may be higher than detector resolution. However, when reading data back, physicists may be interested in restoring the precision of the floating point numbers. This is obviously impossible in the strict sense, as the process of removing bits is irreversible. Nevertheless, given that the data volume is high, some variables are correlated, and follow specific distributions, one may consider a machine learning approach to recover the lossy-compressed floating-point data.

          Task ideas
          Perform lossy compression of data sample from the ATLAS experiment
          Investigate ML techniques for data recovery, prediction and upscaling
          Integrate the chosen technique into HEP workflow
          Expected results
          Implementation of ML-based procedure to restore precision of lossy-compressed floating-point numbers in ATLAS data
          Evaluation of the method’s performance (decompression accuracy) and its applicability in HEP workflow
          Requirements
          C++, Python, Machine Learning
          Links
          IEEE_754
          Implementation of FloatCompressor in Athena
          Mentors
          Maciej Szymański - ANL
          Peter Van Gemmeren - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: July-September
          Corresponding Project
          ATLAS
          Participating Organizations
          ANL
          CERN

          ~~~~~~~~~~

          The rise of the machine (learning) in data compression for high energy physics and beyond
    

          Short description of the project
          The Large Hadron Collider (LHC) hosts multiple large-scale experiments, LHC experiments such as ATLAS, ALICE, LHCb, and CMS. These together produce roughly 1 Petabyte of data per second, but bandwidth and storage limitations force them to only pick the most interesting data, and discard the rest. The final data stored on disk is roughly 1 Petabyte per day [1]. Despite such steep methods of data reduction, the upgraded High Luminosity LHC in 2029 will produce 10 times more particle collisions. This is a great improvement for the potential to discover new physics, but poses a challenge both for data processing and data storage, as the resources needed in both departments are expected to be 3 and 5 times larger than the projected resources available [2][3].

          Data compression would be the go-to solution to this issue, but general data formats used for big data and the ROOT data format used at the LHC are already highly compressed, meaning that the data does not compress much under normal loss-less compression methods like zip [4]. However, since the observables in these experiments benefit from more events and higher statistics, lossy compression is a good alternative. By using lossy compression some data accuracy is lost, but the compression will allow for the storage of more data which will increase the statistical precision of the final analysis.

          BALER is a compression tool undergoing development at the particle physics division of the University of Manchester. BALER uses autoencoder and other neural networks as a type of lossy machine learning-based compression to compress multi-dimensional data and evaluate the accuracy of the dataset after compression.

          Since data storage is a problem in many fields of science and industry, BALER aims to be an open source tool that can support the compression of data formats from vastly different fields of science. For example, catalog data in astronomy and time series data in computational fluid dynamics.

          This project aims to work on the machine learning models in BALER to optimize performance for LHC data and evaluate its performance in real LHC analyses.

          Task ideas
          This internship can focus on a range of work packages, and the project can be tailored to the intern. Possible projects include:

          New auto-encoder models could be developed, better identifying correlations between data objects in a given particle physics dataset entry (event, typically containing thousands of objects and around 1MB each). New models could also improve performance on live / unseen data. These could include transformer, GNN, probabilistic and other tiypes of networks.
          Existing models could be applied on an FPGA, potentially significantly reducing latency and power consumption, opening the possibility of live compression before transmission of data on a network.
          BALER could also be integrated into standard research data storage formats and programs used by hundreds of thousands of physics researchers (ROOT).
          Finally the compression could be applied to particle physics datasets and the effect on the physics discovery sensitivity of an analysis could be assessed and compared to the possible increased sensitivity from additional data bandwidth.
          Ideas from the intern are also welcomed.

          Expected results
          An improved compression performance with documentation and figures of merit that may include:

          Plots made in matplotlib that demonstrate the performance of the new models compared to the old
          Documentation of the design choices made for the improved models
          Documented evaluation of a physics analysis on data before and after compression
          Requirements
          The candidate should have experience with the python language and a Linux environment, familiarity with AI fundamentals, and familiarity with PyTorch.

          Desirable skills include familiarity with AI fundamentals including transformers and/or graph neural networks, particle physics theory and experiments, PyTorch, FPGA programming and/or simulation.

          Links
          BALER GitHub
          BALER Paper

          Previous work:
          Thesis by Eric Wulff, Lund University
          Thesis by Erik Wallin, Lund University
          GSOC 2020 project: Medium post by Honey Gupta
          GSOC 2021 project: Zenodo entry by George Dialektakis
          ROOT
          Jupyter
          PyTorch
          Mentors
          James Smith - UManchester
          Caterina Doglioni - CERN
          Leonid Didukh
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          baler
          Participating Organizations
          baler
          UManchester
          CERN

          ~~~~~~~~~~

          Probabilistic circuit for lossless HEP data compression
          

          Short description of the project
          Neural data compression is an efficient solution for reducing the cost and computational resources of data storage in many LHC experiments. However, it suffers from the ability to precisely reconstruct compressed data, as most of the neural compression algorithms perform the decompression with the information loosage. On another hand, the lossless neural data compression schemas (VAE, IDF) have a lower compression ratio and are not fast enough for file IO. This project’s task is to overcome the disadvantages of the neural compression algorithm by using the probabilistic circuit for HEP data compression.

          Task ideas
          Implement the probabilistic circuit using the PyTorch
          Train and compress the HEP data (Higgs data, TopQuark Dataset)
          Measure the cost and quantify the optimal compression ratio of the probabilistic circuit
          Perform the benchmark, and compare the results with AE, Transformer
          Expected results
          An improved compression performance with documentation and figures of merit that may include:

          Implemented model of the probabilistic circuit
          Documentation of the benchmark and experiment of compression of the HEP data
          Requirements
          Required: Good knowledge of UNIX, Python, matplotlib, Pytorch, Julia, Pandas, ROOT.

          Links
          Previous work:

          GSOC 2021 project: Zenodo entry by George Dialektakis
          Baler – Machine Learning Based Compression of Scientific Data
          ROOT
          Jupyter
          Lossless compression with probabilistic circuits
          iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder
          Integer Discrete Flows and Lossless Compression
          Mentors
          Leonid Didukh
          Caterina Doglioni - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October (with 3 weeks mentor vacation where student will work independently with minimal guidance)
          Corresponding Project
          baler
          Participating Organizations
          CERN

          ~~~~~~~~~~
          Agent-Based Simulation of CAR-T Cell Therapy Using BioDynaMo
          Description
          Chimeric Antigen Receptor T-cell (CAR-T) therapy has revolutionized cancer treatment by harnessing the immune system to target and destroy tumor cells. While CAR-T has demonstrated success in blood cancers, its effectiveness in solid tumors remains limited due to challenges such as poor tumor infiltration, immune suppression, and T-cell exhaustion. To improve therapy outcomes, computational modeling is essential for optimizing treatment parameters, predicting failures, and testing novel interventions. However, existing models of CAR-T behavior are often overly simplistic or computationally expensive, making them impractical for large-scale simulations.

          This project aims to develop a scalable agent-based simulation of CAR-T therapy using BioDynaMo, an open-source high-performance biological simulation platform. By modeling T-cell migration, tumor engagement, and microenvironmental factors, we will investigate key treatment variables such as dosage, administration timing, and combination therapies. The simulation will allow researchers to explore how tumor microenvironment suppression (e.g., regulatory T-cells, hypoxia, immunosuppressive cytokines) affects CAR-T efficacy and what strategies such as checkpoint inhibitors or cytokine support can improve outcomes.

          The final deliverable will be a fully documented, reproducible BioDynaMo simulation, along with analysis tools for visualizing treatment dynamics. The model will provide insights into the optimal CAR-T cell dosing, tumor penetration efficiency, and factors influencing therapy resistance. This project will serve as a foundation for in silico testing of immunotherapies, reducing the need for costly and time-consuming laboratory experiments while accelerating the development of more effective cancer treatments.

          Expected plan of work:
          Phase 1: Initial Setup & Simple T-cell Dynamics
          Phase 2: Advanced CAR-T Cell Behavior & Tumor Interaction
          Phase 3: Integration of Immunosuppressive Factors & Data Visualization
          Expected deliverables
          A fully documented BioDynaMo simulation of CAR-T therapy.
          Analysis scripts for visualizing tumor reduction and CAR-T efficacy.
          Performance benchmarks comparing different treatment strategies.
          A research-style report summarizing findings.
          Requirements
          C++ (for BioDynaMo simulations)
          Agent-based modeling (understanding immune dynamics)
          Basic immunology & cancer biology (optional but helpful)
          Data visualization (Python, Matplotlib, Seaborn)
          Links
          Mapping CAR T-Cell Design Space Using Agent-Based Models
          BioDynaMo: A Modular Platform for High-Performance Agent-Based Simulation
          Computational Modeling of Chimeric Antigen Receptor (CAR) T-Cell Therapy of a Binary Model of Antigen Receptors in Breast Cancer
          Investigating Two Modes of Cancer-Associated Antigen Presentation in CAR T-Cell Therapy Using Agent-Based Modeling
          BioDynaMo: Cutting-Edge Software Helps Battle Cancer
          Mentors
          Vassil Vassilev
          Lukas Breitwieser - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          BioDynamo
          Participating Organizations
          CERN
          CompRes

          ~~~~~~~~~~

          Development of an auto-tuning tool for the CLUEstering library
          Description
          CLUE is a fast and fully parallelizable density-based clustering algorithm, optimized for high- occupancy scenarios, where the number of clusters is much larger than the average number of hits in a cluster (Rovere et al. 2020). The algorithm uses a grid spatial index for fast querying of neighbors and its timing scales linearly with the number of points within the range considered. It is currently used in the CMS and CLIC event reconstruction software for clustering calorimetric hits in two dimensions based on their energy. The CLUE algorithm has been generalized to an arbitrary number of dimensions and to a wider range of applications in CLUEstering, a general purpose clustering library, with the backend implemented in C++ and providing a Python interface for easier use. The backend can be executed on multiple backends (serial, TBB, GPUs, ecc) thanks to the Alpaka performance portability library. One feature currently lacking from CLUEstering and that would be extremely useful for every user, is an autotuning of the parameters, that given the expected number of clusters computes the combination of input parameters that results in the best clustering.
          For this task, one of the options to be explored is “The Optimizer”, a Python library developed by the Patatrack group of the CMS experiment which provides a collection of optimization algorithm, in particular MOPSO (Multi-Objective Particle Swarm Optimization).

          Expected results
          Consider the best techniques and tools for the task
          Develop an auto-tuning tool for the parameters of CLUEstering
          Test it on a wide range of commonly used datasets
          Benchmark and profile to identify the bottlenecks of the tool and optimize it
          Evaluation Task
          Interested students please contact simone.balducci@cern.ch

          Technologies
          C++, Python
          Desirable skills
          Experience with development in C++17/20
          Experience with GPU computing
          Experience with machine learning and optimization techniques
          Experience with development of Python libraries
          Links
          CLUE
          CLUEstering
          Alpaka
          Mentors
          Simone Balducci - CERN UNIBO
          Felice Pantaleo - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Patatrack
          Participating Organizations
          CERN


          ~~~~~~~~~~

          Evaluate Distribution of ML model files on CVMFS
          Description
          Particle physicists studying nature at highest energy scales at the Large Hadron Collider rely on simulations and data processing for their experiments. These workloads run on the “computing grid”, a massive globally distributed computing infrastructure. Deploying software efficiently and reliable to this grid is an important and challenging task. CVMFS is an optimised shared file system developed specifically for this purpose: it is implemented as a POSIX read-only file system in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs. In many cases, it replaces package managers and shared software areas on cluster file systems as means to distribute the software used to process experiment data.

          Task idea
          CVMFS is optimized for the distribution of software (header files, scripts and libraries), taking advantage of the repeated access pattern for its caching, and the possibility to deduplicate files present in several versions. CVMFS is capable to provide a general read-only POSIX file system view on data in external storage. A very common use case is to make conditions databases available to workloads running in distributed computing infrastructure, but various datasets have been published in CVMFS. How efficient CVMFS can be always depends on the details in these use cases - often the benefit for the users is simply in leveraging the existing server and proxy infrastructure.

          In this project proposal, we’d like to evaluate CVMFS as a means to distribute machine learning model files used in inference, for example .onnx files. The main focus will be on creating a test deployment and benchmarking the access, as well as possible coding utilities and scripts to aid in the deployment of models on CVMFS. We’d also like to contrast CVMFS to existing inference servers like KServe, and see if it could integrate as a backend storage.

          Expected results and milestones
          Familiarization with the CVMFS server infrastructure
          Familiarization with the ML model usage at CERN, Survey of different common inference model file formats.
          Test deployment of models relevant to ML4EP
          Benchmark and evaluation of inference using models served from CVMFS
          Addition of the benchmark to the CVMFS continuous benchmarking infrastructure
          Writing a best practices document for the CVMFS documentation
          Requirements
          UNIX/Linux
          Interest in scientific computing devops
          Familiarity with common ML libraries, in particular ONNX
          Links
          CVMFS
          KServe
          Mentors
          Valentin Volkl - CERN
          Lorenzo Moneta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-October
          Corresponding Project
          CernVM-FS
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Implement and improve an efficient, layered tape with prefetching capabilities
          Description
          In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Automatic differentiation is an alternative technique to Symbolic differentiation and Numerical differentiation (the method of finite differences). Clad is based on Clang which provides the necessary facilities for code transformation. The AD library can differentiate non-trivial functions, to find a partial derivative for trivial cases and has good unit test coverage.

          The most heavily used entity in AD is a stack-like data structure called a tape. For example, the first-in last-out access pattern, which naturally occurs in the storage of intermediate values for reverse mode AD, lends itself towards asynchronous storage. Asynchronous prefetching of values during the reverse pass allows checkpoints deeper in the stack to be stored furthest away in the memory hierarchy. Checkpointing provides a mechanism to parallelize segments of a function that can be executed on independent cores. Inserting checkpoints in these segments using separate tapes enables keeping the memory local and not sharing memory between cores. We will research techniques for local parallelization of the gradient reverse pass, and extend it to achieve better scalability and/or lower constant overheads on CPUs and potentially accelerators. We will evaluate techniques for efficient memory use, such as multi-level checkpointing support. Combining already developed techniques will allow executing gradient segments across different cores or in heterogeneous computing systems. These techniques must be robust and user-friendly, and minimize required application code and build system changes.

          This project aims to improve the efficiency of the clad tape and generalize it into a tool-agnostic facility that could be used outside of clad as well.

          Expected Results
          Optimize the current tape by avoiding re-allocating on resize in favor of using connected slabs of array
          Enhance existing benchmarks demonstrating the efficiency of the new tape
          Add the tape thread safety
          Implement multilayer tape being stored in memory and on disk
          [Stretch goal] Support cpu-gpu transfer of the tape
          [Stretch goal] Add infrastructure to enable checkpointing offload to the new tape
          [Stretch goal] Performance benchmarks
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes


          ~~~~~~~~~~

          Enhancing LLM Training with Clad for efficient differentiation
          Description
          This project aims to leverage Clad, an automatic differentiation (AD) plugin for Clang, to optimize large language model (LLM) training primarily in C++. Automatic differentiation is a crucial component of deep learning training, enabling efficient computation of gradients for optimization algorithms such as stochastic gradient descent (SGD). While most modern LLM frameworks rely on Python-based ecosystems, their heavy reliance on interpreted code and dynamic computation graphs can introduce performance bottlenecks. By integrating Clad into C++-based deep learning pipelines, we can enable high-performance differentiation at the compiler level, reducing computational overhead and improving memory efficiency. This will allow developers to build more optimized training workflows without sacrificing flexibility or precision.

          Beyond performance improvements, integrating Clad with LLM training in C++ opens new possibilities for deploying AI models in resource-constrained environments, such as embedded systems and HPC clusters, where minimizing memory footprint and maximizing computational efficiency are critical. Additionally, this work will bridge the gap between modern deep learning research and traditional scientific computing by providing a more robust and scalable AD solution for physics-informed machine learning models. By optimizing the differentiation process at the compiler level, this project has the potential to enhance both research and production-level AI applications, aligning with compiler-research.org’s broader goal of advancing computational techniques for scientific discovery.

          Expected Results
          Develop a simplified LLM setup in C++
          Apply Clad to compute gradients for selected layers and loss functions
          Enhance clad to support it if necessary, and prepare performance benchmarks
          Enhance the LLM complexity to cover larger projects such as llama
          Repeat bugfixing and benchmarks
          Develop tests to ensure correctness, numerical stability, and efficiency
          Document the approach, implementation details, and performance gains
          Present progress and findings at relevant meetings and conferences
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Background in LLM is preferred but not required
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes


          ~~~~~~~~~~

          Enable Clad on ONNX-based models
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. Clad is useful in powering statistical analysis and uncertainty assessment applications. ONNX (Open Neural Network Exchange) provides a standardized format for machine learning models, widely used for interoperability between frameworks like PyTorch and TensorFlow

          This project aims to integrate Clad, an automatic differentiation (AD) plugin for Clang, with ONNX-based machine learning models. Clad can generate derivative computations for C++ functions, making it useful for sensitivity analysis, optimization, and uncertainty quantification. By extending Clad’s capabilities to ONNX models, this project will enable efficient differentiation of neural network operations within an ONNX execution environment.

          Expected Results
          Enumerate ONNX modules with increasing complexity and analyze their differentiation requirements.
          Develop a structured plan for differentiating the identified ONNX operations.
          Implement forward mode differentiation for selected ONNX operations.
          Extend support to reverse mode differentiation for more complex cases.
          Create comprehensive tests to validate correctness and efficiency.
          Write clear documentation to ensure ease of use and future maintenance.
          Present results at relevant meetings and conferences.
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Basic knowledge of Clang is preferred but not mandatory
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable automatic differentiation of OpenMP programs with Clad
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. Clad is useful in powering statistical analysis and uncertainty assessment applications. OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared-memory multiprocessing programming in C, C++, and other computing platforms.

          This project aims to develop infrastructure in Clad to support the differentiation of programs that contain OpenMP primitives.

          Expected Results
          Extend the pragma handling support
          List the most commonly used OpenMP concurrency primitives and prepare a plan for how they should be handled in both forward and reverse accumulation in Clad
          Add support for concurrency primitives in Clad’s forward and reverse mode automatic differentiation.
          Add proper tests and documentation.
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Parallel Programming
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Integrate Clad to PyTorch and compare the gradient execution times
          Description
          PyTorch is a popular machine learning framework that includes its own automatic differentiation engine, while Clad is a Clang plugin for automatic differentiation that performs source-to-source transformation to generate functions capable of computing derivatives at compile time.

          This project aims to integrate Clad-generated functions into PyTorch using its C++ API and expose them to a Python workflow. The goal is to compare the execution times of gradients computed by Clad with those computed by PyTorch’s native autograd system. Special attention will be given to CUDA-enabled gradient computations, as PyTorch also offers GPU acceleration capabilities.

          Expected Results
          Incorporate Clad’s API components (such as clad::array and clad::tape) into PyTorch using its C++ API
          Pass Clad-generated derivative functions to PyTorch and expose them to Python
          Perform benchmarks comparing the execution times and performance of Clad-derived gradients versus PyTorch’s autograd
          Automate the integration process
          Document thoroughly the integration process and the benchmark results and identify potential bottlenecks in Clad’s execution
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable automatic differentiation of C++ STL concurrency primitives in Clad
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. This project focuses on enabling automatic differentiation of codes that utilise C++ concurrency features such as std::thread, std::mutex, atomic operations and more. This will allow users to fully utilize their CPU resources.

          Expected Results
          Explore C++ concurrency primitives and prepare a report detailing the associated challenges involved and the features that can be feasibly supported within the given timeframe.
          Add concurrency primitives support in Clad’s forward-mode automatic differentiation.
          Add concurrency primitives support in Clad’s reverse-mode automatic differentiation.
          Add proper tests and documentation.
          Present the work at the relevant meetings and conferences.
          An example demonstrating the use of differentiation of codes utilizing parallelization primitives:

          #include <cmath>
          #include <iostream>
          #include <mutex>
          #include <numeric>
          #include <thread>
          #include <vector>
          #include "clad/Differentiator/Differentiator.h"

          using VectorD = std::vector<double>;
          using MatrixD = std::vector<VectorD>;

          std::mutex m;

          VectorD operator*(const VectorD &l, const VectorD &r) {
            VectorD v(l.size());
            for (std::size_t i = 0; i < l.size(); ++i)
              v[i] = l[i] * r[i];
            return v;
          }

          double dot(const VectorD &v1, const VectorD &v2) {
            VectorD v = v1 * v2;
            return std::accumulate(v.begin(), v.end(), 0.0);
          }

          double activation_fn(double z) { return 1 / (1 + std::exp(-z)); }

          double compute_loss(double y, double y_estimate) {
            return -(y * std::log(y_estimate) + (1 - y) * std::log(1 - y_estimate));
          }

          void compute_and_add_loss(VectorD x, double y, const VectorD &weights, double b,
                                    double &loss) {
            double z = dot(x, weights) + b;
            double y_estimate = activation_fn(z);
            std::lock_guard<std::mutex> guard(m);
            loss += compute_loss(y, y_estimate);
          }

          /// Compute total loss associated with a single neural neural-network.
          /// y_estimate = activation_fn(dot(X[i], weights) + b)
          /// Loss of a training data point = - (y_actual * std::log(y_estimate) + (1 - y_actual) * std::log(1 - y_estimate))
          /// total loss: summation of loss for all the data points
          double compute_total_loss(const MatrixD &X, const VectorD &Y,
                                    const VectorD &weights, double b) {
            double loss = 0;
            const std::size_t num_of_threads = std::thread::hardware_concurrency();
            std::vector<std::thread> threads(num_of_threads);
            int thread_id = 0;
            for (std::size_t i = 0; i < X.size(); ++i) {
              if (threads[thread_id].joinable())
                threads[thread_id].join();
              threads[thread_id] =
                  std::thread(compute_and_add_loss, std::cref(X[i]), Y[i],
                              std::cref(weights), b, std::ref(loss));
              thread_id = (thread_id + 1) % num_of_threads;
            }
            for (std::size_t i = 0; i < num_of_threads; ++i) {
              if (threads[i].joinable())
                threads[i].join();
            }

            return loss;
          }

          int main() {
            auto loss_grad = clad::gradient(compute_total_loss);
            // Fill the values as required!
            MatrixD X;
            VectorD Y;
            VectorD weights;
            double b;

            // derivatives
            // Zero the derivative variables and make them of the same dimension as the
            // corresponding primal values.
            MatrixD d_X;
            VectorD d_Y;
            VectorD d_weights;
            double d_b = 0;

            loss_grad.execute(X, Y, weights, b, &d_X, &d_Y, &d_weights, &d_b);

            std::cout << "dLossFn/dW[2]: " << d_weights[2] << "\n"; // Partial derivative of the loss function w.r.t weight[2]
            std::cout << "dLossFn/db: " << d_b << "\n"; // Partial derivative of the loss function w.r.t b
          }
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Support usage of Thrust API in Clad
          Description
          The rise of ML has shed light into the power of GPUs and researchers are looking for ways to incorporate them in their projects as a lightweight parallelization method. Consequently, General Purpose GPU programming is becoming a very popular way to speed up execution time.

          Clad is a clang plugin for automatic differentiation that performs source-to-source transformation and produces a function capable of computing the derivatives of a given function at compile time. This project aims to enhance Clad by adding support for Thrust, a parallel algorithms library designed for GPUs and other accelerators. By supporting Thrust, Clad will be able to differentiate algorithms that rely on Thrust’s parallel computing primitives, unlocking new possibilities for GPU-based machine learning, scientific computing, and numerical optimization.

          Expected Results
          Research and decide on the most valuable Thrust functions to support in Clad
          Create pushforward and pullback functions for these Thrust functions
          Write tests that cover the additions
          Include demos of using Clad on open source code examples that call Thrust functions
          Write documentation on which Thrust functions are supported in Clad
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~
          
          Extending the User Interface
          Description
          Constellation is a framework used for lab setups or small-scale experiments in HEP. One of its most important goals is that the framework should be easy to use for both scientists implementing new devices as well as experiment operators.

          Constellation features a Qt-based User Interfaces to control and monitor all devices in the experimental setup. The focus of this GSoC project is to add new user interfaces to Constellation and extend the current ones.

          Project Milestones
          Creating a new GUI to display monitoring data from devices using Qt Charts
          Modularization of UI elements into reusable Qt widgets
          Adding the monitoring widget to the existing GUI for device control
          Requirements
          Modern C++
          Knowledge of Qt is helpful but not required
          Practical experience with Unix and git
          Links
          Repository
          Documentation
          Mentors
          Stephan Lachnit - DESY
          Simon Spannagel - DESY
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          Constellation
          Participating Organizations
          DESY

          ~~~~~~~~~~

          Implement CppInterOp API exposing memory, ownership and thread safety information
          Description
          Incremental compilation pipelines process code chunk-by-chunk by building an ever-growing translation unit. Code is then lowered into the LLVM IR and subsequently run by the LLVM JIT. Such a pipeline allows creation of efficient interpreters. The interpreter enables interactive exploration and makes the C++ language more user friendly. The incremental compilation mode is used by the interactive C++ interpreter, Cling, initially developed to enable interactive high-energy physics analysis in a C++ environment.

          Clang and LLVM provide access to C++ from other programming languages, but currently only exposes the declared public interfaces of such C++ code even when it has parsed implementation details directly. Both the high-level and the low-level program representation has enough information to capture and expose more of such details to improve language interoperability. Examples include details of memory management, ownership transfer, thread safety, externalized side-effects, etc. For example, if memory is allocated and returned, the caller needs to take ownership; if a function is pure, it can be elided; if a call provides access to a data member, it can be reduced to an address lookup. The goal of this project is to develop API for CppInterOp which are capable of extracting and exposing such information AST or from JIT-ed code and use it in cppyy (Python-C++ language bindings) as an exemplar. If time permits, extend the work to persistify this information across translation units and use it on code compiled with Clang.

          Project Milestones
          Collect and categorize possible exposed interop information kinds
          Write one or more facilities to extract necessary implementation details
          Design a language-independent interface to expose this information
          Integrate the work in clang-repl and Cling
          Implement and demonstrate its use in cppyy as an exemplar
          Present the work at the relevant meetings and conferences.
          Requirements
          C++ programming
          Python programming
          Knowledge of Clang and LLVM
          Links
          Repo
          Mentors
          Aaron Jomy - CompRes
          Vassil Vassilev - CompRes
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          CppInterOp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Incorporate a Large Language Model to assist users
          Description
          The amount of data that is processed by individual scientists has grown hugely in the past decade. It is not unusual for a user to have data processed on tens of thousands of processors with these located at tens of different locations across the globe. The Ganga user interface was created to allow for the management of such large calculations. It helps the user to prepare the calculations, submitting the tasks to a resource broker, keeping track of which parts of the task that has been completed, and putting it all together in the end.

          As a scripting and command line interface, there will naturally be users that have problems with getting the syntax correct. To solve this, they will often spend time searching through mailing lists, FAQs and discussion fora or indeed just wait for another more advanced coder to debug their problem. The idea of this project is to integrate a Large Language Model (LLM) into the command prompt in Ganga. This should allow the user to describe in words what they would like to do and get an example that they can incorporate. It should also intercept exceptions thrown by the Ganga interface, help the user to understand them and propose solutions.

          We have an interface based on ollama that will build a RAG that contains extra information about Ganga that has not been available for the training of the underlying LLM.

          Task ideas
          Integrate the interaction with the LLM and RAG into Ganga.
          Integrate past input and output in the CLI to provide context for the CLI.
          Setup a server such that the LLM can run on a remote server requiring minimal installation by the user.
          Test which samples are most useful for adding to the RAG (mailing list discussions, manuals, instant messages)
          Develop continuous integration tests that ensures that LLM integration will keep working.
          Expected results
          For the scientific users of Ganga, this will speed up their development cycle as they will get a faster response to the usage queries that they have.

          As a student, you will gain experience with the challenges of large scale computing where some tasks of a large processing chain might take several days to process, have intermittent failures and have thousands of task processing in parallel. You will get experience with how LLMs can be integrated directly into projects to assist users in the use of the CLI and in understanding error messages.

          Evaluation Task
          Interested students please contact Ulrik (see contact below) to ask questions and for an evaluation task.

          Requirements
          Python programming (advanced), Linux command line experience (intermediate), use of git for code development and continuous integration testing (intermediate)

          Links
          Ganga
          Mentors
          Alex Richards - Imperial College
          Mark Smith - Imperial College
          Ulrik Egede - Monash University
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: May-November
          Corresponding Project
          Ganga
          Participating Organizations
          ImperialCollege
          MonashUniversity

          ~~~~~~~~~~

          Implement a deprecation system to keep code up to date
          Description
          The amount of data that is processed by individual scientists has grown hugely in the past decade. It is not unusual for a user to have data processed on tens of thousands of processors with these located at tens of different locations across the globe. The Ganga user interface was created to allow for the management of such large calculations. It helps the user to prepare the calculations, submitting the tasks to a resource broker, keeping track of which parts of the task that has been completed, and putting it all together in the end.

          As code that has developed over many years, there are part of the API that has become redundant. This means that for a period of time there will be both the old and now deprecated API as well as the new way of doing things. At the moment Ganga is missing a formal way of deprecating code. This means that warnings about using something deprecated are non-uniform and there is also very old code that has never been cleaned up.

          The idea in this project is to formalise the way that code can be declared deprecated and then use the continuous integration to ensure that the code eventually is deleted.

          Task ideas
          Have a well defined way of marking plugins, functions etc as deprecated with a warning about when they will be removed. Building on top of the python package deprecated might be an idea.
          Run tests in the testing framework that will alert developers to that certain parts of the code can now be removed.
          Apply in the testing framework a similar system that will identify when deprecated python features are used when moving to a new python version.
          Apply the deprecation system to parts of the code that is already deprecated.
          Expected results
          Obtain a cleaner code base where very old and since long deprecated code is no longer present. Provide the end user with consistent warnings about their use of deprecated code as well as when it will be removed.

          As a student, you will gain experience with the challenges of large scale computing where some tasks of a large processing chain might take several days to process, have intermittent failures and have thousands of task processing in parallel. You will get experience with working within a large code base that has gone through many developments.

          Evaluation Task
          Interested students please contact Ulrik (see contact below) to ask questions and for an evaluation task.

          Requirements
          Python programming (advanced), Linux command line experience (intermediate), use of git for code development and continuous integration testing (intermediate)

          Links
          Ganga
          Mentors
          Alex Richards - Imperial College
          Mark Smith - Imperial College
          Ulrik Egede - Monash University
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: May-November
          Corresponding Project
          Ganga
          Participating Organizations
          ImperialCollege
          MonashUniversity

          ~~~~~~~~~~

          Geant4-FastSim - Data Representation Optimisation for Generative Model-based Fast Calorimeter Shower Simulation
          Description
          High energy physics experiments such as those operated at the Large Hadron Collider (LHC) fundamentally rely on detailed and realistic simulations of particle interactions with the detector. The state-of-the-art Geant4 toolkit provides a means of conducting these simulations with Monte Carlo procedures. However, the simulation of particle showers in the calorimeter systems of collider detectors with such tools is a computationally intensive task. For this reason, alternative fast simulation approaches based on generative models have received significant attention, with these models now being deployed in production by current experiments at the LHC. In order to develop the next generation of fast simulation tools, approaches are being explored that would be able to handle larger data dimensionalities stemming from the higher granularity present in future detectors, while also being efficient enough to provide a sizable simulation speed-up for low energy showers.

          A shower representation which has the potential to meet these criteria is a point cloud, which can be constructed from the position, energy and time of hits in the calorimeter. Since Geant4 provides access to the (very numerous) individual physical interactions simulated in the calorimeter, it also provides a means to create a representation independent of the physical readout geometry of the detector. This project will explore different approaches to clustering these individual simulated hits into a point cloud, seeking to minimise the number of points while preserving key calorimetric observables.

          First Steps
          Gain a basic understanding of calorimeter shower simulation (G4FastSim)
          Try simulating some electromagnetic particle showers with the Key4hep framework (see test)
          Propose different approaches to clustering, with justification
          Project Milestones
          Survey different approaches to clustering
          Implement and experiment with the different methods
          Investigate the impact of varying the detector granularity on the performance of separate clustering algorithms
          If time allows, hadronic showers could also be investigated
          Expected Results
          A comparison of different approaches to clustering, with a performance evaluation in terms of the effect on calorimetric observables.
          An evaluation of the impact of varying the granularity of the detector readout on the performance of the clustering algorithm
          Requirements
          C++, Python
          Familiarity with PyTorch could be an advantage
          Evaluation Tasks and Timeline
          Find the test here. Please submit it by 9:00 CET 17th March 2025 along with a short proposal (2 pages max) describing how you would approach the problem. See submission instructions in the test doc. Please don’t forget to start the subject line with “GSoC’25 FastSim”.
          We will make the selections based on the test, short proposal and resume by 17:00 CET 24th March.
          Selected candidates will then write the full proposal and submit it according to the official GSoC timeline.
          Links
          G4FastSim
          CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation
          Mentors
          Peter McKeown - CERN
          Piyush Raikwar - CERN
          Anna Zaborowska - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Geant4
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Using ROOT in the field of genome sequencing
          Description
          The ROOT is a framework for data processing, born at CERN, at the heart of the research on high-energy physics. Every day, thousands of physicists use ROOT applications to analyze their data or to perform simulations. The ROOT software framework is foundational for the HEP ecosystem, providing capabilities such as IO, a C++ interpreter, GUI, and math libraries. It uses object-oriented concepts and build-time modules to layer between components. We believe additional layering formalisms will benefit ROOT and its users.

          ROOT has broader scientific uses than the field of high energy physics. Several studies have shown promising applications of the ROOT I/O system in the field of genome sequencing. This project is about extending the developed capability in GeneROOT and understanding better the requirements of the field.

          Expected results
          Reproduce the results based on previous comparisons against ROOT master
          Investigate and compare the latest compression strategies used by Samtools for conversions to BAM, with RAM(ROOT Alignment Maps).
          Explore ROOT’s RNTuple format to efficiently store RAM maps, in place of the previously used TTree.
          Investigate different ROOT file splitting techniques
          Produce a comparison report
          Requirements
          C++ and Python programming
          Familiarity with Git
          Knowledge of ROOT and/or the BAM file formats is a plus.
          Links
          Latest Presentation on GeneROOT
          ROOT
          GeneROOT
          Mentors
          Martin Vasilev - Uni Plovdiv
          Jonas Rembser - CERN
          Fons Rademakers - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-November
          Corresponding Project
          ROOT
          Participating Organizations
          CERN
          CompRes

          ~~~~~~~~~~

          Highly Granular Quantization for CICADA
          Description
          The CICADA (Calorimeter Image Convolutional Anomaly Detection Algorithm) project aims to provide an unbiased detection of new physics signatures in proton-proton collisions at the Large Hadron Collider’s Compact Muon Solenoid experiment (CMS). It detects anomalies in low-level trigger calorimeter information with a convolutional autoencoder, whose behaviour is transferred to a smaller model through knowledge distillation. Careful quantization of the deployed model allows it to meet the requirement of sub-500ns inference times on FPGAs. While CICADA currently employs Quantization Aware Training with different quantization schemes for each layer of the distilled model, a new gradient-based quantization optimization approach published in 2024 offers the possibility of optimizing quantization at the individual weight level. This project would explore implementing this highly granular quantization method to CICADA’s distilled model and evaluating its effects on both model performance and resource consumption on FPGAs. The work would involve implementing the new quantization approach, comparing it with the current implementation, and investigating the impact on both detection performance and hardware resource utilization while maintaining the strict timing requirements.

          Task ideas
          Transition CICADA’s quantization tool from QKeras to HGQ
          Optimize student model’s quantization with higher granularity
          Compare resulting model’s performance with legacy model
          Emulate deployment on FPGA w/ Vivado to evaluate resource consumption
          Expected results
          Extend existing training / quantization scripts to use HGQ in addition to QKeras
          A trained student model with highly granular quantization
          Estimates of that model’s performance and resource consumption on an FPGA
          Requirements
          Python, Tensorflow, Quantization

          Links
          CICADA (homepage)
          CICADA (code)
          HGQ (Paper)
          HGQ (code)
          Mentors
          Lino Gerlach - CERN
          Isobel Ojalvo - Princeton
          Jennifer Ngadiuba - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          CICADA
          Participating Organizations
          princeton

          ~~~~~~~~~~

          Intelligent Log Analysis for the HSF Conditions Database
          Description
          The nopayloaddb project works as an implementation of the Conditions Database reference for the HSF. It provides a RESTful API for managing payloads, global tags, payload types, and associated data.

          Our current system, composed of Nginx, Django, and database (link to helm chart), lacks a centralized logging solution making it difficult to effectively monitor and troubleshoot issues. This task will address this deficiency by implementing a centralized logging system aggregating logs from multiple components, and develop a machine learning model to perform intelligent log analysis. The model will identify unusual log entries indicative of software bugs, database bottlenecks, or other performance issues, allowing us to address problems before they escalate. Additionally, by analyzing system metrics, the model will provide insights for an optimal adjustment of parameters during periods of increased request rates.

          Steps
          Set up a centralized logging system
          Collect and structure logs from Nginx, Django, and the database
          Develop an ML model for log grouping and anomaly detection
          Implement Kubernetes-based database with replication
          Train an ML model to optimize Kubernetes parameters dynamically
          Expected Results
          A centralized logging system for improved monitoring and troubleshooting
          ML-powered anomaly detection
          ML-driven dynamic configuration for optimal performance
          Requirements
          Python and basic understanding of ML frameworks
          Kubernetes, basic understanding, k8s, Helm, Operators, OpenShift
          Django and Nginx, basic understanding of web frameworks and logging
          Database knowledge, PostgreSQL, database replication
          Links
          Django REST API: https://github.com/BNLNPPS/nopayloaddb
          Automized deployment with helm-chart: https://github.com/BNLNPPS/nopayloaddb-charts
          Mentors
          Ruslan Mashinistov - BNL
          John S. De Stefano Jr. - BNL
          Michel Hernandez Villanueva - BNL
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          HSFCondDB
          Participating Organizations
          BNL

          ~~~~~~~~~~

          Interactive Differential Debugging - Intelligent Auto-Stepping and Tab-Completion
          Description
          Differential debugging is a time-consuming task that is not well supported by existing tools. Existing state-of-the-art tools do not consider a baseline(working) version while debugging regressions in complex systems, often leading to manual efforts by developers to achieve an automatable task.

          The differential debugging technique analyzes a regressed system and identifies the cause of unexpected behaviors by comparing it to a previous version of the same system. The idd tool inspects two versions of the executable – a baseline and a regressed version. The interactive debugging session runs both executables side-by-side, allowing the users to inspect and compare various internal states.

          This project aims to implement intelligent stepping (debugging) and tab completions of commands. IDD should be able to execute until a stack frame or variable diverges between the two versions of the system, then drop to the debugger. This may be achieved by introducing new IDD-specific commands. IDD should be able to tab complete the underlying GDB/LLDB commands. The contributor is also expected to set up the necessary CI infrastructure to automate the testing process of IDD.

          Expected Results
          Enable stream capture
          Enable IDD-specific commands to execute until diverging stack or variable value.
          Enable tab completion of commands.
          Set up CI infrastructure to automate testing IDD.
          Present the work at the relevant meetings and conferences.
          Requirements
          Python & C/C++ programming
          Familiarity debugging with GDB/LLDB
          Links
          IDD Repository
          Mentors
          Vipul Cariappa - CompRes
          Martin Vasilev - University of Plovdiv
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          CppInterOp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Julia Interfaces to HepMC3
          Description
          In high-energy physics experiments at CERN it is necessary to simulate physics events in order to compare predicted observations with those that the LHC experiments actually observe. A key piece of the software chain used to do that is the HepMC3 event record library, which encodes the output from physics event generators in a standard way, so that they can be used by downstream detector simulation and analysis codes.

          There is now increasing interest in using Julia as a language for HEP software, as it combines the ease of programming in interactive languages, e.g., Python, with the speed of compiled language, such as C++. As part of building up the ecosystem of supporting packages for Julia in high-energy physics, developing interfaces to read, manipulated and write HepMC3 event records in Julia is the aim of this project.

          Task ideas
          This project would develop a wrapper library for HepMC3 allowing the HepMC3 data objects and methods, in C++, to be called from Julia.

          It would utilise the general underlying wrapper interfaces in CxxWrap and the automated wrapper code generator WrapIt! to allow for as easy and maintainable an interface as possible.

          A key outcome would be a set of unit tests and examples, based on the HepMC3 ones, demonstrating how to use the library and proving that the code is correct.

          Expected results and milestones
          Reading of HepMC3 event files
          Particularly the ASCII format will be targeted first
          Access to event data structures
          Access to particle properties
          Navigation of the event and the vertices between parent and child particles
          Access to run information
          Update of HepMC3 data structures
          Creation of new HepMC3 events
          Re-serialisation of these events to file
          Initially ASCII
          Documentation and examples on how to use the Julia interfaces
          HepMC3.jl package registered in the Julia general registry
          Extension of serialisation to ROOT format (stretch goal)
          Requirements
          Programming experience in C++
          Prior experience in Julia (very advantageous)
          A background understanding of high-energy physics (advantageous)
          Evaluation Exercise
          TBD

          Links
          Julia Programming Language
          JuliaHEP HSF Group
          HepMC3 Repository
          CxxWrap
          WrapIt!
          Mentors
          Graeme Stewart - CERN
          Mateusz Fila - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-July- August
          Corresponding Project
          JuliaHEP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          MCnet/MCviz - graph and 3D-viewer tools for simulated particle collisions
          Description
          Simulations are key to particle-physics research: many modern theoretical models have such complex consequences that we test theory not by comparing measurements of particle collisions to predicted functional forms, but by generating simulated collision-events from the theory and analyzing them identically to the real ones from the particle collider.

          This means that event generators are incredibly important to particle physics, as the most-used link between experiment and theory, and as a crucial data format for exchange of ideas. They are also an excellent way to introduce new researchers and the public to particle-physics concepts. However, the toolset for MC event manipulation and visualisation is less powerful and coherent than it should be, and this project seeks to improve that situation!

          Task ideas
          This project will pick up old ideas and code for MC-event visualisation – both of the interaction graph that illustrates the internal theory computation, and the external appearance of the resulting collision decay-products – and produce a new set of tools useful both to physicists and for public outreach.

          Expected results and milestones
          Extend the mcgraph tool to be usable with both the HepMC and LHE MC-event formats.
          Refactor mcgraph into a library capable of rendering to a web browser in a server app.
          Interface the Phoenix event-viewer library to display 3D events (with and without a dummy detector model) to a web browsers.
          Display interactive particle information and jet clustering in graph and 3D view interfaces.
          Requirements
          Command-line tools
          Python
          Web technologies
          Gitlab CI
          git
          Links
          Phoenix event view library
          Old MCview web-based MC event viewer
          MC event-graph viewer
          Old MCviz event-graph viewer
          HepMC3 event format
          LHE event format
          Mentors
          Andy Buckley - CERN
          Chris Gutschow - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          MCnet
          Participating Organizations
          UofGlasgow


          ~~~~~~~~~~

          MCnet/OpenData - tools and exercises for open-data exploration with MC simulations
          Description
          CERN’s experiments are committed to publishing their data in a form that is accessible to all, both for research purposes and for education. For example, the ATLAS experiment provides Jupyter notebook exercises based on live-analysing reduced forms of the real collider data.

          But particle-physics researchers also use simulations of data as a crucial tool for testing theories and for understanding the background processes that new physics effects have to be isolated from. For this we use Monte Carlo (MC) event-generator codes, which are statistical implementations of the fundamental physics theory that sample real-looking events from the predicted particle types and kinematics. These are not yet represented in open-data exercises.

          Task ideas
          In this project we will develop new tools and exercises for extending open-data analysis resources to include MC event simulations. It will both reduce the entry barriers to outreach with open data and enable more engaging exercises with hypothetical new-physics models.

          Expected results and milestones
          Develop a library of wrapper functions to make open-data analysis more approachable for non-experts.
          Create functions and datasets for loading and analysing MC event samples through Jupyter.
          Develop a new Jupyter+Binder worksheet for outreach-oriented open-data MC analysis.
          Requirements
          Python
          Jupyter
          Binder
          Gitlab CI
          git
          Links
          ATLAS open data
          Example open-data analysis notebook
          Jupyter
          Binder
          Mentors
          Andy Buckley - CERN
          Chris Gutschow - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-October
          Corresponding Project
          MCnet
          Participating Organizations
          UofGlasgow


          ~~~~~~~~~~

          Integrating Support for Google XLS in HLS4ML
          Description
          Google XLS (Accelerated Hardware Synthesis) is an advanced open-source framework for high-level hardware design, offering flexible and efficient synthesis for FPGA and ASIC applications. By integrating XLS into HLS4ML, a framework for translating machine learning models into FPGA-friendly code, we can leverage XLS’s optimizing compiler and domain-specific language to improve resource efficiency, performance, and portability. This integration will enable seamless generation of highly optimized hardware implementations for ML models while maintaining the ease of use that HLS4ML provides.

          HLS4ML currently supports traditional HLS tools like Vivado HLS and Intel HLS, but adding XLS can bring further benefits such as better compilation times, improved hardware efficiency, and wider vendor compatibility. This project will focus on developing an interface between HLS4ML and XLS, allowing ML models to be translated into XLS IR and synthesized efficiently.

          Task Ideas
          Develop a backend in HLS4ML that translates neural network layers into XLS Intermediate Representation (IR).
          Implement the key ML operations (e.g., matrix multiplications, activations, and pooling) via XLS’s DSLX language and map them to HLS4ML operations.
          Benchmark and compare performance, resource utilization, and synthesis results against existing HLS4ML backends.
          Extend HLS4ML’s configuration options to allow selection of XLS as a backend, ensuring ease of integration.
          Expected Results
          A prototype of a backend in HLS4ML supporting XLS-based synthesis.
          Conversion scripts to map ML operations to XLS IR.
          Performance evaluation of XLS and existing HLS backends.
          Documentation and tutorials for using XLS with HLS4ML.
          Requirements
          Proficiency in Python and C++.
          Knowledge of hardware and compiler design.
          Basic familiarity with neural networks.
          Familiarity with version control systems like Git/GitHub.
          Links
          hls4ml documentation
          hls4ml Repository
          Google XLS documentation
          Google XLS repository
          Mentors
          Vladimir Loncar - CERN
          Dimitrios Danopoulos - CERN
          Additional Information
          Difficulty level (low / medium / high): medium/high
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          Optimizing Model Splitting in hls4ml for Efficient Multi-Graph Inference
          Description
          hls4ml is an open-source tool that enables the deployment of machine learning (ML) models on FPGAs using High-Level Synthesis (HLS). It automatically converts pre-trained models from popular deep learning frameworks (e.g., Keras, PyTorch, and ONNX) into optimized firmware for FPGA-based inference.

          Traditionally, the entire ML model is synthesized as a monolithic graph, which can lead to long synthesis times and complicated debugging, especially for large model topologies. Splitting the model graph at specified layers into independent subgraphs allows for parallel synthesis and step-wise optimization. However, finding the ‘optimal’ splitting points and optimizing FIFO buffers in between the subgraphs remains a challenge, especially when dealing with dynamic streaming architectures.

          This project aims to investigate optimal splitting strategies for complex ML models in hls4ml, focusing on efficient FIFO depth optimization across multi-graph designs. The goal is to develop methodologies that can be integrated into hls4ml to enable automated and optimal graph splitting for improved performance.

          Task ideas
          The contributor will start by familiarizing themselves with hls4ml and building ML models using multi-graph designs. They will implement profiling techniques (e.g., VCD logging) to measure FIFO occupancy and backpressure in order to develop a FIFO optimization strategy for multi-graph designs. They will also investigate multi-objective optimization algorithms to determine optimal splitting points based on subgraph resource usage or dataflow patterns. Finally, they will integrate these methodologies with hls4ml and run benchmarks to validate improvements in latency, resource utilization, etc.

          Expected results and milestones
          Familiarization with hls4ml: Understand the hls4ml workflow, including synthesis, and simulation.
          Research and Evaluation: Explore FIFO profiling and optimization strategies along with algorithms to partition the model graph given specific optimization objectives.
          Validation: Benchmark against monolithic implementations and compare differences in latency and resource utilization.
          Requirements
          Proficiency with computer architecture, FPGA design and simulation tools (e.g., Vivado)
          Experience with Python
          Understanding of ML concepts is beneficial.
          Familiarity with version control systems like Git/GitHub.
          Links
          hls4ml documentation
          hls4ml Repository
          Vivado Design Implementation
          Mentors
          Vladimir Loncar - CERN
          Dimitrios Danopoulos - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          RNTuple in JSROOT
          Description
          RNTuple is the next-generation data format for high-energy physics (HEP) data collected from the LHC. It is part of ROOT, a cornerstone software package for the storage, visualization and analysis of scientific data, widely used in the scientific community and particularly in HEP. ROOT is a C++ and Python framework, but it recently became available in the browsers as well through a Javascript implementation of some of its parts: JSROOT. Since RNTuple is still in the experimental phase, it currently lacks a JSROOT interface and its contents cannot be visualized in the browser, a common and desirable property of many ROOT objects. The goal of this project is filling this gap by making JSROOT able to read and display data stored inside an RNTuple.

          Task ideas
          In this project, the student will learn the internals of the RNTuple binary format and use this knowledge to implement a Javascript interface to expose RNTuple to JSROOT.

          Expected results and milestones
          Familiarize with the JSROOT framework, understanding how to integrate new components into it;
          read and implement (a subset of) the RNTuple binary format specifications, in Javascript; this will concretely mean implementing the deserialization code from a binary blob to a RNTuple object that may be used by JSROOT;
          enable the visualization of an RNTuple’s fields in the browser, leveraging the existing framework in JSROOT.
          Requirements
          Knowledge of Javascript / ES6
          Basic knowledge of “low-level” programming (primitive types binary layouts, bit-level manipulations, reinterpreting bytes as different types, …)
          Experience with git / github
          (Bonus): familiarity with any binary format
          Links
          ROOT Project homepage
          ROOT Project repository
          JSROOT homepage
          JSROOT repository
          Introduction to RNTuple
          RNTuple architecture overview
          RNTuple Binary Specification
          Mentors
          Serguei Linev - CERN
          Giacomo Parolini - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          ROOT
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Rucio WebUI Revamp
          Description
          Rucio is an open-source software framework that provides functionality to scientific collaborations to organize, manage, monitor, and access their distributed data and dataflows across heterogeneous infrastructures. Originally developed to meet the requirements of the high-energy physics experiment ATLAS, Rucio has been continuously enhanced to support diverse scientific communities. Since 2016, Rucio has orchestrated multiple exabytes of data access and data transfers globally.

          The Rucio WebUI is a Next.js application utilized by various users within collaborating communities to access, monitor, and manage their distributed data. Key features of the Rucio WebUI include:

          SDK for Streaming: Facilitates seamless data streaming from the Rucio server to page components, ensuring a responsive user interface.
          Typed in TypeScript with Generics: Strict typing ensures code integrity and enhances development efficiency.
          Accessibility and Responsiveness: Designed with accessibility and responsiveness in mind, ensuring usability across various devices.
          Testing and Stability: Extensive testing ensures robustness and reliability in all components.
          Feature Toggles: Dynamic feature toggles provide flexibility in enabling or disabling specific functionalities as needed.
          Component Library: Utilizes Storybook and TailwindCSS to enhance development speed and consistency.
          Tasks
          Upgrade to Next.js 15, React 19, TailwindCSS 4.x:
          Migrate the existing codebase to Next.js 15 to leverage the latest features and performance improvements.
          Utilize Server Side Rendering and React Query in Client Side Components to enhance data-fetching capabilities.
          Migrate tailwind.config.js to new CSS based configuration for TailwindCSS 4.x.
          Enhance User Experience for Site Administrators and Operators:
          Currently the WebUI focuses on List/Get views with the exception of allowing users to Create Rules. Add features to Create/Edit resources for site administrators and operational experts.
          Investigate legacy views in the previous Flask application and migrate them to the new WebUI.
          Redesign these views to be more user-friendly, incorporating feedback from site administrators and operators.
          Migrate Authentication to NextAuth (Auth.js):
          Transition existing x509 and user/password authentication mechanisms to NextAuth.
          Ensure compatibility with various authentication flows, including OAuth and OpenID Connect.
          Develop an RBAC system to ensure users have access only to functionalities relevant to their roles, enhancing security and usability.
          Transition to a Monorepo Structure:
          Migrate the Rucio WebUI to a monorepo structure to improve code organization and facilitate the sharing of common components across different projects.
          Requirements
          Mandatory:

          Proficiency in React.js and Next.js
          Experience with TailwindCSS
          Strong knowledge of JavaScript (ECMAScript 6) and TypeScript
          Familiarity with Python 3 and Flask
          Proficiency with Linux, Git, and Docker
          Good to Have:

          Understanding of NX Monorepos
          Experience with AGGrid Data Tables
          Experience with GitHub Actions
          Knowledge of HTTP REST APIs
          Familiarity with OpenID Connect and x509 protocols
          Expected Results
          By the end of GSoC 2025, we expect to have a revamped Rucio WebUI that:

          Is upgraded to Next.js 15 with integrated React Query.
          Utilizes both client and server-side components as per React 19’s stable features.
          Supports TailwindCSS 4.0 for a modern design system.
          Offers enhanced user experiences tailored for site administrators and operators.
          Employs NextAuth for streamlined authentication processes.
          Implements a robust RBAC system.
          Adopts a monorepo structure for improved code organization and component sharing.
          Links
          Rucio GitHub Repository
          Rucio UI Presentation
          Rucio Documentation
          Rucio System Overview Journal Article (Springer)
          Rucio Operational Experience Article (IEEE Computer Society)
          Mentors
          Mayank Sharma - University of Michigan, Ann Arbor
          Martin Barisits - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-November
          Corresponding Project
          Rucio
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Background Enrichment augmented Anomaly Detection (BEAD) for new physics searches at LHC
          

          Short description of the project
          A long-standing mystery of fundamental physics is the existence of dark matter (DM), a type of matter that has little interaction with ordinary matter but is supported by various astrophysical and cosmological observations and is six times more abundant than ordinary matter in the universe. Several Large Hadron Collider (LHC) experiments are conducting searches aimed at detecting dark matter. Unsupervised and semi-supervised learning outlier detection techniques are advantageous to these searches, for casting a wide net on a variety of possibilities for how dark matter manifests, as they impose minimal constraints from specific physics model details, but rather learn to separate characteristics of rare signals starting from the knowledge of the background they’ve been trained on. Developing innovative search techniques for probing dark matter signatures is crucial for broadening the DM search program at the LHC, and BEAD is a Python package that uses deep learning based methods for anomaly detection in HEP data for such new physics searches. BEAD has been designed with modularity in mind, to enable usage of various unsupervised latent variable models for any task.

          BEAD has five main running modes:

          Data handling: Deals with handling file types, conversions between them and pre-processing the data to feed as inputs to the DL models.

          Training: Trains a model to learn implicit representations of the background data that may come from multiple sources(/generators) to get a single, encriched latent representation of it.

          Inference: Using a model trained on an enriched background, the user can feed in samples where to detect anomalies in.

          Plotting: After running Inference, or Training, one can generate plots. These include performance plots as well as different visualizations of the learned data.

          Diagnostics: Enabling this mode allows running profilers that measure a host of metrics connected to the usage of the compute node to help optimization of the code (using CPU-GPU metrics).

          The package is under active development. The student in this project will work on the machine learning models available in BEAD, and implementing new models to perform anomaly detection, initially on simulated data.

          Task ideas
          Possible projects include:

          New auto-encoder models could be developed, better identifying correlations between data objects in a given particle physics dataset entry (containing event level and/or physics object level information). New models could also improve performance on live / unseen data. These could include transformer, GNN, probabilistic and other tiypes of networks.
          Existing models could be tested on different datasets, potentially identifying distinct latent spaces populated by the different LHC physics processes, that can enable improved anomaly detection.
          Ideas from the student working on this project are also welcome.

          Expected results
          An improved performance of selected models, with documentation and figures of merit that may include:

          Plots made in matplotlib that demonstrate the performance of the new models compared to the old
          Documentation of the design choices made for the improved models
          Documented evaluation of a physics analysis on data before and after compression
          Requirements
          Python
          Linux environment
          ML / unsupervised algorithms key concepts
          PyTorch

          Desired skills: transformers and/or graph neural networks, particle physics theory and experiments, particle physics simulations
          Links
          Paper on unsupervised ML algorithms using HEP datasets
          Review of LHC searches using unsupervised learning
          BEAD GitHub repository (WIP)
          ROOT
          Jupyter
          PyTorch
          Mentors
          Pratik Jawahar - CERN
          Sukanya Sinha - CERN
          Caterina Doglioni - Backup Mentor - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          BEAD
          Participating Organizations
          SMARTHEP
          UManchester

          ~~~~~~~~~~

          Estimating the energy cost of ML scientific software
          Description
          At a time where “energy crisis” is something that we hear daily, we can’t help but wonder whether our research software can be made more sustainable, and more efficient as a byproduct. In particular, this question arises for ML scientific software used in high-throughput scientific computing, where large datasets composed of many similar chunks are analysed with similar operations on each chunk of data. Moreover, CPU/GPU-efficient software algorithms are crucial for the real-time data selection (trigger) systems in LHC experiments, as the initial data analysis necessary to select interesting collision events is executed on a computing farm located at CERN that has finite CPU resources.

          The questions we want to start answering in this work are:

          what is the trade off between performance of a ML algorithm and its energetic efficiency?
          can small efficiency improvements in ML algorithms running on Large Hadron Collider data have a sizable energetic impact?
          how do these energy efficiency improvements vary when using different computing architectures (1) and/or job submission systems (2)?
          Task ideas
          The students in this project will use metrics from the Green Software Foundation and from other selected resources to estimate the energy efficiency of machine learning software from LHC experiments (namely, top tagging using ATLAS Open data) and from machine learning algorithms for data compression (there is another GSoC project developing this code, called Baler). This work will build on previous GSoC / Master’s thesis work, and will expand these results for GPU architectures. If time allows, the student will then have the chance to make small changes to the code to make it more efficient, and evaluate possible savings.

          Expected results and milestones
          Understand and summarise the metrics for software energy consumption, focusing on computing resources at CERN;
          Become familiar with running and debugging the selected software frameworks and algorithms;
          Set up tests and visualisation for applying metrics to the selected software
          Run tests and visualise results (preferably using a Jupyter notebook)
          Vary platforms and job submission systems
          Identify possible improvements, apply them, and run tests again
          Requirements
          Python
          git
          Jupyter notebooks
          PyTorch or equivalent ML toolkit
          Desirable: code profiling experience
          Links
          (1) Green Software Foundation course
          (2) Code by the previous GSoC student
          Mentors
          Caterina Doglioni - CERN
          Tobias Fitschen - Backup Mentor - CERN
          James Smith - Backup Mentor - University of Manchester
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October (with 2-3 weeks mentor vacation where student will work independently with minimal guidance)
          Corresponding Project
          SMARTHEP
          Participating Organizations
          UManchester
          CERN

          ~~~~~~~~~~

          Sustainable Quantum Computing algorithms for particle physics reconstruction
          Description
          Reconstructing the trajectories of charged particles as they traverse several detector layers is a key ingredient for event reconstruction at any LHC experiment. The limited bandwidth available, together with the high rate of tracks per second, makes this problem exceptionally challenging from the computational perspective. With this in mind, Quantum Computing is being explored as a new technology for future detectors, where larger datasets will further complicate this task. Furthermore, when choosing such alternative sustainability will play a crucial role and needs to be studied in detail. This project will consist in the implementation of both Quantum and Classical Machine Learning algorithms for track reconstruction, and using open-source, realistic event simulations to benchmark them from both a physics performance and an energy consumption perspective.

          First steps
          Basic understanding of track reconstruction at LHC using ACTS and/or Allen framework.
          Familiarizing her/himself with trackML simulation datasets https://www.kaggle.com/competitions/trackml-particle-identification/data?select=train_sample.zip.
          Learning how to use the quantum simulator for QML algorithms https://pennylane.ai/.
          Milestones
          Choosing a ML algorithm (or part of) in quantum computing and its classical counterpart for track reconstruction.
          Mapping of track reconstruction problem to Ising-like Hamiltonian.
          Prototype implementation of classical and quantum track reconstruction using trackML simulation inputs.
          Expected results
          Benchmarking physics output and energy consumption of the classical and quantum algorithm.
          Requirements
          CUDA, python, C++
          Evaluation Tasks and Timeline
          To be completed
          Corresponding Project
          QuantumForTracking
          Participating Organizations
          CERN

          ~~~~~~~~~~

          TMVA SOFIE - GPU Support for Machine Learning Inference
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          SOFIE currently supports various Machine Learning operators defined by the ONNX standards, as well as a Graph Neural Network (GNN) implementation. It supports the parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE continues to evolve, there’s a need to enable inference on GPUs. This project aims to explore different GPU stacks (such as CUDA, ROCm, ALPAKA) and implement GPU-based inference functionalities in SOFIE. There is already a SYCL implementation for SOFIE, developed in 2023, which can serve as a reference for future development.

          Task ideas
          In this project, the contributor will gain experience with GPU programming and its role in Machine Learning inference. They will start by understanding SOFIE and running inference on CPUs. After researching GPU stacks and methods of their integration with SOFIE, the contributor will implement GPU support for inference, ensuring the code is efficient and well-integrated with GPU technologies.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understanding the SOFIE architecture, working with its internals, and running inference on CPUs.
          Research and Evaluation: Analyzing various GPU stacks (CUDA, ROCm, ALPAKA, etc.) and determining their alignment with SOFIE.
          Implementation of GPU Inference: Developing functionalities for GPU-based inference in SOFIE.
          [Optional] Benchmarking: Evaluating the performance of the new GPU functionality by benchmarking memory usage, execution time, and comparing results with other frameworks (such as TensorFlow or PyTorch).
          Requirements
          Proficiency in C++ and Python.
          Knowledge of GPU programming (e.g., CUDA).
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          Implementation of SOFIE-SYCL
          Accelerating Machine Learning Inference on GPUs with SYCL
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          TMVA SOFIE - HLS4ML Integration for Machine Learning Inference
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          Currently, SOFIE supports various machine learning operators defined by ONNX standards, as well as a Graph Neural Network implementation. It supports parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE evolves, there is a growing need for inference capabilities on models trained across a variety of frameworks. This project will focus on integrating hls4ml in SOFIE, thereby enabling generation of C++ inference functions on models parsed by hls4ml.

          Task ideas
          In this project, the contributor will gain experience with C++ and Python programming, hls4ml, and their role in machine learning inference. The contributor will start by familiarizing themselves with SOFIE and running inference on CPUs. After researching the possibilities for integration with hls4ml, they will implement functionalities that ensure efficient inference of ML models parsed by hls4ml, which were previously trained in external frameworks like TensorFlow and PyTorch.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understanding the SOFIE architecture, working with its internals, and running inference on CPUs.
          Research and Evaluation: Exploring hls4ml, its support for Keras and PyTorch, and possible integration with SOFIE.
          Integration with hls4ml: Developing functionalities for running inference on models parsed by hls4ml.
          Requirements
          Proficiency in C++ and Python.
          Knowledge of hls4ml
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          hls4ml documentation
          hls4ml Repository
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          TMVA SOFIE - Enhancing Keras Parser and JAX/FLAX Integration
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          SOFIE currently supports various Machine Learning operators defined by the ONNX standards, as well as a Graph Neural Network (GNN) implementation. It supports the parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE continues to evolve, this project aims to:

          Enhance the Keras parser to support models trained in the latest TensorFlow v2.18.0, which introduces NumPy 2.0 compatibility.
          Integrate JAX/FLAX support, enabling SOFIE to generate C++ inference functions for models developed using JAX/FLAX.
          Task ideas
          In this project, the contributor will gain experience with C++ and Python programming, TensorFlow/Keras and its storage formats for trained machine learning models, and JAX/FLAX for accelerated machine learning. They will begin by familiarizing themselves with SOFIE and its Keras parser. After researching the changes required to support the latest TensorFlow version, they will implement functionalities to ensure the successful generation of inference code for the latest Keras models. In the next phase, they will explore the JAX/FLAX library and investigate its potential integration with SOFIE.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understand the SOFIE architecture, run inference using the existing Keras parser, and analyze the current parser’s capabilities.
          Researching latest TensorFlow/Keras: Investigate the latest TensorFlow/Keras developments and assess their alignment with SOFIE.
          Improving the Keras Parser: Implement parser enhancements to support the latest TensorFlow version and validate inference results.
          JAX/FLAX Integration: Design and develop a parsing mechanism for JAX/FLAX models, ensuring compatibility with SOFIE’s IR and further generation of inference code.
          Requirements
          Proficiency in C++ and Python.
          Knowledge of TensorFlow/Keras and JAX/FLAX.
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          Keras: The high-level API for TensorFlow
          JAX Documentation
          FLAX Documentation
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Implementing Debugging Support
          Description
          xeus-cpp is an interactive execution environment for C++ in Jupyter notebooks, built on the Clang-Repl C++ interpreter, provided by CppInterOp. While xeus-cpp enables a seamless workflow for running C++ code interactively, the lack of an integrated debugging experience remains a gap, especially when dealing with code that is dynamically compiled and executed through LLVM’s JIT(Just-In-Time) infrastructure.

          Jupyter’s debugging system follows the Debug Adapter Protocol (DAP), enabling seamless integration of debuggers into interactive kernels. Existing Jupyter kernels, such as the IPython & the xeus-python kernel, have successfully implemented debugging workflows that support breakpoints, variable inspection, and execution control, even in dynamically executed environments. These implementations address challenges such as symbol resolution and source mapping for dynamically generated code, ensuring that debugging within Jupyter remains intuitive and user-friendly.

          However, debugging C++ inside an interactive environment presents unique challenges, particularly due to Clang-Repl’s use of LLVM’s ORC JIT to compile and execute code dynamically. To integrate debugging into xeus-cpp, the project will explore existing solutions for DAP implementations like lldb_dap and debuggers like lldb that can interface with Jupyter while effectively supporting the execution model of Clang-Repl.

          Project Milestones
          Seamless debugging integration, establishing reliable interactions between xeus-cpp, a Debug Adapter Protocol (DAP) implementation, and a debugger.
          Implement a testing framework through xeus-zmq to thoroughly test the debugger. This can be inspired by an existing implementation in xeus-python.
          Present the work at the relevant meetings and conferences.
          Requirements
          C/C++
          Basic understanding of the Debug Adapter Protocol
          Basic understanding of the stack used by xeus-cpp: xeus, cppinterop, clang-repl
          Research on different DAP implementations like lldb_dap and debuggers like lldb/gdb that can be utilized for the project.
          Links
          Repo
          Debug Adaptor Protocol
          Debugging support through Jupyter:
          https://jupyterlab.readthedocs.io/en/stable/user/debugger.html
          https://jupyter-client.readthedocs.io/en/latest/messaging.html#debug-request
          Mentors
          Anutosh Bhat - QuantStack
          Johan Mabille - QuantStack
          Vipul Cariappa - CompRes
          Aaron Jomy - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Xeus-Cpp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable GPU support and Python Interoperability via a Plugin System
          Description
          Xeus-Cpp integrates Clang-Repl with the xeus protocol via CppInterOp, providing a powerful platform for C++ development within Jupyter Notebooks.

          This project aims to introduce a plugin system for magic commands (cell, line, etc.), enabling a more modular and maintainable approach to extend Xeus-Cpp. Traditionally, magic commands introduce additional code and dependencies directly into the Xeus-Cpp kernel, increasing its complexity and maintenance burden. By offloading this functionality to a dedicated plugin library, we can keep the core kernel minimal while ensuring extensibility. This approach allows new magic commands to be developed, packaged, and deployed independently—eliminating the need to rebuild and release Xeus-Cpp for each new addition. Initial groundwork has already been laid with the Xplugin library, and this project will build upon that foundation. The goal is to clearly define magic command compatibility across different platforms while ensuring seamless integration. A key objective is to reimplement existing features, such as the LLM cell magic and the in-development Python magic, as plugins. This will not only improve modularity within Xeus-Cpp but also enable these features to be used in other Jupyter kernels.

          As an extended goal, we aim to develop a new plugin for GPU execution, leveraging CUDA or OpenMP to support high-performance computing workflows within Jupyter.

          Project Milestones
          Move the currently implemented magics and reframe using xplugin
          Complete the on-going work on the Python interoperability magic
          Implement a test suite for the plugins
          Extended: To be able to execute on GPU using CUDA or OpenMP
          Optional: Extend the magics for the wasm use case (xeus-cpp-lite)
          Present the work at the relevant meetings and conferences
          Requirements
          Python
          C/C++
          GPU programming; CUDA/OpenMP
          Links
          Repo
          Related Issues:
          https://github.com/compiler-research/xeus-cpp/issues/4
          https://github.com/compiler-research/xeus-cpp/issues/140
          Mentors
          Anutosh Bhat - QuantStack
          Johan Mabille - QuantStack
          Vipul Cariappa - CompRes
          Aaron Jomy - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Xeus-Cpp
          Participating Organizations
          CompRes

          
    totalCharacters_of_ideas_content_parent: 107186
    totalwords_of_ideas_content_parent: 25550
    totalTokenCount_of_ideas_content_parent: 20259
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cern-hsf/
    idea_list_url: https://hepsoftwarefoundation.org/gsoc/2025/summary.html


  - organization_id: 17
    organization_name: CGAL Project
    no_of_ideas: 11
    ideas_content: |
          Enhancing the 2D Regularized Boolean Operation Demo
          Mentor: Efi Fogel

          Project description: The new demonstration program of the "2D Regularized Boolean Operations" package demonstrates various operations on polygons, such as, union, intersection, and Minkowski sum. It also demonstrates the application of several operations in a pipeline fashion. The demo has not been published yet; it requires a few enhancements, such as the support of Boolean operations on general polygons bounded by non-linear curves.

          Required Skills: Qt6, geometry, code development tools (e.g., git), and C++14 proficiency

          Contact: efifogel@gmail.com

          Duration: 350h

          ~~~~~~~~~~

          Tetrahedral Isotropic Remeshing Parallelization
          Mentor: Jane Tournois

          Project description:

          The goal of this project is to parallelize the code of the Tetrahedral Remeshing algorithm available in CGAL. This multi-material tetrahedral remeshing algorithm [2] is based on local and atomic operations such as edge collapse, edge split and edge flip, that could be performed in parallel to improve the performances of the code. The 3D Triangulations [3] and Tetrahedral Mesh Generation package [4] provide a framework to implement mesh operations concurrently. The same framework will be used to parallelize the remeshing algorithm, with the Intel TBB library [5].

          Resources:

          [1] CGAL Tetrahedral Remeshing package
          [2] The original publication Multi-Material Adaptive Volume Remesher
          [3] CGAL 3D Triangulations
          [4] CGAL Tetrahedral Mesh Generation package
          [5] Intel Threading Building Blocks
          Required Skills: C++17, Mesh Processing, Computational Geometry, Parallelism with TBB

          Contact: jane.tournois@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          New Mesh Subdivision Methods
          Mentor: Mael Rouxel-Labbé

          Project description:

          Subdivision methods are efficient techniques to produce smooth surfaces from polygonal meshes. Within CGAL [1], a handful of classic subdivision techniques already exist (CatmullClark subdivision, Loop subdivision DooSabin subdivision, Sqrt3 subdivision). The goal of this project is two-fold: (a) implement newer subdivision techniques -- such as Interpolatory SQRT(3) Subdivision [2], which builds upon an algorithm that is already found in CGAL -- and compare them to our existing algorithms (b) Investigate the use of these newer techniques as a preprocessing step in some of CGAL's newer remeshing techniques (such as adaptive remeshing).

          Resources:

          [1] CGAL Subdivision package
          [2] Interpolatory SQRT(3) Subdivision
          [3] Gaussian-Product Subdivision Surfaces
          [4] CGAL's upcoming adaptive remeshing algorithms
          Required Skills: C++17, Mesh Processing

          Contact: mael.rouxel.labbe@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Enhanced Dual Contouring
          Mentor: Mael Rouxel-Labbé, Pierre Alliez

          Project description:

          A previous GSoC launched the process of adding classic contouring methods to CGAL: Marching Cubes and Dual Contouring. This package is about to be finalized and will be integrated soon into CGAL (https://github.com/CGAL/cgal/pull/6849). Many enhancements exist for the Dual Contouring method to improve its robustness: placement of the dual point, improved conditioning of the SVD matrices, or on-the-fly refinement of the underlying grid [1]. Another aspect is speed, as a grid structure is well adapted to GPU computation.

          The project will first focus on manifold contouring methods and robustness in standard C++. If there is time and the candidate has the required skills, we can also explore runtime aspects and the conversion to a GPU implementation. If there is time and the candidate does not have the required skills, we shall explore the implementation of other contouring methods such as Dual Marching Cubes [2].

          Resources:

          [1] Manifold Dual Contouring
          [2] Dual Marching Cubes
          Feature-Sensitive Subdivision and Isosurface Reconstruction
          Required Skills: C++17, Dual Contouring, linear algebra / quadric error metrics, possibly GPU algorithms

          Contact: mael.rouxel.labbe@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Topological Filtering of Features in Triangle Meshes
          Mentor: Sebastien Loriot

          Project description:

          Remeshing algorithms in CGAL requires the proper extraction of sharp features so that they can be represented in the output mesh (like here for example). Classical method to detect sharp features are based on collecting edges with sharp dihedral surface angles. However, depending on the quality of the input mesh, some noisy edges might be detected, or some edges might be detected. To workaround these issues, it might be interesting to rely on tools from Topological Data Analysis, like for example persistence. Indeed, extra data or missing data are all related to a notion of scale at which the problem is looked at. The goal of this project is to implement such a strategy for provide curated feature edge graph to the meshing algorithm of CGAL. If time allows, extension to detection of significant handles might also be looked at.

          Resources:

          A Practical Solver for Scalar Data Topological Simplification
          To cut or to fill: a global optimization approach to topological simplification
          Topological Simplification of Nested Shapes
          Gudhi library
          Required Skills: C++17, Mesh Processing, Topological Data Analysis

          Contact: sebastien.loriot@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Improving ARAP in CGAL
          Mentor: Andreas Fabri

          Project description:

          As-Rigid-As-Possible (ARAP) surface modeling is one of the most well known approach for deformation of surfaces. It has been implemented in CGAL, within the Surface Mesh Deformation package (https://doc.cgal.org/latest/Surface_mesh_deformation/index.html#Chapter_SurfaceMeshDeformation). Since the original paper (Sorkine & Alexa, 2007 - As-Rigid-As-Possible Surface Modeling), which is implemented in CGAL, a number of improvements have been proposed. The goal of this project is to investigate these improvements, and enhance the CGAL implementation. Another direction of interest is the extension of the ARAP formulation to the setting of volume deformation of tetrahedral meshes.

          Resources:

          As-Rigid-As-Possible Surface Modeling
          ARAP Revisited Discretizing the Elastic Energy using Intrinsic Voronoi Cells
          Higher Order Continuity for Smooth As-Rigid-As-Possible Shape Modeling
          Required Skills: C++17, linear algebra

          Contact: andreas.fabri@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Extending 2D Arrangement Drawings
          Mentor: Efi Fogel

          Project description: The "2D Arrangement" package partially supports limited drawing of a 2D arrangements. The goal of this project is extend the capabilities of 2D arrangement drawing. In particular:

          The drawing is limited. An instance of the the Arrangement_2<Traits,Dcel> template can be used to represent 2D arrangements on the plane. The 2D Arrangement package supports ten traits classes that can substitute the Traits parameter. A traits class determines the family of curves that induce the arrangement, e.g., Bezier curves. Currently, arrangement induced by curves of several families cannot be drawn.
          The drawing is inefficient and should be optimized.
          The drawing of arrangements induced by geodesic arcs on the sphere in 3D is deficient. Currently only the curves are drawn (and the faces are not). The Earth demo exhibit some drawing of such arrangements, but it applies a trick that restricts the drawing to faces that do not cross the equator of the sphere. Addressing this item requires knowledge and experience in 3D graphics.
          Required Skills: Qt6 and 3D graphics, geometry, code development tools (e.g., git), and C++17 proficiency

          Contact: efifogel@gmail.com

          Duration: 350h

          ~~~~~~~~~~

          Hexahedral mesh generation
          Mentor: Guillaume Damiand

          Project description:

          The goal of this project is to implement the method of the paper [1] "A template-based approach for parallel hexahedral two-refinement", Steven J. Owen, Ryan M. Shih, Corey D. Ernst; in CGAL. This method allows to generate a locally refined hexahedral mesh, starting from a coarse grid, and using different templates for refinement. It will be implemented using a 3D linear cell complex [2] as underlying data-structure. To implement the different templates, we can use the volumic Query-replace operation [3]. The project was started last year and a preliminary version of the method already exists. The goal of this project is to finish this development, and to propose an integration in CGAL. To do so, the work to do is: (1) finish the sequential version, adding displacement of new vertices in order to obtain smooth meshes; (2) validate results on many different input meshes; (3) write the doc and the examples; (4) finish the parallel version.

          Resources:

          [1] The paper to be implemented: "A template-based approach for parallel hexahedral two-refinement"
          [2] CGAL Linear cell complex package
          [3] Query-replace operations for topologically controlled 3D mesh editing and the Gitlab repository
          Required Skills: C++17, Geometry Processing, Mesh Processing, Computational Geometry

          Contact: guillaume.damiand@cnrs.fr

          Duration: 175h

          ~~~~~~~~~~

          Cut by plane a volumetric mesh
          Mentor: Guillaume Damiand and Sebastien Loriot

          Project description:

          The goal of this project is to implement a method allowing to cut a 3D volumetric mesh (represented by a 3D linear cell complex) by a plane. There are some code available for the two first steps of the method (insert vertices on the cut edges, and insert edges between the new vertices to split faces); it remains the last step which consists in inserting new faces along path of edges. The method must be robust, i.e. deal with any configuration of volumetric mesh. To do so, the work to do is: (1) implement the 3 steps in CGAL; (2) validate results on many different input meshes; (3) write the doc and the examples.

          Required Skills: C++17, Geometry Processing, Mesh Processing, Computational Geometry

          Contact: guillaume.damiand@cnrs.fr sloriot.ml@gmail.com

          Duration: 175h

          ~~~~~~~~~~

          Improvement of Named Parameters
          Mentor: Sebastien Loriot and Laurent Rineau

          Project description:

          The goal of this project is to continue the work started in the pull-request https://github.com/CGAL/cgal/pull/7966. This change proposal implements a mechanism that allows the user to check at compile time that the options passed are used by the function (currently a flow of our mechanism). The proof of concept is there, but now we need to apply it globally in CGAL to all the functions using named parameters. As the function are documented, one way to tackle this project is to write a (python?) script that will collect for all the function the expected named parameters and add the macro calls in the function. There are also other improvements that can be implemented during this project if time allows (automatic extraction of a subset of options, more friendly developer interface, ...)

          Required Skills: C++17, Scritping Language such as Python, with knowledge in parsing

          Contact: sloriot.ml@gmail.com

          Duration: 175h

          ~~~~~~~~~~

          Adding Support for New File Formats for Meshes
          Mentor: Sebastien Loriot and Mael Rouxel-Labbé

          Project description:

          The CGAL library provides several functions to read and write meshes (surface and volume) in the Stream Support package. The list of currently supported file format is available here. The goal of this project is to add support to more file formats. We could for example add support for glTF, gmsh format, 3mf v2, ... The duration of the project will depend on the file format proposed for addition.

          Required Skills: C++17

          Contact: sloriot.ml@gmail.com

          Duration: 90h, 175h, or 350h

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://github.com/CGAL/cgal/wiki/Project-Ideas
    idea_list_url: https://github.com/CGAL/cgal/wiki/Project-Ideas


  - organization_id: 18
    organization_name: CHAOSS
    no_of_ideas: 
    ideas_content: |
      Idea: Enhance Conversational Topic Modelling Capabilities in CHAOSS Software
      Hours: 350

      Micro-tasks and place for questions

      This project will add GenSIM logic, and other capabilities to the Clustering Worker inside of Augur Software, and be extended into a generalized Open Source Software Conversational Topic Modeling Instrument.

      CHOASS/augur has several workers that store machine learning information derived from computational linguistic analysis of data in the message table. The message table includes messages from issue, pull request, pull request review, and email messages. They are related to their origin with bridge tables like pull_request_message_ref. The ML/CL workers are all run against all the messages, regardless of origin.

      Clustering Worker (clusters created and topics modeled)
      message analysis worker (sentiment and novelty analysis)
      discourse analysis worker (speech act classification (question, answer, approval, etc.)
      Clustering Worker Notes:

      Clustering Worker: 2 Models.

      Models:
      Topic modeling, but it needs a better way of estimating number of topics.
      Tables - repo_topic - topic_words
      Computational linguistic clustering
      Tables - repo_cluster_messages
      Key Needs
      Add GenSim algorithms to topic modeling section #1199
      The topics, and associated topic words need to be persisted after each run. At the moment, the topic words get overwritten for each topic modeling run.
      Description/optimization of the parameters used to create the computational linguistic clusters.
      Periodic deletion of models (heuristic: If 3 months pass, OR there’s a 10% increase in the messages, issues, or PRs in a repo, rebuild the models)
      Establish some kind of model archiving with appropriate metadata (lower priority)
      Discourse Analysis Worker Notes:

      discourse_insights table (select max(data_collection_date) for each msg_id)

      sequence is reassembled from the timestamp in the message table (look at msg_timestamp)
      issues_msg_ref, pull_request_message_ref, pull_request_review_msg_ref
      Message Analysis Worker

      message_analysis
      message_analysis_summary
      augur-tech

      The aims of the project are as follows:

      Advance topic modeling of open source software conversations captured in GitHub.
      Integrate this information into clearer, more parsimonious CHAOSS metrics.
      Automate the management machine learning insights, and topic models over time.
      (Stretch Goal) Improve the operation of the overall machine learning insights pipeline in CHAOSS/augur, and generalize these capabilities.
      
      ~~~~~~~~~~
      IDEA: Implement Conversion Rate Metric in CHAOSS Software
      Hours: 350

      Micro-tasks and place for questions

      Conversion Rate
      Question: What are the rates at which new contributors become more sustained contributors?

      Description
      The conversion rate metric is primarily aimed at identifying how new community members become more sustained contributors over time. However, the conversion rate metric can also help understand the changing roles of contributors, how a community is growing or declining, and paths to maintainership within an open source community.

      Objectives (why)
      Observe if new members are becoming more involved with an open source project
      Observe if new members are taking on leadership roles within an open source project
      Observe if outreach efforts are generating new contributors to an open source project
      Observe if outreach efforts are impacting roles of existing community members
      Observe if community conflict results in changing roles within an open source community
      Identify casual, regular, and core contributors
      Implementation
      This project could be implemented using either the CHAOSS/Augur, or CHAOSS/Grimoirelab (including stack components noted in references) technology stacks.

      The aims of the project are as follows:

      Implement the Conversion Rate Metric in CHAOSS Software
      After discussion, consider which CHAOSS Software Stack you wish to work with
      In collaboration with mentors, define the technology framework, and initial path to a "hello world" version of the metric
      Iterative development of the metric
      Assist in the deployment of this metric for a pre-determined collection of repositories in a publicly viewable website linked to the CHAOSS project.
      Advance the work of the chaoss metrics models working group.
      Difficulty: Medium
      Requirements: Knowledge of Python is desired. Some knowledge of Javascript or twitter/bootstrap is also desired. Key requirement is a keenness to dig into this challenge!
      Recommended: Python experience.
      Mentors: Sean Goggins
      Filters (optional)
      Commits
      Issue creation
      Issue comments
      Change request creation
      Change request comments
      Merged change requests
      Code Reviews
      Code Review Comments
      Reactions (emoji)
      Chat platform messages
      Maillist messages
      Meetup attendance
      Visualizations


      Source: https://chaoss.github.io/grimoirelab-sigils/assets/images/screenshots/sigils/overall-community-structure.png



      Source: https://opensource.com/sites/default/files/uploads/2021-09-15-developer-level-02.png

      Tools Providing the Metric
      Augur
      openEuler Infra
      Data Collection Strategies
      The following is an example from the openEuler community:

      A group of people who attended an offline event A held by the community, can be identified as Group A. Demographic information of Group A could be fetched from an on-line survey when people register for the event. To identify the conversation rate of these participants:
      Some people from Group A started watching and forking the repos, indicating they have shown some interest in this community. We marked them as subgroup D0 (Developer Level 0) as a subset of Group A.
      Conversion rate from the total number of people in Group A to the number of people in subgroup D0 is: D0/Group A
      Some people from subgroup D0 make more contributions beyond just watching or forking, including creating issues, making comments on an issue, or performed a code review. We marked them as subgroup D1 (Developer Level 1) as a subset of D0.
      Conversion rate from the total number of people in Subgroup D0 to the number of people in subgroup D1 is: D1/D0.
      Some people from subgroup D1 continue to make more contributions, like code contributions, to the project. This could include creating merge requests and merging new project code. We marked them as subgroup D2 (Developer Level 2) as a subset of D1.
      Conversion rate from the total number of people in subgroup D1 to the number of people in subgroup D2 is: D2/D1.


      Definition:

      Developer Level 0 (D0) example: Contributors who have given the project a star, or are watching or have forked the repository
      Developer Level 1 (D1): Contributors who have created issues, made comments on an issue, or performed a code review
      Developer Level 2 (D2): Contributors who have created a merge request and successfully merged code
      Conversion Rate (Group A -> D0): CR (Group A -> D2) = D0/Group A
      Conversion Rate (D0 -> D1): CR (D0 -> D1) = D1/D0
      Conversion Rate (D1 -> D2): CR (D1 -> D2) = D2/D1
      References
      https://opensource.com/article/21/11/data-open-source-contributors
      https://github.com/chaoss/augur
      https://gitee.com/openeuler/website-v2/blob/master/web-ui/docs/en/blog/zhongjun/2021-09-15-developer-level.md
      https://chaoss.github.io/grimoirelab-sigils/common/onion_analysis/
      https://mikemcquaid.com/2018/08/14/the-open-source-contributor-funnel-why-people-dont-contribute-to-your-open-source-project/
      Contributors
      Sean Goggins
      Andrew Brain
      John McGinness

      ~~~~~~~~~~
      IDEA: Open Source Software Health Metrics Visualization Exploration
      Hours: 350

      Micro-tasks and place for questions

      The CHAOSS Community currently delivers pre-packaged visualizations of open source software health data through Augur APIs (https://github.com/chaoss/augur/blob/main/augur/routes/pull_request_reports.py and https://github.com/chaoss/augur/blob/main/augur/routes/contributor_reports.py), and the https://github.com/chaoss/augur-community-reports repository. This project seeks to expand, refine, and standardize the visualization of different classes of community health metrics data. Specifically, some analyses are temporal, others are anomaly driven, and in some cases contrasts across repositories and communities are required. In each case, the visualization of data is an essential component for metrics, and what we are now referring to as metrics models (https://github.com/chaoss/wg-metrics-models).

      Additional resources include: http://new.augurlabs.io/ && https://github.com/augurlabs/augur_view which demonsrate the updated twitter/bootstrap Augur frontend.

      The aims of the project are as follows:

      Experiment with standard metrics visualizations using direct Augur database connections, or through the Augur API.
      Refine metrics, and metrics model visualizations using Jupyter Notebooks are similar technology.
      Transform visualizations, as they are completed, into Augur API endpoints, following the pull request, and contributor reports examples.
      Difficulty: Medium
      Requirements: Strong interest in data visualization.
      Recommended: Experience with Python is desirable, and experience designing, or developing visualizations is desirable.
      Mentors: Isaac Milarsky, Andrew Brain

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/chaoss/
    idea_list_url: https://github.com/chaoss/augur/blob/main/gsoc-ideas.md


  - organization_id: 19
    organization_name: CNCF
    no_of_ideas: 22
    ideas_content: |
        etcd
        etcd cache
        Description: Develop a generic, high-performance caching library for etcd, inspired by the Kubernetes watch cache, to facilitate building scalable and efficient etcd-based applications.
        Expected Outcome:
        A well-tested and performant library providing core caching primitives similar to Kubernetes' watch cache, significantly reducing etcd load and latency for generic etcd use cases.
        The library will offer feature parity with Kubernetes watch cache, including support for:
        Caching watch events and demultiplexing requests.
        Caching non-consistent list requests using a B-tree structure, updated via watch events.
        Handling requests during cache initialization and re-initialization.
        Custom encoders/decoders for data serialization.
        Custom indexing for optimized lookups.
        Consistent reads.
        Exact stale reads via B-tree snapshots.
        Comprehensive documentation, examples, benchmarks, and metrics to enable easy adoption and monitoring. This includes e2e and robustness tests.
        Recommended Skills: Go, Distributes Systems
        Expected project size: small
        Mentor(s):
        Marek Siarkowicz (@serathius, siarkowicz@google.com) - primary
        Madhav Jivrajani (@MadhavJivrajani, madhav.jiv@gmail.com)
        Upstream Issue (URL): etcd-io/etcd#19371

        ~~~~~~~~~~
        Harbor
        Enhance Harbor Satellite for Artifact Replication from Remote Registry to Edge
        Description: The Harbor Satellite project aims to enable decentralized artifact replication in edge computing environments. This project currently focuses on Use Case #1, where Harbor Satellite will pull images from a central Harbor registry and store them in a local OCI-compliant registry for use by edge devices. The solution is designed for environments with limited or intermittent internet connectivity, ensuring continuous access to required artifacts by local edge devices even when connectivity is unavailable.
        Expected Outcome:
        Enhance Harbor Satellite to enable reliable artifact replication from a central Harbor registry to a local OCI-compliant registry at the edge.
        Implement secure synchronization between central and local registries, especially in air-gapped environments.
        Optimize configuration management for edge container runtimes to pull images from the local registry.
        Provide clear documentation and setup guides for deploying Harbor Satellite in edge environments.
        Recommended Skills: Go, OCI-Registries, Distribution-spec
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Vadim Bauer (@vad1mo, vb@container-registry.com) - primary
        Orlin Vasilev (@OrlinVasilev, orlin@orlix.org)
        Prasanth Baskar (@bupd, bupdprasanth@gmail.com)
        Upstream Issue (URL): goharbor/harbor#21605

        ~~~~~~~~~~
        Jaeger
        Service performance analysis on top of Elasticsearch / OpenSearch data
        Description: Jaeger is an open-source, distributed tracing platform designed to monitor and troubleshoot transactions in distributed systems. In its basic deployment it allows collecting tracing data, storing it in a database, and querying & analyzing individual traces in the UI. This workflow is great for deep-diving into individual requests, but it does not answer some higher level questions like "which endpoints in my service are the slowest?" To address those questions Jaeger has a special feature called SPM (Service Performance Management), which allows the user to see the trends of services' and endpoints' performance and to drill down into the outliers. However, this feature requires a more complicated deployment where a special real-time processor is running and extracting metrics from the traces and storing those metrics in a Prometheus-compatible remote storage. Some of the storage backends supported by Jaeger, such as Elasticsearch & OpenSearch, can provide the same aggregate answers directly from the trace data, which can significantly simplify the deployment. This project aims to enable this integration.
        Expected Outcome:
        Support SPM functionality directly in Elasticsearch / OpenSearch backends by implementing the metrics query API
        Enhance existing e2e integration tests to continuously test this new capability
        Recommended Skills: Go, basic familiarity with Elasticsearch
        Expected project size: large (~350 hour projects)
        Mentors:
        Yuri Shkuro (@yurishkuro, github@ysh.us) - primary
        Jonah Kowall (@jkowall, jkowall@kowall.net)
        Upstream Issue (URL): jaegertracing/jaeger#6641

        ~~~~~~~~~~
        KCL
        KCL OCI third-party dependency management enhancement
        Description: KCL is an open-source constraint-based record & functional language mainly used in configuration and policy scenarios. KPM is a package management tool for the KCL language that supports the management of KCL packages in the OCI registry and Git Repo. This topic only applies to third-party dependencies from the OCI registry. Use the layering mechanism in OCI to help KPM implement dependency management of KCL third-party dependencies.

        Expected Outcome:

        Refactor the current KPM dependency management module with the OCI's layered mechanism.
        Recommended Skills: Go, OCI

        Expected project size: medium (~175 hour projects)

        Mentor(s):

        Zhe Zong (@zong-zhe, zongzhe1024@163.com)
        Heipa (@He1pa, he1pa404@gmail.com)
        Upstream Issue (URL): kcl-lang/kpm#598

        ~~~~~~~~~~
        Knative Functions
        Dynamic AI Agent Callbacks
        Description: Knative Functions is well-suited for AI agent integration. The serverless nature and isolated runtime environment of Functions make them ideal for creating lightweight, purpose-built services that can be dynamically invoked and even created by agents.
        Expected Outcome: This project would be a combination of research and practicum. First, an analysis of current AI agent interaction patterns, including emergent protocols and available frameworks. Second, the development of a Proof-of-concept integration between Functions and AI agents. This would involve at a minimum invocation, with a stretch goal of implementation and deployment by the agent based on a human prompt.
        Recommended Skills:
        Strong language and communication skills, with the ability to both research deeply and communicate clearly.
        Experience with AI/ML agents and desire to learn about programmatic LLM integrations.
        Familiarity with the Go programming language (ideal) or Python (secondarily), and web services.
        Familiarity with kubernetes, serveless, and microservices a plus.
        Expected project size: Large
        Mentor(s):
        Luke Kingland @lkingland (kingland AT redhat DOT com) - primary
        Aleksander Slominski @aslom (aslomins AT redhat DOT com)
        Upstream Issue (URL): knative/func#2690
        
        ~~~~~~~~~~
        Konveyor
        Extend usage of Konveyor AI to detect and update deprecated Kubernetes API usage in golang applications
        Description: Konveyor is an application modernization platform that helps organizations migrate legacy applications to Kubernetes at scale. As part of this effort, you will contribute to Konveyor AI (Kai), an intelligent code assistant that automates source code updates using data from static code analysis and changelog histories. Your work will focus on applying Generative AI techniques to detect and update deprecated Kubernetes APIs in Golang applications. You’ll build a tool that uses a LLM to generate Konveyor static code analysis rules from published documentation such as the Kubernetes deprecated API guide. Additionally, you’ll create workflows to identify deprecated API usage in legacy applications and automate code suggestions for updates — all powered by Konveyor AI.

        Expected Outcome:

        Develop a prototype tool to convert Kubernetes API deprecation documentation into static code analysis rules.
        Collaborate with the Konveyor AI team to extend support for Golang applications, identify issues, and contribute improvements.
        Demonstrate Konveyor AI’s ability to detect and suggest fixes for deprecated API usage in Golang projects.
        Recommended Skills: Golang, Python, Kubernetes, Generative AI

        Expected project size: # Large (~350 hours)

        Mentor(s):

        John Matthews (@jwmatthews, jwmatthews@gmail.com) - primary
        Savitha Raghunathan (@savitharaghunathan, saveetha13@gmail.com)
        Upstream Issue (URL): konveyor/kai#644

        ~~~~~~~~~~
        KubeArmor
        Improve KubeArmor Observability Spectrum
        Description: KubeArmor is a security enforcement system that provides runtime protection for Kubernetes workloads. To enhance observability, this task involves exposing key Prometheus metrics related to KubeArmor’s policy enforcement. These metrics will provide insights into security policy activity and alerting within a Kubernetes cluster.

        For starters, the following metrics can be started with:

        Number of Policies Applied
        Number of Alerts Triggered
        List of Active Policies
        Policy Status (Active/Inactive)
        Expected Outcome: Prometheus metrics are successfully integrated into KubeArmor, allowing users to monitor policy enforcement and security events effectively. The metrics should be accessible via a Prometheus endpoint and conform to best practices for Prometheus metric exposition.

        Recommended Skills: Go, Prometheus, Kubernetes.

        Expected project size: 175 hrs

        Mentor(s):

        Rishabh Soni (@rootxrishabh, risrock02@gmail.com)
        Prateek Nandle (@Prateeknandle, prateeknandle@gmail.com)
        Barun Acharya (@daemon1024, barun1024@gmail.com)
        Nishant Singh (@tesla59, talktonishantsingh.ns@gmail.com)
        Upstream Issue (URL): kubearmor/KubeArmor#1902

        ~~~~~~~~~~
        KubeBuilder
        Automating Operator Maintenance: Driving Better Results with Less Overhead
        Description:
        Code-generation tools like Kubebuilder and Operator-SDK have transformed cloud-native application development by providing scalable, community-driven frameworks. These tools simplify complexity, accelerate results, and enable developers to create tailored solutions while avoiding common pitfalls, laying a strong foundation for innovation.
        However, as these tools evolve with ecosystem changes and new features, projects risk becoming outdated. Manual updates are time-consuming, error-prone, and make it challenging to maintain security, adopt advancements, and stay aligned with modern standards.
        This project introduces an automated solution for Kubebuilder, with potential applications for similar tools or those built on its foundation. By streamlining maintenance, projects remain aligned with modern standards, improve security, and adopt the latest advancements. It fosters growth and innovation across the ecosystem, letting developers focus on what matters most: building great solutions.
        Note that the initial idea is to solve this with 3-way Git merges. However, users will face conflicts, and in the first phase, we want to study whether AI could help resolve these conflicts in a future phase to achieve this goal.

        Expected Outcome

        Conduct research on 3-Way Merge & Advanced Merge Options in Git.
        Conduct research on how AI could help resolve conflicts. If open-source solutions are available and align with the proposal, include them for consideration in a second phase.
        Develop a Proof of Concept (POC) implementing a GitHub Action that automatically creates a Pull Request (PR) in a mock repository, demonstrating the feasibility of the proposed solution.
        Successfully complete the proposal for PEP.
        Introduce a new Kubebuilder Plugin that scaffolds the GitHub Action based on the POC. This plugin will be released as an alpha feature, allowing users to opt-in for automated updates. The initial solution does not need to have AI, but AI integration could be a future enhancement if feasible.
        Recommended Skills

        Golang
        GitHub Actions
        Software Automation
        CI/CD
        Git
        IA
        Expected project size: Large (~350 hour projects)

        Mentor(s)

        Camila Macedo (@camilamacedo86, camilamacedo86@gmail.com) - Primary
        Varsha Prasad (@varshaprasad96, varshaprasad1507@gmail.com)
        TianYi(Tony) (@Kavinjsir)
        Upstream Issue: WIP - Proposal: Automating Operator Maintenance: Driving Better Results with Less Overhead
        ~~~~~~~~~~
        KubeStellar
        AI/ML Model Monitoring and Drift Detection in Disconnected Clusters using KubeStellar
        Description: AI/ML models deployed in disconnected environments, such as edge clusters and air-gapped systems, often suffer from model drift—a degradation in model performance due to changes in input data distributions. Without continuous monitoring, models may become inaccurate, leading to unreliable predictions.

        This project aims to integrate model monitoring and drift detection into KubeStellar, enabling Kubernetes-based AI workloads to detect data drift locally and sync monitoring metrics when connectivity is restored. The solution will use lightweight monitoring agents deployed alongside ML models to track data distribution changes and alert mechanisms to trigger model retraining when necessary.

        The system will also include policies for efficient metric storage and synchronization between disconnected and central clusters while minimizing bandwidth usage.

        Expected Outcome:

        A KubeStellar-compatible AI/ML monitoring component that tracks model drift in disconnected clusters.
        Efficient local storage and synchronization of monitoring metrics when connectivity is restored.
        Policies for adaptive model retraining triggers based on drift detection signals.
        Integration with existing ML tools (e.g., Prometheus, TensorFlow Extended, OpenTelemetry).
        Open-source documentation and example workflows demonstrating how KubeStellar manages AI model monitoring across disconnected clusters.
        Recommended Skills:

        Kubernetes and container orchestration
        AI/ML model deployment & monitoring
        Python, Go (for Kubernetes integrations)
        Experience with logging/monitoring tools (Prometheus, OpenTelemetry)
        Familiarity with KubeStellar (preferred but not required)
        Expected Project Size: Large (~350 hours) This project requires implementing multiple components: local monitoring, drift detection, synchronization, and integration with KubeStellar. It also involves research into efficient data synchronization strategies for low-bandwidth environments.

        Mentor(s):

        Andy Anderson (@clubanderson, andy@clubanderson.com) - Primary Mentor
        [Second Mentor's Name] (@second-mentor-github, second-mentor-email)
        Upstream Issue (URL): kubestellar/kubestellar#2791

        ~~~~~~~~~~

        Kubewarden
        Allow policies to be written using JavaScript
        Description: Kubewarden is a Policy Engine powered by WebAssembly policies that enforces security and compliance in Kubernetes clusters. Its policies can be written in CEL, Rego (OPA & Gatekeeper flavours), Rust, Go, YAML, and others.

        Kubewarden does not have a JavaScript SDK yet. Recent work done inside of the Bytecode Alliance made possible to compile Javascript code into WebAssembly . This means It's now possible to create such a SDK. This task consists on writing a JavaScript SDK that provides an idiomatic way to write Kubewarden policies.

        Expected Outcome: A new JavaScript SDK is created. The SDK API is documented, and the policy tutorial as well.

        Recommended Skills: JavaScript, Kubernetes.

        Expected project size: Large

        Mentor(s):

        Victor Cuadrado (@viccuad, vcuadradojuan@suse.com) - primary
        Flavio Castelli (@flavio, fcastelli@suse.com)
        José Guilherme Vanz (@jvanz, jguilhermevanz@suse.com)
        Fabrizio Sestito (@fabriziosestito, fabrizio.sestito@suse.com)
        Upstream Issue (URL): kubewarden/community#37

        ~~~~~~~~~~

        Elevate our .NET SDK into a first class citizen
        Description: Kubewarden is a Policy Engine powered by WebAssembly policies that enforces security and compliance in Kubernetes clusters. Its policies can be written in CEL, Rego (OPA & Gatekeeper flavours), Rust, Go, YAML, and others.

        Kubewarden has a .NET SDK that allows policy authors to write policies in C#. Starting with .NET 8, a big chunk of the work from https://github.com/dotnet/dotnet-wasi-sdk made its way upstream. This means it's a good time to revisit Kubewarden's .NET SDK for policies. This task consists on bringing our .NET SDK up to standard with the rest of our SDKs such as the Go or Rust ones.

        Expected Outcome: Our .NET SDK has been ported to .NET 9, and supports the same capabilities as our other SDKs. The SDK API is documented, and the policy tutorial as well.

        Recommended Skills: C#, .NET, Kubernetes.

        Expected project size: medium

        Mentor(s):

        Victor Cuadrado (@viccuad, vcuadradojuan@suse.com) - primary
        Flavio Castelli (@flavio, fcastelli@suse.com)
        José Guilherme Vanz (@jvanz, jguilhermevanz@suse.com)
        Fabrizio Sestito (@fabriziosestito, fabrizio.sestito@suse.com)
        Upstream Issue (URL): kubewarden/policy-sdk-dotnet#47

        ~~~~~~~~~~
        Lima
        VM plugin subsystem
        Description: Lima (https://lima-vm.io) is a project that provides Linux virtual machines with a focus on running containers. Lima supports several VM backends via built-in drivers: qemu, vz (Apple Virtualization.framework), and wsl2 (see lima/pkg/driverutil/instance.go). The idea for GSoC is to make a plugin subsystem that decouples the built-in VM drivers into separate binaries that communicate with the main Lima binary via some RPC (probably gRPC). This idea will improve the maintainability of the code base, and also help supporting additional VM backends (e.g., vfkit and cloud-based drivers).
        Expected Outcome:
        Design the plugin subsystem and its RPC (probably gRPC)
        Migrate the existing built-in VM drivers to the new plugin subsystem
        Implement additional VM plugins if the time allows
        Recommended Skills: Go, gRPC, QEMU, macOS
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Akihiro Suda (@AkihiroSuda, suda.kyoto@gmail.com) - primary
        Anders Björklund (@afbjorklund, anders.f.bjorklund@gmail.com)
        Upstream Issue (URL): lima-vm/lima#2007
        
        ~~~~~~~~~~
        LitmusChaos
        Terraform Support for LitmusChaos
        Description: LitmusChaos is an open-source Chaos Engineering platform that helps teams uncover weaknesses and potential outages in their applications by running controlled chaos experiments. However, before injecting chaos, several prerequisite steps must be completed, including user and project creation, connecting target infrastructure, and setting up experiments. To streamline this process, developers and SREs often seek automation, especially when integrating chaos testing into CI/CD pipelines. This Google Summer of Code (GSoC) project proposes developing a Terraform provider for LitmusChaos, enabling users to automate these essential setup steps and seamlessly manage chaos experiments through Terraform.

        Expected Outcome:

        LitmusChaos will have a terraform provider supporting user, project, infrastructure, and experiment resource operations along with proper documentation and usage scripts.
        A stretch goal for the mentee would be to become an official maintainer of the Litmus Terraform provider project.
        Recommended Skills: Golang, Terraform

        Expected project size: large (~175 hour projects)

        Mentor(s):

        Saranya Jena (@Saranya-jena, saranya.jena@harness.io)
        Sarthak Jain (@SarthakJain26, sarthak.jain@harness.io)
        Upstream Issue (URL): litmuschaos/litmus#5042

        ~~~~~~~~~~
        Meshery
        Multi-player Collaboration: Resilient Websockets and GraphQL Subscriptions
        Description: Meshery's current implementation of websockets and GraphQL subscriptions is in need of refactoring for increased reliability and resiliency. This client and server-side refactoring includes use of webworkers and separation of concerns for the client-side, and the use of a message broker for the server-side. The project has implications on Meshery's implementation of multi-player collaboration functionality.

        Expected Outcome: Resilient websockets and GraphQL subscriptions for Meshery, enabling multi-player collaboration functionality.

        Recommended Skills: Golang, Kubernetes, Azure, well-written and well-spoken English

        Expected project size: large (~175 hour project)

        Mentor(s):

        Lee Calcote (@leecalcote, leecalcote@gmail.com)
        Aabid Sofi (@aabidsofi19, mailtoaabid01@gmail.com)
        Upstream Issue: meshery/meshery#13554

        ~~~~~~~~~~

        Support for Azure in Meshery
        Description: Enhance Meshery's existing orchestration capabilities to include support for Azure. The Azure Service OperatorAzure Service Operator (ASO) provides a wide variety of Azure Resources via Kubernetes custom resources. as first-class Meshery Models. This involves enabling Meshery to manage and orchestrate Azure services and their resources, similar to how it handles other Kubernetes resources. The project will also include generating support for Azure services and their resources in Meshery's Model generator.

        Expected Outcome: Meshery will be able to orchestrate and manage all Azure services supported by ASO. This includes the ability to discover, configure, deploy, and operate the lifecycle of Azure services through Meshery. The Meshery Model generator will be updated to automatically generate models for Azure services, simplifying their integration and management within Meshery. This will be an officially supported feature of Meshery.

        Recommended Skills: Golang, Kubernetes, Azure, well-written and well-spoken English

        Expected project size: large (~175 hour project)

        Mentor(s):

        Lee Calcote (@leecalcote, leecalcote@gmail.com)
        Mia Grenell (@miacycle, mia.grenell2337@gmail.com)
        Upstream Issue: meshery/meshery#11244

        ~~~~~~~~~~

        Distributed client-side inference (policy evaluation) with WebAssembly (WASM) and OPA in Meshery
        Description: Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https://github.com/open-policy-agent/golang-opa-wasm project into Meshery UI, so that a powerful, real-time user experience is possible.

        Expected Outcome: The goal of this project is to enhance Meshery's infrastructure configuration capabilities by incorporating real-time policy evaluation using the golang-opa-wasm project. This project will integrate the capabilities of golang-opa-wasm into the Meshery UI, enabling users to experience the existing, powerful, server-side policy evaluation client-side.

        Recommended Skills: WebAssembly, Golang, Open Policy Agent, well-written and well-spoken English

        Expected project size: large (~175 hour project)

        Mentor(s): Lee Calcote (@leecalcote, leecalcote@gmail.com), James Horton (@hortison, james.horton2337@gmail.com)

        Upstream Issue: meshery/meshery#13555

        ~~~~~~~~~~
        Open Cluster Management
        Privacy-preserving and efficient AI model training across multi-cluster
        Description: Open Cluster Management (OCM) streamlines multi-cluster workload management through APIs that align with SIG-Multicluster standards. Beyond traditional workload orchestration, OCM enables scalable AI training and inference across distributed environments.

        As machine learning (ML) expands across clusters, data privacy becomes a critical concern. ML models rely on vast datasets, making it essential to safeguard sensitive information across clusters without compromising model performance.

        This project integrates Federated Learning (FL) into OCM, enabling privacy-preserving, collaborative model training without transferring raw data between clusters. Instead, training occurs locally where the data resides, ensuring compliance, enhancing efficiency, and reducing bandwidth and storage costs.

        By leveraging OCM's Placement, ManifestWork, and other APIs. we standardize FL workflows and seamlessly integrate frameworks like Flower and OpenFL through a unified interface. This approach harnesses OCM's capabilities to deliver scalable, cost-efficient, and privacy-preserving AI solutions in multi-cluster environments.

        Expected Outcome:

        Comprehensive Documentation:
        Define the scenarios addressed by the prototype, highlighting its purpose and value.
        Provide an intuitive and architectural comparison between Federated Learning (FL) and OCM, mapping FL terminology to OCM APIs to showcase OCM’s native support for FL.
        Illustrate the complete Federated Learning workflow within Open Cluster Management.
        Extended Prototype (or CRD) Support:
        Enable model aggregation persistence in AWS S3 (currently supports only native PVC).
        Extend compatibility to support additional Federated Learning frameworks like OpenFL (currently supports Flower). This requires understanding how OpenFL works, containerizing it, and integrating it into the prototype.
        Recommended Skills: Golang, Kubernetes, Federated Learning, Open Cluster Management, Scheduling

        Expected project size: medium (~175 hour projects)

        Mentor(s):

        Meng Yan (@yanmxa, myan@redhat.com) - primary
        Qing Hao (@haoqing0110, qhao@redhat.com)
        Upstream Issue (URL): open-cluster-management-io/ocm#825

        ~~~~~~~~~~    
        ORAS
        Enhance Java ORAS SDK
        Description: The ORAS project aims to enhance its Java SDK to support a broader range of features from the OCI Distribution spec. This involves implementing missing functionality, improving existing features, and expanding the SDK’s overall capabilities.
        Expected Outcome:
        Implement missing features from the OCI Distribution and Image Specifications, such as chunked uploads and the Referrers API
        Improve existing features, robustness and tests to ensure full compatibility with the OCI Distribution and Image Specifications.
        Enhance documentation and provide more comprehensive examples.
        Add support for additional authentication methods, including using credentials from docker config.json
        Recommended Skills: java, oci
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Valentin Delaye (@jonesbusy, jonesbusy@gmail.com) - primary
        Feynman Zhou (@FeynmanZhou, zpf0610@gmail.com)
        Upstream Issues: https://github.com/oras-project/oras-java/issues
        The Update Framework (TUF)
        Snapshot Merkle trees
        Description: The TUF snapshot role is responsible for consistency proofs in a TUF repository. In certain high-volume repositories, the related snapshot metadata file can become prohibitively large, and thus impose a significant overhead for TUF clients. TAP 16 proposes a method for reducing the size of snapshot metadata a client must download without significantly weakening the security properties of TUF. In this project you will add TAP 16 support to python-tuf.
        Expected Outcome: Snapshot Merkle trees are implemented in python-tuf Metadata API and ngclient
        Recommended Skills: Python, data structures (merkle trees)
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Lukas Pühringer (@lukpueh) - primary
        Justin Cappos (@JustinCappos)
        Upstream Issue (URL): theupdateframework/taps#134

        ~~~~~~~~~~
        Vitess
        Enhancements for FAQ Chatbot for Vitess
        Description: Vitess is a distributed database system built on MySQL. Developers often need to search through documentation, Slack discussions, and GitHub issues to find answers. We are starting a project to implement an AI-powered FAQ chatbot using Retrieval-Augmented Generation, integrating vector search with an LLM (such as OpenAI, DeepSeek, GPT-4, Mistral, Llama 3). The chatbot will be available via a CLI and Slack bot for developer support.

        In the next phase, which will be implemented in this Summer Of Code (SOC) project, we will be adding more features like:

        Content filtering for chatbot safety and response validation
        Fine-tuning the model for improved accuracy
        Pipelines for refreshing data from new/updated docs
        Caching previous replies to reduce LLM costs
        Rate-limiting
        Better benchmarking for iterative improvements
        User feedback integration to improve relevancy
        Expected Outcome: Improved chatbot that provides accurate Vitess-related answers via CLI and Slack, using indexed documentation and discussions for retrieval.

        Recommended Skills: golang, python, LLM APIs, vector databases

        Expected project size: large (~350 hour projects)

        Mentor(s):

        Rohit Nayak (@rohit-nayak-ps, rohit@planetscale.com)
        Manan Gupta (@GuptaManan100, manan@planetscale.com)
        Upstream Issue: vitessio/vitess#17690

        ~~~~~~~~~~

        WasmEdge
        Virtual filesystem security for WasmEdge plug-ins with exporting WASI APIs
        Description: The WASI proposal defines the variety of rules to guarantee the virtual filesystem security and isolation when executing WASM binaries. However, besides using WASI directly in WASM, developers can also implement the host functions to access the filesystem in their guest programming language. This will break the sandbox of WebAssembly. In this program, our goal is to export the WASI APIs in WasmEdge, and use the APIs in WasmEdge plug-ins to ensure the filesystem security and WebAssembly isolation.
        Expected Outcome:
        Export needed WASI APIs in WasmEdge internal to provide the functions of checking and accessing host filesystem.
        Apply the APIs in some WasmEdge plug-ins which accessing the filesystem, such as WASI-NN.
        Implement test suites to verify the above behaviors.
        Recommended Skills:
        C++
        WebAssembly
        Expected project size: Large (~350 hour projects)
        Mentor(s):
        YiYing He (@q82419 , yiying@secondstate.io) - Primary
        Shen-Ta Hsieh (@ibmibmibm , beststeve@secondstate.io)
        Upstream Issue (URL): WasmEdge/WasmEdge#4012

        ~~~~~~~~~~
        Port WasmEdge and the WASI-NN ggml backend to the s390x platform
        Description: WasmEdge provides cross-platform support for amd64 and arm64 for executing AI/LLM applications. We would like to support as many new hardware platforms as possible, so developers and users will no longer need to worry about the actual hardware. All they need to do is develop their AI agent or LLM applications once and deploy their services anywhere. For more information, please check the upstream issue.
        Expected Outcome:
        Make the WasmEdge toolchain support the s390x platform, including the interpreter and the AOT mode.
        Ensure the WASI-NN ggml plugin can execute without any issues on the s390x platform.
        Implement test suites to verify the above behaviors.
        Write a document discussing the compilation, installation, execution, and verification of the work.
        Recommended Skills:
        C++
        s390x
        LLVM
        Expected project size: Large (~350 hour projects)
        Mentor(s):
        Hung-Ying Tai (@hydai, hydai@secondstate.io) - Primary
        dm4 (@dm4, dm4@secondstate.io)
        Upstream Issue (URL): WasmEdge/WasmEdge#4010

        ~~~~~~~~~~

        Use Runwasi with WasmEdge runtime to test multiple WASM apps as cloud services
        Description: With WasmEdge serving as one of Runwasi’s standard runtimes, and as our C++-implemented library continues to evolve, we also need a verification process integrated into Runwasi to streamline and validate the stability of both container and cloud environments.
        Expected Outcome:
        A concise GitHub workflow demonstrates Runwasi end-to-end testing on Kubernetes.
        Need to design an interactive application scenario that supports multiple nodes
        Try to incorporate the use of the WasmEdge plugin into this scenario
        Document
        Recommended Skills:
        Rust
        C++
        GDB
        git / github workflow
        shell script
        Expected project size: Large (~350 hour projects)
        Mentor(s):
        Vincent (@CaptainVincent, vincent@secondstate.io) - Primary
        yi (@0yi0 yi@secondstate.io)
        Upstream Issue (URL): WasmEdge/WasmEdge#4011

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cncf/
    idea_list_url: https://github.com/cncf/mentoring/blob/main/programs/summerofcode/2025.md


  - organization_id: 20
    organization_name: CRIU 
    no_of_ideas: 5
    ideas_content: |
        Project ideas
        Add support for memory compression
        Summary: Support compression for page images

        We would like to support memory page files compression in CRIU using one of the fastest algorithms (it's matter of discussion which one to choose!).

        This task does not require any Linux kernel modifications and scope is limited to CRIU itself. At the same time it's complex enough as we need to touch memory dump/restore codepath in CRIU and also handle many corner cases like page-server and stuff.

        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Suggested by: Andrei Vagin <avagin@gmail.com>
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Alexander Mikhalitsyn <alexander@mihalicyn.com>, Andrei Vagin <avagin@gmail.com>

        ~~~~~~~~~~
        Use eBPF to lock and unlock the network
        Summary: Use eBPF instead of external iptables-restore tool for network lock and unlock.

        During checkpointing and restoring CRIU locks the network to make sure no network packets are accepted by the network stack during the time the process is checkpointed. Currently CRIU calls out to iptables-restore to create and delete the corresponding iptables rules. Another approach which avoids calling out to the external binary iptables-restore would be to directly inject eBPF rules. There have been reports from users that iptables-restore fails in some way and eBPF could avoid this external dependency.

        Links:

        https://www.criu.org/TCP_connection#Checkpoint_and_restore_TCP_connection
        https://github.com/systemd/systemd/blob/master/src/core/bpf-firewall.c
        https://blog.zeyady.com/2021-08-16/gsoc-criu
        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Prajwal S N <prajwalnadig21@gmail.com>
        Suggested by: Adrian Reber <areber@redhat.com>

        ~~~~~~~~~~
        Files on detached mounts
        Summary: Initial support of open files on "detached" mounts

        When criu dumps a process with an open fd on a file, it gets the mount identifier (mnt_id) via /proc/<pid>/fdinfo/<fd>, so that criu knows from which exact mount the file was initially opened. This way criu can restore this fd by opening the same exact file from topologically the same mount in restored mount tree.

        Restoring fd from the right mount can be important in different cases, for instance if the process would later want to resolve paths relative to the fd, and obviously resolving from the same file on different mount can lead to different resolved paths, or if the process wants to check path to the file via /proc/<pid>/fd/<fd>.

        But we have a problem finding on which mount we need to reopen the file at restore if we only know mnt_id but can't find this mnt_id in /proc/<pid>/mountinfo.

        Mountinfo file shows the mount tree topology of current mntns: parent - child relations, sharing group information, mountpoint and fs root information. And if we don't see mnt_id in it we don't know anything about this mount.

        This can happen in two cases

        1) external mount or file - if file was opened from e.g. host it's mount would not be visible in container mountinfo
        2) mount was lazily unmounted
        In case of 1) we have criu options to help criu handle external dependencies.

        In case of 2) or no options provided criu can't resolve mnt_id in mountinfo and criu fails.

        Solution: We can handle 2) with: resolving major/minor via fstat, using name_to_handle_at and open_by_handle_at to open same file on any other available mount from same superblock (same major/minor) in container. Now we have fd2 of the same file as fd, but on existing mount we can dump it as usual instead, and mark it as "detached" in image, now criu on restore knows where to find this file, but instead of just opening fd2 from actually restored mount, we create a temporary bindmount which is lazy unmounted just after open making the file appear as a file on detached mount.

        Known problems with this approach:

        Stat on btrfs gives wrong major/minor
        file handles does not work everywhere
        file handles can return fd2 on deleted file or on other hardlink, this needs special handling.
        Additionally (optional part): We can export real major/minor in fdinfo (kernel). We can think of new kernel interface to get mount's major/minor and root (shift from fsroot) for detached mounts, if we have it we don't need file handle hack to find file on other mount (see fsinfo or getvalues kernel patches in LKML, can we add this info there?).

        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Mentor: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
        Suggested by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
        Checkpointing of POSIX message queues
        Summary: Add support for checkpoint/restore of POSIX message queues

        POSIX message queues are a widely used inter-process communication mechanism. Message queues are implemented as files on a virtual filesystem (mqueue), where a file descriptor (message queue descriptor) is used to perform operations such as sending or receiving messages. To support checkpoint/restore of POSIX message queues, we need a kernel interface (similar to MSG_PEEK) that would enable the retrieval of messages from a queue without removing them. This project aims to implement such an interface that allows retrieving all messages and their priorities from a POSIX message queue.

        Links:

        https://github.com/checkpoint-restore/criu/issues/2285
        https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/ipc/mqueue.c
        https://www.man7.org/tlpi/download/TLPI-52-POSIX_Message_Queues.pdf
        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Pavel Tikhomirov <ptikhomirov@virtuozzo.com>, Prajwal S N <prajwalnadig21@gmail.com>
        Suggested by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>

        ~~~~~~~~~~


        Add support for arm64 Guarded Control Stack (GCS)
        Summary: Support arm64 Guarded Control Stack (GCS)

        The arm64 Guarded Control Stack (GCS) feature provides support for hardware protected stacks of return addresses, intended to provide hardening against return oriented programming (ROP) attacks and to make it easier to gather call stacks for applications such as profiling (taken from [1]). We would like to support arm64 Guarded Control Stack (GCS) in CRIU, which means that CRIU should be able to Checkpoint/Restore applications using GCS.

        This task should not require any Linux kernel modifications but will require a lot of effort to understand Linux kernel and glibc support patches. We have a good example of support for x86 shadow stack [4].

        Links:

        [1] kernel support https://lore.kernel.org/all/20241001-arm64-gcs-v13-0-222b78d87eee@kernel.org
        [2] libc support https://inbox.sourceware.org/libc-alpha/20250117174119.3254972-1-yury.khrustalev@arm.com
        [3] libc tests https://inbox.sourceware.org/libc-alpha/20250210114538.1723249-1-yury.khrustalev@arm.com
        [4] x86 support https://github.com/checkpoint-restore/criu/pull/2306
        Details:

        Skill level: expert (a lot of moving parts: Linux kernel / libc / CRIU)
        Language: C
        Expected size: 350 hours
        Suggested by: Mike Rapoport <rppt@kernel.org>
        Mentors: Mike Rapoport <rppt@kernel.org>, Andrei Vagin <avagin@gmail.com>, Alexander Mikhalitsyn <alexander@mihalicyn.com>

        ~~~~~~~~~~
        Coordinated checkpointing of distributed applications
        Summary: Enable coordinated container checkpointing with Kubernetes.

        Checkpointing support has been recently introduced in Kubernetes, where the smallest deployable unit is a Pod (a group of containers). Kubernetes is often used to deploy applications that are distributed across multiple nodes. However, checkpointing such distributed applications requires a coordination mechanism to synchronize the checkpoint and restore operations. To address this challenge, we have developed a new tool called criu-coordinator that relies on the action-script functionality of CRIU to enable synchronization in distributed environments. This project aims to extend this tool to enable seamless integration with the checkpointing functionality of Kubernetes.

        Links:

        https://github.com/checkpoint-restore/criu-coordinator
        https://lpc.events/event/18/contributions/1803/
        https://sched.co/1YeT4
        https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/
        Details:

        Skill level: intermediate
        Language: Rust / Go / C
        Expected size: 350 hours
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Prajwal S N <prajwalnadig21@gmail.com>
        Suggested by: Radostin Stoyanov <rstoyanov@fedoraproject.org>

        

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/criu/
    idea_list_url: https://criu.org/Google_Summer_of_Code_Ideas
  

  - organization_id: 21
    organization_name: Center for Translational Data Science
    no_of_ideas: 7
    ideas_content: |
        Google Summer of Code - Project Ideas 2025
        #1 Project Name: Towards Personalized Medicine with FHIR Integration and Data Driven Discovery
        Mentor(s): Alex VanTol, Kyle Burton
        Project Description
        FHIR (Fast Healthcare Interoperability Resources, pronounced “fire”) is the standard for ensuring smooth data exchange and interoperability in electronic health records (EHR) and widely used in hospital settings. This project will focus on integrating a FHIR server into an open-source data platform used for managing and analyzing distributed biomedical data and supporting federated AI for data driven discovery.
        As part of this endeavor, you will work on enabling this platform to support FHIR standards, using synthetic data to ensure the functionality of the solutions developed. You will gain valuable experience at the intersection of healthcare and technology, working on tasks that include cloud infrastructure and modern data standards.
        Gain a deep understanding of the FHIR standard and how it supports interoperability of EHR systems.
        Learn about cloud-based architectures for managing and sharing biomedical data.
        Hone your ability to write, translate, and estimate business and technical requirements through user stories, tickets, and epics.
        Develop and deploy code, as well as create integration tests for proof of concept scenarios.
        Contribute to the enhancement of healthcare data interoperability.
        Expand your technical skills by working with cloud infrastructure and standardized data protocols.
        Acquire hands-on experience contributing to an impactful open-source software project.
        Expected Outcomes
        Automated deployment and configuration of a FHIR server within the data platform.
        Capability for the data platform to import and export FHIR data seamlessly.
        Skills Required/Preferred
        Required: 
        Python programming experience
        Some experience with cloud infrastructure

        Preferred:
        Some familiarity with healthcare data
        Java programming experience
        Javascript programming experience
        Expected Size and Difficulty
        350 hours
        Difficulty - Medium

        ~~~~~~~~~~

        #2 Project Name: Supporting an Open-Source Data Lake Architecture with Graph-like Data
        Mentor(s): Alex VanTol, Michael Lukowski
        Project Description
        This project focuses on enhancing a data platform by implementing a versatile solution for data ingestion and storage using a data lakehouse architecture. The aim is to prototype a method for converting graph-structured data into Avro-based files, specifically utilizing a file format called the Portable Format for Biomedical data (PFB). This format supports exporting and importing bulk clinical and other structured data within healthcare data platforms.
        You will engage in designing and developing tooling to convert data from original sources, representing graph-like models, into serialized Avro files for seamless ingestion into a data lake. This project offers a unique opportunity to delve into data lakehouse architectures, graph data models, and serialization techniques, enriching your expertise in these areas.
        Understand and work with data lakehouse architectures for managing diverse datasets.
        Learn to convert data from graph models into an Avro-based serialized format.
        Develop skills in creating extensible methodologies and tooling for data ingestion.
        Gain familiarity with querying and visualizing graph models contained within serialized files.
        Explore the intersection of data management, bioinformatics, and healthcare data platforms.
        Contribute to the development of advanced data ingestion and storage solutions.
        Enhance your understanding of data lakehouses and serialization formats like Avro.
        Acquire hands-on experience with graph data models and their applications.
        Participate in an open-source project that has real-world impact in the healthcare field.
        Expected Outcomes
        Develop methodology and tools for converting graph-structured data into Avro-based serialized files.
        Potentially include tools for querying and visualizing graph models within the serialized format, depending on project scope.
        Skills Required/Preferred
        Required:
        Python programming experience
        Preferred:
        Knowledge of graph models
        Some familiarity with serialization formats (specifically, Avro)
        Knowledge of UX and/or client side tooling
        Expected Size and Difficulty
        175 hours 
        Difficulty - Medium

        ~~~~~~~~~~

        #3 Project Name: Improve Scalable Data Download Functionality Using Globus and an Open-Source Python SDK & CLI
        Mentor(s): Alex VanTol, Pauline Ribeyre
        Project Description
        This project aims to enhance the data download capabilities of a Python SDK & CLI used to interact with a large-scale data management platform. The goal is to create a consistent and efficient tool for researchers to download and stream large biomedical data sets. This involves integrating with existing RESTful APIs and Globus, a service for secure and reliable data transfer.
        You will work on implementing data download functionality within the Python SDK & CLI, writing unit tests to ensure robust code, and optimizing the download process using asynchronous capabilities in Python. This tool will be extensively used by researchers and scientists to stream large data sets to virtual workspaces for analysis.
        Gain experience in enhancing data download functionalities in a scalable manner.
        Learn to work with RESTful APIs and how to interact with them programmatically.
        Get hands-on experience with Globus for secure data transfer.
        Develop skills in Python programming, particularly in creating efficient and asynchronous code.
        Learn to write thorough unit tests to ensure high code quality.
        Understand the challenges and requirements of streaming large biomedical data sets.
        Contribute to improving tools that support large-scale data management and research.
        Enhance your programming skills, particularly in Python and Golang.
        Work on real-world projects that benefit researchers and scientists in the biomedical field.
        Gain experience in using modern data transfer technologies and optimizing performance.
        Expected Outcomes
        Implemented data download functionality within the Python SDK & CLI, integrated with Globus.
        Achieve 100% unit test coverage for the new code.
        Optimized data download process for improved performance.
        Skills Required/Preferred
        Required:
        Ability to read and understand Golang
        Ability to code in Python
        Preferred:
        Familiarity with RESTful APIs
        Familiarity with testing
        Familiarity with command line interfaces and/or UX
        Familiarity with concepts around concurrency
        Expected Size and Difficulty
        175 hours
        Difficulty - Medium

        ~~~~~~~~~~

        #4 Project Name: Towards a Data Commons Operations Center with Observability and Monitoring
        Mentor(s): Jawad Qureshi
        Project Description
        The goal of this project is to develop an Operations Center dashboard (CSOC) to manage multiple standalone and interconnected data commons running Gen3, a well-established open source platform for biomedical research. This dashboard will streamline the deployment, monitoring, and management of Gen3 data commons and meshes through a unified interface. The project will involve creating a distributed system with a Go-based backend and Next.js frontend, integrating with Kubernetes, Grafana, Helm, and Terraform, and ensuring secure server-agent communication.

        Expected Outcomes
        Production-ready Operations Center: A functional dashboard with a Go backend and Next.js frontend.
        RBAC and Security Policy Administration: Implementation of role-based access control (RBAC) and security policies.
        Observability Platform: Complete observability integrated with Prometheus/Grafana for monitoring.
        User-Friendly Dashboard: An intuitive interface for Gen3 commons deployment and management.
        Secure Server-Agent Communication: Infrastructure to ensure secure communication between server and agents.
        Comprehensive Documentation: Detailed system documentation and deployment guides.

        Skills Required/Preferred
        Go programming experience
        React/Next.js development skills / Javascript
        Understanding of Kubernetes, Helm, and Terraform
        Understanding of Cloud Infrastructure
        Experience with gRPC
        Understanding the pillars of observability (metrics, logs, tracing)
        Expected Size and Difficulty
        Number of hours 1000
        Difficulty - (Medium - Difficult)

        ~~~~~~~~~~

        #5 Project Name: Enhance Data Solutions with Native Graph Database Integration
        Mentor(s): Craig Barnes, Andrew Prokhorenkov, Alex VanTol
        Project Description
        This project aims to significantly improve the efficiency and capabilities of our data platform by transitioning from a custom PostgreSQL backend to a native graph database solution. With the increasing reliability and performance of graph databases like neo4j, this transition will enhance how we manage, analyze, and query complex biomedical data.
        Your role will involve developing a new Python-based microservice that supports the same RESTful APIs as our current submission and query services. Additionally, the microservice will dynamically generate GraphQL (or GraphQL-like) APIs based on a configured data schema, facilitating advanced search and query capabilities.
        Expected Outcomes
        Evaluate and analyze the best native graph database solutions for integration.
        Implement the selected graph database to replace the current PostgreSQL backend.
        Develop a new microservice in Python to support the existing RESTful and GraphQL (or GraphQL-like) APIs.
        Ensure seamless data submission, access, and querying within our data platform.
        This project will enhance our data platform's performance and flexibility, offering more robust solutions for managing biomedical data. Your contribution will provide significant improvements in data querying and management capabilities, benefiting the broader biomedical research community.
        Skills Required/Preferred
        Required: 
        Proficiency in Python
        Basic understanding of GitHub or other version control platforms
        Familiarity with RESTful APIs
        Understanding of graph databases
        Knowledge of user authorization and security principles

        Preferred:
        Familiarity with relational databases
        Understanding of microservice architecture, Docker, and containerization
        Expected Size and Difficulty
        Number of hours: 350 hours
        Difficulty - Hard

        ~~~~~~~~~~

        #6 Project Name: GPU Cluster Orchestration and Observability
        Mentors: Salman Sikandar and Bilal Baqar
        Project Description
        This project focuses on developing a CPU-based control plane to orchestrate jobs on an existing GPU cluster used by ML/AI researchers. The current setup is rudimentary, and the goal is to implement an automated job scheduling system along with comprehensive observability. The intern will design and implement a solution using technologies like Slurm for orchestration, and Prometheus/Grafana for monitoring and alerting.
        Expected Outcomes:
        Implement a job scheduler with priority queues and preemption capabilities for GPU workloads
        Develop a centralized dashboard displaying GPU/CPU utilization, job queue status, and thermal metrics
        Create Terraform/Ansible playbooks for reproducible cluster provisioning
        Design and implement automated alerting based on predefined thresholds and anomaly detection
        Skills Required/Preferred
        Required:
        Python or Go programming
        Linux systems administration
        Basic networking concepts
        Containerization (Docker)
        Preferred:
        Experience with Slurm
        Familiarity with Prometheus and Grafana
        GPU architecture knowledge (CUDA/NCCL)
        Experience with distributed systems

        Expected Size and Difficulty
        Number of Hours: 300-350 hours (8-10 weeks)
        Difficulty: Intermediate to Advanced

        ~~~~~~~~~~


        #7 Project Name: Staging Inference Cluster with CI/CD
        Mentors:
        Salman Sikandar and Bilal Baqar
        Project Description
        This project involves building a staging inference cluster to host production versions of ML models with auto scaling capabilities and CI/CD pipelines. The intern will set up an environment that allows for seamless promotion of trained models to production, implementing canary deployments and rollback mechanisms. The project will utilize technologies such as KEDA or Vertical Pod Autoscaler for autoscaling, and ArgoCD or Flux for GitOps-driven CI/CD.
        Expected Outcomes:

        Establish a staging inference cluster with autoscaling triggered by request latency/throughput thresholds
        Implement a GitHub Actions pipeline for model validation, containerization, and deployment
        Develop GitOps-driven CI/CD pipelines using ArgoCD or Flux for automated deployments
        Create a system for canary deployments and automated rollbacks based on performance metrics

        Skills Required/Preferred
        Required:
        Python or Go programming
        CI/CD concepts and tools
        Containerization (Docker)
        Basic understanding of ML workflows
        Preferred:
        Knowledge of ML model serving frameworks
        Familiarity with GitOps principles

        Expected Size and Difficulty
        Number of Hours: 300-350 hours (8-10 weeks)
        Difficulty: Intermediate to Advanced


          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/center-for-translational-data-science/
    idea_list_url: https://docs.google.com/document/d/1kiEDB6tw2xD8Qj3uxpSELjazW35llHM1a9Nt_LQKwHw/edit?usp=sharing

  - organization_id: 22
    organization_name: Ceph Foundation
    no_of_ideas: 7
    ideas_content: |
      
      Teuthology on Podman ¶
      Mentor name(s): Zack Cerza, Kamoltat (Junior) Sirivadhna Aishwarya Mathuria, Vallari Agrawal
      Mentor email(s): zack1@ibm.com, ksirivad@ibm.com, aishwarya.mathuria@ibm.com, vallari.agrawal@ibm.com
      Difficulty: Hard
      Project Hours: 175
      Skills needed: python, containerisation, linux
      Subcomponent of Ceph: Ceph Integration Test Framework
      Description of project:
      ceph-devstack is an in-development tool that uses rootless podman containers to deploy a scaled-down teuthology lab. It has proven useful for testing changes to teuthology and its related services, allowing us to more easily and flexibly make changes to components without worrying about causing outages.
      It has some basic ability to run Ceph tests, but could benefit significantly from more investment in that area.
      Improve and extend ceph-devstack's ability to perform teuthology tests against Ceph builds. This project will involve writing Python code and tests to orchestrate podman containers, and working with security systems like SELinux, CGroups, and Linux capabilities.
      Standup/weekly call mentee could attend?: Teuthology weekly meeting
      Steps to evaluate an applicant for the project: TBD
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD
      Expected Outcome:
      Extend ceph-devstack's ability to perform teuthology tests

      ~~~~~~~~~~
      smartmontools drivedb.h postprocessor ¶
      Mentor name(s): Anthony D'Atri, Sunil Angadi
      Mentor email(s): anthony.datri@ibm.com, sunil.angadi@ibm.com
      Difficulty: Intermediate
      Project Hours: 90
      Skills needed: c++, maybe python or golang
      Subcomponent of Ceph: Observability
      Description of project:
      smartmontools (smartctl) is pretty much the only game in town for harvesting metrics and counters from storage devices: SMART for SATA, a few things for SAS, and passthrough to nvme-cli for NVMe. It leverages a runtime file named drivedb.h that directs what attributes are to be found with what numeric IDs, and how to interpret them. drivedb.h is a mess, and upstream smartmontools would likely resist wholesale refactoring. For example, SSD wear might be labeled as "lifetime remaining" or "wear level" or multiple other strings. Some devices also report wear used, others wear remaining.
      One task would be to add an interpretation primitive to the c++ code so that a drivedb.h entry can specify that the result should be subtracted from 100.
      The larger task would be to write a postprocessor for drivedb.h that more or less is a sequence of regex invocations that converges the existing freeform attribute label names into a normalized, defined set. Many tools just pass through the text labels, so doing meaningful analysis or queries is difficult; often only a fraction of the data is actually captured as a result. The output also includes numeric attribute IDs, which are less varied, but relying on them instead of the text labels is fraught because these numeric IDs are not strictly standardized either. I have seen drives that report a metric on a different numeric ID than most others, and/or that report a different metric on a specific numeric than most others report on that ID.
      For extra credit, interface with the central telemetry DB as described in project "Public telemetry slice/dice of SMART data".
      Standup/weekly call mentee could attend?: TBD
      Steps to evaluate an applicant for the project: Ability to leverage code libraries and write the glue code.
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD

      ~~~~~~~~~~
      The More The Merrier ¶
      Mentor name(s): Yuval Lifshitz
      Mentor email(s): ylifshit@ibm.com
      Difficulty: Hard
      Project Hours: 350
      Skills needed: C++, Python
      Subcomponent of Ceph: RGW
      Description of project:
      Detailed description of the project and evalution steps can be found here.
      Persistent bucket notifications are a very useful and powerful feature
      tech talk: https://www.youtube.com/watch?v=57Ejl6R-L20
      usecase example: https://www.youtube.com/watch?v=57Ejl6R-L20
      However, they can pose a performance issue, since the notifications regarding a specific bucket are written to a single RADOS queue (unlike the writes to the bucket which are distributed across multiple bucket shards. So, in case that small objects are written to the bucket, the overhead of the notifications is considerable. In this project, our goal would be to create a sharded bucket notifications queue, to allow for better performance of sending persistent bucket notifications.
      Standup/weekly call mentee could attend?: RGW daily Standup, RGW weekly refactoring meeting
      Steps to evaluate an applicant for the project:
      build ceph from source and run basic bucket notification tests
      fix low-hanging-fruit issues in bucket notifications
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD
      Expected outcome:
      sharded implementation of persistent topic queue

      ~~~~~~~~~~
      stretch goal: perf test proving performance improvement
      Public telemetry slice/dice of SMART data ¶
      Mentor name(s): Anthony D'Atri
      Mentor email(s): anthony.datri@ibm.com
      Difficulty: Medium
      Project Hours: 175
      Skills needed: Some coding language, Python or Go, jq or JSON parsing or other text library.
      Subcomponent of Ceph: telemetry
      Description of project:
      Public telemetry today offers a few Grafana panels and downloadable archives of anonymized data. One field is a JSON blob of smartctl output. Parse this, apply a normalization layer, deduplicate, and present in one or more formats that facilitate analysis:
      CSV file containing attributes for only the latest report found for a given device
      The number of data points might be too high, but possibly a Grafana dashboard or even spreadsheet with template variables for manufacturer/model, interface type, etc. with various panes:
      Histograms of power_on hours, normalized endurance used or remaining, etc
      histogram or table of endurance remaining vs power on hours or TBW, i.e. allowing one to predict drive lifetime and inform purchase decisions, vs. assuming that SSDs especially QLC lack endurance or that high-endurance SKUs are required.
      reallocated sectors over time, etc.
      Standup/weekly call mentee could attend?: TBD
      Steps to evaluate an applicant for the project: Coding experience beyond Karel
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship:
      Gain familiarity with the data format, including JSON. Discuss input filtering: skip over invalid entries, handle submissions from older smartmontools, uniqify, learn about SMART -- and how dumb it is, the need for nomalization of counters.
      Expected outcome:
      Described above under Description. More specifically, deriving the rate of wear over time for each specific SSD for which we have more than say a month of data: capture the delta between earliest and latest wear levels reported for each given serial number, and the time delta between those samples. Divide the wear delta by the time delta for rate of wear over time.
      
      ~~~~~~~~~~
      Warm and Fuzzy ¶
      Mentor name(s): Yuval Lifshitz, Pritha Srivastava
      Mentor email(s): ylifshit@ibm.com, Pritha.Srivastava@ibm.com
      Difficulty: Medium
      Project Hours: 175
      Skills needed: C++, Python and also depending with the tool
      Subcomponent of Ceph: RGW
      Description of project:
      The RGW's frontend is an S3 REST API server, and in this project we would like to use a REST API fuzzer to test the RGW for security issues (and other bugs). First step of the project would be to select the right tool (e.g. https://github.com/microsoft/restler-fuzzer), feed it with the AWS S3 OpenAPI spec, and see what happens when we let it connect to the RGW. Fixing issues the fuzzer finds would nice, but the real stretch goal would be to integrate these tests into teuthology.
      Standup/weekly call mentee could attend: RGW daily Standup, RGW weekly refactoring meeting
      Steps to evaluate an applicant for the project:
      Detailed description of the project and evalution steps can be found here.
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD
      Expected outcome:
      find and fix security issues in the RGW found by the fuzzing tool
      stretch goal: integrate tool into automated teuthology runs

      ~~~~~~~~~~
      Ceph Dashboard Usability Improvements ¶
      Mentor name(s): Afreen Misbah
      Mentor email(s): afreen@ibm.com
      Difficulty: Easy
      Project Hours: 175
      Skills needed: Typescript, Angular, and basic understanding of HTML & CSS.
      Subcomponent of Ceph: Dashboard
      Description of project:
      Ceph Dashboard is Ceph's management and monitoring tool. It's a web application tool with Angular/Typescript on frontend side and Python as backend.
      We are in an effort to provide more usability workflows and solve UX issues to make management and monitoring easy for Ceph users.
      The task includes improving the notification system and creating a workflow for managing NVMe-oF devices from dashboard.
      Standup/weekly call mentee could attend?: Dashboard daily sync
      Steps to evaluate an applicant for the project:
      Build ceph dashboard locally via docker-compose and kcli both
      Able to understand issues and ask useful questions
      Eagerness to learn and contribute
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship:
      Learning about ceph and storage and gradually contributing to the dashboard.
      Expected Outcome:
      Improve dashboard usability.

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ceph-foundation/
    idea_list_url: https://ceph.io/en/developers/google-summer-of-code/

  - organization_id: 23
    organization_name: Checker Framework
    no_of_ideas: 4
    ideas_content: |
      Evaluate a type system or a Checker Framework feature
      These projects evaluate a recently-written type system or a feature used by multiple type systems. Using the type systems on real code is our most important source of new ideas and improvements. Many people have started out “just” doing a case study but have ended up making deep, fundamental contributions and even publishing scientific papers about their discoveries.

      One possible outcome is to identify weaknesses in the type-checker so that we can improve it. Another possible outcome is to provide evidence that the type-checker is effective and convince more users to adopt it. You will probably also discover defects (bugs) in the codebase being type-checked.

      Signature strings
      Determine whether the ASM library, or some other library, properly handles signature strings.

      Some challenging aspects of this case study are:

      Some libraries define their own new signature string formats (!), which you need to define in the Signature String Checker.
      Sometimes the library's documentation is incorrect, and in other cases the string format is not defined.
      Preventing mixed signed/unsigned computations
      An unsigned integer's bits are interpreted differently than a signed integer's bits. It is meaningless to add a signed and an unsigned integer — the result will be nonsense bits. The same is true of printing and of other numeric operators such as multiplication and comparison.

      We have a prototype compile-time verification tool that detects and prevents these errors. The goal of this project is to perform case studies to determine how often programmers make signedness errors (our initial investigation suggests that this is common!) and to improve the verification tool.

      The research questions are:

      How often do programmers make signedness errors?
      Is it feasible to automatically detect signedness errors? What techniques are useful?
      What is the false positive rate of a signedness verification tool — that is, false alarms from the tool?
      How much effort is required from a programmer?
      The methodology is:

      find open-source projects that use unsigned arithmetic
      run the verification tool on them
      for each tool warning, determine whether it is a defect in the project or a limitation of the verification tool. For example, the Signedness Checker does not currently handle boxed integers and BigInteger; these haven't yet come up in case studies but could be worthwhile enhancements. You may also need to write more annotations for libraries such as the JDK.
      submit bug reports against the project, or improve the verification tool
      A good way to find projects that use unsigned arithmetic is to find a library that supports unsigned arithmetic, then search on GitHub for projects that use that library.

      Here are some relevant libraries.

      In the JDK's Integer and Long, these include compareUnsigned, divideUnsigned, parseUnsignedInt, remainderUnsigned, and toUnsignedLong.
      Classes like DataInputStream, ObjectInputStream, and RandomAccessFile have readUnsignedByte.
      Arrays has compareUnsigned. The JDK is already annotated; search for @Unsigned within https://github.com/typetools/jdk.
      In Guava, see its unsigned support, such as UnsignedBytes, UnsignedLong, UnsignedLongs, etc. Guava is already annotated; search for @Unsigned within https://github.com/typetools/guava.
      The jOOU library consists of support for unsigned integers.
      Another possibility is to find Java projects that could use an unsigned arithmetic library but do not. For example, bc-java defines its own unsigned libraries, and some other programs might do direct bit manipulation.

      Whole-program type inference
      A type system is useful because it prevents certain errors. The downside of a type system is the effort required to write the types. Type inference is the process of writing the types for a program.

      The Checker Framework includes a whole-program inference that inserts type qualifiers in the user's program. It works well on some programs, but needs more enhancements to work well on all programs.

      Sound checking by default
      By default, the Checker Framework is unsound in several circumstances. “Unsound” means that the Checker Framework may report no warning even though the program can misbehave at run time.

      The reason that the Checker Framework is unsound is that we believe that enabling these checks would cause too many false positive warnings: warnings that the Checker Framework issues because it cannot prove that the code is safe (even though a human can see that the code is safe). Having too many false positive warnings would irritate users and lead them not to use the checker at all, or would force them to simply disable those checks.

      We would like to do studies of these command-line options to see whether our concern is justified. Is it prohibitive to enable sound checking? Or can we think of enhancements that would let us turn on those checks that are currently disabled by default?

      There is no need to annotate new code for this project. Just use existing annotated codebases, such as those that are type-checked as part of the Checker Framework's Azure Pipeline. In other words, you can start by enabling Azure Pipelines for your fork and then changing the default behavior in a branch. The Azure Pipelines job will show you what new warnings appear.

      Comparison to other tools
      Many other tools exist for prevention of programming errors, such as Error Prone, NullAway, FindBugs, JLint, PMD, and IDEs such as Eclipse and IntelliJ. These tools are not as powerful as the Checker Framework (some are bug finders rather than verification tools, and some perform a shallower analysis), but they may be easier to use. Programmers who use these tools wonder, "Is it worth my time to switch to using the Checker Framework?"

      The goal of this project is to perform a head-to-head comparison of as many different tools as possible. You will quantify:

      the number of annotations that need to be written
      the number of bugs detected
      the number of bugs missed
      the number of false positive warnings
      This project will help programmers to choose among the different tools — it will show when a programmer should or should not use the Checker Framework. This project will also indicate how each tool should be improved.

      One place to start would be with an old version of a program that is known to contain bugs. Or, start with the latest version of the program and re-introduce fixed bugs. (Either of these is more realistic than introducing artificial bugs into the program.) A possibility would be to use the Lookup program that has been used in previous case studies.

      Android support annotations
      Android uses its own annotations that are similar to some in the Checker Framework. Examples include the Android Studio support annotations, including @NonNull, @IntRange, @IntDef, and others.

      The goal of this project is to implement support for these annotations. That is probably as simple as creating aliased annotations by calling method addAliasedTypeAnnotation() in AnnotatedTypeFactory.

      Then, do a case study to show the utility (or not) of pluggable type-checking, by comparison with how Android Studio currently checks the annotations.
      
      ~~~~~~~~~~

      Annotate a library
      These projects annotate a library, so that it is easier to type-check clients of the library. Another benefit is that this may find bugs in the library. It can also give evidence for the usefulness of pluggable type-checking, or point out ways to improve the Checker Framework.
      When type-checking a method call, the Checker Framework uses the method declaration's annotations. This means that in order to type-check code that uses a library, the Checker Framework needs an annotated version of the library.
      The Checker Framework comes with a few annotated libraries. Increasing this number will make the Checker Framework even more useful, and easier to use.
      After you have chosen a library, fork the library's source code, adjust its build system to run the Checker Framework, and add annotations to it until the type-checker issues no warnings.
      Before you get started, be sure to read How to get started annotating legacy code. More generally, read the relevant sections of the Checker Framework manual.
      Choosing a library to annotate
      There are several ways to choose a library to annotate:
      The best way to choose a library is to try to annotate a program and notice that library annotations are needed in order to type-check the program.
      Alternately, you can choose a popular Java library.
      When annotating a library, it is important to type-check both the library and at least one client that uses it. Type-checking the client will ensure that the library annotations are accurate.
      Whatever library you choose, you will need to deeply understand its source code. You will find it easier to work with a library that is well-designed and well-documented.
      You should choose a library that is not already annotated. There are two exceptions to this.
      A library might be annotated for one type system, but you add annotations for a different type system. One advantage of this is that the library's build system is already set up to run the Checker Framework. You can tell which type systems a library is annotated for by examining its source code.
      A library might be annotated, but the annotations have not been verified by running the type-checker on the library source code. You would verify that the annotations in the library are correct.
      Guava library
      Guava is already partially annotated with nullness annotations — in part by Guava's developers, and in part by the Checker Framework team. However, Guava does not yet type-check without errors. Doing so could find more errors (the Checker Framework has found nullness and indexing errors in Guava in the past) and would be a good case study to learn the limitations of the Nullness Checker.
      
      ~~~~~~~~~~
      Create a new type system
      The Checker Framework is shipped with about 20 type-checkers. Users can create a new checker of their own. However, some users don't want to go to that trouble. They would like to have more type-checkers packaged with the Checker Framework for easy use.
      Each of these projects requires you to design a new type system, implement it, and perform case studies to demonstrate that it is both usable and effective in finding/preventing bugs.
      Ownership type system
      The lightweight ownership mechanism of the Resource Leak Checker is not implemented as a type system, but it should be. That would enable writing ownership annotations on generic type arguments, like List<@Owning Socket>. It would also enable changing the Resource Leak Checker so that non-@Owning formal parameters do not have their @MustCall annotation erased.
      We have some notes on possible implementation strategies.
      Non-Empty Checker for precise handling of Queue.peek() and poll()
      The Nullness Checker issues a false positive warning for this code:
      import java.util.PriorityQueue;
      import org.checkerframework.checker.nullness.qual.NonNull;
      
      public class MyClass {
          public static void usePriorityQueue(PriorityQueue<@NonNull Object> active) {
              while (!(active.isEmpty())) {
                  @NonNull Object queueMinPathNode = active.peek();
              }
          }
      }
      The Checker Framework does not determine that active.peek() returns a non-null value in this context.
      The contract of peek() is that it returns a non-null value if the queue is not empty and the queue contains no null values.
      To handle this code precisely, the Nullness Checker needs to know, for each queue, whether it is empty. This is analogous to how the Nullness Checker tracks whether a particular value is a key in a map.
      It should be handled the same way: by adding a new subchecker, called the Nonempty Checker, to the Nullness Checker. Its types are:
      @UnknownNonEmpty — the queue might or might not be empty
      @NonEmpty — the queue is definitely non-empty
      There is a start at this type-checker in branch nonempty-checker. It:
      defines the annotations
      creates the integration into the Nullness Checker
      However, it is not done. (In fact, it doesn't even compile.) For information about what needs to be done, see issue #399.
      When you are done, the Nullness Checker should issue only the // :: diagnostics from checker/tests/nullness/IsEmptyPoll.java — no more and no fewer. You can test that by running the Nullness Checker on the file, and when you are done you should delete the // @skip-test line so that the file is run as part of the Checker Framework test suite.
      Iteration Checker to prevent NoSuchElementException
      A Java program that uses an Iterator can throw NoSuchElementException if the program calls next() on the Iterator but the Iterator has no more elements to iterate over. Such exceptions even occur in production code (for example, in Eclipse's rdf4j).
      We would like a compile-time guarantee that this run-time error will never happen. Our analysis will statically determine whether the hasNext() method would return true. The basic type system has two type qualifiers: @HasNext is a subtype of @UnknownHasNext.
      A variable's type is @HasNext if the program calls hasNext() and it returns true. Implementing this is easy (see the dataflow section in the "How to create a new checker" chapter). The analysis can also permit some calls to next() even if the programmer has not called hasNext(). For example, a call to next() is permitted on a newly-constructed iterator that is made from a non-empty collection. (This special case could build upon the Non-Empty Checker mentioned above.) There are probably other special cases, which experimentation will reveal.
      Parts of this are already implemented, but it needs to be enhanced. Once case studies have demonstrated its effectiveness, then it can be released to the world, and a scientific paper can be written.
      Preventing injection vulnerabilities via specialized taint analysis
      Many security vulnerabilities result from use of untrusted data without sanitizing it first. Examples include SQL injection, cross-site scripting, command injection, and many more. Other vulnerabilities result from leaking private data, such as credit card numbers.
      We have built a generalized taint analysis that can address any of these problems. However, because it is so general, it is not very useful. A user must customize it for each particular problem.
      The goal of this project is to make those customizations, and to evaluate their usefulness. A specific research question is: "To what extent is a general taint analysis useful in eliminating a wide variety of security vulnerabilities? How much customization, if any, is needed?"
      The generalized taint analysis is the Checker Framework's a Tainting Checker. It requires customization to a particular domain:
      rename the @Tainted and @Untainted qualifiers to something more specific (such as @Private or @PaymentDetails or @HtmlQuoted), and
      annotate libraries.
      The first part of this project is to make this customization easier to do — preferably, a user will not have to change any code in the Checker Framework (the Subtyping Checker already works this way). As part of making customization easier, a user should be able to specify multiple levels of taint — many information classification hierarchies have more than two levels. For example, the US government separates information into four categories: Unclassified, Confidential, Secret, and Top Secret.
      The second part of this project is to provide several examples, and do case studies showing the utility of compile-time taint checking.
      Possible examples include:
      SQL injection
      OS command injection
      the @PrivacySource and @PrivacySink annotations used by the Meta Infer static analyzer.
      information flow
      many of the CWE/SANS most dangerous software programming errors (and the "on the cusp" ones too)
      For some microbenchmarks, see the Juliette test suite for Java from CWE.
      Warn about unsupported operations
      In Java, some objects do not fully implement their interface; they throw UnsupportedOperationException for some operations. One example is unmodifiable collections. They throw the exception when a mutating operation is called, such as add, addAll, put, remove, etc.
      The goal of this project is to design a compile-time verification tool to track which operations might not be supported. This tool will issue a warning whenever an UnsupportedOperationException might occur at run time. This helps programmers to avoid run-time exceptions (crashes) in their Java programs.
      The research questions include:
      Is it is possible to build a verification tool to prevent UnsupportedOperationException? What design is effective?
      How difficult is such a tool to use, in terms of programmer effort and number of false alarms?
      Are potential UnsupportedOperationException exceptions pervasive in Java programs? Is it possible to eliminate them?
      The methodology is:
      design a static (compile-time) analysis
      implement it
      evaluate it on open-source projects
      report bugs in the projects, and improve the tool
      Here is a possible design, as a pluggable type system.
        @Unmodifiable
             |
        @Modifiable
      In other words, the @Unmodifiable type qualifier is a supertype of @Modifiable. This means that a @Modifiable List can be used where an @Unmodifiable List is expected, but not vice versa.
      @Modifable is the default, and methods such as Arrays.asList and Collections.emptyList must be annotated to return the less-capable supertype.
      Overflow checking
      Overflow is when 32-bit arithmetic differs from ideal arithmetic. For example, in Java the int computation 2,147,483,647 + 1 yields a negative number, -2,147,483,648. The goal of this project is to detect and prevent problems such as these.
      One way to write this is as an extension of the Constant Value Checker, which already keeps track of integer ranges. It even already checks for overflow, but it never issues a warning when it discovers possible overflow. Your variant would do so.
      This problem is so challenging that there has been almost no previous research on static approaches to the problem. (Two relevant papers are IntScope: Automatically Detecting Integer Overflow Vulnerability in x86 Binary Using Symbolic Execution and Integer Overflow Vulnerabilities Detection in Software Binary Code.) Researchers are concerned that users will have to write a lot of annotations indicating the possible ranges of variables, and that even so there will be a lot of false positive warnings due to approximations in the conservative analysis. For example, will every loop that contains i++ cause a warning that i might overflow? That would not be acceptable: users would just disable the check.
      You can convince yourself of the difficulty by manually analyzing programs to see how clever the analysis has to be, or manually simulating your proposed analysis on a selection of real-world code to learn its weaknesses. You might also try it on good and bad binary search code.
      One way to make the problem tractable is to limit its scope: instead of being concerned with all possible arithmetic overflow, focus on a specific use case. As one concrete application, the Index Checker is currently unsound in the presence of integer overflow. If an integer i is known to be @Positive, and 1 is added to it, then the Index Checker believes that its type remains @Positive. If i was already Integer.MAX_VALUE, then the result is negative — that is, the Index Checker's approximation to it is unsound.
      This project involves removing this unsoundness by implementing a type system to track when an integer value might overflow — but this only matters for values that are used as an array index. That is, checking can be restricted to computations that involve an operand of type @IntRange). Implementing such an analysis would permit the Index Checker to extend its guarantees even to programs that might overflow.
      This analysis is important for some indexing bugs in practice. Using the Index Checker, we found 5 bugs in Google Guava related to overflow. Google marked these as high priority and fixed them immediately. In practice, there would be a run-time exception only for an array of size approximately Integer.MAX_INT.
      You could write an extension of the Constant Value Checker, which already keeps track of integer ranges and even determines when overflow is possible. It doesn't issue a warning, but your checker could record whether overflow was possible (this could be a two-element type system) and then issue a warning, if the value is used as an array index. Other implementation strategies may be possible.
      Here are some ideas for how to avoid the specific problem of issuing a warning about potential overflow for every i++ in a loop (but maybe other approaches are possible):
      The loop checks whether i == Integer.MAX_VALUE before incrementing. This wide-scale, disruptive code change is not acceptable.
      Make the default array size (the length of an unannotated array) be @ArrayLenRange(0, Integer.MAX_VALUE-1) rather than @UnknownVal, which is equivalent to @ArrayLenRange(0, Integer.MAX_VALUE-1). Now, every array construction requires the client to establish that the length is not Integer.MAX_VALUE. I don't have a feel for whether this would be unduly burdensome to users.
      Index checking for mutable length data structures
      The Index Checker is currently restricted to fixed-size data structures. A fixed-size data structure is one whose length cannot be changed once it is created, such as arrays and Strings. This limitation prevents the Index Checker from verifying indexing operations on mutable-size data structures, like Lists, that have add or remove methods. Since these kind of collections are common in practice, this is a severe limitation for the Index Checker.
      The limitation is caused by the Index Checker's use of types that are dependent on the length of data structures, like @LTLengthOf("data_structure"). If data_structure's length could change, then the correctness of this type might change.
      A naive solution would be to invalidate these types any time a method is called on data_structure. Unfortunately, aliasing makes this still unsound. Even more, a great solution to this problem would keep the information in the type when a method like add or remove is called on data_structure. A more complete solution might involve some special annotations on List that permit the information to be persisted.
      Another approach would be to run a pointer analysis before type-checking, then use that information for precise information about what lists might be changed by each call to add or remove. One possible pointer analysis would be that of Doop.
      This project would involve designing and implementing a solution to this problem.
      Nullness bug detector
      Verifying a program to be free of errors can be a daunting task. When starting out, a user may be more interested in bug-finding than verification. The goal of this project is to create a nullness bug detector that uses the powerful analysis of the Checker Framework and its Nullness Checker, but omits some of its more confusing or expensive features. The goal is to create a fast, easy-to-use bug detector. It would enable users to start small and advance to full verification in the future, rather than having to start out doing full verification.
      This could be structured as a new NullnessLight Checker, or as a command-line argument to the current Nullness Checker. Here are some differences from the real Nullness checker:
      No initialization analysis; the checker assumes that every value is initialized.
      No map key analysis; assume that, at every call to Map.get, the given key appears in the map.
      No invalidation of dataflow facts. Assume all method calls are pure, so method calls do not invalidate dataflow facts. Assume there is no aliasing, so field updates do not invalidate dataflow facts.
      Assume that boxing of primitives is @Pure: it returns the same value on every call.
      If the Checker Framework cannot infer a type argument, assume that the type argument is @NonNull.
      Each of these behaviors should be controlled by its own command-line argument, as well as being enabled in the NullnessLight Checker.
      The implementation may be relatively straightforward, since in most cases the behavior is just to disable some functionality of existing checkers.
      Tools such as FindBugs, NullAway, NullnessLight, and the Nullness Checker form a spectrum from easy-to-use bug detectors to sound verification. NullnessLight represents a new point in the design space. It will be interesting to compare these checkers:
      How much easier is it to use? For example, how many fewer annotations need to be written?
      How many more fewer true positives does it report — in other words, how many more false negatives does it suffer?
      How many fewer false positives does it report?
      Uber's NullAway tool is also an implementation of this idea (that is, a fast, but incomplete and unsound, nullness checker). NullAway doesn't let the user specify Java Generics: it assumes that every type parameter is @NonNull. Does Uber's tool provide users a good introduction to the ideas that a user can use to transition to a nullness type system later?
      
      ~~~~~~~~~~
      Enhance the toolset
      Indicate library methods that should be used instead
      Sometimes, the best way to avoid a checker warning is to use an annotated library method. Consider this code:
      @FqBinaryName String fqBinaryName = ...;
      @ClassGetName String componentType = fqBinaryName.substring(0, fqBinaryName.indexOf('['));
      The Signature String Checker issues a warning, because it does not reason about arbitrary string manipulations. The code is correct, but it is in bad style. It is confusing to perform string manipulations to convert between different string representations. It is clearer and less error-prone (the above code is buggy when fqBinaryName is not an array type!) to use a library method, and the checker accepts this code because the library method is appropriately annotated:
      import org.plumelib.reflection.Signatures;
      ...
      @ClassGetName String componentType = Signatures.getArrayElementType(fqBinaryName);
      However, users may not know about the library method. Therefore, the Checker Framework should issue a warning message, along with the error message, notifying users of the library method. For example, the Signature String Checker would heuristically mention the Signatures.getArrayElementType() method when it issues an error about string manipulation where some input is a FqBinaryName and the output is annotated as ClassGetName. It would behave similarly for other library methods.
      Improving error messages
      Compiler writers have come to realize that clarity of error messages is as important as the speed of the executable (1, 2, 3, 4). This is especially true when the language or type system has rich features.
      The goal of this project is to improve a compiler's error messages. Here are some distinct challenges:
      Some type errors can be more concisely or clearly expressed than the standard "found type A, expected type B" message.
      Some types are complex. The error message could explain them, or link to the manual, or give suggested fixes.
      Compiler messages currently show the effective type, which may be different than what the user wrote due to defaulting, inference, and syntactic sugar. For example, a user-written @IndexFor("a") annotation is syntactic sugar for @NonNegative @LTLengthOf("a"), and those types are the ones that currently appear in error messages. It might be good to show simpler types or ones that the user wrote.
      Some checkers combine multiple cooperating type systems; the Nullness Checker and the Index Checker are examples. If there is a problem with a variable's lower bound type, then its upper bound type should not be shown in the error message. This will make the message shorter and more specific, and avoid distracting the user with irrelevant information.
      When a checker has multiple type systems, a type error or the lack of one may depend on facts from multiple type systems, and this should be expressed to the user.
      Replace JavaParser by javac
      The Checker Framework uses JavaParser to parse a Java expressions. However, JavaParser is buggy and poorly maintained. The goal of this project is to replace every use of JavaParser by a use of javac-parse.
      Java expression parser
      A number of type annotations take, as an argument, a Java expression. The representation for these is as a JavaExpression. The goal of this project is to remove it.
      The JavaExpression class represents an AST. There is no need for the Checker Framework to define its own AST when the javac AST already exists and is maintained.
      The goals for the project include:
      Replace every use of JavaExpression by a use of the javac class class com.sun.tools.javac.tree.JCTree.JCExpression.html.
      Replace every use of a subclass of JavaExpression (listed in the "Direct Known Subclasses" section of the JavaExpression API documentation) by a use of a subclass of JCTree.JCExpression.html. For example, replace every use of MethodCall by JCTree.JCMethodInvocation.
      Replace the JavaExpressionParseUtil class and delete ExpressionToReceiverVisitor.
      Direct replacement of the classes is not possible, or we would have done it already. For example, JavaExpression contains some methods that javac lacks, such as isUnassignableByOtherCode. As a first step before doing the tasks listed above, you may want to convert these methods from instance methods of JavaExpression into static methods in JavaExpressions, making JavaExpression more like a standard AST that can be replaced by JavaParser classes. You also need to decide how to store the type field of JavaExpression, when JavaExpression is eliminated. An alternate design (or a partial step in the refactoring process) would be to retain the JavaExpression class, but make it a thin wrapper around javac classes that do most of the real work.
      Another aspect of this project is fixing the issues that are labeled "JavaExpression".
      Dataflow enhancements
      The Checker Framework's dataflow framework (manual here) implements flow-sensitive type refinement (local type inference) and other features. It is used in the Checker Framework and also in Error Prone, NullAway, and elsewhere.
      There are a number of open issues — both bugs and feature requests — related to the dataflow framework. The goal of this project is to address as many of those issues as possible, which will directly improve all the tools that use it.
      Side effect analysis, also known as purity analysis
      A side effect analysis reports what side effects a procedure may perform, such as what variable values it may modify. A side effect analysis is essential to other program analyses. A program analysis technique makes estimates about the current values of expressions. When a method call occurs, the analysis has to throw away most of its estimates, because the method call might change any variable. However, if the method is known to have no side effects, then the analysis doesn't need to throw away its estimates, and the analysis is more precise. Thus, an improvement to the foundational side effect analysis can improve many other program analyses.
      The goal of this project is to evaluate existing side effect analysis algorithms and implementations, in order to determine what is most effective and to improve them. The research questions include:
      What side effect analysis algorithms are most effective? What are their limitations?
      Can the most effective algorithms be combined to become even effective? Or can their limitations be overcome?
      How much does accurate side effect analysis improve other programming tasks?
      The methodology is to collect existing side effect analysis tools (two examples are Soot and Geffken); run them on open-source projects; examine the result; and then improve them.
      Javadoc support
      Currently, type annotations are only displayed in Javadoc if they are explicitly written by the programmer. However, the Checker Framework provides flexible defaulting mechanisms, reducing the annotation overhead. This project will integrate the Checker Framework defaulting phase with Javadoc, showing the signatures after applying defaulting rules.
      There are other type-annotation-related improvements to Javadoc that can be explored, e.g. using JavaScript to show or hide only the type annotations currently of interest.
      
      

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/checker-framework/
    idea_list_url: https://rawgit.com/typetools/checker-framework/master/docs/developer/new-contributor-projects.html

  - organization_id: 24
    organization_name: Chromium
    no_of_ideas:
    ideas_content: |
      1. Interaction to Next Paint (INP) "subparts" (Medium Size)




      Project Description
      The web performance specifications define how browsers are supposed to be behaving when it comes to the various web performance APIs, and the Core Web Vitals (CWV) program uses those apis to automatically measure the performance and UX of most websites in the world, and shares the data publicly via the Chrome User Experience Report (CrUX). Last year a new metric, Interaction to Next Paint (INP), was added to the CWV. This year, we would like your help to give developers additional insights into INP latency issues by also reporting INP "subparts", similar to what we did for LCP last year.  For example: a developer should know if any INP responsiveness issues are caused by: long input delay (maybe the page as blocked and busy)
      event processing times (maybe the interaction event listeners were too complex) rendering/pixel presentation delays (maybe the page content is too complex/bloated) …or other task scheduling issues. We’ve recently instrumented the measurement code (i.e. the Event Timing API) to measure these time points, but we need your help to finish the job: moving the data from Renderer code to Browser code (using Mojo IPC), adding it to field metrics reporting (UKM), and eventually helping teammates integrate into CrUX experimental data – while also writing tests for the above. This is an open source project and some of the contributions will be useful to developers for local lab testing, as well (as for other downstream browsers). Finally, there are also several stretch technical opportunities related to the Event Timing API / INP metric.
      Proposal Doc for more information: Link
      Location to ask project specific questions not answered above: here (preferred), or chromium-gsoc-project1@chromium.org
      Beginner Bugs: https://issues.chromium.org/issues/40858679 
      Requirements: C++, JavaScript, Performance tooling (DevTools performance panel). Time zone flexible but hosts are in Eastern time zone.

      ~~~~~~~~~~


      2. Android Virtual Printer Application (Large Size)


      Project Description
      An Android application which can pretend to be a printer (act as a virtual printer), which can handle interactions between Android devices, software and printer itself. Once this virtual printer Android application is installed, the Android device can detect a new printer, set up the printer via network connection, users can use it to print and change settings based on its capability, and the virtual printer can respond to print jobs. This virtual printer application should allow configuration on printer capabilities, and print job response, in this way it can be used to test different functionalities and validate handling errors.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project2@chromium.org
      Requirements: Android application development, Kotlin, Rust, Java, C++, C

      ~~~~~~~~~~

      3. Chrome Extension APIs (Large Size)

      Project Description
      We have a number of new APIs and features that we would like to add to the Chrome extensions platform but don't currently have prioritized (for example, improvements to the declarative network request API used for network filtering, and new features in the `chrome.sidePanel` API). Many of these involve work in the W3C WebExtensions Community Group. We would like to offer a contributor the opportunity to own a work item from this list.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here  (preferred), chromium-gsoc-project3@chromium.org
      Beginner Bugs: 
      Requirements: C++ for the Chromium codebase.  JavaScript needed for writing extensions.


      ~~~~~~~~~~


      4. Structured DNS Errors (Small Size)

      Project Description
      Chrome should implement enough of the emerging Public DNS Errors standards to allow public DNS servers to indicate to clients when certain DNS resolutions are blocked for legal reasons.
      https://github.com/mnot/public-resolver-errors
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project4@chromium.org
      Beginner Bugs: https://issues.chromium.org/hotlists/6689006
      Requirements: C++, Chromium Net Stack. Must overlap with US East Coast timezone.

      ~~~~~~~~~~


      5. FedCM API Test Coverage and Flakiness (Medium Size)

      Project Description
      The FedCM API comprises of multiple tests which require the bot to go through a federated authentication process. The project consists of documenting the existing test coverage, improving it, and fixing test flakiness.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project5@chromium.org
      Beginner Bugs: https://issues.chromium.org/issues/41482163
      Requirements: C++, as well as a little bit of HTML, JS, Python. US East is preferred but not required


      ~~~~~~~~~~


      6. Add 3rd Party Theme Support for Tab Groups (Small Size)

      Project Description
      Add an API to allow 3rd party theme developers to customize the colors used for tab groups.

      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project6@chromium.org
      Beginner Bugs: https://issues.chromium.org/issues/40711397, https://issues.chromium.org/issues/370416945 https://issues.chromium.org/issues/40929354
      Requirements: C++ prior experience is requested, ability to work in PST is requested.

      ~~~~~~~~~~

      7. ChromeOS Platform Input Device Quality Monitoring (Medium Size)

      Project Description
      ChromeOS devices generally contain user-input devices (touchscreens, touchpads, etc.) that contain firmware and communicate over internal buses. This firmware can have bugs, sometimes needs to be updated in the field, and the communication buses can show errors.

      Improved testing during development, and logging for production devices, can reduce shipped issues and reduction of in-field failures, and improved analysis leading to better OEM response to such issues.

      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project7@chromium.org
      Beginner Bugs: https://issuetracker.google.com/397520373, https://issuetracker.google.com/397521060, https://issuetracker.google.com/397520674
      Requirements: ChromeOS development tech stack (C++, shell scripts), experience or patience to learn ChromeOS build system and loading custom software onto a ChromeBook, moderate overlap with US-Pacific timezone.


      ~~~~~~~~~~


      8. Farfetchd: tracing/replay (Medium Size)

      Project Description
      Preloading files into memory (or prefetch) is a useful mechanism for improving application “cold start” performance. On ChromiumOS, this technique is used via ureadahead to speed up boot times by up to 20%. Farfetchd is a general purpose D-Bus service that allows prefetch of application binaries and associated resources and allows the optimization of cold start times.
      This proposal aims to add support to farfetchd for:
      1) Tracing: via tracefs, tracing file pages that are fetched for an application during a given workload and standardized tracepoints.
      2) Replay: Using the collected trace, preload the specific pages from disk and measure improvements in application performance.

      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project8@chromium.org
      Beginner Bugs: https://issuetracker.google.com/issues/397539111, https://issuetracker.google.com/issues/397539767
      Requirements: C++, linux kernel, tracing, no TZ requirements

      ~~~~~~~~~~


      9. Develop fwupd plugin to handle touch firmware updates (Large Size)

      Project Description
      A number of components like storage devices use fwupd to handle firmware updates in ChromeOS. The platform inputs team would like to start using fwupd to manage and handle firmware updates for touch controllers. This project aims to build out a handful of fwupd plugins for touch controllers.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project9@chromium.org
      Beginner Bugs: https://issuetracker.google.com/issues/172340186, https://issuetracker.google.com/issues/397567795
      Requirements: C, C++

      ~~~~~~~~~~


      10. Debug WebUI For Tabstrip states (Medium Size)

      Project Description
      Tabstrip state and session states for browsers are complicated and rely on correct ordering of tabs in the tabstrip model, groups and sessions to restore tabs in the right position and selection state. As a result this has resulted in numerous bugs and it is often a pain point to figure out if it is a tabstrip model issue or a client issue. A webUI that captures the live state of the backend of the browser and tabstrip models will be really helpful in finding issues and edge cases.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project10@chromium.org
      Beginner Bugs: WebUI TabStrip Bug, Print Preview + Tab Dragging Bug, Keyboard Shortcut Bug
      Requirements: Familiarity with HTML, CSS and JS, Proficiency in C++ codebases and concepts, Working from PST 9:00 AM - 5:00 PM hours

      ~~~~~~~~~~

      11. Improve Chromium Web Audio Testing (Medium Size)

      Project Description
      The audit.js WebAudio test helper library was introduced in 2016 and is currently used by over 300 test files in Chromium’s blink/web_test directory. This library was initially created to run tests sequentially with callback/promise support and to provide utility functions for audio-specific assertions. However, the W3C Test Harness (testharness.js) offers superior support for test runners and assertions, rendering many features in audit.js redundant. Removing audit.js from Chromium would reduce resource consumption in the Chrome team’s Continuous Integration Infrastructure and allow us to leverage the well-supported and well-maintained W3C Test Harness library.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project11@chromium.org
      Beginner Bugs: crbug.com/396477778
      Requirements: Proficiency in JavaScript (reading, writing, debugging), Familiarity with HTML and CSS

      ~~~~~~~~~~
      12. ChromeStatus Search UI Enhancement Project (Medium Size)

      Project Description
      Enhance the search functionality on chromestatus.com and webstatus.dev with several new auto-complete details.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project12@chromium.org
      Beginner Bugs: https://github.com/GoogleChrome/chromium-dashboard/issues?q=is%3Aissue%20state%3Aopen%20label%3Agoodfirstbug
      Requirements: HTML, CSS, TypeScript, Python, Go, Web components, UX / Usability

      ~~~~~~~~~~


      13. Enhancing the webstatus.dev User Experience (Medium Size)


      Project Description
      webstatus.dev is a valuable resource for web developers, providing up-to-date information on the browser compatibility of various web technologies. This project aims to enhance the user experience of webstatus.dev by making it more accessible and user-friendly across a wider range of devices and user preferences, with a particular focus on mobile devices and dark mode support. The project will involve redesigning key pages (overview, feature detail, and stats) for optimal mobile viewing, implementing a comprehensive dark theme, and ensuring compatibility with the Lit framework, Google Charts, and Shoelace components.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project13@chromium.org
      Beginner Bugs: https://github.com/GoogleChrome/chromium-dashboard/issues?q=is%3Aissue%20state%3Aopen%20label%3Agoodfirstbug
      Requirements: Lit, TypeScript, HTML, CSS Google Charts, Shoelace Web Test Runner (for unit tests) Playwright (for E2E testing), Docker (for local environment provisioning)

      ~~~~~~~~~~


      14. WebGPU Texel Buffers (Medium Size)


      Project Description
      WebGPU is a new graphics API, shipped in Chromium in 2023, that brings modern GPU capabilities to the web. The goal of this project is to implement new functionality in Chromium to further advance the capabilities of WebGPU, allowing a broader range of applications to target the API. Specifically, the "texel buffers" feature has already been proposed at the W3C standards committee, and the next steps are to prototype the functionality so that we can get feedback from real users and push the standardization process forwards.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project14@chromium.org
      Beginner Bugs: crbug.com/42250870 and crbug.com/42250968
      Requirements: C++ proficiency, timezone compatible with North American mentors


          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/chromium/
    idea_list_url: https://docs.google.com/presentation/d/1ozDiULkf2Gi4HH1XA_Ad9O7rQpP9SP0yPEs_dpcBZKM/edit?usp=sharing

  - organization_id: 25
    organization_name: CircuitVerse.org
    no_of_ideas: 7
    ideas_content: |
      
      Project 1: Circuit Management & Performance Enhancement
      Duration: 175 hours
      Difficulty: Medium
      Technologies: Ruby on Rails, JavaScript
      This project focuses on refining the organizational structure and performance of the CircuitVerse platform. The objective is to enhance user efficiency by introducing a systematic approach to circuit management and optimizing backend operations. Participants will implement a folder-based system for subcircuits, enabling users to categorize their work effectively. Additionally, the project includes developing a feature for circuits with group-specific visibility, ensuring accessibility within designated teams while maintaining privacy. Performance improvements will involve optimizing Ruby on Rails N+1 queries. The scope also extends to creating a comprehensive circuits explore page with search functionality, integrating a leaderboard for weekly contests, and enabling locale switching from the homepage to improve accessibility.
      Learning Path:
      Acquire proficiency in Ruby on Rails through The Odin Project’s Full-Stack Ruby on Rails Course, covering foundational and advanced concepts.
      Enhance Rails expertise with Pragmatic Studio’s Rails Course for structured, practical training.
      Develop JavaScript skills using JavaScript.info to support frontend enhancements.
      Possible Mentors: Vaibhav Upreti, Smriti Garg

      ~~~~~~~~~~
      Project 2: Desktop Application & Vue Frontend Updates
      Duration: 175 hours
      Difficulty: Medium
      Technologies: VueJS, Ruby on Rails, TypeScript, JavaScript
      This project aims to extend CircuitVerse’s reach by improving our lightweight desktop application using Tauri, ensuring efficient performance across operating systems, and establishing an automated release pipeline for streamlined deployment. Participants will refactor web-based components into reusable, platform-agnostic units suitable for both web and desktop environments, optimizing resource usage for desktop contexts. A reliable update mechanism will be implemented to deliver future improvements seamlessly. The project also entails modernizing the frontend by converting the JavaScript codebase to TypeScript, replacing jQuery with Vue’s reactivity system, and addressing issues in the Verilog module and layout rendering. Enhancements to the TestBench will improve usability and output compatibility, supported by thorough testing with Vitest. Synchronization with the main CircuitVerse repository will ensure consistency across platforms.
      Learning Path:
      Gain expertise in VueJS via the Vue.js Official Guide for reactive frontend development.
      Study TypeScript fundamentals at TypeScript Documentation to facilitate codebase migration.
      Learn Tauri application development through its Official Docs.
      Prepare for testing with Vitest Documentation covering unit, integration, and end-to-end tests.
      Possible Mentors: Vedant Jain, Aryann Dwivedi, Niladri Adhikary

      ~~~~~~~~~~
      Project 3: Migrate to View Components & Improve Search Experience
      Duration: 175 hours
      Difficulty: Easy
      Technologies: HTML, CSS, JavaScript, Figma, Ruby on Rails
      This project focuses on modernizing CircuitVerse’s technical foundation and enhancing the user experience. Participants will migrate UI elements to ViewComponents for better maintainability and scalability. Responsive design principles will be applied to ensure compatibility across devices. Candidates will also utilize Figma to design and implement UI improvements while refactoring CSS to reduce redundancy and effectively use grid and flexbox with Bootstrap utility classes.
      Additionally, the project includes an initiative to improve the search experience within CircuitVerse. This will involve analyzing existing search functionality, optimizing query performance, and enhancing the UI/UX to make search results more intuitive and accessible.
      Learning Path:
      Build foundational skills in HTML, CSS, and JavaScript with FreeCodeCamp’s Responsive Web Design.
      Learn UI/UX design with Figma through Figma’s Official Tutorials.
      Study Rails ViewComponents at ViewComponent Docs and Hotwire via Hotwire Docs.
      Possible Mentors: Aman Asrani, Siddhant-K-code

      ~~~~~~~~~~
      Project 4: Assignment Suite Enhancement
      Duration: 175 hours
      Difficulty: Easy
      Technologies: Ruby on Rails, JavaScript
      This project seeks to advance CircuitVerse’s educational tools by enhancing classroom and assignment management capabilities. The scope includes developing a multi-level classroom structure, allowing students to form subgroups for collaborative projects, and creating a flexible assignment management system for both individual and group submissions. Participants will introduce features such as pre-built circuit submissions with integrated test cases, incorporate auto-verification from practice sessions, and refine the assignment submission process for efficiency. Integration with Canvas LMS will be improved to strengthen CircuitVerse’s utility in academic settings, supporting educators and students with robust, user-friendly tools.
      Learning Path:
      Develop Rails proficiency with The Odin Project’s Ruby on Rails Course.
      Enhance JavaScript knowledge at MDN Web Docs for dynamic functionality.
      Review Canvas LMS integration through its Developer Docs.
      Learning tools interoperability (LTI)[https://en.wikipedia.org/wiki/Learning_Tools_Interoperability]
      Possible Mentors: Aman Asrani, Siddharth Asthana, Yashika Jotwani

      ~~~~~~~~~~
      Project 5: Enhanced Verilog Support & Stability
      Duration: 175 hours
      Difficulty: Hard
      Technologies: JavaScript, Canvas
      This project targets the enhancement of CircuitVerse’s Verilog module and overall simulator stability. The goal is to refine the Verilog interface, making it more intuitive and enabling users to generate, view, edit, and test circuits comprehensively. Detailed documentation will accompany these improvements to assist users. Additional enhancements include adding play/pause functionality to simulations, implementing a full-screen view for the Boolean Logic Table This project requires a strong focus on technical precision to strengthen a critical component of the platform.
      Learning Path:
      Master JavaScript for simulation logic with Eloquent JavaScript.
      Learn Canvas rendering techniques at MDN Canvas Tutorial.
      Possible Mentors: Vedant Jain, Niladri Adhikary, Josh Varga

      ~~~~~~~~~~
      Project 6: Open Hardware Component Library
      Duration: 90 hours
      Difficulty: Hard
      Technologies: JavaScript, Ruby on Rails
      This project aims to expand CircuitVerse’s digital component library and introduce hardware integration capabilities. Participants will enrich the platform by adding components such as shift registers, sensors, and counters, broadening its appeal to hardware-focused users. The initiative also involves enabling serial device connectivity to facilitate interaction with physical hardware. Though shorter in duration, this project demands a high level of technical skill and innovation to advance CircuitVerse’s utility in hardware education.
      Learning Path:
      Strengthen Rails knowledge with Rails Guides.
      Study hardware integration concepts via Arduino Tutorials.
      Deepen JavaScript proficiency at You Don’t Know JS.
      Possible Mentors: Smriti Garg
      ~~~~~~~~~~
      Project 7: Flutter Upgrade
      Duration: 90 hours
      Difficulty: Easy
      Technologies: Dart, JavaScript
      This project focuses on updating the CircuitVerse mobile application to ensure compatibility with the latest Flutter framework. Participants will upgrade the app to the current Flutter version, optimizing its performance, and implement circuit embedding functionality to enhance mobile usability. This concise project offers an opportunity to contribute to the platform’s mobile presence with a focus on modern development practices.
      Learning Path:
      Learn Flutter development at Flutter Official Docs.
      Study Dart essentials with the Dart Language Tour.
      Review JavaScript fundamentals at MDN JavaScript Guide.
      

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/circuitverse.org/
    idea_list_url: https://github.com/CircuitVerse/CircuitVerse/wiki/GSoC'25-Project-List


  - organization_id: 26
    organization_name: CloudCV
    no_of_ideas: 3
    ideas_content: |
     
      
      Enhanced Test Suite and Improved User Experience
      SQL Django AngularJS AWS
      This project is focused on significantly improving EvalAI’s usability by enhancing exsiting comprehensive test suite alongside a series of user experience enhancements. By increasing our test coverage, automating critical workflows, and refining the platform’s interface and documentation, this initiative aims to create a more robust, user-friendly, and resilient environment for both challenge hosts and participants.
      The enhanced test suite will ensure that all core functionalities, from challenge creation to submission processing are verified, reducing bugs and increasing system reliability. In parallel, targeted user experience improvements will simplify navigation, enhance error reporting, and streamline user interactions, leading to a more intuitive and supportive EvalAI ecosystem.
      Project Size: Medium (175 hours)
      Difficulty Rating: Medium
      Participate in the issue corresponding to this project to get started
      Enhanced Test Suite and Improved User Experience together with gautamjajoo Yes, let's do it

      ~~~~~~~~~~
      Mitigating Biases & Prompt Effects in Vision-Language Models
      Python PyTorch/TensorFlow NLP Vision-Language Models LLMs Bias and Fairness in AI
      This research project aims to investigate how prompt engineering influences the behaviour and outputs of Vision-Language Models (VLLMs), with a particular focus on the emergence and amplification of biases. By systematically studying the relationship between prompt formulations and model responses, the project seeks to uncover the mechanisms through which biases are introduced and propose effective mitigation strategies.
      Project Size: Medium (175 hours)
      Difficulty Rating: Medium to Advanced
      Sorry but currently we don't have any mentoring project ...

      ~~~~~~~~~~
      RAG-Based Chat Bot for Enhanced Challenge Support
      Python Natural Language Processing (NLP) Machine Learning Retrieval Augmented Generation (RAG) Django SQL
      This project aims to enhance the user experience for both challenge hosts and participants by developing an intelligent, RAG (Retrieval Augmented Generation) based chatbot. The chatbot will efficiently address queries related to challenge hosting, guidelines, troubleshooting, and FAQs. By integrating state-of-the-art NLP techniques with robust retrieval mechanisms, the solution will ensure prompt, accurate, and context-aware responses that reduce support overhead and streamline communication.
      Using the RAG approach, the chatbot will retrieve relevant information from challenge documentation and combine it with generative models to create coherent and helpful answers. This will empower hosts to manage challenges more effectively and assist participants in resolving queries, ultimately contributing to a smoother and more interactive challenge experience.
      Project Size: Medium (175 hours)
      Difficulty Rating: Medium
      Participate in the issue corresponding to this project to get started
      RAG-Based Chat Bot for Enhanced Challenge Support together with gautamjajoo Yes, let's do it
      

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cloudcv/
    idea_list_url: https://gsoc.cloudcv.org/


  - organization_id: 27
    organization_name: D Language Foundation
    no_of_ideas: 4
    ideas_content: |
      Translate DRuntime Hooks to Templates
      Mentor: Teodor Duțu

      Spec	Details
      Difficulty Level	hard
      Project Duration	350 hours
      Number of Contributors	2
      Prerequisites	Familiarity with compilers, build systems; low-level programming
      Description
      High-level language constructs are often compiled into lower-level function calls that implement the same functionality. This process is called lowering It simplifies compiler design by offloading complex code into simpler function calls. These functions are implemented in D’s runtime library (called DRuntime) and are commonly called runtime hooks.

      Below is an example of such a runtime hook:

      struct S { ... }

      S[3] a, b;

      // Original code
      b = a;  // b is a copy of a 

      // Resulting DRuntime hook
      _d_arrayassign_l(b, a)  // copies a into b

      _d_arrayassign_l handles array (re)allocation and assignment operators according to the types of a and b (S[3]) to simplify the compiler’s work.

      As shown in the example above, runtime hooks require type information, such as which assignment operator to call and the arrays’ sizes. D supports templates, but runtime hooks pre-date the introduction of templates to D. Therefore, they retrieve the necessary type information at runtime by receiving an extra argument, whose type is TypeInfo. This object contains the size, constructors, destructors and overloaded operators of a given type. However, because this information is processed at run time, this approach is slower than the alternative of having the compiler send it to the runtime hook via template arguments. Due to D’s high flexibility regarding metaprogramming, translating each hook to a template function would allow its code to specialise according to the types of its arguments at compile time.

      In general, these are the steps required to convert a runtime hook to a template:

      Implement a new template version of the hook in DRuntime.
      Change the lowering in the compiler to use the new hook.
      Run a benchmark to measure the increase in performance generated by using the new hook.
      Remove the old hook from DRuntime.
      The hooks that are yet to be templated can be split into 2 categories:

      rt/aa.d implements associative arrays as language builtins. This module is made up of multiple hooks, all of them using TypeInfo. Therefore, this module is to be reimplemented using templates. Associative arrays are defined as an opaque structure called Impl. It receives a TypeInfo argument in its constructor. This structure is accessed via the runtime hooks listed below. The plan for this direction is the following:
      Template the hooks below and temporarily extract the TypeInfo structure required by Impl from the template arguments using typeid. Each hook will require changes to DRuntime and the compiler.
      Template the Impl structure and modify the previously templated hooks to use the template arguments itself instead of TypeInfo.
      The list of hooks for associative arrays is:

      _aaApply
      _aaApply2
      _aaDelX
      _aaEqual
      _aaGetRvalueX
      _aaGetX
      _aaInX
      _aaLen
      _d_assocarrayliteralTX

      Somewhat more independent hooks, which still have interdependencies. They are mostly implemented in rt/lifetime.d. A goal of this project is to remove this file and replace all its code with templated implementations. Each hook can be handled individually and separately. There was previous work on _d_arrayliteralTX so it might be a good starting point. Another promising starting point are _d_arrayset{capacity,lengthT,lengthiT} or _d_arrayappendcTX. The latter three already have wrapper template hooks that call the functions from rt/lifetime.d. What is needed in their case is to fully move the underlying implementation to the template hooks. The full list of hooks is below:

      _d_arraysetcapacity
      _d_arraysetlengthiT
      _d_arraysetlengthT
      _d_arrayshrinkfit
      _d_arrayappendcTX
      _d_arrayliteralTX
      _d_interface_cast
      _d_isbaseof
      _d_isbaseof2
      _adEq2

      Resources
      Initial project proposal and discussion
      PRs converting some of the DRuntime hooks to templates
      Weekly reports regarding the earlier work
      DConf presentations from 2022 and 2024 on the work on DRuntime hooks
      Instructions on how to build the reference compiler - DMD

      ~~~~~~~~~~

      Separate Semantic Routines From AST Nodes
      Mentor: Razvan Nitu

      Spec	Details
      Difficulty Level	easy-medium
      Project Duration	175 hours
      Number of Contributors	1
      Prerequisites	Familiarity with compiler organization, visitor pattern, object oriented programming
      Description
      In the DMD compiler codebase, AST nodes are defined as classes within various files. The ideal structure for these nodes is to have minimal fields and methods focused solely on field queries. However, the current state of the DMD frontend deviates from this ideal. AST nodes are laden with numerous methods that either perform or are dependent on semantic analysis. Furthermore, many AST node files contain free functions related to semantic analysis. Our objective is to decouple AST nodes from these functions.

      How to start working on this project
      Clone the compile repository - check this guideline.
      Choose an AST node file: start by selecting a file from this list of AST node definition files.
      Examine Imports: open your chosen file and scrutinize the top-level imports.
      Isolate semantic imports: temporarily comment out one of the imports that includes semantic routines, particularly those ending in sem (e.g., dsymbolsem, expressionsem, etc.).
      Build and identify dependencies: compile DMD and observe any unresolved symbols that emerge.
      Relocate functions: shift the functions reliant on the unresolved symbols to the semantic file where the import was commented out.
      Move and test a function: select a function for relocation and ensure it functions correctly in its new location.
      Submit a Pull Request: Once you’re satisfied with the changes, create a PR that follows the guidelines.
      Check this PR for an illustration of the above steps.

      Sometimes, more intricate solutions are required. For instance, if an overridden method in an AST node calls a semantic function, it can’t be simply relocated. In these cases, using a visitor to collate all overrides, along with the original method, into the appropriate semantic file is the way forward. A notable instance of this approach is detailed in this pull request.

      Other complex scenarios may arise, especially when dealing with AST nodes that interact with the backend. Finding solutions to those will be the fun part of the project.

      This project helps advance the development of the compiler library by creating a clear separation between compilation phases.

      This project is ideal for someone that has no prior experience with real-life compilers but wants to start by doing valuable work.

      Resources
      Compiler codebase
      List of files that need to have semantic separated out of them
      How to start
      Building the compiler

      ~~~~~~~~~~

      Performance Regression Publisher
      Mentor: Dennis Korpel

      Spec	Details
      Difficulty Level	easy-medium
      Project Duration	175 hours
      Number of Contributors	1
      Prerequisites	Github actions, webdev, performance testing
      Description
      The D compiler currently does not have an automated performance regression test. Oftentimes pull requests that claim to improve compiler performance are being made (be it spatial or temporal). However, it’s up to the reviewer to actually believe the committer or to test things on their own. To make things simpler and more transparent, we want to implement a bot that monitors all the pull requests made to the compiler codebase and analyzes the compiler’s performance with and without the pull request.

      The list of stats should include, but not be limited to:

      size of some predefined binaries (like a “hello world” program)
      compile time of popular projects
      compiler size
      runtime of test suite
      Adding more performance tests, such as stress tests also falls under the scope of this project. Ideally the bot could also store a history of performance regressions within a web page.

      Project milestones:
      Analyze the best way to publish the results: it could be a GitHub action, a bot that sends data to a website. Depending on your skills and preferences, we expect you propose something and toghether we will decide what’s best.
      Implement the initial part that simply collects how long running the testing pipeline took and publishes the result.
      Decide on other metrics that need to be collected and implement them.
      Add more stress tests to the compiler testing suite.
      Resources
      Compiler pull request queue

      ~~~~~~~~~~
      Json Library
      Mentor: Adam Wilson

      Spec	Details
      Difficulty Level	medium
      Project Duration	175 hours
      Number of Contributors	1
      Prerequisites	json, parsers, object oriented programming
      Description
      D currently does not have a json parser integrated in the standard library and it is the year 2025. There are 3rd party libraries that implement some pieces to a certain extent, however they are not at industry level requirements. The purpose of this project is to create a json library that offers all the facilities required for working with json objects. The goal is to eventually integrate it in the standard library, however, an initial step is to publish as a dub (D package manager) package.

      Although there is no json library in the D ecosystem, there are multiple json parser implementations. We can start by picking up a notable json parser as a basis for our implementation. Next, we can built higher level functionalities on top of the existing parser.

      This project is of high priority and impact, as json objects essentially rule the world.

      Project Milestones
      Initial Package Setup and Porting from jsoniopipe.
      Object Model Design/Implementation.
      Object Model Serialization/Deserialization
      Streaming Serialization/Deserialization
      Stretch Goal: Object Serialization/Deserialization (Direct serialization/deserialization from a D object instead of using the object model.)
      Resources
      jsoniopipe

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/d-language-foundation/
    idea_list_url: https://dlang.github.io/GSoC/gsoc-2025/project-ideas.html


  - organization_id: 28
    organization_name: DBpedia
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/dbpedia/
    idea_list_url: https://forum.dbpedia.org/tag/gsoc2025-ideas

  

  - organization_id: 29
    organization_name: Dart
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/dart/
    idea_list_url: https://github.com/dart-lang/sdk/blob/main/docs/gsoc/Dart-GSoC-2025-Project-Ideas.md


  - organization_id: 30
    organization_name: Data for the Common Good
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/data-for-the-common-good/
    idea_list_url: https://docs.pedscommons.org/GSoC/ideas
  

  - organization_id: 31
    organization_name: Debian
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/debian/
    idea_list_url: https://wiki.debian.org/SummerOfCode2025/Projects

  - organization_id: 32
    organization_name: DeepChem
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/deepchem/
    idea_list_url: https://forum.deepchem.io/t/deepchem-gsoc-2025-project-ideas/1568

  - organization_id: 33
    organization_name: Department of Biomedical Informatics, Emory University
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/department-of-biomedical-informatics-emory-university/
    idea_list_url: https://github.com/NISYSLAB/Emory-BMI-GSoC/?tab=readme-ov-file#list-of-ideas

  - organization_id: 34
    organization_name: Django Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/django-software-foundation/
    idea_list_url: https://code.djangoproject.com/wiki/SummerOfCode2025

  - organization_id: 35
    organization_name: Drupal Association
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/drupal-association/
    idea_list_url: https://www.drupal.org/project/gsoc


  - organization_id: 36
    organization_name: Eclipse Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/eclipse-foundation/
    idea_list_url: https://gitlab.eclipse.org/eclipsefdn/emo-team/gsoc-at-the-ef/-/issues/?sort=due_date&state=opened&label_name%5B%5D=Project%20Idea&label_name%5B%5D=GSoC%202025&first_page_size=50


  - organization_id: 37
    organization_name: Electron
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/electron/
    idea_list_url: https://electronhq.notion.site/Electron-Google-Summer-of-Code-2025-Ideas-List-1851459d1bd1811894dad8b48a68596d

  - organization_id: 38
    organization_name: FFmpeg
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ffmpeg/
    idea_list_url: https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2025

  - organization_id: 39
    organization_name: FLARE
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/flare/
    idea_list_url: https://github.com/mandiant/flare-gsoc/blob/2025/doc/project-ideas.md


  - organization_id: 40
    organization_name: FOSSASIA
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fossasia/
    idea_list_url: https://docs.google.com/document/d/1Tz1KxYefreKzBr98C4vbCv9UwnchZoyTwO8rBrz_lmg/edit?tab=t.0#heading=h.9sjk3ie7l2o2
  

  - organization_id: 41
    organization_name: FOSSology
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fossology/
    idea_list_url: https://fossology.github.io/gsoc/docs/2025/GSoC-projects

  - organization_id: 42
    organization_name: Fedora Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fedora-project/
    idea_list_url: https://docs.fedoraproject.org/en-US/mentored-projects/gsoc/2025/ideas/

  - organization_id: 43
    organization_name: Fortran-lang
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fortran-lang/
    idea_list_url: https://github.com/fortran-lang/webpage/wiki/GSoC-2025-Project-ideas

  - organization_id: 44
    organization_name: Free and Open Source Silicon Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/free-and-open-source-silicon-foundation/
    idea_list_url: https://fossi-foundation.org/gsoc/gsoc25-ideas

  - organization_id: 45
    organization_name: FreeCAD
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/freecad/
    idea_list_url: https://wiki.freecad.org/Google_Summer_of_Code_2025


  - organization_id: 46
    organization_name: GNOME Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnome-foundation/
    idea_list_url: https://gsoc.gnome.org/2025/


  - organization_id: 47
    organization_name: GNSS-SDR
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnss-sdr/
    idea_list_url: https://gnss-sdr.org/google-summer-code-2025-ideas-list/

  - organization_id: 48
    organization_name: GNU Compiler Collection (GCC)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-compiler-collection-(gcc)/
    idea_list_url: https://gcc.gnu.org/wiki/SummerOfCode

  - organization_id: 49
    organization_name: GNU Image Manipulation Program
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-image-manipulation-program/
    idea_list_url: https://developer.gimp.org/core/internship/ideas/


  - organization_id: 50
    organization_name: GNU Octave
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-octave/
    idea_list_url: https://wiki.octave.org/Summer_of_Code_-_Getting_Started#Suggested_projects
  

  - organization_id: 51
    organization_name: GNU Radio
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-radio/
    idea_list_url: https://wiki.gnuradio.org/index.php?title=GSoCIdeas

  - organization_id: 52
    organization_name: GRAME
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/grame/
    idea_list_url: https://github.com/grame-cncm/faustideas

  - organization_id: 53
    organization_name: GeomScale
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/geomscale/
    idea_list_url: https://github.com/GeomScale/gsoc25/wiki/table-of-proposed-coding-projects

  - organization_id: 54
    organization_name: Git
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/git/
    idea_list_url: https://git.github.io/SoC-2025-Ideas/

  - organization_id: 55
    organization_name: GitLab
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gitlab/
    idea_list_url: https://gitlab.com/gitlab-org/developer-relations/contributor-success/google-summer-of-code-2025/-/issues


  - organization_id: 56
    organization_name: Google DeepMind
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/google-deepmind/
    idea_list_url: https://goo.gle/deepmind-gsoc-projects-2025


  - organization_id: 57
    organization_name: Graphite
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/graphite/
    idea_list_url: https://graphite.rs/volunteer/guide/student-projects/

  - organization_id: 58
    organization_name: Haskell.org
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/haskell.org/
    idea_list_url: https://summer.haskell.org/ideas.html

  - organization_id: 59
    organization_name: HumanAI
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/humanai/
    idea_list_url: https://humanai.foundation/


  - organization_id: 60
    organization_name: INCF
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/incf/
    idea_list_url: https://docs.google.com/document/d/1T7U4nYFbVbZuIUw_2bB89YASj-ra26hRNtG2a20Gjlw/edit?tab=t.0
  

  - organization_id: 61
    organization_name: IOOS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ioos/
    idea_list_url: https://github.com/ioos/gsoc/blob/main/2025/ideas-list.md

  - organization_id: 62
    organization_name: Inkscape
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/inkscape/
    idea_list_url: https://gitlab.com/inkscape/inkscape/-/blob/master/doc/gsoc/summerofcode.md

  - organization_id: 63
    organization_name: International Catrobat Association
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/international-catrobat-association/
    idea_list_url: https://developer.catrobat.org/pages/development/google-summer-of-code/2025/

  - organization_id: 64
    organization_name: Internet Archive
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/internet-archive/
    idea_list_url: https://docs.google.com/document/d/1oHNwPNYmHV5q3puBfv6IQFs-4gTe9XLN2iz2Lgse-1k/edit?tab=t.0

  - organization_id: 65
    organization_name: Internet Health Report
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/internet-health-report/
    idea_list_url: https://github.com/InternetHealthReport/gsoc/blob/main/ideas.md


  - organization_id: 66
    organization_name: Invesalius
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/invesalius/
    idea_list_url: https://github.com/invesalius/gsoc/blob/main/gsoc_2025_ideas.md


  - organization_id: 67
    organization_name: JAX and Keras
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jax-and-keras/
    idea_list_url: https://docs.google.com/document/d/16uAZEldrXUBHjrszSO22Hr81OmVSjzVXza0szZl6Fxw/edit?usp=sharing

  - organization_id: 68
    organization_name: JSON Schema
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/json-schema/
    idea_list_url: https://github.com/json-schema-org/community/blob/main/programs/mentoring/gsoc/gsoc-2025.md


  - organization_id: 69
    organization_name: JabRef e.V.
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jabref-e.v./
    idea_list_url: https://github.com/JabRef/jabref/wiki/GSoC-2025-ideas-list


  - organization_id: 70
    organization_name: JdeRobot
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jderobot/
    idea_list_url: https://jderobot.github.io/activities/gsoc/2025#ideas-list
  

  - organization_id: 71
    organization_name: Jenkins
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jenkins/
    idea_list_url: https://www.jenkins.io/projects/gsoc/2025/project-ideas/

  - organization_id: 72
    organization_name: Jitsi
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jitsi/
    idea_list_url: https://github.com/jitsi/gsoc-ideas/blob/master/2025/README.md

  - organization_id: 73
    organization_name: Joomla!
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/joomla!/
    idea_list_url: https://docs.joomla.org/GSoC_2025_Project_Ideas

  - organization_id: 74
    organization_name: KDE Community
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kde-community/
    idea_list_url: https://community.kde.org/GSoC/2025/Ideas

  - organization_id: 75
    organization_name: Keploy
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/keploy/
    idea_list_url: https://github.com/keploy/gsoc/tree/main/2025


  - organization_id: 76
    organization_name: Kiwix
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kiwix/
    idea_list_url: https://kiwix.org/en/google-summer-of-code/


  - organization_id: 77
    organization_name: Kornia
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kornia/
    idea_list_url: https://github.com/kornia/kornia-rs/wiki/Google-Summer-of-Code-Ideas-2025

  - organization_id: 78
    organization_name: Kotlin Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kotlin-foundation/
    idea_list_url: https://kotlinlang.org/docs/gsoc-2025.html


  - organization_id: 79
    organization_name: KubeVirt
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kubevirt/
    idea_list_url: https://github.com/kubevirt/community/wiki/Google-Summer-of-Code-2025


  - organization_id: 80
    organization_name: Kubeflow
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kubeflow/
    idea_list_url: https://www.kubeflow.org/events/gsoc-2025/
  

  - organization_id: 81
    organization_name: LLVM Compiler Infrastructure
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/llvm-compiler-infrastructure/
    idea_list_url: https://llvm.org/OpenProjects.html

  - organization_id: 82
    organization_name: LabLua
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/lablua/
    idea_list_url: https://github.com/labluapucrio/gsoc

  - organization_id: 83
    organization_name: Learning Equality
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/learning-equality/
    idea_list_url: https://docs.google.com/document/d/e/2PACX-1vT49WKgbsAIUoJ0jJS8LTWWm7UTByM0Pw3qt0KgyU_BShB1oGtZBkrEWuannFsRMVvGd0QBytZt8blh/pub

  - organization_id: 84
    organization_name: LibreOffice
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/libreoffice/
    idea_list_url: https://wiki.documentfoundation.org/Development/GSoC/Ideas

  - organization_id: 85
    organization_name: Liquid Galaxy project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/liquid-galaxy-project/
    idea_list_url: https://www.liquidgalaxy.eu/2024/11/unique-post-for-gsoc-2025-at-liquid.html


  - organization_id: 86
    organization_name: MBDyn
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mbdyn/
    idea_list_url: https://public.gitlab.polimi.it/DAER/mbdyn/-/wikis/GSoc-Project-Ideas


  - organization_id: 87
    organization_name: MDAnalysis
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mdanalysis/
    idea_list_url: https://github.com/MDAnalysis/mdanalysis/wiki/GSoC-2025-Project-Ideas

  - organization_id: 88
    organization_name: MIT App Inventor
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mit-app-inventor/
    idea_list_url: https://github.com/mit-cml/appinventor-sources/wiki/Google-Summer-of-Code-2025

  - organization_id: 89
    organization_name: Machine Learning for Science (ML4SCI)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/machine-learning-for-science-(ml4sci)/
    idea_list_url: https://ml4sci.org/


  - organization_id: 90
    organization_name: MariaDB
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mariadb/
    idea_list_url: https://mariadb.com/kb/en/google-summer-of-code-2025/
  

  - organization_id: 91
    organization_name: Meshery
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/meshery/
    idea_list_url: https://meshery.io/programs/gsoc/2025

  - organization_id: 92
    organization_name: MetaBrainz Foundation Inc
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/metabrainz-foundation-inc/
    idea_list_url: https://wiki.musicbrainz.org/Development/Summer_of_Code/2025

  - organization_id: 93
    organization_name: Micro Electronics Research Lab - UITU
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/micro-electronics-research-lab-uitu/
    idea_list_url: https://github.com/merledu/Google-Summer-of-Code/wiki/GSoC'25-Project-Ideas-List

  - organization_id: 94
    organization_name: Mixxx
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mixxx/
    idea_list_url: https://github.com/mixxxdj/mixxx/wiki/GSOC-2025-Ideas

  - organization_id: 95
    organization_name: National Resource for Network Biology (NRNB)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/national-resource-for-network-biology-(nrnb)/
    idea_list_url: https://github.com/nrnb/GoogleSummerOfCode/issues


  - organization_id: 96
    organization_name: Neovim
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/neovim/
    idea_list_url: https://github.com/neovim/neovim/wiki/Google-Summer-of-Code#gsoc-ideas-2025


  - organization_id: 97
    organization_name: Neuroinformatics Unit
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/neuroinformatics-unit/
    idea_list_url: https://neuroinformatics.dev/get-involved/gsoc

  - organization_id: 98
    organization_name: NumFOCUS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/numfocus/
    idea_list_url: https://github.com/numfocus/gsoc/blob/master/2025/ideas-list.md


  - organization_id: 99
    organization_name: OSGeo (Open Source Geospatial Foundation)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/osgeo-(open-source-geospatial-foundation)/
    idea_list_url: https://wiki.osgeo.org/wiki/Google_Summer_of_Code_2025_Ideas


  - organization_id: 100
    organization_name: OWASP Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/owasp-foundation/
    idea_list_url: https://owasp.org/www-community/initiatives/gsoc/gsoc2025ideas
  

  - organization_id: 101
    organization_name: Open Climate Fix
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-climate-fix/
    idea_list_url: https://docs.google.com/document/d/1nh6pdIjCTSLWwgFhgIV1gpG8CbIqGyoCFDHOfWmKdwA/edit?usp=sharing

  - organization_id: 102
    organization_name: Open Food Facts
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-food-facts/
    idea_list_url: https://wiki.openfoodfacts.org/GSOC/2025_ideas_list

  - organization_id: 103
    organization_name: Open HealthCare Network
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-healthcare-network/
    idea_list_url: https://contributors.ohc.network/projects

  - organization_id: 104
    organization_name: Open Robotics
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-robotics/
    idea_list_url: https://github.com/osrf/osrf_wiki/wiki/GSoC25

  - organization_id: 105
    organization_name: Open Science Initiative for Perfusion Imaging
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-science-initiative-for-perfusion-imaging/
    idea_list_url: https://docs.google.com/document/d/e/2PACX-1vSuYh57hsLUXbmrA5tozX4Ucne0sRXnmFt5xBA88gzDZJKZYD4-Bq04J9acer2d_i6NP6xhimmz4m5i/pub


  - organization_id: 106
    organization_name: Open Science Labs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-science-labs/
    idea_list_url: https://opensciencelabs.org/opportunities/gsoc/project-ideas/


  - organization_id: 107
    organization_name: Open Technologies Alliance - GFOSS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-technologies-alliance-gfoss/
    idea_list_url: https://ellak.gr/wiki/index.php?title=Google_Summer_of_Code_2025_proposed_ideas

  - organization_id: 108
    organization_name: Open Transit Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-transit-software-foundation/
    idea_list_url: https://opentransitsoftwarefoundation.org/2025/01/google-summer-of-code-2025-project-ideas/


  - organization_id: 109
    organization_name: OpenAFS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openafs/
    idea_list_url: https://www.openafs.org/gsoc/project-ideas.html


  - organization_id: 110
    organization_name: OpenAstronomy
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openastronomy/
    idea_list_url: https://openastronomy.org/gsoc/gsoc2025/#/projects
  

  - organization_id: 111
    organization_name: OpenCV
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/opencv/
    idea_list_url: https://github.com/opencv/opencv/wiki/GSoC_2025

  - organization_id: 112
    organization_name: OpenELIS Global
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openelis-global/
    idea_list_url: https://uwdigi.atlassian.net/wiki/spaces/OG/pages/321224705/GSoC+2025

  - organization_id: 113
    organization_name: OpenMRS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openmrs/
    idea_list_url: https://openmrs.atlassian.net/wiki/spaces/RES/pages/322404353/Summer+of+Code+2025

  - organization_id: 114
    organization_name: OpenMS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openms/
    idea_list_url: https://www.openms.org/news/gsoc2025/

  - organization_id: 115
    organization_name: OpenStreetMap
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openstreetmap/
    idea_list_url: https://wiki.openstreetmap.org/wiki/Google_Summer_of_Code/2025/Project_ideas


  - organization_id: 116
    organization_name: OpenVINO Toolkit
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openvino-toolkit/
    idea_list_url: https://github.com/openvinotoolkit/openvino/wiki/Google-Summer-Of-Code#project-ideas-for-2025


  - organization_id: 117
    organization_name: OpenWISP
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openwisp/
    idea_list_url: https://openwisp.io/docs/dev/developer/gsoc-ideas-2025.html

  - organization_id: 118
    organization_name: Oppia Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/oppia-foundation/
    idea_list_url: https://github.com/oppia/oppia/wiki/Google-Summer-of-Code-2025#oppias-project-ideas-list


  - organization_id: 119
    organization_name: Organic Maps
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/organic-maps/
    idea_list_url: https://github.com/organicmaps/organicmaps/wiki/GSoC-2025-ideas


  - organization_id: 120
    organization_name: PAL Robotics
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/pal-robotics/
    idea_list_url: https://pal-robotics.com/2025-google-summer-code-proposals/#tips-successful-gsoc
  

  - organization_id: 121
    organization_name: PEcAn Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/pecan-project/
    idea_list_url: https://pecanproject.github.io/gsoc_ideas

  - organization_id: 122
    organization_name: Pharo Consortium
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/pharo-consortium/
    idea_list_url: https://gsoc.pharo.org/ideas

  - organization_id: 123
    organization_name: Plone Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/plone-foundation/
    idea_list_url: https://plone.org/community/gsoc/2025

  - organization_id: 124
    organization_name: PostgreSQL
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/postgresql/
    idea_list_url: https://wiki.postgresql.org/wiki/GSoC_2025

  - organization_id: 125
    organization_name: Processing Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/processing-foundation/
    idea_list_url: https://github.com/processing/Processing-Foundation-GSoC/wiki/Project-Ideas-List-(GSoC-2025)


  - organization_id: 126
    organization_name: Project Mesa
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/project-mesa/
    idea_list_url: https://github.com/projectmesa/mesa/wiki/Google-Summer-of-Code-2025


  - organization_id: 127
    organization_name: Prometheus-Operator
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/prometheus-operator/
    idea_list_url: https://github.com/prometheus-operator/community/blob/main/mentoring/gsoc/2025/project_ideas.md

  - organization_id: 128
    organization_name: Python Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/python-software-foundation/
    idea_list_url: https://python-gsoc.org/ideas.html

  

  - organization_id: 129
    organization_name: QC-Devs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/qc-devs/
    idea_list_url: https://qcdevs.org/join/qcdevs_gsoc/


  - organization_id: 130
    organization_name: QEMU
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/qemu/
    idea_list_url: https://wiki.qemu.org/Google_Summer_of_Code_2025
  

  - organization_id: 131
    organization_name: R project for statistical computing
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/r-project-for-statistical-computing/
    idea_list_url: https://github.com/rstats-gsoc/gsoc2025/wiki/table-of-proposed-coding-projects

  - organization_id: 132
    organization_name: RTEMS Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rtems-project/
    idea_list_url: https://projects.rtems.org/gsoc/

  - organization_id: 133
    organization_name: Rizin
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rizin/
    idea_list_url: https://rizin.re/gsoc/2025/

  - organization_id: 134
    organization_name: Rspamd
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rspamd/
    idea_list_url: https://rspamd.com/gsoc2025_ideas.html

  - organization_id: 135
    organization_name: SQLancer
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sqlancer/
    idea_list_url: https://github.com/sqlancer/sqlancer/wiki/GSoC-2025-Ideas


  - organization_id: 136
    organization_name: SW360
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sw360/
    idea_list_url: https://eclipse.dev/sw360/gsoc/gsoc-projects-2025/


  - organization_id: 137
    organization_name: SageMath
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sagemath/
    idea_list_url: https://wiki.sagemath.org/GSoC/2025

  - organization_id: 138
    organization_name: Scala Center
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/scala-center/
    idea_list_url: https://github.com/scalacenter/GoogleSummerOfCode

  - organization_id: 139
    organization_name: ScummVM
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/scummvm/
    idea_list_url: https://www.scummvm.org/gsoc-2025-ideas


  - organization_id: 140
    organization_name: Society for Arts and Technology (SAT)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/society-for-arts-and-technology-(sat)/
    idea_list_url: https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/categories/ideas/
  

  - organization_id: 141
    organization_name: Software and Computational Systems Lab at LMU Munich
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/software-and-computational-systems-lab-at-lmu-munich/
    idea_list_url: https://www.sosy-lab.org/gsoc/gsoc2025.php

  - organization_id: 142
    organization_name: St. Jude Children's Research Hospital
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/st.-jude-children's-research-hospital/
    idea_list_url: https://docs.google.com/document/d/1ze8OpsltCbAkksjmhd3hCeS6VDqAaolAhWAQQRJZBpc/edit?tab=t.0

  - organization_id: 143
    organization_name: Ste||ar group
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/steororar-group/
    idea_list_url: https://github.com/STEllAR-GROUP/hpx/wiki/Google-Summer-of-Code-%28GSoC%29-2025#2025-hpx-project-ideas

  - organization_id: 144
    organization_name: Stichting SU2
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/stichting-su2/
    idea_list_url: https://www.cfd-online.com/Forums/su2-news-announcements/259420-gsoc-2025-projects.html

  - organization_id: 145
    organization_name: Sugar Labs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sugar-labs/
    idea_list_url: https://github.com/sugarlabs/GSoC/blob/master/Ideas-2025.md


  - organization_id: 146
    organization_name: Swift
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/swift/
    idea_list_url: https://www.swift.org/gsoc2025/


  - organization_id: 147
    organization_name: SymPy
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sympy/
    idea_list_url: https://github.com/sympy/sympy/wiki/GSoC-Ideas

  - organization_id: 148
    organization_name: Synfig
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/synfig/
    idea_list_url: https://github.com/synfig/synfig-docs-dev/blob/master/docs/gsoc/2025/ideas.rst

  - organization_id: 149
    organization_name: TARDIS RT Collaboration
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/tardis-rt-collaboration/
    idea_list_url: https://tardis-sn.github.io/summer_of_code/ideas/


  - organization_id: 150
    organization_name: The Apache Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-apache-software-foundation/
    idea_list_url: https://s.apache.org/gsoc2025ideas
  

  - organization_id: 151
    organization_name: The FreeBSD Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-freebsd-project/
    idea_list_url: https://wiki.freebsd.org/SummerOfCodeIdeas

  - organization_id: 152
    organization_name: The Honeynet Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-honeynet-project/
    idea_list_url: https://www.honeynet.org/gsoc/gsoc-2025/google-summer-of-code-2025-project-ideas/

  - organization_id: 153
    organization_name: The JPF team
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-jpf-team/
    idea_list_url: https://github.com/javapathfinder/jpf-core/wiki/GSoC-2025-Project-Ideas

  - organization_id: 154
    organization_name: The Julia Language
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-julia-language/
    idea_list_url: https://julialang.org/jsoc/projects/

  - organization_id: 155
    organization_name: The Linux Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-linux-foundation/
    idea_list_url: https://wiki.linuxfoundation.org/gsoc/google-summer-code-2025


  - organization_id: 156
    organization_name: The Mifos Initiative
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-mifos-initiative/
    idea_list_url: https://mifosforge.jira.com/wiki/spaces/RES/pages/4271669249/Google+Summer+of+Code+2025+Ideas


  - organization_id: 157
    organization_name: The NetBSD Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-netbsd-foundation/
    idea_list_url: https://wiki.netbsd.org/projects/gsoc/

  - organization_id: 158
    organization_name: The P4 Language Consortium
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-p4-language-consortium/
    idea_list_url: https://github.com/p4lang/gsoc/blob/main/2025/ideas_list.md

  - organization_id: 159
    organization_name: The Palisadoes Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-palisadoes-foundation/
    idea_list_url: https://developer.palisadoes.org/docs/internships/gsoc/gsoc-ideas


  - organization_id: 160
    organization_name: The Rust Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-rust-foundation/
    idea_list_url: https://github.com/rust-lang/google-summer-of-code
  

  - organization_id: 161
    organization_name: The Tor Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-tor-project/
    idea_list_url: https://gitlab.torproject.org/tpo/team/-/wikis/GSoC

  - organization_id: 162
    organization_name: The ns-3 Network Simulator Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-ns-3-network-simulator-project/
    idea_list_url: https://www.nsnam.org/wiki/GSOC2025Projects

  - organization_id: 163
    organization_name: Typelevel
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/typelevel/
    idea_list_url: https://typelevel.org/gsoc/ideas

  - organization_id: 164
    organization_name: UC OSPO
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/uc-ospo/
    idea_list_url: https://ucsc-ospo.github.io/osre25/#projects

  - organization_id: 165
    organization_name: Unicode, Inc.
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/unicode-inc./
    idea_list_url: https://docs.google.com/document/u/2/d/e/2PACX-1vQbj0-VFkRjYdnivuPPXuHM3IW4LuHxK6E0LVO3O8ZU_-k8CYH_eFMZ_IwFg_r-oBw3FCEOmHCb5jrn/pub


  - organization_id: 166
    organization_name: Unikraft
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/unikraft/
    idea_list_url: https://github.com/unikraft/gsoc/blob/staging/gsoc-2025/ideas.md


  - organization_id: 167
    organization_name: Uramaki LAB
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/uramaki-lab/
    idea_list_url: https://github.com/ruxailab/gsoc/blob/main/ideas2025.md

  - organization_id: 168
    organization_name: VideoLAN
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/videolan/
    idea_list_url: https://wiki.videolan.org/SoC_2025/


  - organization_id: 169
    organization_name: Wagtail
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/wagtail/
    idea_list_url: https://github.com/wagtail/gsoc/blob/main/project-ideas.md


  - organization_id: 170
    organization_name: Waycrate
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/waycrate/
    idea_list_url: https://waycrate.github.io/outreach/gsoc/2025/idea-list/
  

  - organization_id: 171
    organization_name: Wellcome Sanger Tree of Life
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/wellcome-sanger-tree-of-life/
    idea_list_url: https://docs.google.com/document/d/1dRHwnHAXlwQbRDstnd0OZTDJYA9cTsl1jG_hOempqII/edit?usp=sharing

  - organization_id: 172
    organization_name: Zendalona
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/zendalona/
    idea_list_url: https://docs.google.com/document/d/1HshVGE-oQq_xx09zG2LfqfVYWRhyrASteXzabCV9eIQ/edit?usp=sharing

  - organization_id: 173
    organization_name: Zulip
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/zulip/
    idea_list_url: https://zulip.readthedocs.io/en/latest/outreach/gsoc.html

  - organization_id: 174
    organization_name: cBioPortal for Cancer Genomics
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cbioportal-for-cancer-genomics/
    idea_list_url: https://github.com/cBioPortal/gsoc/issues?q=is%3Aissue%20state%3Aopen%20label%3AGSoC-2025

  - organization_id: 175
    organization_name: checkstyle
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/checkstyle/
    idea_list_url: https://github.com/checkstyle/checkstyle/wiki/Checkstyle-GSoC-2025-Project-Ideas


  - organization_id: 176
    organization_name: dora-rs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/dora-rs/
    idea_list_url: https://github.com/dora-rs/dora/wiki/GSoC_2025


  - organization_id: 177
    organization_name: freifunk
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/freifunk/
    idea_list_url: https://projects.freifunk.net/

  - organization_id: 178
    organization_name: gprMax
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gprmax/
    idea_list_url: https://github.com/gprMax/GSoC/blob/main/project-ideas-2025.md


  - organization_id: 179
    organization_name: kro | Kube Resource Orchestrator
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kro-or-kube-resource-orchestrator/
    idea_list_url: https://github.com/kro-run/kro/issues/288


  - organization_id: 180
    organization_name: libssh
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/libssh/
    idea_list_url: https://www.libssh.org/development/google-summer-of-code/
  

  - organization_id: 181
    organization_name: omegaUp
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/omegaup/
    idea_list_url: https://github.com/omegaup/omegaup/wiki/Google-Summer-of-Code-2025

  - organization_id: 182
    organization_name: openSUSE Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/opensuse-project/
    idea_list_url: https://github.com/openSUSE/mentoring/issues

  - organization_id: 183
    organization_name: rocket.chat
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rocket.chat/
    idea_list_url: https://github.com/RocketChat/google-summer-of-code/blob/main/google-summer-of-code-2025.md#-project-ideas

  - organization_id: 184
    organization_name: stdlib
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/stdlib/
    idea_list_url: https://github.com/stdlib-js/google-summer-of-code/blob/main/ideas.md

  - organization_id: 185
    organization_name: webpack
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/webpack/
    idea_list_url: https://docs.google.com/document/d/1JOtAdpoqHGieg_nJpjkWDv1BoErPQ9L1GnZmX55E-W0/edit?usp=sharing


  




  


  

 