organizations:
  - organization_id: 1
    organization_name: 52°North Spatial Information Research GmbH
    no_of_ideas: 3
    ideas_content: |
                1. KomMonitor
                Angular migration of the KomMonitor Web Client
                Explanation
                KomMonitor is a web based tool that combines methods of GIS (Geographic information System) and statistical data and helps in providing a simpler and easier way to monitor geo-spatial data. Many municipalities have established KomMonitor to address a wide range of challenges in fields such as urban planning, environmental management, and disaster response. The current version of the KomMonitor Web Client has been developed using AngularJS, which has served as a reliable foundation for its functionalities. However, AngularJS has been deprecated for some years now. Therefore, relying on the current code base has several potential drawbacks associated with using AngularJS, such as compatibility issues, limited community support, reduced performance, and version support. To overcome these challenges and take KomMonitor to the next level, it is necessary to adopt the KomMonitor Web Client to the more modern and widely-supported framework Angular. As part of GSoC 2023, essential work has been done by developing a general approach for the Angular migration. The Web Client has been restructured so that it can be deployed as a hybrid web application, which runs both legacy AngularJS components and migrated or new Angular components. This year, the project aims to continue the migration tasks. Hence, the goal of this project is to reimplement several selected components of the KomMonitor Web Client by using the Angular framework.

                Expected Results
                As a result of the project, it is expected that several selected components of the KomMonitor Web Client will have been reimplemented with the Angular framework. The resulting UI of the reimplemented components should be as close as possible to the previous design to preserve the current look&feel. As an additional requirement, the reimplementation should take into account best practices and common design patterns in Angular. This results in also restructuring some of the existing components rather than simply transferring a component from AngularJS to Angular. Finally, the hybrid Web Client, including legacy AngularJS components and new Angular components side-by-side, should run properly without any bugs.

                Code Challenge
                Migrate the kommonitorToastHelperService of the KomMonitor Web Client to Angular and make use of it in a new Angular component as part of the Web Client. Follow the steps below:

                Create a fork of https://github.com/KomMonitor/web-client and checkout the GSoC2025 Branch
                Create a new Angular service as part of the KomMonitor Web Client that provides the same functionality as the existing AngularJS version of the kommonitorToastHelperService
                Create a new Angular component that makes use of the previously implemented kommonitorToastHelperService. Take into account these requirements:
                The component should be opened and closed by clicking on a button on the left sidebar.
                The component should include a text area and a button.
                The required functionality should be to display a message as toast on the screen by filling the text area and clicking on the button. For this purpose the kommonitorToastHelperService should be used.
                Push the code to your fork at GitHub
                Link to the fork within your official GSoC application. Your GSoC application should also include a description of which components you plan to migrate during GSoC as well as an estimation of time required for implementing it.


                Community and Code License
                Apache Software License, Version 2

                Mentors
                Sebastian Drost (s.drost @52north.org), Christoph Wagner (c.wagner @52north.org)

                Project Duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                Chat
                TBD

                ~~~~~~~~~~

                2. LLM and GeoData
                Explanation
                During 52°North’s Student Innovation Challenge in 2024, a first open-source implementation connecting spatial data and Large Language Models (LLM) was developed.

                The ambition was to address the pain points of searchability in Research and Spatial Data Infrastructures (RDI/SDI). Search functionality in such systems is typically limited to a metadata-based approach. However, geospatial data – whether vector or raster based – provides a wealth of interesting data that can currently only be identified by looking at the individual dataset. The challenge of the 2024 Student Innovation Prize was to develop a concept and a possible implementation that allows searching within datasets of/and RDI/SDI, e.g. on the attribute level. There are many interesting aspects related to this challenge: technical solutions, taxonomies and semantics, language/i18n, searching in raster data, and many more such as LLMs.

                The available Proof of Concept (PoC) features a prompt that makes it easier to search and access to spatial data. More user stories are documented in the Innovation Prize project backlog on GitHub: https://github.com/52North/innovation-prize.

                Expected Results
                The PoC should be hardened and developed beyond its current state. For example, less verbose prompts are needed as more sophisticated LLMs emerge. Also, improved software frameworks may provide a better development experience. Various extensions are possible and a selection should be outlined in the proposal. Additional user stories from the backlog in the github project (see above) could be addressed. Another interesting extension could also entail a federated architecture. Furthermore, the use of different LLMs is also a possible option for further development.

                Code Challenge
                Set up the entire working environment based on the existing open source code

                https://github.com/52North/innovation-prize/tree/2024

                and add two more data sets. Share the code and the deployed system.

                Community and Code License
                TBChecked: Apache Software License, Version 2

                Mentors
                Henning Bredel (h.bredel @52north.org), Simeon Wetzel

                Project duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                ~~~~~~~~~~

                3. Weather Routing Tool
                Explanation
                The open-source 52°North Weather Routing Tool (WRT) was initially developed during the MariData project. It provides means to find an optimal ship route that minimizes fuel consumption under varying weather conditions. In the optimization process, several constraints can be integrated, e.g. water depth and traffic separation zones. Currently, there are two algorithms available: an isofuel algorithm and a genetic algorithm. Details of the MariData project and example applications of the Weather Routing Tool can be found in the following publication: https://proceedings.open.tudelft.nl/imdc24/article/view/875.

                Expected Results
                The Weather Routing Tool should be extended by new features and its robustness should be improved. There are three major directions of possible developments:

                Ship speed optimization
                Currently, only the geometry of the route is optimized while the ship speed is assumed to be constant. To cover a broader range of real-world use cases, the Weather Routing Tool should provide the option to optimize ship speed. This could be along a fixed route or simultaneous with the route geometry.
                Genetic algorithm
                The implementation of the genetic algorithm is still very basic. Possible improvements include the generation of the initial population and the strategies for crossover and mutation. Moreover, a multi-objective optimization could be implemented.
                General consumption model
                An important aspect of the Weather Routing Tool is the underlying (fuel) consumption model. The best results can generally be obtained by using a consumption model which is developed specifically for a ship, e.g. based on hydrodynamic modeling or machine learning models. However, developing such specific models is cumbersome and restricts the applicability of the tool. Thus, having a general consumption model which only requires a few characteristics of a ship (e.g. type of vessel, length, breadth, displacement) would be a great improvement. The model should have reasonable accuracy. As this feature includes research aspects and can only be successfully developed with the necessary background knowledge, interested candidates have to provide a clear plan of their approach.
                The features can be implemented in different ways. How they are implemented is up to the candidate and might include deterministic, machine learning or AI methods.

                Code Challenge
                New ship class:

                Implement a new ship class
                It should inherit from the Boat base class
                The get_ship_parameters method has to be implemented; it should return a “synthetic” fuel rate which depends on at least one environmental parameter (e.g. wave height)
                Make sure the fuel rates (kg per second) are within a reasonable value range. Besides the weather conditions, typical fuel rates also depend on the ship size, type (e.g. container ship, tanker, fishing vessel) and speed.
                The choice of the considered environmental parameters and the type of the function is free
                You can take the ConstantFuelBoat class as an example
                Prepare weather conditions
                Options:
                Create your own synthetic weather conditions
                Download actual historical or forecast data from public portals (Copernicus, NOAA, …). You can use the Python package maridatadownloader directly or indirectly by setting “DATA_MODE” to “automatic“.
                Run the Weather Routing Tool with your new ship class and a route of your free choice
                Hint: because the Python package mariPower is not publicly available, you need to comment or delete the corresponding lines in ship.py.
                Configuration:
                Set “ALGORITHM_TYPE” to “isofuel”
                Provide the expected results for review
                Mandatory:
                Final route as GeoJSON file
                Python code of new ship class
                Optional:
                Log file (info.log)
                Snapshots of routing steps (WRT_FIGURE_PATH)
                Used weather data
                Community and Code License
                MIT License

                Mentors
                Martin Pontius (m.pontius @52north.org), Katharina Demmich (k.demmich @52north.org)

                Project Duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                TBD

                Cloud Native OGC SensorThings API 2
                enviroCar
    totalCharacters_of_ideas_content_parent: 10019
    totalwords_of_ideas_content_parent: 1397
    totalTokenCount_of_ideas_content_parent: 2059
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/52north-spatial-information-research-gmbh/
    idea_list_url: https://52north.org/outreach-dissemination/google-summer-of-code/project-ideas/


  - organization_id: 2
    organization_name: AFLplusplus
    no_of_ideas: 4
    ideas_content: |
      Proposal 1: Tool for Automated generic/bounds simplification
        Create a (general, not LibAFL-specific) rust tool to simplify/minimze bounds

        Description
        As commented by many users and maintainers of LibAFL, our codebase is absolutely full of complicated generics. We use these to allow for structured and statically-checked compatibility between various components provided in our codebase, and is a critical part of how LibAFL is structured.

        Unfortunately, these can be very difficult to maintain. Our goal is to develop a tool capable of assisting developers in this maintenance process.

        Please check out issue #2868 for more details.

        Expected Outcomes
        A tool that works on any rust code, tries to minimize the used bounds, and fixes the code

        Skills Expected
        Rust
        A good understanding of Generics and the Rust Type system
        Possible Mentors
        @addisoncrump
        @tokatoka
        Expected size of the project
        The project is expected to take either 175 or 350 hours.

        Difficulty Rating
        The overall difficulty of this project is expected to be medium.
        ~~~~~~~~~~
        Proposal 2: Adapt qemuafl Frontend to LibAFL QEMU
        The project consists of adapting the frontend of qemuafl, the AFL++'s QEMU fork, with LibAFL QEMU.

        Description
        The end goal of this project would be to run fuzzers built for qemuafl while using LibAFL QEMU as the backend, in a retrocompatible way.
        A draft PR is already available and can be used as a starting point by the student.
        Ideally, the student would measure the performance (in terms of exec/s and coverage) of the new qemuafl adaptation with some fuzzers to evaluate how the final implementation compares with the reference.

        Expected Outcomes
        In short, we expect the student to make the new frontend work for most fuzzers developed for qemuafl while maintaining (at least) similar performance.

        See #1983 for an initial implementation that still lacks features.

        The main tasks the student would have to perform are the following:

        Speak the AFL++ forkserver protocol (check the draft PR).
        Add TCG caching to the LibAFL QEMU forkserver
        Use LibAFL QEMU snapshots where possible
        Add as many env variable features as possible
        Skills Expected
        We expect the student to:

        have a strong background in the Rust and C languages.
        be familiar with fuzzing.
        ideally, have some experience using AFL++ and / or LibAFL.
        ideally, have prior experience with the QEMU project.
        Possible Mentors
        The possible mentors for this project are:

        @domenukk
        @rmalmain
        Expected size of the project
        The project is expected to take either 175 or 350 hours.

        Difficulty Rating
        The overall difficulty of this project is expected to be medium.

        Original post
        This proposition is mostly an adaptation of issue #2964.
        ~~~~~~~~~~
        Proposal 3: Network Emulation for LibAFL_QEMU
        Implement syscall emulation for filesystem and network in libafl_qemu.

        Description
        The student must implement something similar to preeny and FitM to hook the network API and an emulator filesystem that can be snapshot-restored always hooking the syscall in libafl_qemu user mode

        Expected Outcomes
        A working network emulation layer for LibAFL_QEMU

        Required Skills
        Good understanding of Rust, C, system programming
        Ideally: prior knowledge in emulators and fuzzing
        Difficulty Rating
        The overall difficulty of this project is expected to be medium.

        Possible mentors
        @domenukk
        @rmalmain
        Expected size of the project
        The project is expected to take either 175 or 350 hours, depending on details
        ~~~~~~~~~~
        Proposal 4: Remote Worker Stage
        Mutations and execution of a Stage is always on the machine LibAFL runs at. For very slow targets it may be beneficial to offload the actual executions to stateless worker.

        Description
        We could add a RemoteWorkerLauncherStage that builds n work packages, each including a next scheduled corpus entry, all metadata for this Testcase, the current feedback state, as well as additional random corpus entries for splicing.
        The work package should then be posted to Redis or some other queue db (very much like celery, whatever a rust alternative is).
        After the execution, the results should be collected in an extra stage

        Expected Outcome:
        The implementation and a set of working examples, including:
        LibAFL Workers / RemoteWorkerLauncherStage + RemoteWorkerCollectorStage

        Required Skills
        Rust
        Prior knowledge in distributed computing and/or fuzzing are a plus
        Difficulty Rating
        easy to medium

        Possible mentors
        @domenukk
        @tokatoka
        @addisoncrump
        Length
        175 hours
    totalCharacters_of_ideas_content_parent: 4418
    totalwords_of_ideas_content_parent: 600
    totalTokenCount_of_ideas_content_parent: 1003
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aflplusplus/
    idea_list_url: https://github.com/AFLplusplus/LibAFL/issues/2992

  - organization_id: 3
    organization_name: AOSSIE
    no_of_ideas: 13
    ideas_content: |
      Agora Blockchain
        Project Type: Medium
        Description:
        Agora Blockchain is a decentralized voting platform designed to enhance electoral integrity and accessibility. It enables transparent, tamper-proof voting through smart contracts, leveraging Chainlink CCIP for cross-chain functionality. Agora ensures fair participation and trust in election results by eliminating centralized control and providing a verifiable, immutable ledger of votes.

        Key features include:

        Multi-algorithm voting: Supports different voting mechanisms like ranked choice, quadratic voting, and stake-based voting.
        Cross-chain voting: Uses Chainlink CCIP to enable voting across multiple blockchains.
        Gas-efficient smart contracts: Optimized Solidity contracts reduce transaction costs.
        Decentralized governance: Community-driven elections and decision-making.
        User-friendly interface: Built with Next.js, Wagmi, and MetaMask for seamless interaction.
        Expected Outcomes:
        Smart Contract Enhancements:

        Implement private elections for confidential voting.
        Further optimize election factory contracts for gas efficiency.
        Cross-Chain Expansion:

        Extend Chainlink CCIP integration to support multiple blockchains.
        Frontend & dApp Integration:

        Build an intuitive UI using Next.js and Wagmi.
        Ensure smooth wallet connectivity and real-time vote updates.
        Analytics & Insights:

        Develop a real-time dashboard for election statistics.
        Track voter participation and engagement metrics.
        Required Skills:
        Solidity
        Hardhat
        Chainlink CCIP
        Next.js
        MetaMask
        Wagmi
        TailwindCSS
        Zustand
        Mentors:
        Ronnie

        ~~~~~~~~~~

        BabyNest
        Project Type: Large
        Description:
        Pregnancy is a life-changing journey filled with crucial medical appointments, tests, and healthcare decisions. However, expecting parents often struggle to keep track of these milestones, which can lead to missed appointments and added stress. Studies show that adherence to prenatal checkups directly impacts pregnancy outcomes, yet there is no universally accessible tool to assist parents in navigating healthcare requirements based on their country and trimester.

        BabyNest is designed to solve this problem through a minimalist React Native app integrated with an AI-powered assistant. This intelligent assistant acts as a personal pregnancy planner and guide, ensuring that expecting parents stay informed, organized, and stress-free.

        Users can benefit from features such as:

        Automated tracking of trimester-specific medical appointments
        Country-specific healthcare requirement notifications
        Offline access to pregnancy care guidelines
        AI-powered personalized recommendations and reminders
        Expected Outcomes:

        Mobile application built with React Native for cross-platform support
        AI agent integration for intelligent pregnancy milestone scheduling, reminders and tracking
        Offline-first architecture with local storage of healthcare guidelines
        Required Skills:

        Frontend Development (React Native)
        Backend Development (Node.js, FastAPI)
        AI and NLP (Python, LangChain)
        Database Management (SQLite, Pinecone)
        Mentors: Bhavik Mangla

        ~~~~~~~~~~
        DebateAI
        Project Type: Large
        Description:
        DebateAI is an interactive, AI-enhanced debate game platform designed to improve users communication skills through structured competitive debates. Users can engage in real-time debates against both human opponents and AI-driven challengers on a wide range of real-world topics. The platform mimics formal debate competition structures, making it an effective practice and competitive tool.

        Expected Outcomes:
        User vs. User Debates:

        Real-time interaction using WebSockets and WebRTC for audio, video, and text communication.
        Structured debate formats with timed rounds, including opening statements, rebuttals, cross-examinations, and closing arguments.
        User vs. AI Debates:

        AI-driven opponents using LLMs to generate realistic counterarguments and adapt to user inputs.
        User Management and Profiles:

        Secure authentication and access control.
        Personal dashboards to track debate history and manage settings.
        Elo rating system for matchmaking and ranking users.
        Custom Debate Spaces:

        Users can create private rooms to debate on topics of their choice.
        Platform Enhancement & Codebase Refactoring:

        Refactor the existing codebase for better maintainability, scalability, and performance.
        Improve real-time communication efficiency and backend services.
        Required Skills:
        ReactJS
        TypeScript
        GoLang
        Python
        Databases
        LLMs
        Mentors:
        Bruno Keshav


        ~~~~~~~~~~
        Devr.AI
        Project Type: Large
        Description:
        Devr.AI is an AI-powered Developer Relations (DevRel) assistant designed to seamlessly integrate with open-source communities across platforms like Discord, Slack, GitHub, and Discourse. It acts as a virtual DevRel advocate, helping maintainers engage with contributors, onboard new developers, and provide real-time project updates.

        By leveraging LLMs, knowledge retrieval, and workflow automation, the assistant enhances community engagement, simplifies contributor onboarding, and ensures open-source projects remain active and well-supported.

        Expected Outcomes:
        AI-Driven Contributor Engagement

        Automates interactions, welcomes new contributors, and guides them through onboarding.
        Automated Issue Triage & PR Assistance

        Helps maintainers prioritize issues and assists contributors in resolving them efficiently.
        Knowledge Base & FAQ Automation

        Provides instant answers to common queries, reducing repetitive maintainer workload.
        AI-Powered Community Analytics

        Tracks engagement metrics, identifies active contributors, and generates insights.
        Required Skills:
        GenAI
        Supabase
        FastAPI
        Integrations:
        Discord
        Slack
        GitHub
        Mentors:
        Chandan


        ~~~~~~~~~~
        DocPilot
        Project Type: Large
        Description:
        Build a new age EMR application using conversational AI at its best. Existing EMR solutioning is Age-old! Doctors resist the overwhelming software which is high on costs and difficult to operate. Last innovation was made in 1990's. DocPilot listens to the whole consultation conversation between a doctor and patient, and generates a prescription for the doctor to just sign, print and save digitally.

        The app should be able to separate out things like symptoms, diagnosis, medications and tests from the conversation it listens to. These are just the basic requirements. Research more on OPD appointments and include them in our solutioning.

        Expected Outcomes:
        Conversational AI-powered EMR that listens and auto-generates prescriptions.
        Eliminates outdated, complex, and costly software for doctors.
        Affordable and easy to use, reducing resistance from medical professionals.
        Extracts symptoms, diagnosis, medications, and tests from conversations.
        Allows doctors to review, sign, print, and save prescriptions digitally.
        Integrates OPD appointment management for a seamless experience.
        A modern solution replacing decades-old EMR systems.
        Required Skills:
        Flutter
        AI
        Appwrite
        Mentors:
        Jaideep

        ~~~~~~~~~~

        EduAid
        Project Type: Medium
        Description:
        EduAid is an AI-driven tool designed to enhance online learning by generating quizzes from educational content, helping students improve retention and engagement. Currently available as a browser extension, we aim to expand it into a full-fledged platform with a website, optimized model pipelines, and better system performance.

        Our current model supports difficulty-controlled quizzes for short-answer and multiple-choice questions (MCQs). We plan to extend this functionality to other formats, including fill-in-the-blanks, boolean, and match-the-following, by improving our models for diverse question generation. Additionally, we seek to integrate EduAid with other educational platforms to make it a seamless part of the learning ecosystem.

        Expected Outcomes:
        Fully deploy the EduAid browser extension and website.
        Optimize model pipelines for better accuracy and response time.
        Improve system performance for a smoother user experience.
        Expand difficulty-controlled question generation to new formats.
        Enhance UI/UX for better usability.
        Integrate with other educational platforms for wider adoption.
        Required Skills:
        Frontend Development
        Backend Development
        PyTorch & NLP
        System Design & Architecture
        Mentors:
        Aditya Dubey


        ~~~~~~~~~~
        Ell-ena
        Project Type: Large
        Description:
        Ell-ena is an AI-powered product manager that automates task management by creating to-do items, tickets, and transcribing meetings while maintaining full work context. It is input-agnostic and features a chat interface where users can interact naturally.

        Users can ask Ell-ena to perform tasks such as:

        Create a ticket to work on the dark mode feature.
        Add a to-do list item for my math assignment.
        The AI understands the context and adds relevant details automatically. Advanced algorithms like Graph RAG can be leveraged for efficient context retrieval and decision-making.

        Expected Outcomes:
        AI-powered system that generates tasks, tickets, and meeting transcriptions.
        Seamless chat-based interface for intuitive user interactions.
        Context-aware automation to enrich task details automatically.
        Implementation of Graph RAG or similar techniques for intelligent processing.
        Scalable backend to support real-time task creation and management.
        Required Skills:
        ReactJS / NextJS
        NodeJS / Any backend tech stack
        AI / NLP
        Graph RAG
        Mentors:
        Jaideep


        ~~~~~~~~~~

        Inpact
        Project Type: Large
        Description:
        Inpact is an AI-powered creator collaboration and sponsorship matchmaking platform designed to connect content creators, brands, and agencies through data-driven insights. This open-source platform enables influencers to discover relevant sponsorship deals, collaborate with like-minded creators, and optimize brand partnerships.

        By leveraging GenAI, audience analytics, and engagement metrics, Inpact ensures highly relevant sponsorship opportunities for creators while maximizing ROI for brands investing in influencer marketing.

        Expected Outcomes:
        AI-Driven Sponsorship Matchmaking

        Automatically connects creators with brands based on audience demographics, engagement rates, and content style.
        AI-Powered Creator Collaboration Hub

        Facilitates partnerships between creators with complementary audiences and content niches.
        AI-Based Pricing & Deal Optimization

        Provides fair sponsorship pricing recommendations based on engagement, market trends, and historical data.
        AI-Powered Negotiation & Contract Assistant

        Assists in structuring deals, generating contracts, and optimizing terms using AI insights.
        Performance Analytics & ROI Tracking

        Enables brands and creators to track sponsorship performance, audience engagement, and campaign success.
        Required Skills:
        ReactJS
        GenAI
        Supabase
        FastAPI
        Mentors:
        Chandan

        ~~~~~~~~~~
        Monumento
        Project Type: Large
        Description:
        Monumento is an AR-integrated social app that transforms how you connect with the world’s most iconic landmarks. Through Monumento, you can check in to popular monuments, explore famous sites, and discover new people, all within a social platform dedicated to cultural and historical experiences. Whether you're a traveler or a history enthusiast, Monumento offers an immersive way to engage with the world’s most treasured locations.

        Expected Outcomes:
        Improved UI responsiveness

        Improve the app's responsiveness across different devices and screen sizes.
        Ensure a seamless user experience on various platforms.
        Better social system

        Improve the social aspect of the app by improving the feed and user profiles and the ability to interact with other users.
        Introduce new features like events, communities to keep users engaged
        Make Popular Monumnets Dynamic

        Introduce a dynamic system where popular monuments can be updated with new information and images by the users.
        Allow users to add new monuments to the app and make them available for users to check in to.
        Itineray

        Introduce a itinerary feature to help users plan their trips and discover new places.
        Allow users to save their favorite monuments and create personalized itineraries.
        Required Skills:
        Flutter
        Appwrite/Pocketbase/Supabase
        Generative AI
        ARCore/ARKit
        UI/UX Design
        Mentor:
        Mohammed Mohsin

        ~~~~~~~~~~

        Neurotrack
        Project Type: Medium
        Description:
        Neurotrack is an AI-powered platform designed for schools and therapy centers to detect, assess, and manage neurodevelopmental conditions like Autism, ADHD, and learning difficulties. By automating assessments, personalized education plans, and therapy tracking, it empowers educators, therapists, and parents to provide more effective, data-driven support.

        Expected Outcomes:
        AI-Powered Student Grouping

        Identifies patterns and groups students with similar needs for tailored interventions.
        Automated Individualized Education Plans (IEPs)

        Creates personalized learning strategies with AI-driven recommendations.
        Digital Assessments

        Conducts efficient, research-backed evaluations to track progress.
        Real-Time Reports & Insights

        Provides actionable data for educators, therapists, and parents.
        Comprehensive Therapy Tracking

        Logs sessions, progress, and improvements over time.
        Parent Support Assistant

        AI-driven chat support for guidance and resource recommendations.
        Seamless Scheduling

        Simplifies session planning for educators and therapists.
        Required Skills:
        GenAI
        Supabase/Appwrite
        Flutter
        Mentors:
        Mohsin
        ~~~~~~~~~~
        Perspective
        Project Type: Large
        Description:
        In today's digital landscape, personalized content algorithms and social media feeds often create echo chambers of various news and different perspectives and narratives. Users are repeatedly exposed to viewpoints confirming their beliefs. This reinforcement of confirmation bias leads to increased polarization and limits critical thinking.

        The Perspective app tackles the issue of echo chambers and confirmation bias by actively presenting users with well-researched, alternative viewpoints alongside their regularly consumed content. It analyzes the current narrative of a news article, social media post, or online discussion, then curates counterarguments from credible sources. This exposure encourages critical evaluation and helps users see beyond the single perspective they might be constantly fed, ultimately fostering a more balanced and nuanced understanding of complex facts. You don't need to rely on truncated news, get complete facts.

        Users can benefit from features such as:

        Counter-perspective: Instantly see counterarguments and narration of why other perspective.
        Reasoned Thinking: The tool will provide a counter-narrative of the same fact with strongly connected facts.
        Updated Facts: With the help of context-aware LLMs, we will provide the latest facts and counter-facts.
        Seamless Integration: Works with news, blogs, and social media applications.
        Real-Time Analysis: You don't need to wait for any author, make Perspective your companion for immediate insights as you browse.
        Expected Outcomes:

        Less Bias in narratives: Break out of echo chambers.
        Wider Perspectives: Broaden your understanding of the news you are watching.
        Better Discourse: More balanced discussions.
        Sharper Analysis: Improved critical thinking and decreased your mind's polarisation.
        Required Skills:

        Frontend Development (ReactJS)
        Backend Development (Python, FastAPI)
        AI and NLP (Python, LangChain, Langgraph, Prompt Engineering)
        Database Management (Any VectorDB)
        Mentors: Manav Sarkar

        ~~~~~~~~~~

        Pictopy
        Project Type: Medium
        Description:
        Pictopy is currently built using Tauri, relying on Rust, but it comes with platform-specific dependencies that make it difficult to containerize and ship. Electron has been considered as an alternative, but issues with rendering local machine photos and bypassing security have caused challenges in the past. This has led to difficulty in onboarding new contributors as many give up during the setup process, resulting in fewer active contributors.

        The backend has been stable but stagnant and could use refactoring and design enhancements to improve its growth and functionality. While the backend is working without issues, there is potential for improvement and future scaling.

        Expected Outcomes:
        Rework the frontend to explore other options that can simplify setup and containerization.
        Address issues related to Electron, including photo rendering and security bypassing.
        Increase contributions from new developers by simplifying the setup process.
        Refactor and enhance the backend for better growth and scalability.
        Provide design improvements to the backend for smoother development and future expansions.
        Required Skills:
        Rust
        Electron
        Backend Development
        Frontend Development
        Mentors:
        Pranav Aggarwal
        ~~~~~~~~~~
        Resonate
        Project Type: Medium
        Description:
        Resonate is an open-source social voice platform designed to enable real-time audio interactions, storytelling, and voice-based social networking. The project is built with a strong focus on open collaboration, accessibility, and innovation in voice communication. Whether it's live discussions, pair chats, or immersive story experiences, Resonate is designed to put voice at the center of social engagement.

        Expected Outcomes:
        Expanded Audio Story Marketplace

        Develop a fully-fledged marketplace for audio stories, allowing users to create, browse, and follow creators.
        Implement profile pages with a follower system, showcasing user content and social interactions.
        User & Creator Search Functionality

        Enhance the explore page by adding user search functionality.
        Enable users to follow creators, view their profiles, and stay updated on their latest audio stories.
        Friend System for Personal Communication

        Implement a friend request and acceptance system.
        Enable direct personal chats and voice calls between friends.
        Improved Pair Chat Experience

        Introduce a lobby system where users can see the number of people waiting before joining a pair chat.
        Improve UI/UX to enhance user engagement and interaction.
        Required Skills:
        Flutter
        Appwrite
        LiveKit
        WebRTC
        UI/UX Design
        Mentor:
        Aarush Acharya
    totalCharacters_of_ideas_content_parent: 17886
    totalwords_of_ideas_content_parent: 2039
    totalTokenCount_of_ideas_content_parent: 3492
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aossie/
    idea_list_url: https://aossie.org/ideas

  
  - organization_id: 4
    organization_name: API Dash
    no_of_ideas: 10
    ideas_content: |

          1. DashBot
          Related Issue - #621

          Develop DashBot - the AI assistant for API Dash which supercharges developer productivity by helping developers automate tedious tasks, follow best practices, interact & obtain contextual suggestions, all via natural-language input. DashBot must be designed in a modular and extensible manner and provide the following list of features (suggestive, not exhaustive):

          Explain responses & identify any discrepancy
          Debug requests based on Status codes & Error messages
          Generate API documentation
          Understand API and generate tests
          Generate plots & visualizations for API responses along with ability to customize
          Generate API integration frontend code for frontend frameworks like React, Flutter, etc.
          For each of the tasks you are also required to prepare benchmark evaluations so that it is easier for end users to choose the right backend LLM.

          Skills: AI, Agent, LLM Evaluation, Testing, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          2. AI Agent for API Testing & Tool Generation
          Related Issue - #620

          Develop an AI Agent which leverages the power of Large Language Models (LLMs) to automate and enhance the process of testing APIs. Also, simplify the process of converting APIs into structured tool definitions to enable seamless integration with popular AI agent frameworks like crewAI, smolagents, pydantic-ai, langgraph, etc.

          Traditional API testing involves manually crafting requests, validating responses, and writing test cases. However, AI Agents can significantly streamline this process by generating test cases, validating API responses against expected outputs, and even suggesting improvements based on API documentation. Developers can describe test scenarios in natural language, and the agent can automatically generates API requests, parameter variations, and edge cases. It can also interpret API responses, checking for correctness, consistency, and performance benchmarks. This reduces manual effort while increasing coverage and efficiency, making API testing smarter and more efficient.

          You are also required to prepare benchmark dataset & evaluations so that the right backend LLM can be selected for the end user.

          Skills: AI, Agent, LLM Evaluation, Testing, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          3. API Explorer
          Related Issue - #619

          This project is designed to enhance the API Dash user experience by integrating a curated library of popular and publicly available APIs. This feature allows users to discover, browse, search, and directly import API endpoints into their workspace for seamless testing and exploration. Developers can access pre-configured API request templates, complete with authentication details, sample payloads, and expected responses. This eliminates the need to manually set up API requests, reducing onboarding time and improving efficiency. APIs spanning various domains—such as AI, finance, weather, and social media—are organized into categories, making it easy for users to find relevant services. You are required to develop the entire process backend in the form of an automation pipeline which parses OpenAPI/HTML files, auto-tag it to relevant category, enrich the data, create templates. You can also add features such as user ratings, reviews, and community contributions (via GitHub) to ensure accurate and up-to-date resources.

          Skills: UX Design, OpenAPI, Automation, Dart, Flutter
          Difficulty: Low-Medium
          Length: 175 hours

          ~~~~~~~~~~

          4. AI API Eval Framework
          Related Issue - #618

          Develop an end-to-end AI API eval framework and integrate it in API Dash. This framework should (list is suggestive, not exhaustive):

          Provide an intuitive interface for configuring API requests, where users can input test datasets, configure request parameters, and send queries to various AI API services
          Support evaluation AI APIs (text, multimedia, etc) across various industry task benchmarks
          Allow users to add custom dataset/benchmark & criteria for evaluation. This custom scoring mechanisms allow tailored evaluations based on specific project needs
          Visualize the results of API eval via tables, charts, and graphs, making it easy to identify trends, outliers, and performance variations
          Allow execution of batch evaluations
          Work with both offline & online models and datasets
          Skills: AI, Evaluations, Dart, Python, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          5. API Testing Support for - WebSocket, SSE, MQTT & gRPC
          Related Issue - #15 #115 #116 #14

          Testing WebSocket, MQTT (Message Queuing Telemetry Transport), and SSE (Server-Sent Events) protocols is crucial for ensuring the reliability, scalability, and security of real-time communication systems. Whereas, gRPC (Remote Procedure Call) facilitates efficient communication between distributed systems using Protocol Buffers (protobuf) as its interface definition language (IDL) and offers features such as bi-directional streaming, authentication, and built-in support for load balancing and health checking. Each of these API protocols/styles serves different purposes and is utilized in various applications ranging from finance to web applications to IoT (Internet of Things) devices. The objective of this project is to design the architecture of the core library, understand the specs & implement the support for testing, visualization & integration code generation of these APIs in API Dash.

          Skills: Understanding Specs/Protocols, UX Design, Dart, Flutter
          Difficulty: Medium-High
          Length: 350 hours

          ~~~~~~~~~~

          6. AI UI Designer for APIs
          Related Issue - #617

          Develop an AI Agent which transforms API responses into dynamic, user-friendly UI components, enabling developers to visualize and interact with data effortlessly. By analyzing API response structures—such as JSON or XML—the agent automatically generates UI elements like tables, charts, forms, and cards, eliminating the need for manual UI development. One can connect an API endpoint, receive real-time responses, and instantly generate UI components that adapt to the data format. It must also support customization options, allowing developers to configure layouts, styles, and interactive elements such as filters, pagination, and sorting. Finally, users must be able to easily export the generated UI and integrate it in their Flutter or Web apps.

          Skills: AI, UX, Parsing, XML, JSON, Python, Dart, Flutter
          Difficulty: Easy-Medium
          Length: 90 hours

          ~~~~~~~~~~

          7. API Testing Suite, Workflow Builder, Collection Runner & Monitor
          Related Issues - #96 #100 #120

          The objective of this project to design and implement an API testing & workflow builder suite which allows various types of API testing:

          Validation Testing: Verify that the API meets functional and business requirements. Automate the testing & validation of responses received from an API against predefined expectations (assertions), Schema validations, etc.
          Integration Testing: Checks proper interaction between different APIs
          Security Testing: Identifies vulnerabilities and safeguards data
          Performance Testing: Measures speed, responsiveness, and stability under varying loads
          Scalability Testing: Evaluates the system's ability to grow with demand
          Users should be able to easily create collections of APIs for testing. It will also be useful to provide a API workflow builder (a drag and drop environment) to create API workflows and chain requests. The UI must allow users to execute this collection of API requests and test it in a systematic and automated manner (Collection Runner) and finally monitor the results.

          Skills: UI/UX Design, Automation, Testing, Dart, Flutter
          Difficulty: Medium-High
          Length: 350 hours

          ~~~~~~~~~~

          8. Adding Support for API Authentication Methods
          Issue - #609

          Add support for various API authentication methods:

          Basic authentication: Sending a verified username and password with API request Add API Auth: Basic authentication #610
          API key: Sending a key-value pair to the API either in the request headers or query parameters Add API Auth: API key #611
          Bearer token: Authenticate using an access key, such as a JSON Web Token (JWT) Add API Auth: Bearer token #612
          JWT Bearer: Generate JWT bearer tokens to authorize requests Add API Auth: JWT Bearer #613
          Digest Auth: Client must send two requests. First request sent to the server receives a nonce value, which is then used to produce a one-time-use hash key to authenticate the request Add API Auth: Digest Auth #614
          OAuth 1.0 Add API Auth: OAuth 1.0 #615
          OAuth 2.0 Implement OAuth 2.0 authentication #481
          Skills: Authentication, Dart, Flutter
          Difficulty: Low-Medium
          Length: 90 hours

          ~~~~~~~~~~

          9. mem0 for Dart
          mem0 is the goto memory layer for developing personalized AI Agents in Python. It offers comprehensive memory management, self-improving memory capabilities, cross-platform consistency, and centralized memory control. It leverages advanced LLMs and algorithms to detect, store, and retrieve memories from conversations and interactions. It identifies key information such as facts, user preferences, and other contextual information, smartly updates memories over time by resolving contradictions, and supports the development of an AI Agent that evolves with the user interactions. When needed, mem0 employs a smart search system to find memories, ranking them based on relevance, importance, and recency to ensure only the most useful information is presented.

          Currently, we lack this memory layer in Flutter AI applications and your task is to port mem0 to Dart.

          Skills: AI, Database, Data Structures, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~    

          10. API Dash Feature Improvements
          We always believe in improving our core features to help the end user. A suggestive list of features that can be improved are:

          Adding pre-request script/post request script Pre-request and post-request for api collections #557
          Importing from/Exporting to OpenAPI/Swagger specification Importing Requests from OpenAPI Specification file #121
          Adding support for more content types in request Support for application/x-www-form-urlencoded Content-Type as body type formdata currently only supports multipart/form-data #337 Support File as Request Body #352
          JSON body syntax highlighting, beautification, validation - Enhance Request Body Editor: JSON formatting, syntax highlighting, validation and other features #22 Add option to automatically/manually beautify JSON request body #581 Add syntax highlighting for JSON request body #582 Add validation for JSON request body #583 Add environment variable support in request body #590 Env. Variable Support for Text request body #591 Env. Variable Support for JSON request body #592 Env. Variable Support for Form request body #593
          Support for comments in JSON body Support comments in JSON request body #599
          Reading environment variables from OS environment Reading environment variables directly from OS environment #600
          Adding color support for environments (like RED for prod, GREEN for dev) Adding color support for environments #601
          Tab & whitespace settings
          Notification when new app updates are available [feat] in-app update check #373
          Better GraphQL editor
          Beautify and expand/collapse feature for GraphQL query
          Allow inspecting GraphQL schema
          Support for GraphQL variables, fragments, mutation, subscription, etc.
          More widget & integration tests
          More code coverage
          Skills: UX Design, Dart, Flutter
          Difficulty: Easy-Medium
          Length: 175 hours
    totalCharacters_of_ideas_content_parent: 12602
    totalwords_of_ideas_content_parent: 2654
    totalTokenCount_of_ideas_content_parent: 2473
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/api-dash/
    idea_list_url: https://github.com/foss42/apidash/discussions/565


  
  - organization_id: 5
    organization_name: AboutCode
    no_of_ideas: 10
    ideas_content: |
        PURLdb - DeadCode: track End-Of-Life code
        Code Repositories: https://github.com/aboutcode-org/purldb

        Description:

        Eventually old code goes unmaintained and dies. The goal of this project are:

        To add data structures, models, and APIs in purldb to track end-of-life code and in general package and projects activities
        To improve purl coverage at endoflife.date, see https://github.com/endoflife-date/endoflife.date/issues/763
        To import and sync data from projects such as https://github.com/endoflife-date/endoflife.date
        To design a module that can detect when
        a project is turning end-of-life (using the above)
        a project is unmaintained (use metrics from scorecard/other tools)
        Note that on the endoflife.date side, we need to help improve PURL coverage of the database there, as this would be key to integrate with purldb. There

        Priority: High

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        EOL
        End of life
        Mentors:

        @pombredanne
        @JonoYang
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/purldb/issues/42
        https://github.com/aboutcode-org/vulnerablecode/issues/722
        https://github.com/endoflife-date/endoflife.date/issues/763

        ~~~~~~~~~~
        PURLdb - PopularCode - Find and track actually used and most popular open source code
        Code Repositories: https://github.com/aboutcode-org/purldb

        Description:

        There are between 100 and 200 million open source projects and repos out there. Not all of them are equal. Some are much more useful than others, and some could be safely ignored. For instance, the linux kernel is more important, used and popular than a 1st year computer student school assignment project. The goal of this project is to determine when a project is popular and what are the most popular projects. If we do not know what code is used, we can spend a lot of resources to index less used code.

        There are some simple approaches to this, using available statistics for downloads or Github stars, but that is not satisfying alone.

        An idea would be to consider multiple factors to rank popularity and usage.

        For instance: create a (current and updated) graph of dependencies and compute something like a pagerank but for packages
        Then create with a metric on the freshness of the code like when last release and how much downloaded or based on git activity (excluding bots). This would grow for used code and decay for declining packages
        Then combine this with the dependencies "connectedness"
        Or, just a use the graph connections and no download stats, just a giant graph on top of purldb

        Or something like this:

        Finding strongly connected components
        Relate packages ignoring versions
        Find most connected
        Discount distant connections, boost closest
        Apply decay based on version freshness or git activity
        The approach would be to start small with a single ecosystem as PoC and then extend this to all packages types.

        Ideally, this should be exposed in PurlDB API and integrated in data collection operations.

        Priority: High

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        Popularity
        Mentors:

        @pombredanne
        @JonoYang
        @AyanSinhaMahapatra

        ~~~~~~~~~~
        VulnerableCode project ideas
        There are two main categories of projects for VulnerableCode:

        A. COLLECTION: this category is to mine and collect or infer more new and improved data. This includes collecting new data sources, inferring and improving existing data or collecting new primary data (such as finding a fix commit of a vulnerability)

        B. USAGE: this category is about using and consuming the vulnerability database and includes the API proper, the GUI, the integrations, and data sharing, feedback and curation.

        VulnerableCode: Process unstructured data sources for vulnerabilities (Category A)
        Code Repositories:

        https://github.com/aboutcode-org/vulnerablecode
        Description:

        The project would be to provide a way to effectively mine unstructured data sources for possible unreported vulnerabilities.

        For a start this should be focused on a few prominent repos. This project could also find Fix Commits.

        Some sources are:

        mailing lists
        changelogs
        reflogs of commit
        bug and issue trackers
        This requires systems to "understand" vulnerability descriptions: as often security advisories do not provide structured information on which package and package versions are vulnerable. The end goal is creating a system which would infer vulnerable package name and version(s) by parsing the vulnerability description using specialized techniques and heuristics.

        There is no need to train a model from scratch, we can use AI models pre-trained on code repositories (maybe https://github.com/bigcode-project/starcoder?) and then fine-tune on some prepared datasets of CVEs in code.

        We can either use NLP/machine Learning and automate it all, potentially training data masking algorithms to find these specific data (this also involved creating a dataset) but that's going to be super difficult.

        We could also start to craft a curation queue and parse as much as we can to make it easy to curate by humans and progressively also improve some mini NLP models and classification to help further automate the work.

        References: https://github.com/aboutcode-org/vulnerablecode/issues/251

        Priority: Medium

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        Security
        Vulnerability
        NLP
        AI/ML
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        @Hritik14
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues/251

        ~~~~~~~~~~
        VulnerableCode: Add more data sources and mine the graph to find correlations between vulnerabilities (Category A)
        Code Repositories:

        https://github.com/aboutcode-org/vulnerablecode
        Description:

        See https://github.com/aboutcode-org/vulnerablecode#how for background info. We want to search for more vulnerability data sources and consume them.

        There is a large number of pending tickets for data sources. See https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        Also see tutorials for adding new importers and improvers:

        https://vulnerablecode.readthedocs.io/en/latest/tutorial_add_new_importer.html
        https://vulnerablecode.readthedocs.io/en/latest/tutorial_add_new_improver.html
        More reference documentation in improvers and importers:

        https://vulnerablecode.readthedocs.io/en/latest/reference_importer_overview.html
        https://vulnerablecode.readthedocs.io/en/latest/reference_improver_overview.html
        Note that this is similar to this GSoC 2022 project (a continuation):

        https://summerofcode.withgoogle.com/organizations/aboutcode/projects/details/7d7Sxtqo
        References: https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        Priority: High

        Size: Medium/Large

        Difficulty Level: Intermediate

        Tags:

        Django
        PostgreSQL
        Security
        Vulnerability
        API
        Scraping
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        @Hritik14
        @jmhoran
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        ~~~~~~~~~~
        VulnerableCode: On demand live evaluation of packages (Category A)
        Code Repositories: https://github.com/aboutcode-org/vulnerablecode

        Description:

        Currently VulnerableCode runs importers in bulk where all the data from advisories are imported (and reimported) at once and stored to be displayed and queried.

        The objective of this project is to have another endpoint and API where we can dynamically import available advisories for a single PURL at a time.

        At a high level this would mean:

        Support querying a specific package by PURL. This is not for an approximate search but only an exact PURL lookup.

        Visit advisories/package ecosystem-specific vulnerability data sources and query for this specific package. For instance, for PyPi, the vulnerabilities may be available when querying the main API. An example is https://pypi.org/pypi/lxml/4.1.0/json that lists vulnerabilities. In some other cases, we may need to fetch larger datasets, like when doing this in batch.

        This is irrespective of whether data related to this package being present in the db (i.e. both for new packages and refreshing old packages).

        A good test case would be to start with a completely empty database. Then we call the new API endpoint for one PURL, and the vulnerability data is fetched, imported/stored on the fly and the API results are returned live to the caller. After that API call, the database should now have vulnerability data for that one PURL.

        This would likely imply to modify or update importers to support querying by purl to get advisory data for a specific package. The actual low level fetching should likely be done in FetchCode.

        This is not straightforward as many advisories data source do not store data keyed by package, as they are not package-first, but they are stored by security issue. See specific issues/discussions on these importers for more info. See also how things are done in vulntotal.

        Priority: Medium

        Size: Medium/Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        PostgreSQL
        Security
        web
        Vulnerability
        API
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues/1046
        https://github.com/aboutcode-org/vulnerablecode/issues/1008

        ~~~~~~~~~~
        ScanCode.io project ideas
        ScanCode.io: Create file-system tree view for project scans
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        Description:

        When large packages/containers are scanned in scancode.io it is useful to have a tree-view to explore thorugh the file-tree for that package/container to look into scan data for a particular subset of the file-tree/directory or to research more into detections and detection issues.

        This would be something similar to what we have at scancode-workbench for example: https://scancode-workbench.readthedocs.io/en/develop/ui-reference/directory-tree.html

        I.e. we need the following features:

        To be able to toggle showing the directory contents from the directory icon
        Show nested directory contents in a tree like structure
        Have this view ideally in a pane left to the table-view of resources
        Show only info from the selected directory in the table-view of resources
        Note that we do have a ProjectCodebaseView in the projects page currently in scancode.io but this is fairly limited as it only lets you browse through the codebase one directory at a time (only shows the files/directories in one directory), and lets you navigate to directories in the current directory or the parent directory from there.

        Priority: High

        Size: Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        UI/UX
        File-system
        Navigation
        Mentors:

        @tdruez
        @pombredanne
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/697

        ~~~~~~~~~~
        ScanCode.io: Add ability to store/query downloaded packages
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        Description:

        Packages which are downloaded and scanned in SCIO can be optionally stored and accessed to have a copy of the packages which are being used for a specific product for reference and future use, and could be used to meet source redistribution obligations.

        The specific tasks would be:

        Store all packages/archives which are downloaded and scanned in SCIO
        Create an API and index by URL/checksum to get these packages on-demand
        Create models to store metadata/history and logs for these downloaded/stored packages
        Additionally support and design external storage/fetch options
        There should be configuration variable to turn this on to enable these features, and connect external databases/storage.

        Priority: Low

        Size: Medium

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        CI
        Security
        Vulnerability
        SBOM
        Mentors:

        @tdruez
        @keshav-space
        @jyang
        @pombredanne
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/1063


        ~~~~~~~~~~

        ScanCode.io: Update SCIO/SCTK for use in CI/CD:
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        https://github.com/aboutcode-org/scancode-action
        Description:

        Enhance SCIO/SCTK to be integrated into CI/CD pipelines such as Github Actions, Azure Piplines, Gitlab, Jenkins. We can start with any one CI/CD provider like GitHub Actions and later support others.

        These should be enabled and configured as required by scancode configuration files to enable specific functions to be carried out in the pipeline.

        There are several types of CI/CD pipelines to choose from potentially:

        Generate SBOM/VDRs/VEX with scan results:

        Scan the repo to get all purls: packages, dependencies/requirements
        Scan repository for package, license and copyrights
        Query public.vulnerablecode.io for Vulnerabilities by PackageURL
        Generate SPDX/CycloneDX SBOMs from them with scan and vulnerability data
        License/other Compliance CI/CD pipelines

        Scan repo for licenses and check for detection accuracy
        Scan repo for licenses and check for license clarity score
        Scan repo for licenses and check compliance with specified license policy
        Check for OpenSSF scorecard data and specified policy on community health metrics
        The jobs should pass/fail based on the scan results of these specific cases, so we can have:
        a special mode to fail with error codes
        description of issues and failure reasons, and docs on how to fix these
        ways to configure and set up for these cases with configuration files
        Dependency checkers/linters:

        download and scan all package dependencies, get scan results/SBOM/SBOMs
        check for vulnerable packages and do non-vulnerable dependency resolutuion
        check for test failures after dependency upgrades and add PR only if passes
        Jobs which checks and fixes for misc other errors:

        Replaces standard license notices with SPDX license declarations
        checks and adds ABOUT files for vendored code
        We have an initial CI runner at https://github.com/nexB/scancode-action but we need to improve this with more functions, specially checking against predefined policies and failing/successful CI based on that.

        References:

        https://github.com/aboutcode-org/scancode.io/issues/599
        https://github.com/aboutcode-org/scancode.io/issues/1582
        Priority: High

        Size: Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        CI
        Security
        License
        SBOM
        Compliance
        Mentors:

        @pombredanne
        @tdruez
        @keshav-space
        @tg1999
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/599

        ~~~~~~~~~~
        ScanCode Toolkit project ideas
        Have variable license sections in license rules:
        Code Repositories:

        https://github.com/aboutcode-org/scancode-toolkit
        Description:

        There are lots of variability in license notices and declarations in practice, and one example of modeling this is the SPDX matching guidelines. Note that this was also one of the major ways scancode used to detect licenses earlier.

        Support grammar for variability in license rules (brackets, no of words)
        Do a massive analysis on license rules and check for similarity and variable sections This can be used to add variable sections (for copyright/names/companies) and reduce rules.
        Support variability in license detection post-processing for extra-words case
        Add scripts to add variable sections to rules from detection issues (like bsd detections)
        Priority: Medium

        Size: Medium

        Difficulty Level: Intermediate

        Tags:

        Python
        Licenses
        LicenseDetection
        SPDX
        Matching
        Mentors:

        @AyanSinhaMahapatra
        @pombredanne
        @jyang
        @DennisClark
        Related Issues:

        https://github.com/aboutcode-org/scancode-toolkit/issues/3601

        ~~~~~~~~~~

        Mark required phrases for rules automatically using NLP/AI:
        Code Repositories:

        https://github.com/aboutcode-org/scancode-toolkit
        Description:

        Required phrases are present in rules to make sure the rule is not matched to text in a case where the required phrase is not present in the text, which would be a false-positive detection.

        We are marking required phrases automatically based on what is present in other rules and license attributes, but this still leaves a lot of rules without them. See https://github.com/aboutcode-org/scancode-toolkit/pull/3924 where we are also adding a script to add required phrases as individual rules if applicable and also adding required phrases added to other rules.

        research and choose a model pre-trained on code (StarCoder?)
        use the dataset of current SCTK rules to train a model
        Mark required phrases in licenses automatically with the model
        Test required phrase additions, improve and iterate
        Bonus: Create a minimal UI to review rule updates massively
        Priority: Medium

        Size: Medium

        Difficulty Level: Advanced

        Tags:

        Python
        ML/AI
        Licenses
        Mentors:

        @AyanSinhaMahapatra
        @tg1999
        @pombredanne
        Related Issues:

        https://github.com/aboutcode-org/scancode-toolkit/issues/2878

          
    totalCharacters_of_ideas_content_parent: 19350
    totalwords_of_ideas_content_parent: 4455
    totalTokenCount_of_ideas_content_parent: 4378
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aboutcode/
    idea_list_url: https://github.com/aboutcode-org/aboutcode/wiki/GSOC-2025-Project-Ideas




  - organization_id: 6
    organization_name: Accord Project
    no_of_ideas: 7
    ideas_content: |
        1. Linter for Concerto
        Write a linter in TypeScript for Concerto Source files. It should make use of existing functionality to validate the Concerto DSL syntax and JSON AST of Concerto model against a set of rules. Rules should be defined in Typescript and which rules are run should be configurable. You may be able to make use of a tool like Spectral as the framework for defining our own rules over the Concerto AST (JSON).

        Expected Outcomes:
        A tool that allow users to:

        Specify the naming of declarations. E.g. all names of scalars should be in camel case.
        Specify the naming of properties, enum cases e.t.c
        Specify which language features can be used. E.g. disallow maps, disallow forward references in regex validators.
        Enforce the use of certain features. E.g. all string properties should have a length validator.
        Enforce the use of @Term decorators on all declarations and properties e.t.c
        All concepts in a namespace should extend a given concept
        All concepts in a namespace must have unique names across multiple namespaces
        Skills required/preferred:
        Algorithms, Functional programming, Back end development, NodeJS, TypeScript

        Possible Mentors:
        Jamie Shorten, Sanket Shevkar

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        2. Decorator Command Set JSON<->YAML Convertor
        Design and implement a convertor that would convert Decorator Command Sets JSON Objects to a much more human readable YAML format and vice-versa. Currently DCS JSON objects are very verbose to read, write and edit. With the new custom YAML format we aim to make DCS objects much more easier to read, write and edit.

        Expected Outcomes:
        A utility/method in DecoratorManager to convert DCS JSON to YAML and from YAML to JSON.
        1:1 conversion is not expected. YAML should have a custom format that is less verbose and more readable.
        Skills required/preferred:
        NodeJS, Typescript, Javascript, Basic understanding of Data Formats like JSON and YAML

        Possible Mentors:
        Sanket Shevkar

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        3. Accord Project Agreement Protocol
        The Accord Project Agreement Protocol (APAP) defines the protocol used between a document generation engine or contract management platform and an agreement server that provides agreement features like template management, document generation, format conversion etc.

        Expected Outcomes:
        Updated Open API specification
        Updated reference implementation for the specification
        Address (some of) open issues
        Skills required/preferred:
        NodeJS, Typescript, Javascript, REST API design

        Possible Mentors:
        Dan Selman, Niall Roche

        Expected size of project:
        350 hours (large)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        4. Specification Conformance Tests
        Our specification conformance testing is in need of an overhaul! We'd like to migrate to a robust, proven testing framework like Vitest which would support ESM, and be performant and have a new dedicated concerto package used for Concerto conformance testing. An AI tool may be useful in helping with the migration, so feel free to mention how AI could help you with this project! The goal is to have a set of tests that can be run against any Concerto implementation to assess whether it is conformant with the specification.

        Expected Outcomes:
        Migration to Vitest (or other appropriate framework)
        Consolidation of testing methodology and tooling
        New concerto package for tests, focused on conformance
        Build a set of tests for the Concerto validation rules
        Skills required/preferred:
        Node / Javascript
        Unit testing (Mocha / Jest for example)
        Behaviour driven testing (optional, Cucumber, for example)
        Possible Mentors:
        Dan Selman, Ertugrul Karademir

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        5. Incorporating AI into Template Playground
        Our Template Playground web application is used to help onboard users to our technologies. We'd love to make this even easier by adding AI features to make it easier to create, edit, and preview contract templates. This project will build upon the work that was carried out last year in the context of VS Code.

        Expected Outcomes:
        Allow users to upload a file and we'd use AI to convert it to an Accord Project template
        Possibly incorporate auto-complete suggestions when editing using the code editors built into the web app
        Skills required/preferred:
        ReactJS, AI tooling

        Possible Mentors:
        Diana Lease

        Expected size of project:
        350 hours (large)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        6. Testing for Code Generation Targets
        We have tools that allow users to generate code from their Concerto models, supporting several languages. We would like to introduce a way of testing this code generation that compiles code for each language we are generating.

        Expected Outcomes:
        Set of Docker images for each code generation target
        Run code gen tests within the correct image using GitHub actions, for example, generate Java code and then compile and run it using javac to ensure the generated code is correct
        Skills required/preferred:
        Systems engineering, CI/CD
        Docker, Docker compose
        GitHub actions
        Possible Mentors:
        Dan Selman, Ertugrul Karademir

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        7. Migration of Template Playground to use Tailwind CSS
        As mentioned previously, our Template Playground web application is used to help onboard users to our technologies. By using a popular, well-maintained CSS framework like Tailwind, we could improve performance and code maintainability.

        Expected Outcomes:
        Template Playground updated to use Tailwind CSS
        Existing UI tests updated
        Possibly other UI changes to make user experience better, more performant, and/or optimized for multiple screen sizes
        Skills required/preferred:
        ReactJS, Tailwind CSS

        Possible Mentors:
        Diana Lease

        Expected size of project:
        175 hours (medium) - 350 hours (large)

        Expected difficulty:
        Medium


          
    totalCharacters_of_ideas_content_parent: 6850
    totalwords_of_ideas_content_parent: 1663
    totalTokenCount_of_ideas_content_parent: 1391
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/accord-project/
    idea_list_url: https://github.com/accordproject/techdocs/wiki/Google-Summer-of-Code-2025-Ideas-List



  - organization_id: 7
    organization_name: Alaska 
    no_of_ideas: 15
    ideas_content: |
        [1] Automated coastline extraction for erosion modeling in Alaska.

        Mentors: Frank Witmer (fwitmer -at- alaska.edu) and Rawan Elframawy (rawann.elframawy -at- gmail.com)

        Overview: The rapidly warming Arctic is leading to increased rates of coastal erosion, placing hundreds of Alaska communities at the frontline of climate change. Understanding current rates of coastline change and accurately forecasting future changes is critical for communities to mitigate and adapt to these changes. Current modeling approaches typically use a simple linear model based solely on historical coastline positions to measure rates of change and extrapolate them into the future. In doing so, these models fail to capture the dynamic effects associated with decreasing sea ice, increasing annual wave energy, and increasing temperatures. To improve the quality of these coastal models, we need to increase the quantity of digitized coastlines, but manual photointerpretation is slow and laborious.

        Current Status: An initial model and pipeline have been developed to automatically extract coastlines from PlanetLabs imagery. An auto-download script is available to retrieve PlanetLabs imagery (3-5m spatial resolution) by specifying any timeframe, cloud coverage percentage, and geometry. Additionally, NDWI with a majority sliding window has been introduced, allowing a specific threshold for each window to improve water detection accuracy. The DeepWaterMap algorithm was originally trained with the Global Surface Water (GSW) dataset at 30 m resolution from Landsat imagery, but the model did not not work well applied to PlanetLabs imagery. We are working to re-train the model using PlanetLabs imagery automatically labeled using the NDWI thresholding method. This project extends and expands on the progress made in 2024.

        Potential areas of improvement:

        Data Expansion (Deering 2017–2019 and Beyond): Currently using data from 2017 to 2019 for Deering; we plan to include more recent data to extend the time series.
        Improved Cliff Area Segmentation: Enhance segmentation performance specifically in steep or cliff-like coastal areas.
        Handling Challenging Conditions: Improve segmentation in regions with water shadows, buildings, satellite artifacts, and other data quality issues.
        SWIR and Elevation Data Integration: Investigate combining short-wave infrared (SWIR) data and elevation data (e.g., DEMs) to further refine segmentation accuracy.
        Expected Outcomes: A finished model with high accuracy that automatically extracts a vectorized coastline representation from PlanetLabs satellite imagery. Then, the model can be applied to large amounts of imagery to model coastline changes over time.

        Required Skills: Python

        Code Challenge: Experience with multi-band satellite imagery, geospatial data processing, and machine learning.

        Source Code: https://github.com/fwitmer/CoastlineExtraction

        Discussion Forum: https://github.com/fwitmer/CoastlineExtraction/discussions

        Effort: 350 Hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [2] Support for Logarithmic Number Systems in a Deep-Learning Framework.

        Mentors: Mark Arnold (markgarnold -at- yahoo.com), Ed Chester (ed.chester -at- gmail.com), and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: The Logarithmic Number System (LNS) is an alternative to built-in Floating Point (FP), which makes multiplication and division easy at the expense of more difficult addition. Using overloaded operators, xlns provides an open-source Python library for LNS arithmetic. Interest in fabricating LNS hardware has grown since it may reduce power consumption for applications that tolerate approximate results, such as deep learning (see [1]-[5]). The problem is deep learning often relies on open-source Python frameworks (like Tensorflow or Pytorch) that are hardcoded to use FP hardware. A key feature of these frameworks is the ability to automatically compute gradients (based on the chain rule) by recording extra information about the computation stored in FP format. Such gradients are used during backpropagation training to update network weights.

        Current Status: xlns, Tensorflow and Pytorch are all interoperable with the widely-used open-source Numpy library, but xlns is not interoperable with the Tensorflow and Pytorch frameworks because both frameworks are hard coded to use built-in int or FP data internally instead of LNS.

        Expected Outcomes: The goal of this project is to provide support for a deep learning framework that uses xlns instead of FP internally (including network specification, automatic gradient, forward inference, and back-propagation training) while keeping high-level compatibility with the framework. This might be as part of xlns, or as a forked version of the chosen framework, or both. The contributor may choose either Pytorch or Tensorflow. The contributor should justify these decisions as part of the proposed design.

        Required Skills: Calculus, Python, Numpy, and either Pytorch or Tensorflow

        Code Challenge: The following three challenges illustrate the breath of issues involved. Each involves only a few lines of Python. Each involves working with both xlns and the framework. Doing all three in both Tensorflow and Pytorch might give evidence for which framework is more likely to lead to the expected outcome.

        Currently, when the data starts in xlns format, Pytorch/Tensorflow converts to FP. As part of the code challenge, we expect the contributor to provide short Python code snippets that demonstrate that if the data starts in xlns format, the computation cannot be carried out in the xlns format.

        xlns/examples/arn_generic.py is a hard-coded illustration of training a fully connected MLP with 28*28 input nodes, 100 hidden nodes and 10 output nodes using MNIST digit set. The hidden layer uses RELU and the output layer uses softmax. The FP weights for this are initialized as:

        W1 = np.array((list(np.random.normal(0, 0.1, (785, 100)))))                    
        W2 = np.array((list(np.random.normal(0, 0.1, (101, 10)))))
        Because there is an extra weight for a constant 1.0 input in each layer, the number of rows is one larger than the inputs to the layer. The example can be run with various data types, for example with xlnsnp (LNS internally implemented with int64 Numpy ufuncs):

        python3 arn_generic.py --type xlnsnp --num_epoch 7
        or more conventionally

        python3 arn_generic.py --type float --num_epoch 7
        The code challenge is to implement a similar size fully connected network (in FP) using the provided features of Pytorch or Tensorflow and compare its convergence with arn_generic.py (Note: arn_generic.py uses manual differentiation, ie, the derivative of RELU is a constant, which depends on the sign of the argument, and elementary backpropagation implements the chain rule).

        Consider LNS addition (1+2=3 and 3-1=2). The following illustrates the overloaded operator and xlnsnp internal representation (sign is LSB of the int64 value; the log portion is the rest):
        >>> import xlns as xl
        >>> x=xl.xlnsnp([2.0, 3.0])
        >>> x
        xlnsnp([xlns(1.9999999986889088) xlns(2.9999999688096786)])
        >>> x.nd
        array([16777216, 26591258])
        By default, the log portion here is given with 23 bits of precision (see help for xl.xlnssetF for details on how to lower the precision as would be useful in machine learning), which is why the log(2.0) is given as 16777216.

        >>> 2*np.int64(np.log2([2.0, 3.0])*2**23)
        array([16777216, 26591258])
        The expression with log2 double checks the answer for x in 23-bit format (with the additional *2 to make room for the sign bit). Had the +2.0 been -2.0, the representation would have been 16777217.

        >>> y=xl.xlnsnp([1.,-1.])
        >>> y
        xlnsnp([xlns(1.0) xlns(-1.0)])
        >>> y.nd
        array([0, 1])
        The above illustrates that the log(1.0)=0, and that the sign bit is one for negative values.

        >>> x+y
        xlnsnp([xlns(2.9999999688096786) xlns(1.9999999986889088)])
        >>> (x+y).nd
        array([26591258, 16777216])
        Although the Pytorch/Tensorflow frameworks don’t support LNS, LNS can be constructed from int64 and float operations (which is how xlnsnp works). In xlns/src/xlns.py, there is a function sbdb_ufunc_ideal(x,y). If you call this with the following code:

        >>> import numpy as np
        >>> def myadd(x,y):  
                  return np.maximum(x,y)+xl.sbdb_ufunc_ideal(-np.abs(x//2-y//2), (x^y)&1) ))
        it performs the same operation internally on int64 values as the overloaded operator:

        >>> myadd(x.nd,y.nd)
        array([26591258, 16777216])
        Such operations are supported by the frameworks (rather than here from np). This code challenge is to do a similar toy example within the tensor types provided by the framework, which gives a small taste of the difficulty involved in this project. (The code above for myadd is a slight oversimplification of xl.xlnsnp.__add__; see this for details on the treatment of 0.0.)

        References:

        [1] G. Alsuhli, et al., “Number Systems for Deep Neural Network Architectures: A Survey,” https://arxiv.org/abs/2307.05035, 2023.

        [2] M. Arnold, E. Chester, et al., “Training neural nets using only an approximate tableless LNS ALU”. 31st International Conference on Application-specific Systems, Architectures and Processors. IEEE. 2020, pp. 69–72. https://doi.org/10.1109/ASAP49362.2020.00020

        [3] O. Kosheleva, et al., “Logarithmic Number System Is Optimal for AI Computations: Theoretical Explanation of Empirical Success”, https://www.cs.utep.edu/vladik/2024/tr24-55.pdf

        [4] D. Miyashita, et al., “Convolutional Neural Networks using Logarithmic Data Representation,” https://arxiv.org/abs/1603.01025, Mar 2016.

        [5] J. Zhao et al., “LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update,” IEEE Trans. Computers, vol. 71, no. 12, pp.3179–3190, Dec. 2022, https://doi.org/10.1109/TC.2022.3202747

        Source Code: https://github.com/xlnsresearch/xlns

        Discussion Forum: https://github.com/xlnsresearch/xlns/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [3] Developing Distributed Algorithm for Metagenomic Error Correction and Assembly.

        Mentors: Arghya Kusum Das (akdas -at- alaska.edu) and Yali Wang (ywang35 -at- alaska.edu)

        Overview: A metagenomics study of Alaska would explore the diverse microbial communities in its unique environments, including the Arctic, marine, and terrestrial ecosystems. Such research could uncover insights into microbial adaptation to extreme conditions and contribute to understanding environmental and climate-related changes in the region. Metagenomic study has an immense impact on multiple science and engineering projects in Alaska such as, arctic healthcare, arctic water pollution, bio leaching on rare earth elements, arctic environmental sustainability and resilience, understanding boreal forest dynamics, wildfire mitigation, and so on. The list is never ending. Shotgun metagenomics, which involves sequencing DNA from a mixed sample of genomes within a community, offers a high-throughput approach to examine the genomic diversity of microbial populations. A key step in metagenomic analysis is assembling the shotgun reads into longer contiguous sequences, or contigs. However, genome assemblies from short reads are often highly fragmented, potentially generating millions of contigs per sample, especially in diverse communities. This challenge arises due to issues like sequence repeats within and between genomes, low coverage of certain species, and strain variability.

        Current Status: Because of the variability in abundance in multiple species in the mixed sample of genomes, it is hard to design a theoretically solid algorithm to rectify the error in the sample and assemble it accurately. The low abundance species in the mixed sample are often wrongly classified as error if we use a traditional/existing algorithms that can rectify the error in a single species’ whole genome sequence. For the similar reason, the existing metagenomic assemblers are perform sub-optimally. Further, the existing software are limited in terms of their data handling capability. Most of them are capable to operate in a single node only. So, their data nailing is severely limited by the RAM available in one node. Also the time consumed for large datasets are often unreasonable.

        Expected Outcomes: In this project, we will address the first two steps in metagenomic analysis i.e., error correction and assembly which are paramount for any downstream project. Metagenomic data is often large in size spanning to hundreds of gigabytes to terabyte scale. Our motivation is to develop distributed, HPC compatible solution for metagenomic error correction and assembly

        (1) We are looking for working solutions (a solid algorithm and its implementation) for metagenomic error correction and assembly. The solutions should be theoretically justifiable and/or biologically meaningful. (2) The algorithm and the software implementation for both error correction and assembly should be distributed in nature. (3) We are open for AI/ML-enabled solutions but that is not a requirement. (4) GPU-enabled solutions are also encouraged but, it’s also not a requirement.

        Required Skills: Python and experience with Deep Neural Networks

        Code Challenge: Prior experience creating deep learning models is expected.

        Source Code: https://github.com/akdasUAF/Metagenome

        Discussion Forum: https://github.com/akdasUAF/Metagenome/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium/Hard

        ~~~~~~~~~~

        [4] Telehealth over L4S.

        Mentors: Kolawole Daramola (koladaramola -at- icloud.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service 1, 2, 3 has shown promising performance, by rethinking congestion control. Can we have a telehealth deployment with pairs of L4S nodes? Perhaps starting with something simple, such as two DICOM endpoints to send radiographic images in between? Linux kernel with L4S patches can be a good point to start for the endpoints. How L4S, with telehealth and other applications, as well as classic non-L4S traffic, share the network will be an interesting test.

        Current Status: A prototype has been built as part of the GSoC 2024. As rural Alaska is largely unconnected by the road network, people often need to fly into larger towns such as Fairbanks and Anchorage for their healthcare needs. This state of affairs has steered the telehealth initiatives in Alaska much more than elsewhere in the US. Our research partners from healthcare organizations such as Alaska Native Tribal Health Consortium (ANTHC) utilize telehealth in their daily operations. Improved telehealth access and performance can significantly benefit the patients and providers in terms of patient satisfaction and comfort.

        Expected Outcomes: This project will review the latest advances from the research, deployment, and testing perspectives with using L4S in telehealth. The contributor will look into how this can be deployed in practice for various telehealth applications – sending DICOM images for diagnostics (high volume of data but tolerance for high latency), telemonitoring via wearable devices (low volume of data but demand for low latency), televisits (a video call through apps such as Zoom – high volume of data and demand for high latency). As a result of this project, we will understand whether we need any optimizations for L4S to use for telehealth applications and potential alternative approaches.

        Required Skills: Python

        Code Challenge: Experience with network protocols and installing Linux servers is a plus. Coding experience demonstrating such experiences is considered positive.

        Source Code: https://github.com/KathiraveluLab/L4SBOA

        Discussion Forum: https://github.com/KathiraveluLab/L4SBOA/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [5] Creating shareable "albums" from locally stored DICOM images

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM data sets downloaded from PACS environments typically remain in the local environments, such as a research server or a cluster where the DICOM retriever (C-MOVE) is run. To use this data, researchers must identify certain subsets of data. This can be achieved by querying the retrieved data. DICOM images consist of textual metadata. By querying the metadata, subsets of images can be identified. However, currently, creating "albums" from locally stored DICOM images is not seamless.

        Current Status: This feature does not exist in our open-source frameworks. We share images through other orthogonal approaches (via rclone, for example). This project will implement a stand-alone utility to effectively create albums from locally stored DICOM images.

        Expected Outcomes: Several approaches to implementing such album features exist. One approach is to use Kheops to provide an interface to create and view the albums. MEDIator can be extended to create subsets and share the images via a unique URL as well. The proposed feature will make the images accessible to more researchers for their experiments by replacing the current manual data sharing efforts. Moreover, Kheops natively integrates with OHIF Viewer. As such, images retrieved locally can be viewed through OHIF Viewer by creating albums with Kheops. Contributors are encouraged to use Kheops or alternatives rather than reinventing the wheel (unless there is a convincing reason).

        Required Skills: Python and Java.

        Code Challenge: Experience working with DICOM images from previous projects or through a sample dummy project will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Easy

        ~~~~~~~~~~

        [6] Beehive: Integrated Community Health Metrics Framework for Behavioral Health to Supplement Healthcare Practice in Alaska.

        Mentors: David Moxley (dpmoxley -at- alaska.edu) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: This project, a collaboration between the University of Alaska Anchorage Departments of Computer Science and Human Services, seeks to create a digital approach to translating the digitalization of art and photographic images into a digital database that stores in retrievable formats those images for use in advancing the delivery of human services and health care to people who experience considerable vulnerability and marginalization within the community. One of the project goals is to create a digital repository of these images, many of which reflect Outsider Art since the people who produce them are not formally trained as artists and experience considerable discrimination. The repository can be used to support research on Outsider art and Outsider Artists, education of health and human services practitioners about the impact of negative stereotypes on the health and well-being of people who are highly vulnerable, and arts programs devoted to advancing the health of vulnerable people.

        This project aims to develop Beehive, a prototype implementation as an open-source data federation framework that can be used in research environments in Alaska and elsewhere.

        Current Status: A prototype has been built as part of Alaska Season of Code. We are researching the approach for its use with our community partners in Anchorage, aiming to support marginalized folks such as the unhoused.

        Expected Outcomes: In this project, the contributor will develop the Beehive platform for (1) translating digital images into the database, (2) developing the database to support user interactions with content, and (3) facilitating retrieval of images. The contributor will obtain an orientation to the project, instruction in how the arts and photography can represent health and well-being, and insight into using digital representations as an advocacy tool for improving the well-being of highly vulnerable people.

        Required Skills: Database (MySQL or Mongo) and Python or Java. A build management tool such as Apache Maven is recommended if using Java.

        Code Challenge: Prior experience with database management through established coding examples.

        Source Code: https://github.com/kathiraveluLab/beehive.

        Discussion Forum: https://github.com/KathiraveluLab/Beehive/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [7] DICOM Image Retrieval and Processing in Matlab.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM (Digital Imaging and Communications in Medicine) is a radiographic imaging standard for how various modalities of scanners, PACS (Picture archiving and communication system) and other imaging systems communicate. As a storage protocol, it defines how images are stored in a standard way. It also functions as a messaging protocol, an extension to TCP.

        Many DICOM processing tools exist. They support receiving images from the scanners and PACS to a research cluster in real-time as an imaging stream or on-demand selectively. They also provide means to anonymize the DICOM images to preserve patient privacy, export the DICOM images into a format such as PNG or JPEG, and extract the textual metadata from DICOM files to store it in a CSV file format or a database. Machine learning pipelines cannot be executed in clinical systems such as scanners and PACS. Therefore, the DICOM images and their metadata in the research clusters can be used to run machine learning pipelines.

        Matlab has some out-of-the-box support for certain DICOM functions, and it could make our job easy in certain projects. This facilitates processing the files from the file system 2. Region-of-Interest is natively supported for DICOM-RT files in Matlab 3. It also supports deep learning on DICOM and NifTi files 4. Matlab currently does not support receiving images from DICOM systems such as PACS and Scanners over the network. Matlab used to have functions that utilize the Dicom toolkit to pull images from another server. It was available through Matlab's file exchange at one point called "dicom server connection." This is not publicly available anymore. However, we have the implementation available locally. The code was not recently tested, and therefore, its usability with the latest Matlab versions needs to be confirmed.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This project aims to create an easy-to-use open-source Matlab DICOM processing framework. We start with processing DICOM images since the current status of the DICOM networking in Matlab is unknown. But we will explore it, if possible and time permitting. Since this is a research project, we should study the existing projects first to avoid re-inventing the wheel. From Google Scholar, we see many processing and pipelines (ROI, deep learning, ...) on DICOM/DICOM-RT have been implemented using Matlab. Regardless of the scientific novelty, we can get an open-source solution to help with further ML stuff using Matlab on the DICOM files. However, we should also observe how this could be a scientific contribution and its merits beyond what is already available. We can use readily available public DICOM data sources to test our implementations, such as the Cancer Imaging Archive (TCIA), as that avoids having to deal with sensitive patient data with PHI. We will narrow down on a specific research use case to highlight the framework's usage in research.

        Required Skills: Matlab

        Code Challenge: Experience working with DICOM images from previous projects and prior experience with Matlab, as demonstrated through code examples, will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Hard


        ~~~~~~~~~~

        [8] Making ZeroMQ a first-class feature of concore.

        Mentors: Shivang vijay (shivangvijay -at- gmail.com), Rahul Jagwani (rahuljagwani1012 -at- gmail.com), and Mayuresh Kothare (mvk2 -at- lehigh.edu)

        Overview: concore is a lightweight framework for closed-loop peripheral neuromodulation control systems. concore consists of a file-sharing based concore protocol to communicate between the programs in a study. concore also allows a shared-memory based communication between programs. This project will implement a ZeroMQ-based communication between programs, as an alternative to the file-sharing based and shared-memory based communications. ZeroMQ is a message-oriented middleware implemented in multiple languages, which natively supports communications across computing nodes. Such an implementation will improve the usability of concore in distributed environments.

        The study with 0MQ

        Current Status: We experimented with an osparc-control based communication as an alternative to this default file-sharing based concore protocol. osparc-control is an extension of ZeroMQ. Our experimental osparc-control based implementation replaces the file-sharing mechanism restricted to one local machine with message queues that can be transmitted between locally networked machines. The contributor will use this osparc-control based communication as an inspiration for the proposed ZeroMQ-based implementation, which will function as a first-class approach to implement the edges of concore without using osparc-control. In our current experimental osparc-control based implementation, these ZeroMQ edges are not visible in the concore editor, the browser-based visual editor for concore. Consequently, studies with osparc-control are represented as forests instead of directed hypergraphs due to the "invisible" ZeroMQ communication. This also means to run a concore study with ZeroMQ communication, we have to run each hypergraph in the forest separately.

        Expected Outcomes: We need to promote a unified experience in concore, whether the edges are implemented via the default file-sharing approach, shared-memory approach, or through this ZeroMQ message-based approach. In the concore file-sharing approach, we label the edges with alphabetical characters. In the concore shared-memory approach, we label the edges starting with positive decimal integers (specifying the memory channels used for the sharing). Therefore, to denote the concore ZeroMQ-based edges, the contributor should assume that all the ZeroMQ-edges must start with "0" in their labels, followed by a hexadecimal port, followed by an underscore (_). For example, edge 0x1234_Y assigns the logical Y to port 1234 and edge 0xabcd_U assigns the logical U to port abcd. Once such a graph with ZeroMQ-edges is made (a single directed hypergraph, rather than a forest with disjoint two or more directed hypergraphs), we should be able to seamlessly build and run the study regardless of the underlying communication mechanism. Thus, we aim to demonstrate the possibility of a seamless local vs. distributed execution in a cluster through ZeroMQ.

        As the expected outcome of this project, we propose a ZeroMQ-based communication for concore with Python. In addition, the contributor may also implement the ZeroMQ-based communication with other programming languages supported by concore such as Matlab and C++. The contributor may also get inspiration from how the shared-memory based communication is implemented in concore.

        Required Skills: Python

        Code Challenge: Prior experience in Python must be demonstrated. Prior experience with message-oriented middleware frameworks such as ZeroMQ can be a plus, although not mandatory.

        Source Code: https://github.com/ControlCore-Project/concore

        Discussion Forum: https://github.com/ControlCore-Project/concore/discussions

        Effort: 350 Hours

        Difficulty Level: Medium


        ~~~~~~~~~~

        [9] Dynamic DICOM Endpoints.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM (Digital Imaging and Communications in Medicine) is a radiographic imaging standard for how various modalities of scanners, PACS (Picture archiving and communication system), and other imaging systems communicate. As a storage protocol, it defines how images are stored in a standard way. It also functions as a messaging protocol, an extension to TCP. DICOM implementations often have a queue to hold the images sent from the source. Since this is a networking communication, a queue may degrade the performance or introduce data loss. DICOM communications are defined by static source, query, and destination endpoints. Each endpoint is defined by hostname/IP address, port, and AE (Application Entity) Title. A DICOM endpoint such as a PACS or a scanner usually has these endpoints statically configured to ensure security and patient privacy.

        This project attempts to send data from a source to dynamic destinations based on the queue and the performance. This can be a use case for teleradiology with multiple remote healthcare/radiologist sites present or a potential framework to enable federated learning on radiographic images. Orthanc can be set up as a DICOM endpoint that mimics a PACS 1. With multiple Orthanc servers configured, such a federated deployment can be prototyped. Ultimately, this project aims to study the possibilities and opportunities of supporting dynamic DICOM endpoints in practice.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: A prototype implementation that supports dynamic DICOM endpoints.

        Required Skills: Python

        Code Challenge: Experience working with DICOM images from previous projects or through a sample dummy project will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [10] Bio-Block: A Blockchain-based Data Repository and Payment Portal.

        Mentors: Chalinda Weerasinghe (chalindaweerasinghe -at- gmail.com), Erik Zvaigzne (erik.zvaigzne-at-gmail.com), and Forrester Kane Manis (Forrester-at-headword.co)

        Overview: Most biological, genomic, genetic, medical, and behavioral data are currently collected, stored, and sold by vendors who initially offer products and services to clients in order to accumulate this data. The data, once given to companies, remains the property of the company, with very little compensation and autonomy offered to customers who provided the data in the first place. Can we create a secure, decentralized, and scalable data repository of such information for humans and animals, a true bio-block available to all and open-sourced, whereby the data owners get directly compensated? This project offers a response in the affirmative and leverages blockchains for data distribution, archiving, recording, and payments using a dual-chain structure on the Ethereum blockchain.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This overall project will be one of the first offerings of an open-source platform for all biological/medical/genomic/behavioral data that leverages the advantages of blockchains. While proprietary dual-chain blockchain architectures are used by companies in this space, our endeavor, through its sub-projects, aims to proof up the architecture that can be scaled and extended to all forms of client-submitted data and multiple retrieval and payment options. A proof of concept of the architecture will be tested using multivariate, heterogenous synthetic data.

        Required Skills: Python is proposed as the programming language. However, students can also propose their preferred alternative programming language and frameworks. Prior experience developing on Ethereum is a plus.

        Code Challenge: Prior experience in Python (or the proposed alternative language) and, preferably, Ethereum blockchain through established coding examples. Students are expected to establish their experience with Blockchain technologies and architecting and programming them through previous projects - ideally through their respective GitHub repository (or similar code repositories).

        Source Code: https://github.com/bio-block/healthy (New Project).

        Discussion Forum: https://github.com/bio-block/healthy/discussions

        Effort: 350 hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [11] Adopting Nunaliit for Alaska Native Healthcare Practices.

        Mentors: Jessica Ross (jmross2 -at- alaska.edu) and Maria Williams (mdwilliams6 -at- alaska.edu)

        Overview: Nughejagh is an Alaska Native holistic healthcare application. It uses Nunaliit as its map-based interface to store its data. The data is curated from various sources in the form of images, stories, and videos - which are stored using the Nunaliit map-based interface, supported by its CouchDB database. However, currently, Nunaliit lacks several desired features for Nughejagh. This project aims to fill the gap by implementing those features and developing scripts to automate the installation, configuration, and data loading process.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: The expected goal is to have Nunaliit fine-tuned and configured to run Nughejagh with all its requirements. The project outcome might be a new stand-alone repository that uses Nunaliit, a forked version of Nunaliit, or more likely both. The contributor should justify their design decisions as part of the proposed design.

        Required Skills: Prior experience in Javascript, Java, and Python.

        Code Challenge: Deploy and configure Nunaliit locally and share a screenshot of a locally-running Nunaliit instance. Nunaliit runs well on Ubuntu 24.04.

        Source Code: https://github.com/Nughejagh/nughejagh (New Project).

        Discussion Forum: https://github.com/Nughejagh/nughejagh/discussions

        Effort: 350 hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [12] AWANTA: A Virtual Router based on RIPE Atlas Internet Measurements.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: RIPE Atlas is an Internet Measurement network composed of small network devices, known as RIPE Atlas Probes and Anchors, connected to the participating volunteers' routers. Using RIPE Atlas, we can measure the Internet latency and routing path through ping and traceroute measurements. This project aims to develop a software router that dynamically uses RIPE Atlas measurements to change the scheduling path. Before the implementation of the project, we should study the existing works on using RIPE Atlas probe for such network optimization tasks at the Internet scale to quickly understand the state-of-the-art and ensure scientific novelty in our approach.

        Current Status: A prototype has been built as part of the GSoC 2024. We observe the use of such a framework in the Circumpolar North. Such an approach can provide significant benefits, especially in Alaska and the Canadian North, where Internet connectivity can be spotty.

        Expected Outcomes: This project extends the RIPE Atlas client to use the measurements in network scheduling decisions. First, the measurements should be streamlined to perform periodically across several probes set as sources and destinations. The measurements across several probes in a single city can provide a more generalized measurement for a city rather than restricting to individual changes of any given probe when multiple such probes are available to a given city. Second, we will build a virtual router to use these measurements to dynamically influence the network scheduling decisions across several nodes. As the network performance changes with time, we can observe how the network path changes with time. We have more than 60 million RIPE Atlas credits that we accumulated by hosting a RIPE Atlas probe for the past five years. So, we have sufficient resources for these Internet measurement experiments.

        Required Skills: Python.

        Code Challenge: Prior experience in Python through established coding examples.

        Source Code: https://github.com/KathiraveluLab/AWANTA

        Discussion Forum: https://github.com/KathiraveluLab/AWANTA/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium


        ~~~~~~~~~~

        [13] Alaska Wildfire Prediction Using Satellite Imagery.

        Mentors: Yali Wang (ywang35 -at- alaska.edu) and Arghya Kusum Das (akdas -at- alaska.edu)

        Overview: Given Alaska’s unique wildfire patterns, where large-scale fires occur annually in boreal forests, tundra, and remote wilderness, predicting fire-prone areas can help mitigate disasters and optimize resource allocation. The presence of vegetation (fuel) is necessary for a fire, but the determining factors are weather conditions (humidity, wind speed, temperature) and an ignition source (lightning, human activity, etc.). This project aims to develop a hybrid deep learning model to predict wildfire risk in Alaska by integrating optical, thermal, and synthetic aperture radar (SAR) satellite imagery with ground-based weather data. Traditional wildfire prediction relies on weather data, historical fire records, and human observations, which can be delayed or inaccurate in remote areas like Alaska. In contrast, satellite imagery provides real-time, high-resolution insights into vegetation health, thermal anomalies, burn severity mapping, soil moisture, fuel dryness, and even cloud-penetrating fire detection.

        Satellite choices:

        Satellite	Resolution	Revisit Frequency	Why Use It?
        Landsat 8 & 9 (NASA/USGS)	30m (multispectral), 100m (thermal)	16 days	Tracks pre/post-fire vegetation and burn severity with great detail.
        Sentinel-2 (ESA)	10m (RGB, NIR), 20m (SWIR)	5 days	High-resolution images for fire risk classification and early warnings.
        MODIS (Terra/Aqua, NASA)	250m (fire detection), 1km (thermal)	Daily	Provides historical fire perimeters and active fire locations.
        VIIRS (Suomi NPP & NOAA-20)	375m (fire detection), 750m (thermal)	Daily	Real-time fire monitoring, capturing active hotspots.
        Sentinel-1 (ESA)	5m - 20m	6-12 days	SAR imaging for vegetation moisture & burned area mapping.
        ALOS-2 (JAXA)	10m - 100m	14 days	L-band SAR for detecting dry fuel and terrain changes.
        Additional ground data sources:

        1). ERA5 Climate Reanalysis (ECMWF): Provides historical & real-time temperature, wind, and humidity data.

        2). NOAA NWS Weather Data: Near real-time humidity, wind, and temperature.

        3). Alaska Fire Service (AFS) Wildfire Data: Historical ignition source data (lightning, human activity).

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This project aims to develop a deep-learning model that predicts wildfire risk in Alaska using a combination of satellite and ground-based weather data. The expected outcome of this project would involve both the dataset preprocessing pipeline and the performance of the developed model. Especially, the dataset preprocessing would include how to process the pre-fire and post-fire images efficiently and integrate the ground-based data with satellite imagery. Expected outcomes include:

        Minimum viable product (MVP):

        Fire risk classification: Given pre-fire satellite images, the model predicts the probability of a fire occurring within a defined time frame like 1 month, 3 months, or 6 months. The classifications should be "High Fire Risk," "Moderate Risk," or "No Risk."

        1). Data pipeline development:

        Preprocessing satellite images: Band selection, geospatial cropping, cloud removal (For this step, we are mostly interested in analyzing Sentinel-2 data);

        Synthetic Aperture Radar (SAR) analysis: Extracting fuel moisture & terrain features (For this step, we are mostly interested in extracting information like vegetation density and soil moisture from Sentinel-1 SAR data);

        Time-series weather data integration: Incorporating temperature, wind, and humidity. We have access to past decades of weather data for almost the past 30 years for multiple different places in Alaska.

        2). Model training and prediction:

        A hybrid model such as CNN-LSTM that analyzes satellite data and time-series weather trends (CNN-LSTM is just an example. We are open to multiple different types of analysis methodology);

        A web-based GIS dashboard to visualize fire-prone regions in Alaska;

        A report on model performance and fire risk metrics.

        Required Skills: Python. Experience with deep learning and machine learning.

        Code Challenge: Experience with multi-band satellite imagery, geospatial data processing (like ArcGIS Pro), and remote sensing.

        Source Code: https://github.com/YaliWang2019/AK-Satellite-Imagery-Wildfire-Prediction (New Project)

        Discussion Forum: https://github.com/YaliWang2019/AK-Satellite-Imagery-Wildfire-Prediction/discussions

        Effort: 350 Hours

        Difficulty Level: Medium/Hard

        You are welcome to propose new open-source project ideas, especially those that serve the state of Alaska and its people. Please use the below template to create new project ideas. However, if you are proposing a new project idea as a contributor, make sure they are relevant to Alaska specifically and the circumpolar north in general. Also, contact potential mentors from the above-listed mentors and confirm their interest in your project idea before drafting an entire proposal based on your own idea.

        ~~~~~~~~~~
        
        [14] Support for Logarithmic Number Systems in Large Language Models.

        Mentors: Mark Arnold (markgarnold -at- yahoo.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: The Logarithmic Number System (LNS) is an alternative to built-in Floating Point (FP), which makes multiplication and division easy at the expense of more difficult addition. Using overloaded operators, xlnscpp provides an open-source C++ library for both 16- and 32-bit LNS arithmetic. Interest in fabricating LNS hardware has grown since it may reduce power consumption for applications that tolerate approximate results, such as deep learning (see [1]-[5]). Although LNS has been studied extensively for feed-forward networks, only recently [6] has LNS been considered for Large Language Models (LLMs).

        LLMs consist of two main computations: a) feed-forward neural networks for which LNS has been shown to be useful, and b) an operation known as attention. The training of an LLM produces weights for both of these computations, which are often quantized to reduce data storage requirements. These quantized weights are reconstructed (usually in either 16- or 32-bit FP) and operated on by vectors of tokens (usually in similar FP format).

        Existing LLM engines, such as the open-source llama.cpp, perform vector/matrix/tensor operations (mostly matrix multiply) between the FP tokens and the weights (in a variety of formats, not including LNS).

        llama.cpp uses a library called ggml to do the actual math. The design of ggml supports a variety of FP hardware, such as CPUs and GPUs.

        Current Status: xlnscpp is not supported by llama.cpp or ggml. Weights can be stored in a variety of built-in int or FP formats instead of LNS. Matrix operations are carried out in FP.

        Expected Outcomes: The goal of this project is to provide support for xlnscpp instead of FP in ggml (and indirectly) llama.com. At a minimum, this involves modifying ggml to support a "virtual" LNS "machine" using xlnscpp to perform the actual LNS computation, but which appears to the calling llama.cpp like another hardware platform, like a GPU. The storage format of the quantized weights would still be the same, but they would be converted to LNS for computations like attention on LNS-format tokens. It is not expected that the speed would be as fast as if hardware FP were used, although a design that minimizes the slowdown is desirable (for instance, converting to LNS once, and reusing LNS many times, much as data is transferred to GPU memory and reused many times there). The purpose is a proof of concept that LNS yields valid output from an LLM. The design needs to implement enough ggml features to support an actual LLM, like Deepseek.

        Required Skills: C++ and some familarity with LLMs

        Code Challenge:

        Run the xlns16test.cpp and xlns32test.cpp examples.

        Go through the ggml example for 32-bit FP matrix multiplication on CPU ( https://huggingface.co/blog/introduction-to-ggml) which illustrates concepts like: ggml_backend (the code that does the computation on a GPU or CPU), ggml_context (a "container" that holds data), ggml_cgraph: (what computation the backend performs), ggml_backend_buffer: (hold the data of multiple tensors), and ggml_backend_buffer_type: (a "memory allocator" connected to each ggml_backend). This is quite involved because of the ggml_backend concept. Such experience will help you design a new ggml_backend for LNS (which your design proposal will describe as running on CPU using xlnscpp).

        Write a standalone C++ program that has a function to do 32-bit FP matrix multiply with a main program that prints the FP result. Test it with the same matrix data as the previous ggml example. (Hint: use nested for loops to compute the sum of products that form the matrix product).

        Modify this program to include xlns32.cpp (define xlns_ideal first) and perform the internal computation in LNS format. The main program and the signature of the function it calls remain the same (32-bit FP), which requires that the function convert to/from LNS before and after the matrix multiply. (Hint: if you do it properly, the overloaded xlnscpp assignment operator takes care of this automatically.) The sum of products should be computed entirely in LNS (not FP). Notice the numeric results are close to what FP produces.

        Modify the program to include xlns16.cpp instead. Notice the numeric results are slightly less accurate (the 16-bit LNS product is stored in the 32-bit FP result). This illustrates the tradeoff of using reduced precision LNS, which is what we want to experiment with in this project.

        These code challenges provide possible insight as to how the LNS-CPU backend your design proposal will describe can "look like" an FP backend to llama.cpp. When data would be transferred to the backend, it is converted to LNS. When data is transfered back to llama.cpp, it is converted back to 32-bit FP. This is one idea for this project. You may incorporate improvements to this concept in your design proposal that considers the features of ggml.

        References:

        [1] G. Alsuhli, et al., “Number Systems for Deep Neural Network Architectures: A Survey,” https://arxiv.org/abs/2307.05035, 2023.

        [2] M. Arnold, E. Chester, et al., “Training neural nets using only an approximate tableless LNS ALU”. 31st International Conference on Application-specific Systems, Architectures and Processors. IEEE. 2020, pp. 69–72. https://doi.org/10.1109/ASAP49362.2020.00020

        [3] O. Kosheleva, et al., “Logarithmic Number System Is Optimal for AI Computations: Theoretical Explanation of Empirical Success”, https://www.cs.utep.edu/vladik/2024/tr24-55.pdf

        [4] D. Miyashita, et al., “Convolutional Neural Networks using Logarithmic Data Representation,” https://arxiv.org/abs/1603.01025, Mar 2016.

        [5] J. Zhao et al., “LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update,” IEEE Trans. Computers, vol. 71, no. 12, pp.3179–3190, Dec. 2022, https://doi.org/10.1109/TC.2022.3202747

        [6] P. Haghi, C. Wu, Z. Azad, Y. Li, A. Gui, Y. Hao, A. Li, and T. T. Geng, “Bridging the Gap Between LLMs and LNS with Dynamic Data Format and Architecture Codesign ,” in 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO). Los Alamitos, CA, USA: IEEE Computer Society, Nov. 2024, pp. 1617–1631. https://doi.ieeecomputersociety.org/10.1109/MICRO61859.2024.00118

        Source Code: https://github.com/xlnsresearch/xlnscpp

        Discussion Forum: https://github.com/xlnsresearch/xlnscpp/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [15] Time and Ordering in Beehive.

        Mentors: Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu) and David Moxley (dpmoxley -at- alaska.edu)

        Overview: Beehive was initiated as a collaboration between the University of Alaska Anchorage Departments of Computer Science and Human Services, but grew largely into a software platform through open-source contributions. Beehive seeks to create a digital approach to translating the digitalization of art and photographic images into a digital database that stores in retrievable formats those images for use in advancing the delivery of human services and health care to people who experience considerable vulnerability and marginalization within the community. One of the project goals is to extend the current Beehive software as a repository of photomemories (a 2D projection of 3D spaces) X time. This project aims to extend Beehive with these additional capacities and develop data mining algorithms to support this use case of photos as frozen snapshots in an individual's life.

        Current Status: The current Beehive prototype does not consider the complexities of time and ordering in the use of behavioral patterns and narratives in the journey to recovery.

        Expected Outcomes: In this project, the contributor will (1) extend the Beehive platform to support time and ordering as attributes across images, (2) develop algorithms to understand the impact of past events through the series of images and their narratives, and (3) implement data mining algorithms that could fetch and understannd evolving narratives around photomemories. We see spaces as 3D or 2D if we are referring to geolocations. Photos are 2D projections of a 3D space. There is one dimension that we omit in most of these projections. That is time. Time as a 4th dimension is not entirely new in research and applications. A search on spatiotemporal data and space-time continuum will give you plenty of examples, from climate change to science novels. Time, or more specifically, ordering, is an essential variable in behavior. Don't you wonder how you see places differently just because you have seen the same or something similar before? Where it gets more interesting or challenging (depending on how you see it) is how the time affects the exact location and even those "near" it. When we say "near," it is in terms of data, not necessarily in terms of geographical proximity. Sometimes, it is just a minor change, and the location is the same! In data mining, we call this "near duplicates." A change in the name of a place (can be a city or a restaurant!). Other times, these are two entirely different places. Perhaps, Kivalina has moved over time due to the Arctic Erosion (sadly). But that is still geographical proximity. For instance, your visit to Portugal will influence your visits to other Portuguese-speaking nations (such as Angola and Brazil) because they share a language and culture, although they are oceans apart. On a smaller scale, your experience in a library will impact how you perceive another library in a different location. How do we consider time (or in a more accurate sense, "relative time" or "ordering") in our analysis/perception? This is intertwined as the 4th dimension (or 3rd dimension, if you are already projecting the 3D world into a 2D map/photo). This project aims to understand these complexities in a prototype version over simulated/synthetic data.

        Required Skills: Database (MySQL or Mongo) and Python or Java. Experience and interest in data mining is a plus.

        Code Challenge: Prior experience with database management through established coding examples.

        Source Code: https://github.com/kathiraveluLab/beehive.

        Discussion Forum: https://github.com/KathiraveluLab/Beehive/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium

        [N] PROJECT TITLE.

        Mentors: FIRSTNAME1 LASTNAME1 (email-address) and FIRSTNAME2 LASTNAME2 (email-address)

        Overview:

        Current Status:

        Expected Outcomes:

        Required Skills:

        Code Challenge:

        Source Code:

        Discussion Forum:

        Effort: 90/175/350 Hours

        Difficulty Level: Easy/Medium/Hard


          
    totalCharacters_of_ideas_content_parent: 54320
    totalwords_of_ideas_content_parent: 9380
    totalTokenCount_of_ideas_content_parent: 12130
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/alaska/
    idea_list_url: https://github.com/uaanchorage/GSoC




  - organization_id: 8
    organization_name: AnkiDroid
    no_of_ideas: 5
    ideas_content: |
    
          Multiple Profiles (175 hours)
          Problem
          Currently, AnkiDroid only allows a single profile as opposed to Anki (Desktop) and AnkiMobile (iphone) which allow multiple profiles. In particular, a family using a shared device will be able to have one profile per user, instead of sharing data. If you have multiple ankiweb accounts, you can sync all of them on your android. You can see discussion about this topic on https://github.com/ankidroid/Anki-Android/issues/2545.


          Expected Outcomes
          A user should be able to add a profile, sync this profile with an ankiweb account, switch between those accounts, and update the preference on a profile-by-profile level.
          Each profile should have its own collection, as in Anki. 

          There are at least three big questions that you need to consider. The UI, the backend, and the data structure. 
          The UI
          Firstly, you should design the UI. Your proposal should show us what would be the path the user will take to add a second profile, and switch to another profile. 
          The data structure
          AnkiDroid follows anki data structure. We have a folder, called Ankidroid, which contains all the files used to store user data. We should figure out a way to change the data structure in order to allow us to store data of multiple profiles.
          When the device does not use scoped storage, the AnkiDroid folder is in the top level general purpose folder of the AndroidDevice. We expect and hope most users understand this folder is used by ankidroid and leave it alone. We can’t just add other top level folders as this may clutter the user storage system and increase the risk a user delete folder, hence losing their data.
          The backend
          We need to figure out everything that will need to be changed. At the very least:
          We need a way to determine which profile is currently being used, and remember this information when ankidroid is restarted
          Each access to the data structure needs to take the profile into account, in order to access the right collection and media folder. This information should be available to the backend; so we should determine whether any change to the backend is required (probably not)
          We need to determine which preferences are profile based and which are used by the whole app. We need to update all preferences access to ensure we use the profile ones. We probably will need a lint rule that ensure that most preferences are profile-dependent unless there is a good reason not to. When creating an account, we need to determine whether to use default preferences or copy existing ones.


          Stretch goals
          If you have remaining time after the main project is done, there are two extensions that could be worth considering.

          Advertise this feature

          On the first update where multiple accounts are available, show a message to the user inviting them to add other accounts. This message should be similar to the message to new users.
          Saving space by avoiding to duplicate the media

          If multiple accounts have the same media (let’s say, the device is used by multiple users, who all should learn Ultimate Geography or Anking deck), ensure that the media are not duplicated in order to save storage on the device. This may require collaboration with the backend, because this optimization is not done on desktop. 
          It is probably worth doing it because storage is very precious on mobile.
          Language:
          Kotlin & XML
          Potentially some rust if we need to touch the backend for the stretch goal
          Difficulty: Medium
          Mentor(s):
          Arthur Milchior
          Shridhar Goel



          ~~~~~~~~~~



          Review Reminder (175 hours)
          Problem
          The user can request AnkiDroid to send them a daily notification in Android to remind them to review their cards if there are cards to review today.
          At least in theory. In practice those notifications have been broken for a long time. We tried years ago to solve the issue. Our solution was to remove most features, but what remains is still far from ideal.  It’s now clear that we should just scratch the current notification system and recreate one from scratch (and automatically migrate users from the previous feature to the new one). 
          Expected Outcomes
          In your proposal, you should tell us what the notification system will look like.
          We need to know what user interface you plan to implement. Every journey the user can take.

          The most basic idea is to have a notification shown if any card is due. This is what we currently have. Also let the user decide at which hour the notification should be shown. Maybe the notification should have a snooze button, to remind the user later (when?)
          We could also consider adding notification for specific decks.
          If so, there should probably be a way to see all notifications currently planned, in order to easily remove them, or edit them together.
          We should also find a way to test those notifications, manually and with automated tests. Ensuring they only trigger once a day in order not to overwhelm the user.
          You may consider reaching users over reddit and the forum in order to gather feedback 

          Language:
          EITHER:
          Kotlin & XML
          Difficulty: Hard
          Mentor(s):
          Arthur Milchior
          criticalAY


          ~~~~~~~~~~

          Note Editor: Note Type Preview (175 hours)
          Problem
          AnkiDroid is a flashcard app with a complex HTML/field-based templating engine. We currently have difficulties explaining a number of concepts to new users, both while onboarding, and for intermediate users:
          The unexpected fact that the user adds ‘notes’ to the app, not ‘cards’
          One note can generate multiple cards
          Various ‘Note Types’ have unique properties
          A user can create or download additional note types
          Currently, the user is provided with a text-based selection screen:

          Expected Outcomes
          In order to resolve the above issues, we want to modify this screen to provide a preview of each note type available in the system, showing
          The number of cards which will be produced when the note type is used
          A visual preview of how each of the cards will look
          Each card has a separate HTML template, so the designs may vary
          Taking into account some special features: 
          A note type may request that the user types in the answer
          Cloze deletions: 1 input produces 1…n cards
          Image occlusion: 1 input
          The ability to open our Card Template Editor
          The screen should allow a user to open up our Note Type Management screen and our manual. We should aim for the screen to prefer graphical elements over text
          Language:
          EITHER:
          Kotlin & XML
          If the screen is Android-specific
          Svelte (Typescript + HTML)
          If the screen is to be integrated into all Anki clients
          Difficulty: Medium
          Mentor(s):
          David Allison
          criticalAY

          ~~~~~~~~~~



          Tablet & Chromebook UI (175/350 hours)
          Problem
          AnkiDroid was initially designed for Android mobile phones. Over the years, Android has come to tablets and Chromebooks, but our UI has continued to be designed around the mobile phone.

          We currently have ~10% of our users on Tablets or Chromebooks, and we want to improve their user experience using the app, both with the aim to improve the user experience for our existing users, and increasing the number of users who can effectively use our app on larger devices

          Sanjay Sargam greatly improved the user experience on table and chromebook through GSoC 24’, and I invite you to read his report. Still, much remains to do, and what was done can certainly be polished.
          Expected Outcomes
          This primarily depends on your proposal. 
          Any screen in the app is open for your suggestions. 

          Suggestions
          Show NoteEditor and Previewer Side by Side
          Currently, the NoteEditor and Previewer are separate screens in AnkiDroid. On mobile devices, this makes sense due to limited scree n space. However,on tablets and Chromebooks, users have larger displays, and constantly switching between editing and previewing can feel cumbersome.

          Resizable Layout in DeckPicker and CardTemplateEditor
          The goal is to introduce a draggable slider that lets users dynamically adjust the size of two sections.

          Language: Kotlin, XML
          Difficulty: Medium
          Mentor(s):
          David Allison
          Arthur Milchior
          Sanjay Sargam


          ~~~~~~~~~~

          Additional Widgets (175/350 hours)
          Problem
          Widgets were introduced to AnkiDroid in 2010. These provide significant benefit to our power users and we started using them through GSoC 24. I invite you to read last year’s contributor’s report to see what was done.

          
          Expected Outcomes
          Android 12 Widget-based functionality is evaluated and integrated with the widgets when appropriate


          The GSoC proposal is expected to propose additional widgets that would be useful to our users
          Language: Kotlin, XML, UI & UX
          Difficulty: Medium
          Mentor(s):
          David Allison
          criticalAY





          
    totalCharacters_of_ideas_content_parent: 9691
    totalwords_of_ideas_content_parent: 2448
    totalTokenCount_of_ideas_content_parent: 1986
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ankidroid/
    idea_list_url: https://docs.google.com/document/d/1Va6IWEYcWTkK4KDtyoFxtOKzpdcYe54GdrVpuMcFvlI/edit?pli=1&tab=t.0



  - organization_id: 9
    organization_name: Apache DataFusion
    no_of_ideas: 11
    ideas_content: |
        
        Implement Continuous Monitoring of DataFusion Performance
        Description and Outcomes: DataFusion lacks continuous monitoring of how performance evolves over time – we do this somewhat manually today. Even though performance has been one of our top priorities for a while now, we didn’t build a continuous monitoring system yet. This linked issue contains a summary of all the previous efforts that made us inch closer to having such a system, but a functioning system needs to built on top of that progress. A student successfully completing this project would gain experience in building an end-to-end monitoring system that integrates with GitHub, scheduling/running benchmarks on some sort of a cloud infrastructure, and building a versatile web UI to expose the results. The outcome of this project will benefit Apache DataFusion on an ongoing basis in its quest for ever-more performance.

        Category: Tooling

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and mertak-synnada

        Skills: DevOps, Cloud Computing, Web Development, Integrations

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Supporting Correlated Subqueries
        Description and Outcomes: Correlated subqueries are an important SQL feature that enables some users to express their business logic more intuitively without thinking about “joins”. Even though DataFusion has decent join support, it doesn’t fully support correlated subqueries. The linked epic contains bite-size pieces of the steps necessary to achieve full support. For students interested in internals of data systems and databases, this project is a good opportunity to apply and/or improve their computer science knowledge. The experience of adding such a feature to a widely-used foundational query engine can also serve as a good opportunity to kickstart a career in the area of databases and data systems.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): jayzhan-synnada and xudong963

        Skills: Databases, Algorithms, Data Structures, Testing Techniques

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Improving DataFusion DX (e.g. 1 and 2)
        Description and Outcomes: While performance, extensibility and customizability is DataFusion’s strong aspects, we have much work to do in terms of user-friendliness and ease of debug-ability. This project aims to make strides in these areas by improving terminal visualizations of query plans and increasing the “deployment” of the newly-added diagnostics framework. This project is a potential high-impact project with high output visibility, and reduce the barrier to entry to new users.

        Category: DX

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): eliaperantoni and mkarbo

        Skills: Software Engineering, Terminal Visualizations

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Robust WASM Support
        Description and Outcomes: DataFusion can be compiled today to WASM with some care. However, it is somewhat tricky and brittle. Having robust WASM support improves the embeddability aspect of DataFusion, and can enable many practical use cases. A good conclusion of this project would be the addition of a live demo sub-page to the DataFusion homepage.

        Category: Build

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and waynexia

        Skills: WASM, Advanced Rust, Web Development, Software Engineering

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        High Performance Aggregations
        Description and Outcomes: An aggregation is one of the most fundamental operations within a query engine. Practical performance in many use cases, and results in many well-known benchmarks (e.g. ClickBench), depend heavily on aggregation performance. DataFusion community has been working on improving aggregation performance for a while now, but there is still work to do. A student working on this project will get the chance to hone their skills on high-performance, low(ish) level coding, intricacies of measuring performance, data structures and others.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): jayzhan-synnada and Rachelint

        Skills: Algorithms, Data Structures, Advanced Rust, Databases, Benchmarking Techniques

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Improving Python Bindings
        Description and Outcomes: DataFusion offers Python bindings that enable users to build data systems using Python. However, the Python bindings are still relatively low-level, and do not expose all APIs libraries like Pandas and Polars with a end-user focus offer. This project aims to improve DataFusion’s Python bindings to make progress towards moving it closer to such libraries in terms of built-in APIs and functionality.

        Category: Python Bindings

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): timsaucer

        Skills: APIs, FFIs, DataFrame Libraries

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Optimizing DataFusion Binary Size
        Description and Outcomes: DataFusion is a foundational library with a large feature set. Even though we try to avoid adding too many dependencies and implement many low-level functionalities inside the codebase, the fast moving nature of the project results in an accumulation of dependencies over time. This inflates DataFusion’s binary size over time, which reduces portability and embeddability. This project involves a study of the codebase, using compiler tooling, to understand where code bloat comes from, simplifying/reducing the number of dependencies by efficient in-house implementations, and avoiding code duplications.

        Category: Core/Build

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): comphead and alamb

        Skills: Software Engineering, Refactoring, Dependency Management, Compilers

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Ergonomic SQL Features
        Description and Outcomes: DuckDB has many innovative features that significantly improve the SQL UX. Even though some of those features are already implemented in DataFusion, there are many others we can implement (and get inspiration from). This page contains a good summary of such features. Each such feature will serve as a bite-size, achievable milestone for a cool GSoC project that will have user-facing impact improving the UX on a broad basis. The project will start with a survey of what is already implemented, what is missing, and kick off with a prioritization proposal/implementation plan.

        Category: SQL FE

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): berkaysynnada

        Skills: SQL, Planning, Parsing, Software Engineering

        Expected Project Size: 350 hours


        ~~~~~~~~~~

        Advanced Interval Analysis
        Description and Outcomes: DataFusion implements interval arithmetic and utilizes it for range estimations, which enables use cases in data pruning, optimizations and statistics. However, the current implementation only works efficiently for forward evaluation; i.e. calculating the output range of an expression given input ranges (ranges of columns). When propagating constraints using the same graph, the current approach requires multiple bottom-up and top-down traversals to narrow column bounds fully. This project aims to fix this deficiency by utilizing a better algorithmic approach. Note that this is a very advanced project for students with a deep interest in computational methods, expression graphs, and constraint solvers.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): ozankabak and berkaysynnada

        Skills: Algorithms, Data Structures, Applied Mathematics, Software Engineering

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Spark-Compatible Functions Crate
        Description and Outcomes: In general, DataFusion aims to be compatible with PostgreSQL in terms of functions and behaviors. However, there are many users (and downstream projects, such as DataFusion Comet) that desire compatibility with Apache Spark. This project aims to collect Spark-compatible functions into a separate crate to help such users and/or projects. The project will be an exercise in creating the right APIs, explaining how to use them, and then telling the world about them (e.g. via creating a compatibility-tracking page cataloging such functions, writing blog posts etc.).

        Category: Extensions

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and andygrove

        Skills: SQL, Spark, Software Engineering

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        SQL Fuzzing Framework in Rust
        Description and Outcomes: Fuzz testing is a very important technique we utilize often in DataFusion. Having SQL-level fuzz testing enables us to battle-test DataFusion in an end-to-end fashion. Initial version of our fuzzing framework is Java-based, but the time has come to migrate to Rust-native solution. This will simplify the overall implementation (by avoiding things like JDBC), enable us to implement more advanced algorithms for query generation, and attract more contributors over time. This project is a good blend of software engineering, algorithms and testing techniques (i.e. fuzzing techniques).

        Category: Extensions

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): 2010YOUY01

        Skills: SQL, Testing Techniques, Advanced Rust, Software Engineering

        Expected Project Size: 175 to 350 hours*

        *There is enough material to make this a 350-hour project, but it is granular enough to make it a 175-hour project as well. The student can choose the size of the project based on their availability and interest.

          
    totalCharacters_of_ideas_content_parent: 10170
    totalwords_of_ideas_content_parent: 1940
    totalTokenCount_of_ideas_content_parent: 2086
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/apache-datafusion/
    idea_list_url: https://datafusion.apache.org/contributor-guide/gsoc_project_ideas



  - organization_id: 10
    organization_name: ArduPilot
    no_of_ideas: 6 
    ideas_content: |
          Non-GPS Position Estimation Using 3D Camera and Pre-Generated Map¶
          Skills required: Python, C++

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Hard

          Expected Outcome: Copter with low-cost 3D camera estimates its local position by comparing the camera point cloud to a pre-generated 3D map

          The goal of this project is to allow a Copter to estimate its local position using a low-cost 3D camera (e.g. Intel D465) by comparing the camera’s point cloud to a pre-generated 3D map. The steps involved include:

          Create a tool to capture a 3D map of the flight area. The resulting map should be loaded onto the vehicle’s companion computer (e.g. RPI5)

          Mount a low-cost 3D camera (e.g. Intel D465) onto an ArduPilot copter (e.g. EDU650 or similar) equipped with a companion computer

          Write localisation software (e.g. python code) to compare the output of the 3D camera to the pre-generated 3D map and send the estimated position to the vehicle’s EKF (see Non-GPS Position Estimation)

          Implement a simulator of the system (e.g. gazebo)

          Document the setup and operation for future developers and users

          Funding will be provided for hardware including a copter (e.g. Hexsoon EDU650), companion computer and 3D camera (e.g. Intel D465) if necessary

          ~~~~~~~~~~

          AI Chat WebTool for use with MP and/or QGC
          Skills required: JavaScript, OpenAI, Google Gemini

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Web tool capable following a pilot’s verbal commands and converting them to MAVLink in order to control an ArduPilot multicopter

          This project involves re-implementing the MAVProxy’s AI chat module (see blog here) to run as a WebTool

          Once complete the WebTool should be capable of:

          Connecting to the vehicle via Mission Planner or QGC

          Responding to verbal or written questions and commands from the pilot

          Arming the vehicle

          Issuing takeoff commands and flying the vehicle a specified distance in any direction

          Changing the vehicle’s flight mode

          Most of the development can be completed using the SITL simulator and any OpenAI or Google Gemini usage costs will be covered

          ~~~~~~~~~~
           
          AI Chat Integration with all WebTools¶
          Skills required: JavaScript, OpenAI, Google Gemini

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: All WebTools include AI chat to help users understand and use the tool

          This project involves adding an OpenAI or Google Gemini chat window into some or all of the ArduPilot Webtools

          Once complete some or all of the WebTools should:

          Include a new chat widget allowing users to ask an AI assistant questions about the tool using text or voice

          Allow the AI assistant to operate the tool based on user input (e.g. push buttons, change zoom of graphs, etc)

          The top priority WebTool is the “UAV Log viewer” although simpler tools like the “Hardware Report” could be a good starting point

          Most of the development can be completed using the SITL simulator and any OpenAI or Google Gemini usage costs will be covered
          ~~~~~~~~~~


          Gazebo Plug-in Model of a Motor¶
          Skills required: Gazebo, C++

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: ArduPilot Gazebo plugin simulates a Motor

          As part of the ArduPilot_Gazebo plugin, we ask a student to model the electromechanical properties of a motor (no thrust/aero, just the motor angular acceleration/power itself)
          ~~~~~~~~~~

          SITL AI Reinforcement Learning Concept Script¶
          Skills required: Gaazebo, Lua, AI

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Lua script that uses re-inforcement learning to automate changing some parameters

          An AP-SITL reinforcement learning script concept, focuses on using Lua applets or some python to automate parameter changes according to some basic implementation of online reinforcement learning (actor-critic/SARSA/Q-learning)
          ~~~~~~~~~~

          SITL Test Script for Controls Testing¶
          Skills required: Gaazebo, Python

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Python code that allows easily setting up an AP vehicle in SITL for controls testing

          A safe “for education/rookies” SITL test script that strips away the majority of complexity in set-up and gives a Copter (and Plane if time permits) that requires some basic tuning and gives hints/pointers in a UI (this could lower the threshold for earlier year mech/electrical engineers to get their hands dirty on some software and try out basic controls testing)

          
    totalCharacters_of_ideas_content_parent: 5201
    totalwords_of_ideas_content_parent: 1290
    totalTokenCount_of_ideas_content_parent: 1143
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ardupilot/
    idea_list_url: https://ardupilot.org/dev/docs/gsoc-ideas-list.html


  - organization_id: 11
    organization_name: AsyncAPI
    no_of_ideas: 9
    ideas_content: |
          1) Enhancing Performance and Reliability of AsyncAPI CLI
          Improve the AsyncAPI CLI by optimizing performance, enhancing test reliability, and introducing long-requested features such as publishing and syncing AsyncAPI files with remote repositories.

          🎯 Outcome: Achieve a faster CLI execution, stable tests, file sync/publish support, and enhanced validation.
          🛠️ Skills Required: JavaScript/TypeScript, Node.js, Testing Frameworks, API, and testing automation.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AayushSaini101 | @Souvikns
          ⏳ Length: 175 Hours


          ~~~~~~~~~~
          2) AI-Powered Assistant for AsyncAPI
          Build an AI-powered assistant fine-tuned on AsyncAPI to provide accurate answers, generate code snippets, debug specifications, and recommend best practices.

          🎯 Outcome: A fine-tuned LLM-powered chatbot integrated with AsyncAPI’s ecosystem for enhanced developer support.
          🛠️ Skills Required: Javascript/Typescript, Machine Learning (LLMs), NLP, OpenAI/Llama, Chatbot Integration.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AceTheCreator
          ⏳ Length: 175 Hours

          ~~~~~~~~~~
          3) AsyncAPI Generator Maintainership
          This initiative aims to guide you from contributing to maintaining the project. You'll gain insight into the responsibilities of a maintainer, which involve tasks beyond mere coding.

          🎯 Outcome: Responsible for the project's future and continuous improvement.
          🛠️ Skills: JavaScript/TypeScript, testing libraries, Docker, virtualization, and test automation.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @derberg
          ⏳ Length: 350 Hours

          ~~~~~~~~~~
          4) AsyncAPI Conference Website UI Kit Development
          Develop a comprehensive UI Kit to enhance design consistency, modularity, and maintainability of the AsyncAPI Conference website.

          🎯 Outcome: A structured UI Kit with reusable components, Storybook integration, and improved design consistency.
          🛠️ Skills Required: React, TypeScript, Storybook, UI/UX Design, Component Development.
          🧩 Difficulty: Medium
          👩🏿‍🏫 Mentor(s): @AceTheCreator
          ⏳ Length: 175 Hours

          ~~~~~~~~~~
          5) VS Code Extension Maintainership
          This initiative will guide you from contributing to becoming a maintainer of the VS Code AsyncAPI Preview extension. You'll learn the responsibilities of a maintainer, including code contributions, issue triaging, release management, and community engagement.

          🎯 Outcome: Taking ownership of the VS Code extension to ensure its long-term stability and improvement.
          🛠️ Skills Required: TypeScript/JavaScript, VS Code Extensions, Spectral Linting, Testing, and Open Source Contribution.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @ivangsa
          ⏳ Length: 350 Hours

          ~~~~~~~~~~
          6) Java + Quarkus Template for AsyncAPI Generator
          Develop a new AsyncAPI Generator template for Java with Quarkus, leveraging its growing adoption in cloud-native development.

          🎯 Outcome: A fully functional Java + Quarkus template for generating AsyncAPI-based applications.
          🛠️ Skills Required: Java, Quarkus, Templating Engines (Nunjucks/Handlebars), AsyncAPI Generator.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AayushSaini101, @Souvikns
          ⏳ Length: 350 Hours
          ~~~~~~~~~~
          7) Refactor the Scripts inside the website and add Integration tests
          Add the script execution to a new folder inside the website, and add integration tests for those scripts.

          🎯 Outcome: A full Unit + Integration tests setup will be added for the scripts to fully test the functionalities
          🛠️ Skills Required: Typescript, Node js, Jest, Github actions
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @akshatnema
          ⏳ Length: 350 Hours
          ~~~~~~~~~~
          8) Add E2E tests for the Website critical flows
          Add E2E tests for the website where some of the critical flows (that are centered around user experience are tested thoroughly).

          🎯 Outcome: This project will ensure that we are not breaking any critical flows where user experience is our topmost priority
          🛠️ Skills Required: Typescript, Node js, E2E Testing, Github actions
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @sambhavgupta0705
          ⏳ Length: 175 hours
          ~~~~~~~~~~
          9) Redesign of website and addition of Dark theme
          Create new designs for the website pages based on the theme chosen by @Mayaleeeee and replicate those designs inside the website, along with the Dark mode theme.

          🎯 Outcome: This project will ensure that we are not breaking any critical flows where user experience is our topmost priority
          🛠️ Skills Required: Typescript, Node js, Figma, TailwindCSS
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @Mayaleeeee, @devilkiller-ag
          ⏳ Length: 350 hours

          
    totalCharacters_of_ideas_content_parent: 5231
    totalwords_of_ideas_content_parent: 1242
    totalTokenCount_of_ideas_content_parent: 1147
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/asyncapi/
    idea_list_url: https://github.com/asyncapi/community/blob/master/mentorship/summerofcode/2025/asyncapi-gsoc-ideas-page.md



  - organization_id: 12 
    organization_name: BRL-CAD 
    no_of_ideas: 25
    ideas_content: |
          Improving the k-File to BRL-CAD Converter
          Outline
          In the past years, we put some effort in the development of a LS-DYNA keyword file to BRL-CAD converter. Although we made great progress there, we still can't convert every k-file to g, i.e. the native BRL-CAD format. The goal of this project is to increase the number of covertable k-files.

          Details
          The sources of the current k-file to BRL-CAD converter can be found in the brlcad repository at src/conv/g. You have to compile BRL-CAD from its sources to work on this project and see the effects of your changes.

          Examples of k-files, which cannot be converted with k-g, can be found here: THUMS You can however use your own examples.

          Expected Outcome
          We expect an improved k-g LS-DYNA keyword file to BRL-CAD converting program as the outcome from this project.

          Project Properties
          Skills
          C/C++
          LS-DYNA or a similar FE solver software, which can read k-files (needed as reference for how the geometry should look like)
          Difficulty
          medium

          Size
          This project could have any size, short (90h), medium (175h) or long (350h), depending on the amount of functionality you want to add.

          Additional Information
          Potential mentor(s):
          Ali Haydar
          Daniel Rossberg
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Development and Build Support for Native Windows
          Outline
          OpenSCAD is multi-platform desktop application, with official support for Windows, macOS and Linux, and unoffical support for various other Unix-like OSes like FreeBSD.

          While OpenSCAD does support Windows, Windows development and build setup is a bit suboptimal:

          Windows binaries are built on Linux using the MXE cross compilation environment (https://mxe.cc). Build setup
          Windows native builds, including tests are done on msys2 (https://www.msys2.org) Build setup
          Goal of this project is to improve Windows support for native Windows development.

          Details
          Adjust OpenSCAD source code as needed to build OpenSCAD using MSVC
          Adjust the CMake build system as needed to support MSVC
          Find a way of managing building (and packaging if needed) 3rd party dependencies. OpenSCAD depends on a number of 3rd party packages (e.g. Qt, QScintilla2, CGAL, Manifold, GMP, MPFR, boost, OpenCSG, GLEW, Eigen, glib2, fontconfig, freetype2, harfbuzz, libzip, Bison, Flex, double-conversion). Not all of these have great Windows build support.
          Integrate library building/packaging into our Continuous Integration framework (e.g. GitHub Actions or CircleCI).
          Build OpenSCAD on MSVC + run tests natively for every PR, as we do for other build environments. This to make sure contributions don't fall out of maintenance.
          Establish support for debugging OpenSCAD in the MSVC debugger, and documentation on how to set that up, if necessary
          Write/update documentation on how to establish a native WIndows development environment
          Consider switching the official OpenSCAD Windows build to use MSVC.
          Prior work: openscad/openscad#4976

          Expected Outcome
          A native MSVC build environment for OpenSCAD is reasonable easy to set up, and such an environment is continuously integrated.

          Project Properties
          Skills
          Good understanding of the Windows OS and native components (DLLs, executables) on a low enough level to be able to debug odd behaviors.
          Experience using MSVC and native Windows development for C++ projects
          Good understanding of how 3rd party libraries are built and distributed.
          Experience with or interest in acquiring skills using CMake and writing custom CMake configs and macros
          Experience with GitHub CI or CircleCI is a bonus
          Difficulty
          medium

          Size
          long (350h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Note: None of the mentors have relevant Windows skills, but have excellent understanding on how all the relevant technologies work and how they are integrated with OpenSCAD. It's important that the candidate is able to acquire any necessary Windows-specific skills independently.
          Organization website: https://www.openscad.org/

          ~~~~~~~~~~

          Integrated language help feature in OpenSCAD #100
          Outline
          Add more interactive help features for built-in functions and modules. Right now there's already a nice summary of parameters linked as cheat sheet. Scope of this projects would be to use this information in extended form and make it available in a more direct way in the editor.

          Details
          Convert the cheat sheet information into machine readable format
          Find a way to generate the existing HTML format based on the core data
          Add context help to editor giving help for built-in functions and modules, e.g. by adding formatted help output to the console window, including the links to further documentation like the language manual on Wikibooks
          Expected Outcome
          Cheat sheet is integrated into the application and additional context help for built-in functions and modules is available.

          Project Properties
          Skills
          Programming language is C++
          GUI programming with the Qt framework
          Difficulty
          Easy

          Size
          Medium (175h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Organization website: https://www.openscad.org/


          ~~~~~~~~~~

          Create a compelling interface and functionality for the IfcOpenShell WASM / pyodide module #99

          Outline
          http://wasm.ifcopenshell.org/

          Details
          Expected Outcome
          Future Possibilities
          Project Properties
          Skills
          Difficulty
          Size
          Additional Information
          Potential mentor(s): NAME
          Organization website: https://
          Communication channels: https://******.zulipchat.com


          ~~~~~~~~~~

          Authoring interface for IFC4.3 alignment geometry in Bonsai #98
          Outline
          Industry Foundation Classes (IFC) offer the ability for rich information exchanges between modeling, analysis, planning, and other software tools in the Architecture, Engineering, and Construction (AEC) industry. Specifically, the latest release of IFC (version 4.3, also referred to as IFC4X3) adds linear referencing via alignment modeling, which is core to describing the construction and maintenance of infrastructure assets such as roads, bridges, and railways.

          Details
          Alignment import (read) capabilities have been added to IfcOpenShell and the Bonsai add-in for Blender. They have reached a state of maturity such that the next logic step is to enable alignment authoring (write) capabilities.

          Expected Outcome
          Alignment authoring will take place in Blender via the Bonsai add-in. A user-focused workflow has been developed and documented, along with preliminary user interface mockups. This project would add alignment authoring capabilities via new panels and other items within Blender. The ifcopenshell.api namespace will also need to be enhanced incrementally to support the new user interface tools.

          Project Properties
          Skills
          Understanding and general working knowledge of python.

          Difficulty
          Medium

          Size
          Medium (175 h)
          The participant focuses on authoring horizontal alignments via the PI method. This could be via interactive icons or primarily through a table-based interface. The user would need to be able to add, edit, and remove PI (point of intersection) points. Additionally the user would need to be able to adjust the radius that corresponds to each PI point. Though not strictly required for this project, the authoring tool would also enable definition and editing of entry and exit transition curve type (clothoid, sine spiral, polynomial spiral, etc.) and length.

          Long (350 h)
          PI-based alignment would be added for vertical and cant as well. A basic corridor modeling UI tools would be implemented to allow for sweeping geometry (open or closed profile) along an alignment curve to generate 3D linear geometry via IfcSectionedSolidHorizontal and related IFC entities.

          Additional information
          Mentors: Rick Brice @RickBrice & Scott Lecher @civilx64

          Organization website: https://ifcopenshell.org

          Communication channels: https://github.com/IfcOpenShell/IfcOpenShell/discussions

          Technical resources:

          https://docs.bonsaibim.org/guides/development/index.html

          Blender 4.3: Precise Modeling for Architecture, Engineering, and 3D Printing

          Python Scripting in Blender


          ~~~~~~~~~~

          Manifoldness repair 
          Outline
          Repair triangle soup that are not manifold.

          Details
          Basically, a valid solid mesh should be both manifold and has no self-intersection. However, models from the internet may contain defects. This project is about coming up with an algorithm that converts and repair a triangle soup into a manifold mesh.

          This will contain a lot of heuristics, basically what we need is:

          Stitching faces together, and maybe join faces that are close enough.
          Fill holes.
          Duplicate vertices and edges such that the result is a manifold in terms of connectivity.
          Expected Outcome
          Implementation of said algorithm.

          Future Possibilities
          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): @elalish @pca006132
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions

          ~~~~~~~~~~

          Overlap removal
          Outline
          Remove overlaps in meshes that contain self-intersection, assuming the mesh is a manifold.

          Details
          Basically, a valid solid mesh should be both manifold and has no self-intersection. However, models from the internet may contain defects. This project is about coming up with an algorithm that removes self-intersections.

          See elalish/manifold#289 for details about ideas for the algorithm.

          Expected Outcome
          Implementation of said algorithm.

          Future Possibilities
          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): @elalish
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions

          ~~~~~~~~~~

          Creation of an IFC geometry library in IfcOpenShell that uses Manifold

          Outline
          For the past 10 years, IfcOpenShell has had a tight coupling with OpenCASCADE as its only geometry library and OCCT providing the datatypes in the IfcOpenShell C++ APIs.

          In IfcOpenShell v0.8 an additional abstraction is introduced over the geometric concepts in IFC (taxonomy.h) and the evaluation of such concepts using pre-existing geometry libraries (AbstractKernel).

          Also in v0.8, CGAL is introduced as an additional runtime selectable choice besides OpenCASCADE, because of (a) it's extensive set of modules for analysis (e.g convex decomposition, skeleton, ...) and (b) it's arbitrarily robust (and precise) implementation of boolean operations using Nef polyhedra on a number type represents a binary tree of operands taking part in the construction of that number.

          Both OpenCASCADE and CGAL are high quality efforts, but quite complex and resulting in fairly large compiled object sizes. This project proposal aims at introducing Manifold as a 3rd geometry library implementation. Manifold is modern, efficient and robust.

          https://github.com/elalish/manifold

          cc @elalish just fyi.

          Expected Outcome
          Another AbstractKernel implementation that uses Manifold to evaluate a small set of geometrical concepts (boolean, extrusion, brep for example) in IFC. Expecting reasonable outcomes on a small building model (such as the Duplex A model) without necessarily resolving all complexities and corner cases encountered in that model.

          Future Possibilities
          Comparison between implementations and development of a hybrid composition of these libraries that based on prior inspection picks the most suitable implementation for a specific IfcProduct or representation item. For example, OpenCASCADE will likely still excel at curved surfaces (e.g nurbs), but suffers a monumental performance overhead when ingesting detailed triangular meshes (that are also prevalent in IFC) due the overheads of it BRep data model.

          Additional Information
          Potential mentor(s): Thomas Krijnen (aothms)



          ~~~~~~~~~~

          Turn BlenderBIM into a client for remote BIM-collaboration on existing OpenCDE-API-server with a graph backend

          Outline
          The project aim is to turn BlenderBIM into a client for remote BIM-collaboration and a client for remote BIM-model-sharing through a Common Data Environment (a CDE working as BIM/IFC-server) using the already developed OpenCDE API server and the OpenCDE API specifications provided by buildingSMART: BCF API and Documents API.

          OpenCDE API:s are open standards. This project will hence enable usage of BlenderBIM as a client on other BIM-servers that implements the OpenCDE API:s.

          Details
          An OpenCDE API server that implements all buildingSMART OpenCDE API:s (BCF API, Documents API and Foundation API) has been developed in python and the FastAPI framework. Solibri Office was used as a client for testing this server software during development.

          The code of the OpenCDE server is located in the IfcOpenShell repository here: https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/opencdeserver

          The OpenCDE API:s is a set of open API-specifications provided by buildingSMART. https://github.com/buildingSMART/OpenCDE-API

          BIM Collaboration Format (BCF) API is used for collaboration on shared BIM models through a remote BCF-server. BCF API has the same purpose as BCF XML (which is a file format) but the difference is that the data is communicated as JSON through a BCF-server, instead of sending XML-files. https://github.com/buildingSMART/BCF-API
          Documents API is used communication between a client and a CDE (acc. ISO 19650-1). The purpose is a common data environement for sharing models, documents et.c. https://github.com/buildingSMART/documents-API
          Foundation API is used for authentication et.c. and must be implemented by any client or server that implements anyone of the other two OpenCDE API:s. https://github.com/buildingSMART/foundation-API
          To summarize: The model (the IFC data) will normally be shared to the server using Documents API, and downloaded form the server using Documents API. BCF API can be used for remote collaboration on the models located on the server et.c.

          The purpose of the open API specification is to enable independent development of clients and servers that can communication with eachother. A server has already been developed and shared as open source on IfcOpenShell. However, at the moment there is no open source client with a graphical user interface for the OpenCDE API:s. A python library for BCF API communication is available: https://pypi.org/project/bcf-client/.

          The aim of this project is to turn BlenderBIM into a OpenCDE-client (a client that already have BIM-capabilities) that can communicate remotely with (and make use of) the existing open source OpenCDE-server on: https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/opencdeserver

          An add-on for GIT-collaboration have already been developed:

          https://blenderbim.org/docs/users/git_support.html
          https://www.youtube.com/watch?v=cJZhSCSSWdA
          Collaboration using Documents API and BCF API is just another way of collaboration. This way of collaboration might be more suitable for AEC-professionals who does not have experience of GIT. Som features of GIT are not possible. But other features that are possible when using the OpenCDE API:s are not possible using GIT.

          Expected Outcome
          The BlenderBIM can connect as a client to the OpenCDE API server using the Foundation API
          User management functionality is added to OpenCDE API server: Register, invite, delete users
          BlenderBIM is a BCF API client - can collaborate on BIM-models remotely using the already developed OpenCDE-server
          BlenderBIM is a Documents API client - can share/download BIM-models remotely with the already developed OpenCDE-server
          User interface in BlenderBIM for setting up OpenCDE-server on localhost and user management (inviting collegues, adding/deleting users et.c.)
          User interface in BlenderBIM for remote BIM collaboration using BCF API
          User interface in BlenderBIM for remote model download and sharing using Documents API
          Simplify the process of turning your computer into an OpenCDE-server and inviting colleages to collaborate on your BIM model
          Simplify the process of deploying the OpenCDE-server as a BIM-server to the cloud for remote collaboration with BlenderBIM as a client
          Extra: Implement som of the routes/endpoints of the OpenCDE API specifications that Solibri Office does not support. I.e. implement the full official buildingSMART open specifications using the open source server (OpenCDE API server) and open source client (BlenderBIM).

          Future Possibilities
          IFC-server capabilities: Round-tripping of IFC data between IFC STEP (or python or C++ objects in IfcOpenShell) and the OpenCDE API server graph DB. More info on storing IFC data as label property graph (LPG) here: https://www.sciencedirect.com/science/article/pii/S0926580523000389
          Potential synergies with the other GSoC project "Web-based UI integration with Blender" Web-based UI integration with Blender #87 because the Web-based UI could be hosted by the same OpenCDE API server as in this project.
          Visualization of IFC data as graph in that Web-based UI. For example using pyviz or similar tools.
          Project Properties
          Skills
          Python, including how to setup a minimal basic server on localhost using FastAPI. https://fastapi.tiangolo.com/
          Blender Python API to develop user interfaces in BlenderBIM.
          Cypher query language and any graph DB that implements Cypher (such as Neo4j or MemGraph). https://neo4j.com/docs/cypher-manual/current/introduction/
          API development.
          Difficulty
          Hard

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): [Martin Wiss]
          Organization website: https://blenderbim.org/ http://ifcopenshell.org/
          Communication channels: OSArch

          ~~~~~~~~~~

          Geometry Verification and Validation GUI in Qt (AI Project)

          Outline
          Help develop a new GUI application that checks geometry for common issues and/or helps fix them.

          Details
          A new GUI is in prototype development (built on Arbalest) that checks geometry files for common verification and validation (V&V) issues such as topology errors, solidity errors, and more. It's very much an experimental work in progress and we'd like your help to make it complete. The overarching goal of this effort is to extend our prototype in a significant way, either improving usability, checking for more issues, improving the Qt GUI infrastructure integration, integrating workflow(s) for review and repair, or leveraging AI to identify and/or fix issues.

          Expected Outcome
          You will propose a complete project description that identifies the specific objectives you'll aim to achieve. It's expected that you'll leverage the previous work (talk with us to get access to those materials). The proposal should identify 3-10 primary objectives that are researched and specific, starting with our previous effort.

          We essentially want a tool that "compiles" geometry reporting warnings and errors for issues encountered, akin to compiling source code in Visual Studio or Eclipse. There are questions of application architecture to resolve (e.g., whether to extend 'arbalest', integrate 'qged', integrate 'gist', etc). We want the tool to be graphical and interactive. We want it to have the ability to generate reports for auditing. Some of those capabilities exist in isolation, but none exist as a tool tailor-made for 3D geometry V&V.

          Future Possibilities
          This is a long term priority project with future possibilities in:

          GUI infrastructure
          AI integration
          geometry healing and repair workflows
          geometry auditing
          geometry standards development
          Skills
          Qt, C/C++

          Difficulty
          Easy or Medium depending on the objectives

          Size
          long (350h) preferred, but medium (175h) also possible

          Additional Information
          Potential mentor(s): Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com
          https://brlcad.org/design/v&v/


          ~~~~~~~~~~

          Blender UI / integration with voxelisation toolkit software

          Outline
          There is software known as the Voxelisation Toolkit (pip install voxec). It converts the 3D model into voxels (e.g. 3D cubes that represent geometry), analyses and transforms those voxels, and outputs statistics (e.g. distance between voxels, etc).

          Image

          Image

          Voxels are super cool and can be used to calculate head heights, resolve complex non-manifold geometry, egress distances, or concrete formwork areas and strutting distances, and air volume for mechanical calculations. All of this stuff is useful for engineers and construction professionals.

          This project is to add a UI in Blender to start making this general purpose analysis tool available to non-programmers.

          Details
          You will be expected to design an interface for the voxelisation toolkit, prepack some simple recipes, and write scripts that take the output (currently visualised as images or plots) and instead visualise the results in 3D by generating 3D coloured meshes that represent the output.

          Expected Outcome
          Bundle the voxelisation toolkit software with Blender.
          A UI to execute the voxelisation toolkit.
          Simple presets to run the toolkit.
          Visualise the output of the voxelisation analysis as a 3D coloured mesh.
          Future Possibilities
          Bundle scripts for common usecases, like formwork calculation, air volume calculation, or external / internal metadata addition.

          Project Properties
          Skills
          Python
          Difficulty
          Medium

          Size
          Medium to Long

          Additional Information
          Potential mentor(s): Dion Moult, Thomas Krijnen
          Organization website: https://blenderbim.org http://ifcopenshell.org
          Communication channels: https://osarch.org/chat


          ~~~~~~~~~~

          Features for CG artists to visualise beautiful IFC models in Blender 
          Outline
          The architecture, engineering, and construction industry creates 3D models of buildings. These models are generally quite poor and do not contain any textures, lighting, or high quality objects that are suitable for 3D rendering. They often hire artists to help create beautiful renders of their designs.

          This project will build utility functions and workflows to easily get beautiful pictures of 3D models.

          Details
          3D artists typically do the following steps to make a 3D model look beautiful. They:

          Set camera angles with specific camera settings, with "clay" (e.g. all white colours) materials.
          Add lights and sun / sky settings.
          Add simple colours and textures.
          Remodel low quality geometry
          Add new objects (e.g. entourage) to decorate the scene, like trees, grass, people, extra furniture, walruses, shrimp, etc.
          Set common compositing and post processing rules
          You will use the Blender Python API to set simple presets for most of these steps to allow less skilled artists to quickly setup renders. You will also setup a workflow to guide artists on how to organise their files relative to the IFC model and keep the IFC model separate so that when the IFC model is changed, the artists doesn't need to start from scratch or play spot the difference.

          You do not need to be an expert in 3D modeling or CG visualisation or rendering. You will be taught what type of settings and options are appropriate for presets and the details of the workflow. However, you will be expected to automate that detail (every aspect of the Blender settings can be set using Python trivially).

          You will also be expected to create a Blender interface to interact with the settings, e.g. a button to add camera, a button to set a preset sky, etc.

          Expected Outcome
          Note: scope is flexible and you may achieve less or more or different to the below:

          A graphical interface in Blender that relate to the 6 steps above
          Buttons to add cameras, set common camera aspect ratios and settings. Buttons to add common types of lights, set sun angles and sky settings with bundled HDRI textures.
          Buttons to add simple material presets.
          Buttons to mark an object to be replaced by another
          A few preset assets using Blender's built in asset tools to drag and drop in entourage.
          Future Possibilities
          Project Properties
          Skills
          Python (definitely required!)
          Artistic sense (do you like 3D graphics? rendering?) If you have ever rendered a 3D scene before, this is the project for you!
          Difficulty
          Easy to Medium

          Size
          Medium to Long

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://blenderbim.org http://ifcopenshell.org
          Communication channels: https://osarch.org/chat

          ~~~~~~~~~~

          Implement 3D mesh offset
          Outline
          Implement efficient 3D mesh offset, instead of using minkowski sum with high resolution spheres. (elalish/manifold#192)

          Details
          3D mesh offset is a useful feature that many users asked for, but is difficult to implement efficiently. Many users use minkowski sum with sphere to perform positive offset, but this can be very slow due to the need for exact convex decomposition.

          Our approach will only work for positive offset, negative offset can be implemented by performing additional mesh boolean operations, so this is not an issue. The approach has four phases:

          Figure out all pairs of faces that do not share any vertex and may overlap after offsetting. (let's call them conflict pairs)
          Cut the mesh in a way such that for each part, no two faces are in the same conflict pair. (decomposition step, requires monte carlo tree search)
          Perform the positive offset on each part, using a modified algorithm from Offset Triangular Mesh Using the Multiple Normal Vectors of a Vertex. Note that we need to figure out how to blend the surfaces for smooth results.
          Union the parts.
          Expected Outcome
          A fast 3D mesh decomposition algorithm!

          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.
          Size
          Long.
          Additional Information
          Potential mentor(s): @elalish @pca006132 @zalo
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions


          ~~~~~~~~~~

          Add fuzzing tests

          Outline
          Add more fuzzing tests for both 2D and 3D operations.

          Details
          Fuzzing is an effective technique to expose bugs in software. Fuzzing tests randomly generate structured inputs (according to specification), and test if the program crashes/failed assertions.

          This project aims to test 2D and 3D CSG operations on geometrically valid polygons/meshes. To do this, we will define a very simple AST for our CSG operations, and use the recursive domain feature of fuzztest for the tests.

          We will also randomly apply slight perturbation to make the valid geometry only epsilon-valid, to test for robustness of the algorithm.

          Expected Outcome
          Fuzz tests that test for union, intersection, difference, 2D extrude/revolve, etc.

          Project Properties
          Skills
          C++
          Basic understanding of graph data structure.
          Difficulty
          Medium
          Size
          Medium
          Additional Information
          Potential mentor(s): @pca006132
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions


          ~~~~~~~~~~

          Physically-Based Rendering (PBR) advanced shaders
          Outline
          Get BRL-CAD physically-based rendering working with advanced shaders.

          Details
          BRL-CAD recently integrated with Appleseed which provides physically-based rendering. It's presently a command-line renderer called 'art'. For art rendering to work, a shader and colors are specified on geometry. BRL-CAD has preliminary support for material objects including OSL shaders and MaterialX shaders in art, however their support has only been tested with basic shaders such as the Disney Principled Shader. It's hard-wired to single-file shaders.

          Expected Outcome
          The goal of this task will be to make art read and work with any OSL or MaterialX shader networks, including ones using texturing, emission, subsurface scattering, etc. applied to BRL-CAD geometry.

          Project Properties
          Skills
          Decent C/C++ skills
          Some basic familiarity with PBR.
          Basic familiarity with shaders.
          Difficulty
          medium

          Size
          long

          Additional Information
          Potential mentor(s): Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          Improve FreeCAD Hidden Line Removal
          Outline
          FreeCAD's Technical Drawing module (TechDraw) relies heavily on the OpenCascade Hidden Line Removal algorithms. These algorithms can be very slow, do not provide progress reporting and do not provide any linkage between the input shape and the output.

          Details
          The TechDraw module provides projections, section views and detail views of 3D model components and assemblies developed in FreeCAD modules such as Part, PartDesign and Draft.

          Expected Outcome
          a) develop new code for projecting shapes and creating the geometry for technical drawings.
          -or-
          b) modify the existing OpenCascade code as an enhancement.

          Project Properties
          Both OpenCascade and TechDraw are written in C++.

          Skills
          The student should have a good knowledge of C++ and be familar with graphics topics such as the painters algorithm, face detection and hidden line removal.
          Knowledge of technical drawing standards and previous exposure to Qt will be helpful. Familiarity with OpenCascade is a definite plus.

          Difficulty
          Hard

          Size
          long

          Additional Information
          Potential mentor(s): wandererfan
          Organization website: https://freecadweb.org
          Communication channels: https://forum.freecadweb.org


          ~~~~~~~~~~

          Continuation of a prior BRL-CAD GSoC effort
          Outline
          BRL-CAD has been participating in GSoC for over 10 years with nearly 100 students! Any past accepted projects can be submitted as a continuation project.

          Details
          You can find all past participants documented on BRL-CAD's wiki by selecting a given year (e.g., 2018). Even the most successful and completely integrated projects have room for improvement! If any of those past efforts for any prior year sound very interesting to you, you can propose a continuation effort for it.

          Of course, you will need to research the prior effort to determine the status of the work, whether code was integrated or is sitting pending integration in a patch, whether it's functional or was in an intermediate state, etc. You'll also want to come chat with us on Zulip to make sure there is mentoring support for it, but there usually is if you're passionate and independently productive.

          For your proposal, note that it's a continuation effort. Explain what you are doing and how it relates to the prior effort. It's strongly recommended that your development plan focus on production-quality integration aspects such as making sure there are no usability or user experience (UX) issues, no build integration issues, that testing is covered adequately, and with focus on UX.

          Expected Outcome
          The expected outcome of a continuation effort is new capability and features that are "complete", integrated, bug-free, and issue-free, in the hands of users. This means your project covers all vertical integration aspects of development integration including build system and usability / UX concerns. Not prototyped. Not simply rewritten or re-attempted.

          If the prior effort was integrated, your outcome will be specific polish, adaptiveness, and robustness improvements.

          If the prior effort was not integrated, your outcome will be issue-free integration that addresses prior issues preventing integration (which will require research and understanding on your part).

          Project Properties
          Skills
          This varies greatly by continuation. There are continuation projects for C/C++, Python, Javascript/Node.js, Tcl/Tk, OpenCL, OpenGL, Qt, GPGPU, and more.

          Difficulty
          Varies.

          Size
          You are welcome to scope your project medium (175h) or long (350h) depending on the objectives and development scope.

          Additional Information
          Potential mentor(s): Morrison (contact devs@brlcad.org)
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Implement AP242 STEP geometry importer for BRL-CAD
          Outline
          Implement a geometry importer for the ISO 10303 STEP AP242 standard.

          Details
          BRL-CAD has geometry import support for STEP AP203 (v1), but AP242 has emerged as its industry replacement. This project entails implementing as comprehensive import support as possible in BRL-CAD.

          In order to track implementation progress and manage development risk, you will need to track implementation coverage by setting up a dashboard similar to what is used by the CAx-IF -- it can be a simple text file or web page.

          Existing conversion support can be examined for AP203 and other formats in BRL-CAD's repository under src/conv/step

          Expected Outcome
          New AP242 importer that converts STEP entities into BRL-CAD's .g geometry file format.

          Future Possibilities
          AP242 export support...

          Project Properties
          Skills
          C/C++
          STEPcode

          Difficulty
          Hard.

          Size
          This project can be scoped medium (175h) or long (350h) depending on your familiarity and expertise, or you can propose a subset of entities in a shorter timeframe (note though that advanced boundary representation entities should be prioritized).

          Additional Information
          Potential mentor(s): Morrison (contact devs@brlcad.org)
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          BRL-CAD Python bindings
          Outline
          Implement bindings for the BRL-CAD functionality to Python programming language

          Details
          There are long time on-going efforts to wrap BRL-CAD functionality with python code, e.g.

          https://github.com/kanzure/python-brlcad
          https://github.com/nmz787/python-brlcad-tcl
          These projects are however still in early development stages.

          Expected Outcome
          A Python module which can read and write BRL-CAD databases, and provide access to their contents to read, create, and modify the objects.

          Project Properties
          Skills
          C/C++
          Python
          Difficulty
          This project may be of easy or medium difficulty, depending on your familiarity and expertise.

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Daniel Rossberg
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          Webapp to create and check BIM project exchange requirements for IfcOpenShell
          Outline
          When projects exchange data, we often need to set contractual requirements about what data we expect to see in their CAD or Building information data. The is an international standard for describing project requirements in XML called the Information Delivery Specifications (IDS).

          There is a half-built webapp which allows viewing and minor editing of IDS files here: https://blenderbim.org/ifctester/

          Your job would be to finish this web app, building features for more editing, drag and drop from a library of specifications, adding and removing requirements, etc.

          Expected Outcome
          A working example of the web application.

          Project Properties
          Skills
          HTML, CSS, and vanilla Javascript (i.e. no frameworks).
          Difficulty
          Easy

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://ifcopenshell.org
          Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues


          ~~~~~~~~~~

          Scripts for generating simple animations (e.g. appear / disappear, bounce, appear left to right, fade in from above, etc)

          Outline
          Often, construction firms need to visualise animations of construction sequencing. A project timeline will be created, and related to individual model elements. For example, when a concrete slab is poured, it is linked to a 3D object called a slab. We need the ability to automatically generate animations from Blender where objects appear / disappear in various different ways when they start / end their task in the project timeline. The systems for describing project timelines is already in place, so now we need a little animation generator!

          Details
          Expected Outcome
          A series of small scripts that take objects and can automatically animate the visibility, locations, or staggered appearances of building elements, as well as sub elements, and basic scripts that correlate real world time to animation frames, and frames per second, and generate an animated timeline bar in various styles.

          Future Possibilities
          This animation system can be then used from BIM models either in Blender, FreeCAD, or via other software altogether, so it has quite a large impact on the ecosystem.

          Skills
          Basic knowledge of the principles of animation (keyframing)
          Basic Blender animation (you can do some tutorials and get up to speed pretty quick)
          Python
          Artistic sense! We should offer beautiful and elegant animations!
          Difficulty
          Easy

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://ifcopenshell.org
          Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues


          ~~~~~~~~~~

          NURBS Editing Support in BRL-CAD

          Outline
          Implement the prerequisites for NURBS editing in BRL-CAD's GUIs

          Details
          BRL-CAD has support for raytracing of NURBS surfaces implemented, but they are handed over as BLOBs to the openNURBS library. Beyond basic operations such as rotation and translation, the BRL-CAD core has no ability to edit them. This project would implement support for editing NURBS curves and surfaces in the BRL-CAD core, thus creating the prerequisites to handle them with higher level (i.e. GUI) tools.

          See this task's description in former GSoCs for some more information: https://brlcad.org/wiki/NURBS_Editing_Support

          The key-feature would be to have ged command(s) that lets you build NURBS objects from scratch. This could be done by having a declarative ASCII description of these entities and/or wrapping the openNURBS library by a scripting language.

          Describe in your proposal which approach you want to use and why. You may let inspire you by solutions in other programs:

          NURBS-Python: https://github.com/orbingol/NURBS-Python
          Blender: https://blender.stackexchange.com/questions/7020/create-nurbs-surface-with-python
          Web3D: https://www.web3d.org/x3d/content/examples/Basic/NURBS/
          3DSMax/Maya: https://help.autodesk.com/view/3DSMAX/2016/ENU/?guid=__files_GUID_75CD4DE9_8024_4E25_B147_0A0EC8B10031_htm
          Ayam: http://ayam.sourceforge.net/docsdraft/ayam-6.html
          Expected Outcome
          Implementing the necessary logic for NURBS handling in librt, libbrep, and libged

          Future Possibilities
          Implementing a visual NURBS editor in a BRL-CAD GUI (mged, Archer, Arbalest)

          Project Properties
          Skills
          C/C++
          Difficulty
          medium

          Size
          long (350h)

          Additional Information
          Potential mentor(s): Daniel Rossberg, Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          New BRL-CAD GUI

          Outline
          Develop further the new GUI for BRL-CAD!

          Details
          BRL-CAD has two main graphical applications called 'mged' and 'archer' which look like they were developed in the 80's and 90's respectively (because they were). We need a modern GUI, ideally using Qt.

          This new GUI will need to leverage our existing libraries in a big way. This includes the C++ coreInterface ( see https://brlcad.org/wiki/Object-oriented_interfaces) or its successor MOOSE (see https://github.com/BRL-CAD/MOOSE) and LIBGED (see src/libged). The latter is basically all commands available to both mged and archer.

          During past GSoCs an amazing start was made with arbalest. Based on this, the development of a GUI called 'qged' (see src/qged) was started, which you should include in your considerations too. This program implements the traditional BRL-CAD workflow under a modern Qt-based user interface.

          You may propose a complete different approach, but we recommend to use arbalest as starting point for your work. Which additions would you like to program in this years GSoC? You can use the results of the former prototype CAD GUI Google Code-in tasks (http://brlcad.org/gci/data/uncategorized/, search for CAD_GUI there) for inspiration.

          Keep your proposal lean and simple. The main emphasis should be on adding features and/or improvements to our next generation GUI.

          Expected Outcome
          An improved BRL-CAD GUI.

          Project Properties
          Skills
          C/C++
          Qt
          Difficulty
          medium

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Daniel Rossberg
          Himanshu Sekhar Nayak
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Online Geometry Viewer (OGV)

          Outline
          Write a proposal that leverages rewrite of the existing application in the latest tech stack for frontend and backend.

          Details
          We have been working on OGV for over many years. It started with PHP and then was revamped to meteor.js. We want to focus on the backend of OGV, making sure it works properly, converts the models properly, and basically finish a 1.0 version of OGV so we can launch it for the masses! For that, we are planning to change the legacy backend to be rewritten along with the frontend.

          You can use any tech stack (react, vue) for frontend and node, C/C++ for the backend. We faced some problems like removing certain deprecated dependencies and adding new features with Meteor. We are planning to port the application and add all specified features.

          Possible New Features
          Integrating BRL-CAD GCV(Geometry Conversion Vocabulary) to add support for more file formats like .stl, .obj, and .3dm.
          Automated Conversion for Web Display. Convert uploaded files automatically into polygonal formats for web visualization. Ensure smooth rendering and compatibility with web-based 3D viewers.
          Implementing a Model Repository based project architecture for storing and downloading 3d models.
          Conduct a full STIG compliance audit (Security Technical Implementation Guide), which involves ~200+ security checks.
          Run security scans using OWASP and Dependency-Check, addressing any reported vulnerabilities.
          However, you don't have to limit yourself to those ideas.

          Checklist to write proposal for OGV
          Download and clone OGV from https://github.com/BRL-CAD/OGV-meteor
          Setup and Run OGV on your local machine.
          Fork OGV repo
          Understand the flow of existing application
          Talk to mentors
          Choose list of issues that you would like to solve this summer
          Make a detailed weekly implementation plan
          Share your proposal with your mentors
          Submit it to the GSoC website
          Expected Outcome
          You're expected to propose an outcome useful to end-users. That is a broad range of possibilities that will depend on your interests and experience level. For example, you might propose focusing on the backend conversion to triangles for display (C/C++/Node.js). Or you might propose changing the backend to NURBS surfaces (C/C++) and using verbnurb or three.js (Javascript) to display them instead of triangles. Or you might propose keeping the backend the way it is and focus on front-end robustness (Vue, React), website features, or deployment infrastructure. You hopefully get the idea.

          Project Properties
          Skills
          JavaScript (Vue, React)
          Node.js (required)
          C/C++ (optional)
          Verbnurb (optional)
          Three.js (optional)
          Difficulty
          Hard

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Amanjot Singh
          Daniel Rossberg
          Divyanshu Garg
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Add OpenSCAD support for exporting models in STEP format
          Outline
          The STEP format is widely used in the industry to transfer CAD data between different systems. Currently OpenSCAD does not support STEP import or export. Adding STEP export would open up a number of new usecases or simplify the workflow as no external conversion tools are needed to convert to STEP. This includes the design of 3D models for other CAD tools, e.g. for KiCAD where STEP models are used to render 3D representations of PCBs. Other use cases are for manufacturing where sometimes only STEP files are accepted as input, e.g. for CNC milling services.

          Details
          The main focus of this project is to get the ground work done for exporting more detailed models, as opposed to just exporting the fully rendered single mesh which is the normal case right now.

          Topics that need to be solved

          Research options of usable libraries
          Investigate what type of STEP files are accepted as input by various tools
          Select library and integrate into OpenSCAD
          Implement base functionality to export single meshes
          Add test cases to verify the new export functionality
          Update build system to include the new library into installers
          Prototype how more advanced models can be exported
          Expected Outcome
          OpenSCAD supports exporting single meshes as STEP
          (optional) Understanding/Plan of how to support additional features supported by STEP
          Project Properties
          Skills
          Programming language is C++
          Understand and use APIs from external libraries
          Integrate new libraries into the build system for the 3 supported platforms
          Add test cases with files using the new features to allow regression testing
          Difficulty
          Hard

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Organization website: https://www.openscad.org/
          Known libraries:

          StepCode - http://stepcode.org/ (https://github.com/stepcode/stepcode)
          OpenCASCADE - https://www.opencascade.com/


          
    totalCharacters_of_ideas_content_parent: 51942
    totalwords_of_ideas_content_parent: 12320
    totalTokenCount_of_ideas_content_parent: 10905
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/brl-cad/
    idea_list_url: https://github.com/opencax/GSoC/issues?q=is%3Aissue+is%3Aopen+label%3A%22GSoC+2025%22



  - organization_id: 13
    organization_name: BeagleBoard.org
    no_of_ideas: 5
    ideas_content: |
        Deep Learning 
        Medium complexity 175 hours

        A Conversational AI Assistant for BeagleBoard using RAG and Fine-tuning
        BeagleBoard currently lacks an AI-powered assistant to help users troubleshoot errors. This project aims to address that need while also streamlining the onboarding process for new contributors, enabling them to get started more quickly.

        Goal: Develop a domain-specific chatbot for BeagleBoard using a combination of RAG and fine-tuning of an open-source LLM (like Llama 3, Mixtral, or Gemma). This chatbot will assist users with troubleshooting, provide information about BeagleBoard products, and streamline the onboarding process for new contributors.
        Hardware Skills: Ability to test applications on BeagleBone AI-64/BeagleY-AI and optimize for performance using quantization techniques.
        Software Skills: Python, RAG, Scraping techniques, Fine tuning LLMs, Gradio, Hugging Face Inference Endpoints, NLTK/spaCy, Git
        Possible Mentors: Aryan Nanda

        ~~~~~~~~~~



        Linux kernel improvements
         Medium complexity 350 hours

        Update beagle-tester for mainline testing
        Utilize the beagle-tester application and Buildroot along with device-tree and udev symlink concepts within the OpenBeagle continuous integration server context to create a regression test suite for the Linux kernel and device-tree overlays on various Beagle computers.

        Goal: Execution on Beagle test farm with over 30 mikroBUS boards testing all mikroBUS enabled cape interfaces (PWM, ADC, UART, I2C, SPI, GPIO and interrupt) performing weekly mainline Linux regression verification
        Hardware Skills: basic wiring, embedded serial interfaces
        Software Skills: device-tree, Linux, C, OpenBeagle CI, Buildroot
        Possible Mentors: Deepak Khatri, Anuj Deshpande, Dhruva Gole

        ~~~~~~~~~~

        Linux kernel improvements
         Medium complexity 175 hours

        Upstream wpanusb and bcfserial
        These are the drivers that are used to enable Linux to use a BeagleConnect Freedom as a SubGHz IEEE802.15.4 radio (gateway). They need to be part of upstream Linux to simplify on-going support. There are several gaps that are known before they are acceptable upstream.

        Goal: Add functional gaps, submit upstream patches for these drivers and respond to feedback
        Hardware Skills: wireless communications
        Software Skills: C, Linux
        Possible Mentors: Ayush Singh, Jason Kridner

        ~~~~~~~~~~

        Automation and industrial I/O Medium complexity 175 hours

        librobotcontrol support for newer boards
        Preliminary librobotcontrol support for BeagleBone AI, BeagleBone AI-64 and BeagleV-Fire has been drafted, but it needs to be cleaned up. We can also work on support for Raspberry Pi if UCSD releases their Hat for it.

        Goal: Update librobotcontrol for Robotics Cape on BeagleBone AI, BeagleBone AI-64 and BeagleV-Fire
        Hardware Skills: basic wiring, motors
        Software Skills: C, Linux
        Possible Mentors: Deepak Khatri, Jason Kridner

        ~~~~~~~~~~

        RTOS/microkernel imporvements
        Medium complexity 350 hours

        Upstream Zephyr Support on BBAI-64 R5
        Incorporating Zephyr RTOS support onto the Cortex-R5 cores of the TDA4VM SoC along with Linux operation on the A72 core. The objective is to harness the combined capabilities of both systems to support BeagleBone AI-64.

        Goal: submit upstream patches to support BeagleBone AI-64 and respond to feedback
        Hardware Skills: Familiarity with ARM Cortex R5
        Software Skills: C, RTOS
        Possible Mentors: Dhruva Gole, Nishanth Menon
        Upstream Repository: The primary repository for Zephyr Project

          
    totalCharacters_of_ideas_content_parent: 3803
    totalwords_of_ideas_content_parent: 786
    totalTokenCount_of_ideas_content_parent: 816
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/beagleboard.org/
    idea_list_url:  https://gsoc.beagleboard.io/ideas/


  - organization_id: 14
    organization_name: Blender
    no_of_ideas: 15
    ideas_content: |
        Flamenco¶
        Flamenco is Blender's render farm management system. For all the ideas below, a requirement is that you've set up Flamenco for yourself and used it to render things. Of course also some experience with Blender itself is needed.

        Statistics¶
        Description: Design & build a system for Flamenco to collect and display statistics.
        The goal is to show per-job, per-task, and per-worker statistics.
        This should make it possible for users to predict how long a render job will be, as they can look up things like render times of similar render jobs.
        The underlying design should be generic enough to store all kinds of statistics and events, not specific to Blender render times only.
        This project would include the technical design of this feature, the frontend / UI / UX design, and the implementation of both front- and back-end.
        Optional: record more statistics, such as per-frame render time, memory usage, etc.
        Optional: show progress of jobs in the jobs list.
        Expected outcomes: A way for users to get a better understanding of how a new render job will behave, as they can look up information about past & currently running jobs.
        Skills required: Familiarity with Go and unit testing. Familiarity with web languages (HTML/CSS/JavaScript, VueJS).
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        RNA Overrides¶
        Description: Render jobs should be able to specify "RNA overrides" (#101569). In other words, the job definition should be able to include some Python code that sets certain properties in Blender to certain values.
        Design a way to include this in a job definition, and how it affects tasks & commands.
        This could include the ability to add new RNA overrides to existing jobs.
        This should include the ability to update those override values via the web interface.
        Design how such additions / changes affect already-created tasks/commands in the database. Or a way to make this work without changing things in the database?
        This can be used both when a user wants to change something (like re-rendering with increased sample count), or for Flamenco itself to adjust things in the blend file without having to save those values in the blend file itself (#104264)
        Expected outcomes: Give users a simpler way to configure Flamenco for their needs.
        Skills required: Familiarity with Blender (for making the RNA overrides themselves work). Familiarity with Go and unit testing for adjusting Flamenco. Familiarity with web languages (HTML/CSS/JavaScript, VueJS) for the web frontend.
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        Configuration Web Interface¶
        Description: Flamenco's configuration file can be created via its Setup Assistant, but that's only for the initial configuration. For managing more complex things, like the two-way variables for cross-platform support, users still have to manually edit the YAML file. This project is about introducing a configuration editor in the web frontend, and potentially new backend API functions to support that.
        New tab in the frontend for managing the configuration.
        A way to retrieve and visualise the configuration.
        New front-end widgets to represent these, including more complex cases like one-way and two-way variables.
        A way to save the edited configuration.
        Optional: A validator for the configuration options, so that changes can be checked before they take hold.
        Optional: A way for Flamenco Manager to load and apply the configuration without restarting the process.
        Optional: A custom job type for validating configuration paths, so that, for example, a macOS path can be actually checked on a Worker running macOS.
        Expected outcomes: Give users a simpler way to configure Flamenco for their needs.
        Skills required: Familiarity with web languages (HTML/CSS/JavaScript, VueJS, OpenAPI). Potentially also familiarity with Go in case of backend work.
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        Polish & Shine¶
        Description: Fix various issues & implement missing pieces to solve common bumps in the road.
        Reconsider some design aspects of the web frontend, so that it works better on narrower / smaller screens.
        Allow finishing setup assistant without Blender on Manager (#100195).
        Introduce per-Worker logs on the Manager, for introspection and debugging.
        Fix issues with two-way variables (#104336, #104293)
        Add a web interface for the mass deletion of jobs. There is already an API call for this, which deletes all jobs older than a certain timestamp.
        Other issues from the tracker.
        Expected outcomes: Improve the overall experience people have when working with Flamenco.
        Skills required: Familiarity with web languages (HTML/CSS/JavaScript, VueJS) for front-end work. Familiarity with Go and OpenAPI for backend work.
        Possible mentors: Sybren Stüvel.
        Expected project size: 175 hours
        Difficulty: medium

        ~~~~~~~~~~
        Geometry Nodes¶
        Regression Testing¶
        Description: As people build more assets on top of Geometry Nodes, it becomes more and more important to ensure good backwards compatibility. This project focuses on improving our regression tests to cover more issues as early as possible. This involves:
        Adding new tests in our existing test framework.
        Extending the test framework to cover node tools, baking and maybe other areas we still have to find.
        Preparing more complex production files for use in regression tests.
        Expected outcomes: Improved stability of Geometry Nodes.
        Skills required: Good in C/C++ and Python.
        Possible mentors: Jacques Lucke, Hans Goudey
        Expected project size: 90 or 175 hours depending on how many of the mentioned areas are covered
        Difficulty: Easy (using existing framework) and Medium (extending framework)

        ~~~~~~~~~~
        Modeling¶
        Improve Edit-Mesh Mirror¶
        Description: Blender's mesh mirroring in mesh edit-mode works for basic transformations, but does not work for most other operations such as sliding, smoothing, marking seams, etc. In practice, this makes the edit-mode mirror only useful in very specific circumstances and not for general modeling.

        While supporting every operation isn't practical, enabling it for a subset of operators such as those that only adjust existing geometry (rather than adding or removing geometry) would be immediately useful for artists.

        Expected outcomes: Improved edit-mesh mirror support for existing tools.
        Skills required: Proficient in C/C++.
        Possible mentors: Campbell Barton
        Expected project size: 175 or 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        Sculpt & Paint¶
        Mesh Sculpting Performance Improvements¶
        Description: Last year's sculpting rewrite project gave a large improvement in performance, but the team didn't have the time to pursue everything. This task lists possible future improvements. This GSoC project would explore one or more of those ideas with in depth performance testing and experimentation.
        Expected outcomes: More interactive sculping with large meshes
        Skills required: Proficient in C++, familiarity with data-oriented-design.
        Possible mentors: Hans Goudey
        Expected project size: 175 or 350 hours
        Difficulty: medium or hard
        
        ~~~~~~~~~~
        VFX & Video¶
        Hardware accelerated video encoding/decoding¶
        Description: Currently Blender encodes and decodes video though ffmpeg C libraries, on the CPU. ffmpeg also has support for hardware video processing (various kinds depending on platform), this project would enable usage of that.
        Build ffmpeg with hardware video processing support included.
        Note: Blender can't include "non-free" ffmpeg libraries (which means cuda_nvcc, cuda_sdk, libnpp can't be used).
        On Blender's video decoding and encoding side, implement code that would use any relevant ffmpeg C libraries parts for hardware video processing, when supported.
        Decide which additional UI settings need to be exposed to the user, to control hardware video processing.
        Implement code needed to transfer video frames between hardware memory and CPU memory as needed (the rest of VSE processing pipeline is purely on CPU currently).
        Expected outcomes: Video encoding or decoding is more efficient by using dedicated hardware.
        Skills required: Proficient in C/C++, familiarity with ffmpeg.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        High Dynamic Range (HDR) support for video¶
        Description: This project has several partially dependent parts that are all about HDR support within VSE:
        Make Sequencer preview window be able to display HDR content on a capable display (like 3D viewport or Image window can).
        Make blender movie reading code be able to decode HDR videos into proper scene-linear or sequencer color space as needed. HDR video data might be PQ or HLG encoded, and this might need special decoding into destination color space.
        Make blender movie writing code be able to encode HDR videos. Blender already can encode 10/12 bit videos, but only for regular LDR. Additional PQ or HLG data encoding and necessary video metadata is not currently done.
        Expected outcomes: HDR video handling is improved within Blender.
        Skills required: Proficient in C/C++, familiarity with ffmpeg, knowledge of color spaces and color science.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: OpenTimelineIO support¶
        Description: built-in support for OpenTimelineIO import/export within Blender VSE. Blender Studio has experimented with it in 2021, by using and extending a 3rd party addon vse_io. It might be useful to have built-in support for this.
        Expected outcomes: Blender VSE can import and export .otio files.
        Skills required: Proficient in C/C++, familiarity with video editing workflows.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Pitch correction for sound playback¶
        Description: Currently when audio is retimed, the pitch changes, would be nice to have an option to preserve pitch. Different approaches could be researched and implemented (e.g. pitch correction for mostly human speech might be different from pitch correction of music). Might need integration of some 3rd party library if it is suitable for the task, or implementing the correction algorithms manually.
        Expected outcomes: Retimed sound playback has options to preserve original pitch.
        Skills required: Proficient in C/C++, sound processing algorithms.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Animation retiming¶
        Description: Modify animation of strips, when changing their playback speed. Retiming allows changing playback speed of strips, but when strips are animated, the animation keys are fixed in position. These could be moved, such that animation is seemingly mapped to frames of the content.
        Expected outcomes: Animation proportionally is scaled with strip when retiming.
        Skills required: Proficient in C++.
        Possible mentors: Richard Antalik
        Expected project size: 175 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Keyframing in preview¶
        Description: Open workflow quick animation in VSE preview region. In 3D viewport it is possible to transform object and press I key to add key for its position. This also could be done in sequencer preview. This feature should follow same rules and preferences.
        Expected outcomes: Possibility of quick and easy animation in VSE preview
        Skills required: Proficient in C++.
        Possible mentors: Richard Antalik
        Expected project size: 90 hours
        Difficulty: medium

        ~~~~~~~~~~
        Compositor: Implement new nodes¶
        Description: The compositor has been rewritten to be more efficient and future proof. Moving forward, it would be nice to implement new nodes to make the compositor as powerful as it can be. Interested students are encouraged to propose their own ideas. Some example nodes include:
        Define low / high points
        Expected outcomes: Implemented one or more nodes for both CPU and GPU backends.
        Skills required: Good in C/C++, image processing algorithms, familiarity with shaders.
        Possible mentors: Habib Gahbiche / Omar Emara
        Expected project size: 90 or 175 hours depending on the node
        Difficulty: Easy or Medium depending on the node

        ~~~~~~~~~~
        Compositor: UI improvements¶
        Description: The compositor has been rewritten to be more efficient and future proof. It would be nice to improve the UI of some nodes as well as the workflow overall. Interested students are encouraged to suggest ideas. Some examples include:
        Implement 2D gizmos for exisiting nodes.
        Re-design the UI of exisiting nodes
        Expected outcomes: Improved UI within node editors.
        Skills required: Proficient in C/C++, familiarity with design patterns.
        Possible mentors: Habib Gahbiche / Omar Emara
        Expected project size: 90 or 175 hours depending on the scope of the project
        Difficulty: Easy or Medium depending on the scope

          
    totalCharacters_of_ideas_content_parent: 14181
    totalwords_of_ideas_content_parent: 3076
    totalTokenCount_of_ideas_content_parent: 2963
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/blender-foundation/
    idea_list_url: https://developer.blender.org/docs/programs/gsoc/ideas/

  - organization_id: 15
    organization_name: CCExtractor Development
    no_of_ideas: 13
    ideas_content: |
      
        CCExtractor Release 1.00	This is our ambitious project for the summer - work directly with the core team to prepare 1.00 - our first major version bump ever, by getting our PR's from last year vetted, tested and integrated	Some of these: Rust, C, Flutter, Docker, GitHub actions	The rest from the previous list.	Hard	350 hours
        ~~~~~~~~~~
        Ultimate Alarm Clock III	The ultimate alarm clock, with features no other one has. And free!	Flutter	Good application design	Medium	350 hours
        ~~~~~~~~~~
        Beacon Watch Companion	Beacon was started in 2021 and it got a great push also during 2022 and 2024. It aims to ease the group travelling (or hiking). This project is intended to be a companion for the beacon project for smart watches.	Flutter	Scalability	Medium	175 hours
        ~~~~~~~~~~
        Ultimate Alarm Clock Watch Companion	Ultimate Alarm Clock launched in 2023 and gained significant momentum in 2024. It aims to offer unique features that set it apart from other alarm clock apps—all for free!. This project is intended to be a companion for the ultimate alarm clock project for smart watches.	Flutter	Scalability	Medium	175 hours
        ~~~~~~~~~~
        Smart Health Reminder	A fun and interactive health tracking app with smart reminders, challenges, and gamification. Stay healthy effortlessly!	Flutter	Gamification & UX design	Medium	350 hours
        ~~~~~~~~~~
        support more torrent clients	We'd like to add support for other clients to our ruTorrent mobile interface (which of course will get a new name): Flood and Deluge.	Flutter	API, Teamwork	Medium	Discuss
        ~~~~~~~~~~
        URL shortener, with a twist	A URL shortener converts a long URL into a shorter one. There are many use cases. Some times it's just the shortening itself we want, for example to share it on twitter. Other times it's about obfuscation. We want to create our own, but with some unique features.	Any language you want	Internet infrastructure	Medium	175 hours
        ~~~~~~~~~~
        COSMIC Session For Regolith	COSMIC is a wayland based desktop environment written from scratch in rust, with modularity in mind. We're interested in swapping the GNOME components of Regolith DE with COSMIC.	Rust	Wayland, Iced, DBus, etc	Medium	350 hours
        ~~~~~~~~~~
        Add complex layouts to sway	Sway is a drop-in replacement for i3, a popular windows manager for Linux that finally gets rid of the ancient X11 protocol. It's fantastic, but it's still missing support for complex scenarios. We'd like you to work on that support.	C	Sway	Hard	350 hours
        ~~~~~~~~~~
        Expose ectool functionality as a library	ectool is a CLI that lets you interact with an embedded controller for laptops. Expose its functionality as a library so it's possible to use it without spawning the CLI.	C, Python	Interlanguage connectivity	Medium	350 hours
        ~~~~~~~~~~
        CCSync	This project aims to develop a comprehensive platform that can be used sync tasks with taskserver.A hosted solution for syncing your TaskWarrior client.Setting up your own TaskServer takes some effort.And platforms like inthe.am,freecinc have shut down their services.So we want to create a platform similar to inthe.am , freecinc and wingtask.	Any language you want	Internet infrastructure	Medium	175 hours
        ~~~~~~~~~~
        Mouseless for Linux v2 - i3 edition	Mouseless is a nice tool to practice keyboard shortcuts for a few popular apps. Unfortunately it's only available for Mac. Last year we created an open source one that runs on Linux. Using that work or not (this is your choice) we want to create one that helps use i3vm (the fantastic windows manager) using keys only.	Your choice	??	Unknown	175 hours
        ~~~~~~~~~~
        Desktop Actions in Ilia	Desktop Actions defined in .desktop files are used by app launcher to provide access to additional functionalities, typically via context menus. Ilia is an app launcher that currently doesn't support for Desktop Actions due to its keyboard based approach.	Vala, GTK	GTK	Medium	175 hours




          
    totalCharacters_of_ideas_content_parent: 4077
    totalwords_of_ideas_content_parent: 734
    totalTokenCount_of_ideas_content_parent: 964
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ccextractor-development/
    idea_list_url: https://ccextractor.org/docs/ideas_page_for_summer_of_code_2025/

  - organization_id: 16
    organization_name: CERN-HSF
    no_of_ideas: 37
    ideas_content: |
          Precision Recovery in Lossy-Compressed Floating Point Data for High Energy Physics
          Description
          ATLAS is one of the particle physics experiments at the Large Hadron Collider (LHC) at CERN. With the planned upgrade of the LHC (the so-called High Luminosity phase), allowing for even more detailed exploration of fundamental particles and forces of nature, it is expected that the recorded data rate will be up to ten times greater than today. One of the methods of addressing this storage challenge is data compression. The traditional approach involves lossless compression algorithms such as zstd and zlib. To further reduce storage footprint, methods involving lossy compression are being investigated. One of the solutions in High Energy Physics is the reduction of floating point precision, as stored precision may be higher than detector resolution. However, when reading data back, physicists may be interested in restoring the precision of the floating point numbers. This is obviously impossible in the strict sense, as the process of removing bits is irreversible. Nevertheless, given that the data volume is high, some variables are correlated, and follow specific distributions, one may consider a machine learning approach to recover the lossy-compressed floating-point data.

          Task ideas
          Perform lossy compression of data sample from the ATLAS experiment
          Investigate ML techniques for data recovery, prediction and upscaling
          Integrate the chosen technique into HEP workflow
          Expected results
          Implementation of ML-based procedure to restore precision of lossy-compressed floating-point numbers in ATLAS data
          Evaluation of the method’s performance (decompression accuracy) and its applicability in HEP workflow
          Requirements
          C++, Python, Machine Learning
          Links
          IEEE_754
          Implementation of FloatCompressor in Athena
          Mentors
          Maciej Szymański - ANL
          Peter Van Gemmeren - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: July-September
          Corresponding Project
          ATLAS
          Participating Organizations
          ANL
          CERN

          ~~~~~~~~~~

          The rise of the machine (learning) in data compression for high energy physics and beyond
    

          Short description of the project
          The Large Hadron Collider (LHC) hosts multiple large-scale experiments, LHC experiments such as ATLAS, ALICE, LHCb, and CMS. These together produce roughly 1 Petabyte of data per second, but bandwidth and storage limitations force them to only pick the most interesting data, and discard the rest. The final data stored on disk is roughly 1 Petabyte per day [1]. Despite such steep methods of data reduction, the upgraded High Luminosity LHC in 2029 will produce 10 times more particle collisions. This is a great improvement for the potential to discover new physics, but poses a challenge both for data processing and data storage, as the resources needed in both departments are expected to be 3 and 5 times larger than the projected resources available [2][3].

          Data compression would be the go-to solution to this issue, but general data formats used for big data and the ROOT data format used at the LHC are already highly compressed, meaning that the data does not compress much under normal loss-less compression methods like zip [4]. However, since the observables in these experiments benefit from more events and higher statistics, lossy compression is a good alternative. By using lossy compression some data accuracy is lost, but the compression will allow for the storage of more data which will increase the statistical precision of the final analysis.

          BALER is a compression tool undergoing development at the particle physics division of the University of Manchester. BALER uses autoencoder and other neural networks as a type of lossy machine learning-based compression to compress multi-dimensional data and evaluate the accuracy of the dataset after compression.

          Since data storage is a problem in many fields of science and industry, BALER aims to be an open source tool that can support the compression of data formats from vastly different fields of science. For example, catalog data in astronomy and time series data in computational fluid dynamics.

          This project aims to work on the machine learning models in BALER to optimize performance for LHC data and evaluate its performance in real LHC analyses.

          Task ideas
          This internship can focus on a range of work packages, and the project can be tailored to the intern. Possible projects include:

          New auto-encoder models could be developed, better identifying correlations between data objects in a given particle physics dataset entry (event, typically containing thousands of objects and around 1MB each). New models could also improve performance on live / unseen data. These could include transformer, GNN, probabilistic and other tiypes of networks.
          Existing models could be applied on an FPGA, potentially significantly reducing latency and power consumption, opening the possibility of live compression before transmission of data on a network.
          BALER could also be integrated into standard research data storage formats and programs used by hundreds of thousands of physics researchers (ROOT).
          Finally the compression could be applied to particle physics datasets and the effect on the physics discovery sensitivity of an analysis could be assessed and compared to the possible increased sensitivity from additional data bandwidth.
          Ideas from the intern are also welcomed.

          Expected results
          An improved compression performance with documentation and figures of merit that may include:

          Plots made in matplotlib that demonstrate the performance of the new models compared to the old
          Documentation of the design choices made for the improved models
          Documented evaluation of a physics analysis on data before and after compression
          Requirements
          The candidate should have experience with the python language and a Linux environment, familiarity with AI fundamentals, and familiarity with PyTorch.

          Desirable skills include familiarity with AI fundamentals including transformers and/or graph neural networks, particle physics theory and experiments, PyTorch, FPGA programming and/or simulation.

          Links
          BALER GitHub
          BALER Paper

          Previous work:
          Thesis by Eric Wulff, Lund University
          Thesis by Erik Wallin, Lund University
          GSOC 2020 project: Medium post by Honey Gupta
          GSOC 2021 project: Zenodo entry by George Dialektakis
          ROOT
          Jupyter
          PyTorch
          Mentors
          James Smith - UManchester
          Caterina Doglioni - CERN
          Leonid Didukh
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          baler
          Participating Organizations
          baler
          UManchester
          CERN

          ~~~~~~~~~~

          Probabilistic circuit for lossless HEP data compression
          

          Short description of the project
          Neural data compression is an efficient solution for reducing the cost and computational resources of data storage in many LHC experiments. However, it suffers from the ability to precisely reconstruct compressed data, as most of the neural compression algorithms perform the decompression with the information loosage. On another hand, the lossless neural data compression schemas (VAE, IDF) have a lower compression ratio and are not fast enough for file IO. This project’s task is to overcome the disadvantages of the neural compression algorithm by using the probabilistic circuit for HEP data compression.

          Task ideas
          Implement the probabilistic circuit using the PyTorch
          Train and compress the HEP data (Higgs data, TopQuark Dataset)
          Measure the cost and quantify the optimal compression ratio of the probabilistic circuit
          Perform the benchmark, and compare the results with AE, Transformer
          Expected results
          An improved compression performance with documentation and figures of merit that may include:

          Implemented model of the probabilistic circuit
          Documentation of the benchmark and experiment of compression of the HEP data
          Requirements
          Required: Good knowledge of UNIX, Python, matplotlib, Pytorch, Julia, Pandas, ROOT.

          Links
          Previous work:

          GSOC 2021 project: Zenodo entry by George Dialektakis
          Baler – Machine Learning Based Compression of Scientific Data
          ROOT
          Jupyter
          Lossless compression with probabilistic circuits
          iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder
          Integer Discrete Flows and Lossless Compression
          Mentors
          Leonid Didukh
          Caterina Doglioni - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October (with 3 weeks mentor vacation where student will work independently with minimal guidance)
          Corresponding Project
          baler
          Participating Organizations
          CERN

          ~~~~~~~~~~
          Agent-Based Simulation of CAR-T Cell Therapy Using BioDynaMo
          Description
          Chimeric Antigen Receptor T-cell (CAR-T) therapy has revolutionized cancer treatment by harnessing the immune system to target and destroy tumor cells. While CAR-T has demonstrated success in blood cancers, its effectiveness in solid tumors remains limited due to challenges such as poor tumor infiltration, immune suppression, and T-cell exhaustion. To improve therapy outcomes, computational modeling is essential for optimizing treatment parameters, predicting failures, and testing novel interventions. However, existing models of CAR-T behavior are often overly simplistic or computationally expensive, making them impractical for large-scale simulations.

          This project aims to develop a scalable agent-based simulation of CAR-T therapy using BioDynaMo, an open-source high-performance biological simulation platform. By modeling T-cell migration, tumor engagement, and microenvironmental factors, we will investigate key treatment variables such as dosage, administration timing, and combination therapies. The simulation will allow researchers to explore how tumor microenvironment suppression (e.g., regulatory T-cells, hypoxia, immunosuppressive cytokines) affects CAR-T efficacy and what strategies such as checkpoint inhibitors or cytokine support can improve outcomes.

          The final deliverable will be a fully documented, reproducible BioDynaMo simulation, along with analysis tools for visualizing treatment dynamics. The model will provide insights into the optimal CAR-T cell dosing, tumor penetration efficiency, and factors influencing therapy resistance. This project will serve as a foundation for in silico testing of immunotherapies, reducing the need for costly and time-consuming laboratory experiments while accelerating the development of more effective cancer treatments.

          Expected plan of work:
          Phase 1: Initial Setup & Simple T-cell Dynamics
          Phase 2: Advanced CAR-T Cell Behavior & Tumor Interaction
          Phase 3: Integration of Immunosuppressive Factors & Data Visualization
          Expected deliverables
          A fully documented BioDynaMo simulation of CAR-T therapy.
          Analysis scripts for visualizing tumor reduction and CAR-T efficacy.
          Performance benchmarks comparing different treatment strategies.
          A research-style report summarizing findings.
          Requirements
          C++ (for BioDynaMo simulations)
          Agent-based modeling (understanding immune dynamics)
          Basic immunology & cancer biology (optional but helpful)
          Data visualization (Python, Matplotlib, Seaborn)
          Links
          Mapping CAR T-Cell Design Space Using Agent-Based Models
          BioDynaMo: A Modular Platform for High-Performance Agent-Based Simulation
          Computational Modeling of Chimeric Antigen Receptor (CAR) T-Cell Therapy of a Binary Model of Antigen Receptors in Breast Cancer
          Investigating Two Modes of Cancer-Associated Antigen Presentation in CAR T-Cell Therapy Using Agent-Based Modeling
          BioDynaMo: Cutting-Edge Software Helps Battle Cancer
          Mentors
          Vassil Vassilev
          Lukas Breitwieser - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          BioDynamo
          Participating Organizations
          CERN
          CompRes

          ~~~~~~~~~~

          Development of an auto-tuning tool for the CLUEstering library
          Description
          CLUE is a fast and fully parallelizable density-based clustering algorithm, optimized for high- occupancy scenarios, where the number of clusters is much larger than the average number of hits in a cluster (Rovere et al. 2020). The algorithm uses a grid spatial index for fast querying of neighbors and its timing scales linearly with the number of points within the range considered. It is currently used in the CMS and CLIC event reconstruction software for clustering calorimetric hits in two dimensions based on their energy. The CLUE algorithm has been generalized to an arbitrary number of dimensions and to a wider range of applications in CLUEstering, a general purpose clustering library, with the backend implemented in C++ and providing a Python interface for easier use. The backend can be executed on multiple backends (serial, TBB, GPUs, ecc) thanks to the Alpaka performance portability library. One feature currently lacking from CLUEstering and that would be extremely useful for every user, is an autotuning of the parameters, that given the expected number of clusters computes the combination of input parameters that results in the best clustering.
          For this task, one of the options to be explored is “The Optimizer”, a Python library developed by the Patatrack group of the CMS experiment which provides a collection of optimization algorithm, in particular MOPSO (Multi-Objective Particle Swarm Optimization).

          Expected results
          Consider the best techniques and tools for the task
          Develop an auto-tuning tool for the parameters of CLUEstering
          Test it on a wide range of commonly used datasets
          Benchmark and profile to identify the bottlenecks of the tool and optimize it
          Evaluation Task
          Interested students please contact simone.balducci@cern.ch

          Technologies
          C++, Python
          Desirable skills
          Experience with development in C++17/20
          Experience with GPU computing
          Experience with machine learning and optimization techniques
          Experience with development of Python libraries
          Links
          CLUE
          CLUEstering
          Alpaka
          Mentors
          Simone Balducci - CERN UNIBO
          Felice Pantaleo - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Patatrack
          Participating Organizations
          CERN


          ~~~~~~~~~~

          Evaluate Distribution of ML model files on CVMFS
          Description
          Particle physicists studying nature at highest energy scales at the Large Hadron Collider rely on simulations and data processing for their experiments. These workloads run on the “computing grid”, a massive globally distributed computing infrastructure. Deploying software efficiently and reliable to this grid is an important and challenging task. CVMFS is an optimised shared file system developed specifically for this purpose: it is implemented as a POSIX read-only file system in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs. In many cases, it replaces package managers and shared software areas on cluster file systems as means to distribute the software used to process experiment data.

          Task idea
          CVMFS is optimized for the distribution of software (header files, scripts and libraries), taking advantage of the repeated access pattern for its caching, and the possibility to deduplicate files present in several versions. CVMFS is capable to provide a general read-only POSIX file system view on data in external storage. A very common use case is to make conditions databases available to workloads running in distributed computing infrastructure, but various datasets have been published in CVMFS. How efficient CVMFS can be always depends on the details in these use cases - often the benefit for the users is simply in leveraging the existing server and proxy infrastructure.

          In this project proposal, we’d like to evaluate CVMFS as a means to distribute machine learning model files used in inference, for example .onnx files. The main focus will be on creating a test deployment and benchmarking the access, as well as possible coding utilities and scripts to aid in the deployment of models on CVMFS. We’d also like to contrast CVMFS to existing inference servers like KServe, and see if it could integrate as a backend storage.

          Expected results and milestones
          Familiarization with the CVMFS server infrastructure
          Familiarization with the ML model usage at CERN, Survey of different common inference model file formats.
          Test deployment of models relevant to ML4EP
          Benchmark and evaluation of inference using models served from CVMFS
          Addition of the benchmark to the CVMFS continuous benchmarking infrastructure
          Writing a best practices document for the CVMFS documentation
          Requirements
          UNIX/Linux
          Interest in scientific computing devops
          Familiarity with common ML libraries, in particular ONNX
          Links
          CVMFS
          KServe
          Mentors
          Valentin Volkl - CERN
          Lorenzo Moneta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-October
          Corresponding Project
          CernVM-FS
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Implement and improve an efficient, layered tape with prefetching capabilities
          Description
          In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Automatic differentiation is an alternative technique to Symbolic differentiation and Numerical differentiation (the method of finite differences). Clad is based on Clang which provides the necessary facilities for code transformation. The AD library can differentiate non-trivial functions, to find a partial derivative for trivial cases and has good unit test coverage.

          The most heavily used entity in AD is a stack-like data structure called a tape. For example, the first-in last-out access pattern, which naturally occurs in the storage of intermediate values for reverse mode AD, lends itself towards asynchronous storage. Asynchronous prefetching of values during the reverse pass allows checkpoints deeper in the stack to be stored furthest away in the memory hierarchy. Checkpointing provides a mechanism to parallelize segments of a function that can be executed on independent cores. Inserting checkpoints in these segments using separate tapes enables keeping the memory local and not sharing memory between cores. We will research techniques for local parallelization of the gradient reverse pass, and extend it to achieve better scalability and/or lower constant overheads on CPUs and potentially accelerators. We will evaluate techniques for efficient memory use, such as multi-level checkpointing support. Combining already developed techniques will allow executing gradient segments across different cores or in heterogeneous computing systems. These techniques must be robust and user-friendly, and minimize required application code and build system changes.

          This project aims to improve the efficiency of the clad tape and generalize it into a tool-agnostic facility that could be used outside of clad as well.

          Expected Results
          Optimize the current tape by avoiding re-allocating on resize in favor of using connected slabs of array
          Enhance existing benchmarks demonstrating the efficiency of the new tape
          Add the tape thread safety
          Implement multilayer tape being stored in memory and on disk
          [Stretch goal] Support cpu-gpu transfer of the tape
          [Stretch goal] Add infrastructure to enable checkpointing offload to the new tape
          [Stretch goal] Performance benchmarks
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes


          ~~~~~~~~~~

          Enhancing LLM Training with Clad for efficient differentiation
          Description
          This project aims to leverage Clad, an automatic differentiation (AD) plugin for Clang, to optimize large language model (LLM) training primarily in C++. Automatic differentiation is a crucial component of deep learning training, enabling efficient computation of gradients for optimization algorithms such as stochastic gradient descent (SGD). While most modern LLM frameworks rely on Python-based ecosystems, their heavy reliance on interpreted code and dynamic computation graphs can introduce performance bottlenecks. By integrating Clad into C++-based deep learning pipelines, we can enable high-performance differentiation at the compiler level, reducing computational overhead and improving memory efficiency. This will allow developers to build more optimized training workflows without sacrificing flexibility or precision.

          Beyond performance improvements, integrating Clad with LLM training in C++ opens new possibilities for deploying AI models in resource-constrained environments, such as embedded systems and HPC clusters, where minimizing memory footprint and maximizing computational efficiency are critical. Additionally, this work will bridge the gap between modern deep learning research and traditional scientific computing by providing a more robust and scalable AD solution for physics-informed machine learning models. By optimizing the differentiation process at the compiler level, this project has the potential to enhance both research and production-level AI applications, aligning with compiler-research.org’s broader goal of advancing computational techniques for scientific discovery.

          Expected Results
          Develop a simplified LLM setup in C++
          Apply Clad to compute gradients for selected layers and loss functions
          Enhance clad to support it if necessary, and prepare performance benchmarks
          Enhance the LLM complexity to cover larger projects such as llama
          Repeat bugfixing and benchmarks
          Develop tests to ensure correctness, numerical stability, and efficiency
          Document the approach, implementation details, and performance gains
          Present progress and findings at relevant meetings and conferences
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Background in LLM is preferred but not required
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes


          ~~~~~~~~~~

          Enable Clad on ONNX-based models
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. Clad is useful in powering statistical analysis and uncertainty assessment applications. ONNX (Open Neural Network Exchange) provides a standardized format for machine learning models, widely used for interoperability between frameworks like PyTorch and TensorFlow

          This project aims to integrate Clad, an automatic differentiation (AD) plugin for Clang, with ONNX-based machine learning models. Clad can generate derivative computations for C++ functions, making it useful for sensitivity analysis, optimization, and uncertainty quantification. By extending Clad’s capabilities to ONNX models, this project will enable efficient differentiation of neural network operations within an ONNX execution environment.

          Expected Results
          Enumerate ONNX modules with increasing complexity and analyze their differentiation requirements.
          Develop a structured plan for differentiating the identified ONNX operations.
          Implement forward mode differentiation for selected ONNX operations.
          Extend support to reverse mode differentiation for more complex cases.
          Create comprehensive tests to validate correctness and efficiency.
          Write clear documentation to ensure ease of use and future maintenance.
          Present results at relevant meetings and conferences.
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Basic knowledge of Clang is preferred but not mandatory
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable automatic differentiation of OpenMP programs with Clad
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. Clad is useful in powering statistical analysis and uncertainty assessment applications. OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared-memory multiprocessing programming in C, C++, and other computing platforms.

          This project aims to develop infrastructure in Clad to support the differentiation of programs that contain OpenMP primitives.

          Expected Results
          Extend the pragma handling support
          List the most commonly used OpenMP concurrency primitives and prepare a plan for how they should be handled in both forward and reverse accumulation in Clad
          Add support for concurrency primitives in Clad’s forward and reverse mode automatic differentiation.
          Add proper tests and documentation.
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Parallel Programming
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Integrate Clad to PyTorch and compare the gradient execution times
          Description
          PyTorch is a popular machine learning framework that includes its own automatic differentiation engine, while Clad is a Clang plugin for automatic differentiation that performs source-to-source transformation to generate functions capable of computing derivatives at compile time.

          This project aims to integrate Clad-generated functions into PyTorch using its C++ API and expose them to a Python workflow. The goal is to compare the execution times of gradients computed by Clad with those computed by PyTorch’s native autograd system. Special attention will be given to CUDA-enabled gradient computations, as PyTorch also offers GPU acceleration capabilities.

          Expected Results
          Incorporate Clad’s API components (such as clad::array and clad::tape) into PyTorch using its C++ API
          Pass Clad-generated derivative functions to PyTorch and expose them to Python
          Perform benchmarks comparing the execution times and performance of Clad-derived gradients versus PyTorch’s autograd
          Automate the integration process
          Document thoroughly the integration process and the benchmark results and identify potential bottlenecks in Clad’s execution
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable automatic differentiation of C++ STL concurrency primitives in Clad
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. This project focuses on enabling automatic differentiation of codes that utilise C++ concurrency features such as std::thread, std::mutex, atomic operations and more. This will allow users to fully utilize their CPU resources.

          Expected Results
          Explore C++ concurrency primitives and prepare a report detailing the associated challenges involved and the features that can be feasibly supported within the given timeframe.
          Add concurrency primitives support in Clad’s forward-mode automatic differentiation.
          Add concurrency primitives support in Clad’s reverse-mode automatic differentiation.
          Add proper tests and documentation.
          Present the work at the relevant meetings and conferences.
          An example demonstrating the use of differentiation of codes utilizing parallelization primitives:

          #include <cmath>
          #include <iostream>
          #include <mutex>
          #include <numeric>
          #include <thread>
          #include <vector>
          #include "clad/Differentiator/Differentiator.h"

          using VectorD = std::vector<double>;
          using MatrixD = std::vector<VectorD>;

          std::mutex m;

          VectorD operator*(const VectorD &l, const VectorD &r) {
            VectorD v(l.size());
            for (std::size_t i = 0; i < l.size(); ++i)
              v[i] = l[i] * r[i];
            return v;
          }

          double dot(const VectorD &v1, const VectorD &v2) {
            VectorD v = v1 * v2;
            return std::accumulate(v.begin(), v.end(), 0.0);
          }

          double activation_fn(double z) { return 1 / (1 + std::exp(-z)); }

          double compute_loss(double y, double y_estimate) {
            return -(y * std::log(y_estimate) + (1 - y) * std::log(1 - y_estimate));
          }

          void compute_and_add_loss(VectorD x, double y, const VectorD &weights, double b,
                                    double &loss) {
            double z = dot(x, weights) + b;
            double y_estimate = activation_fn(z);
            std::lock_guard<std::mutex> guard(m);
            loss += compute_loss(y, y_estimate);
          }

          /// Compute total loss associated with a single neural neural-network.
          /// y_estimate = activation_fn(dot(X[i], weights) + b)
          /// Loss of a training data point = - (y_actual * std::log(y_estimate) + (1 - y_actual) * std::log(1 - y_estimate))
          /// total loss: summation of loss for all the data points
          double compute_total_loss(const MatrixD &X, const VectorD &Y,
                                    const VectorD &weights, double b) {
            double loss = 0;
            const std::size_t num_of_threads = std::thread::hardware_concurrency();
            std::vector<std::thread> threads(num_of_threads);
            int thread_id = 0;
            for (std::size_t i = 0; i < X.size(); ++i) {
              if (threads[thread_id].joinable())
                threads[thread_id].join();
              threads[thread_id] =
                  std::thread(compute_and_add_loss, std::cref(X[i]), Y[i],
                              std::cref(weights), b, std::ref(loss));
              thread_id = (thread_id + 1) % num_of_threads;
            }
            for (std::size_t i = 0; i < num_of_threads; ++i) {
              if (threads[i].joinable())
                threads[i].join();
            }

            return loss;
          }

          int main() {
            auto loss_grad = clad::gradient(compute_total_loss);
            // Fill the values as required!
            MatrixD X;
            VectorD Y;
            VectorD weights;
            double b;

            // derivatives
            // Zero the derivative variables and make them of the same dimension as the
            // corresponding primal values.
            MatrixD d_X;
            VectorD d_Y;
            VectorD d_weights;
            double d_b = 0;

            loss_grad.execute(X, Y, weights, b, &d_X, &d_Y, &d_weights, &d_b);

            std::cout << "dLossFn/dW[2]: " << d_weights[2] << "\n"; // Partial derivative of the loss function w.r.t weight[2]
            std::cout << "dLossFn/db: " << d_b << "\n"; // Partial derivative of the loss function w.r.t b
          }
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Support usage of Thrust API in Clad
          Description
          The rise of ML has shed light into the power of GPUs and researchers are looking for ways to incorporate them in their projects as a lightweight parallelization method. Consequently, General Purpose GPU programming is becoming a very popular way to speed up execution time.

          Clad is a clang plugin for automatic differentiation that performs source-to-source transformation and produces a function capable of computing the derivatives of a given function at compile time. This project aims to enhance Clad by adding support for Thrust, a parallel algorithms library designed for GPUs and other accelerators. By supporting Thrust, Clad will be able to differentiate algorithms that rely on Thrust’s parallel computing primitives, unlocking new possibilities for GPU-based machine learning, scientific computing, and numerical optimization.

          Expected Results
          Research and decide on the most valuable Thrust functions to support in Clad
          Create pushforward and pullback functions for these Thrust functions
          Write tests that cover the additions
          Include demos of using Clad on open source code examples that call Thrust functions
          Write documentation on which Thrust functions are supported in Clad
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~
          
          Extending the User Interface
          Description
          Constellation is a framework used for lab setups or small-scale experiments in HEP. One of its most important goals is that the framework should be easy to use for both scientists implementing new devices as well as experiment operators.

          Constellation features a Qt-based User Interfaces to control and monitor all devices in the experimental setup. The focus of this GSoC project is to add new user interfaces to Constellation and extend the current ones.

          Project Milestones
          Creating a new GUI to display monitoring data from devices using Qt Charts
          Modularization of UI elements into reusable Qt widgets
          Adding the monitoring widget to the existing GUI for device control
          Requirements
          Modern C++
          Knowledge of Qt is helpful but not required
          Practical experience with Unix and git
          Links
          Repository
          Documentation
          Mentors
          Stephan Lachnit - DESY
          Simon Spannagel - DESY
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          Constellation
          Participating Organizations
          DESY

          ~~~~~~~~~~

          Implement CppInterOp API exposing memory, ownership and thread safety information
          Description
          Incremental compilation pipelines process code chunk-by-chunk by building an ever-growing translation unit. Code is then lowered into the LLVM IR and subsequently run by the LLVM JIT. Such a pipeline allows creation of efficient interpreters. The interpreter enables interactive exploration and makes the C++ language more user friendly. The incremental compilation mode is used by the interactive C++ interpreter, Cling, initially developed to enable interactive high-energy physics analysis in a C++ environment.

          Clang and LLVM provide access to C++ from other programming languages, but currently only exposes the declared public interfaces of such C++ code even when it has parsed implementation details directly. Both the high-level and the low-level program representation has enough information to capture and expose more of such details to improve language interoperability. Examples include details of memory management, ownership transfer, thread safety, externalized side-effects, etc. For example, if memory is allocated and returned, the caller needs to take ownership; if a function is pure, it can be elided; if a call provides access to a data member, it can be reduced to an address lookup. The goal of this project is to develop API for CppInterOp which are capable of extracting and exposing such information AST or from JIT-ed code and use it in cppyy (Python-C++ language bindings) as an exemplar. If time permits, extend the work to persistify this information across translation units and use it on code compiled with Clang.

          Project Milestones
          Collect and categorize possible exposed interop information kinds
          Write one or more facilities to extract necessary implementation details
          Design a language-independent interface to expose this information
          Integrate the work in clang-repl and Cling
          Implement and demonstrate its use in cppyy as an exemplar
          Present the work at the relevant meetings and conferences.
          Requirements
          C++ programming
          Python programming
          Knowledge of Clang and LLVM
          Links
          Repo
          Mentors
          Aaron Jomy - CompRes
          Vassil Vassilev - CompRes
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          CppInterOp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Incorporate a Large Language Model to assist users
          Description
          The amount of data that is processed by individual scientists has grown hugely in the past decade. It is not unusual for a user to have data processed on tens of thousands of processors with these located at tens of different locations across the globe. The Ganga user interface was created to allow for the management of such large calculations. It helps the user to prepare the calculations, submitting the tasks to a resource broker, keeping track of which parts of the task that has been completed, and putting it all together in the end.

          As a scripting and command line interface, there will naturally be users that have problems with getting the syntax correct. To solve this, they will often spend time searching through mailing lists, FAQs and discussion fora or indeed just wait for another more advanced coder to debug their problem. The idea of this project is to integrate a Large Language Model (LLM) into the command prompt in Ganga. This should allow the user to describe in words what they would like to do and get an example that they can incorporate. It should also intercept exceptions thrown by the Ganga interface, help the user to understand them and propose solutions.

          We have an interface based on ollama that will build a RAG that contains extra information about Ganga that has not been available for the training of the underlying LLM.

          Task ideas
          Integrate the interaction with the LLM and RAG into Ganga.
          Integrate past input and output in the CLI to provide context for the CLI.
          Setup a server such that the LLM can run on a remote server requiring minimal installation by the user.
          Test which samples are most useful for adding to the RAG (mailing list discussions, manuals, instant messages)
          Develop continuous integration tests that ensures that LLM integration will keep working.
          Expected results
          For the scientific users of Ganga, this will speed up their development cycle as they will get a faster response to the usage queries that they have.

          As a student, you will gain experience with the challenges of large scale computing where some tasks of a large processing chain might take several days to process, have intermittent failures and have thousands of task processing in parallel. You will get experience with how LLMs can be integrated directly into projects to assist users in the use of the CLI and in understanding error messages.

          Evaluation Task
          Interested students please contact Ulrik (see contact below) to ask questions and for an evaluation task.

          Requirements
          Python programming (advanced), Linux command line experience (intermediate), use of git for code development and continuous integration testing (intermediate)

          Links
          Ganga
          Mentors
          Alex Richards - Imperial College
          Mark Smith - Imperial College
          Ulrik Egede - Monash University
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: May-November
          Corresponding Project
          Ganga
          Participating Organizations
          ImperialCollege
          MonashUniversity

          ~~~~~~~~~~

          Implement a deprecation system to keep code up to date
          Description
          The amount of data that is processed by individual scientists has grown hugely in the past decade. It is not unusual for a user to have data processed on tens of thousands of processors with these located at tens of different locations across the globe. The Ganga user interface was created to allow for the management of such large calculations. It helps the user to prepare the calculations, submitting the tasks to a resource broker, keeping track of which parts of the task that has been completed, and putting it all together in the end.

          As code that has developed over many years, there are part of the API that has become redundant. This means that for a period of time there will be both the old and now deprecated API as well as the new way of doing things. At the moment Ganga is missing a formal way of deprecating code. This means that warnings about using something deprecated are non-uniform and there is also very old code that has never been cleaned up.

          The idea in this project is to formalise the way that code can be declared deprecated and then use the continuous integration to ensure that the code eventually is deleted.

          Task ideas
          Have a well defined way of marking plugins, functions etc as deprecated with a warning about when they will be removed. Building on top of the python package deprecated might be an idea.
          Run tests in the testing framework that will alert developers to that certain parts of the code can now be removed.
          Apply in the testing framework a similar system that will identify when deprecated python features are used when moving to a new python version.
          Apply the deprecation system to parts of the code that is already deprecated.
          Expected results
          Obtain a cleaner code base where very old and since long deprecated code is no longer present. Provide the end user with consistent warnings about their use of deprecated code as well as when it will be removed.

          As a student, you will gain experience with the challenges of large scale computing where some tasks of a large processing chain might take several days to process, have intermittent failures and have thousands of task processing in parallel. You will get experience with working within a large code base that has gone through many developments.

          Evaluation Task
          Interested students please contact Ulrik (see contact below) to ask questions and for an evaluation task.

          Requirements
          Python programming (advanced), Linux command line experience (intermediate), use of git for code development and continuous integration testing (intermediate)

          Links
          Ganga
          Mentors
          Alex Richards - Imperial College
          Mark Smith - Imperial College
          Ulrik Egede - Monash University
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: May-November
          Corresponding Project
          Ganga
          Participating Organizations
          ImperialCollege
          MonashUniversity

          ~~~~~~~~~~

          Geant4-FastSim - Data Representation Optimisation for Generative Model-based Fast Calorimeter Shower Simulation
          Description
          High energy physics experiments such as those operated at the Large Hadron Collider (LHC) fundamentally rely on detailed and realistic simulations of particle interactions with the detector. The state-of-the-art Geant4 toolkit provides a means of conducting these simulations with Monte Carlo procedures. However, the simulation of particle showers in the calorimeter systems of collider detectors with such tools is a computationally intensive task. For this reason, alternative fast simulation approaches based on generative models have received significant attention, with these models now being deployed in production by current experiments at the LHC. In order to develop the next generation of fast simulation tools, approaches are being explored that would be able to handle larger data dimensionalities stemming from the higher granularity present in future detectors, while also being efficient enough to provide a sizable simulation speed-up for low energy showers.

          A shower representation which has the potential to meet these criteria is a point cloud, which can be constructed from the position, energy and time of hits in the calorimeter. Since Geant4 provides access to the (very numerous) individual physical interactions simulated in the calorimeter, it also provides a means to create a representation independent of the physical readout geometry of the detector. This project will explore different approaches to clustering these individual simulated hits into a point cloud, seeking to minimise the number of points while preserving key calorimetric observables.

          First Steps
          Gain a basic understanding of calorimeter shower simulation (G4FastSim)
          Try simulating some electromagnetic particle showers with the Key4hep framework (see test)
          Propose different approaches to clustering, with justification
          Project Milestones
          Survey different approaches to clustering
          Implement and experiment with the different methods
          Investigate the impact of varying the detector granularity on the performance of separate clustering algorithms
          If time allows, hadronic showers could also be investigated
          Expected Results
          A comparison of different approaches to clustering, with a performance evaluation in terms of the effect on calorimetric observables.
          An evaluation of the impact of varying the granularity of the detector readout on the performance of the clustering algorithm
          Requirements
          C++, Python
          Familiarity with PyTorch could be an advantage
          Evaluation Tasks and Timeline
          Find the test here. Please submit it by 9:00 CET 17th March 2025 along with a short proposal (2 pages max) describing how you would approach the problem. See submission instructions in the test doc. Please don’t forget to start the subject line with “GSoC’25 FastSim”.
          We will make the selections based on the test, short proposal and resume by 17:00 CET 24th March.
          Selected candidates will then write the full proposal and submit it according to the official GSoC timeline.
          Links
          G4FastSim
          CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation
          Mentors
          Peter McKeown - CERN
          Piyush Raikwar - CERN
          Anna Zaborowska - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Geant4
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Using ROOT in the field of genome sequencing
          Description
          The ROOT is a framework for data processing, born at CERN, at the heart of the research on high-energy physics. Every day, thousands of physicists use ROOT applications to analyze their data or to perform simulations. The ROOT software framework is foundational for the HEP ecosystem, providing capabilities such as IO, a C++ interpreter, GUI, and math libraries. It uses object-oriented concepts and build-time modules to layer between components. We believe additional layering formalisms will benefit ROOT and its users.

          ROOT has broader scientific uses than the field of high energy physics. Several studies have shown promising applications of the ROOT I/O system in the field of genome sequencing. This project is about extending the developed capability in GeneROOT and understanding better the requirements of the field.

          Expected results
          Reproduce the results based on previous comparisons against ROOT master
          Investigate and compare the latest compression strategies used by Samtools for conversions to BAM, with RAM(ROOT Alignment Maps).
          Explore ROOT’s RNTuple format to efficiently store RAM maps, in place of the previously used TTree.
          Investigate different ROOT file splitting techniques
          Produce a comparison report
          Requirements
          C++ and Python programming
          Familiarity with Git
          Knowledge of ROOT and/or the BAM file formats is a plus.
          Links
          Latest Presentation on GeneROOT
          ROOT
          GeneROOT
          Mentors
          Martin Vasilev - Uni Plovdiv
          Jonas Rembser - CERN
          Fons Rademakers - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-November
          Corresponding Project
          ROOT
          Participating Organizations
          CERN
          CompRes

          ~~~~~~~~~~

          Highly Granular Quantization for CICADA
          Description
          The CICADA (Calorimeter Image Convolutional Anomaly Detection Algorithm) project aims to provide an unbiased detection of new physics signatures in proton-proton collisions at the Large Hadron Collider’s Compact Muon Solenoid experiment (CMS). It detects anomalies in low-level trigger calorimeter information with a convolutional autoencoder, whose behaviour is transferred to a smaller model through knowledge distillation. Careful quantization of the deployed model allows it to meet the requirement of sub-500ns inference times on FPGAs. While CICADA currently employs Quantization Aware Training with different quantization schemes for each layer of the distilled model, a new gradient-based quantization optimization approach published in 2024 offers the possibility of optimizing quantization at the individual weight level. This project would explore implementing this highly granular quantization method to CICADA’s distilled model and evaluating its effects on both model performance and resource consumption on FPGAs. The work would involve implementing the new quantization approach, comparing it with the current implementation, and investigating the impact on both detection performance and hardware resource utilization while maintaining the strict timing requirements.

          Task ideas
          Transition CICADA’s quantization tool from QKeras to HGQ
          Optimize student model’s quantization with higher granularity
          Compare resulting model’s performance with legacy model
          Emulate deployment on FPGA w/ Vivado to evaluate resource consumption
          Expected results
          Extend existing training / quantization scripts to use HGQ in addition to QKeras
          A trained student model with highly granular quantization
          Estimates of that model’s performance and resource consumption on an FPGA
          Requirements
          Python, Tensorflow, Quantization

          Links
          CICADA (homepage)
          CICADA (code)
          HGQ (Paper)
          HGQ (code)
          Mentors
          Lino Gerlach - CERN
          Isobel Ojalvo - Princeton
          Jennifer Ngadiuba - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          CICADA
          Participating Organizations
          princeton

          ~~~~~~~~~~

          Intelligent Log Analysis for the HSF Conditions Database
          Description
          The nopayloaddb project works as an implementation of the Conditions Database reference for the HSF. It provides a RESTful API for managing payloads, global tags, payload types, and associated data.

          Our current system, composed of Nginx, Django, and database (link to helm chart), lacks a centralized logging solution making it difficult to effectively monitor and troubleshoot issues. This task will address this deficiency by implementing a centralized logging system aggregating logs from multiple components, and develop a machine learning model to perform intelligent log analysis. The model will identify unusual log entries indicative of software bugs, database bottlenecks, or other performance issues, allowing us to address problems before they escalate. Additionally, by analyzing system metrics, the model will provide insights for an optimal adjustment of parameters during periods of increased request rates.

          Steps
          Set up a centralized logging system
          Collect and structure logs from Nginx, Django, and the database
          Develop an ML model for log grouping and anomaly detection
          Implement Kubernetes-based database with replication
          Train an ML model to optimize Kubernetes parameters dynamically
          Expected Results
          A centralized logging system for improved monitoring and troubleshooting
          ML-powered anomaly detection
          ML-driven dynamic configuration for optimal performance
          Requirements
          Python and basic understanding of ML frameworks
          Kubernetes, basic understanding, k8s, Helm, Operators, OpenShift
          Django and Nginx, basic understanding of web frameworks and logging
          Database knowledge, PostgreSQL, database replication
          Links
          Django REST API: https://github.com/BNLNPPS/nopayloaddb
          Automized deployment with helm-chart: https://github.com/BNLNPPS/nopayloaddb-charts
          Mentors
          Ruslan Mashinistov - BNL
          John S. De Stefano Jr. - BNL
          Michel Hernandez Villanueva - BNL
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          HSFCondDB
          Participating Organizations
          BNL

          ~~~~~~~~~~

          Interactive Differential Debugging - Intelligent Auto-Stepping and Tab-Completion
          Description
          Differential debugging is a time-consuming task that is not well supported by existing tools. Existing state-of-the-art tools do not consider a baseline(working) version while debugging regressions in complex systems, often leading to manual efforts by developers to achieve an automatable task.

          The differential debugging technique analyzes a regressed system and identifies the cause of unexpected behaviors by comparing it to a previous version of the same system. The idd tool inspects two versions of the executable – a baseline and a regressed version. The interactive debugging session runs both executables side-by-side, allowing the users to inspect and compare various internal states.

          This project aims to implement intelligent stepping (debugging) and tab completions of commands. IDD should be able to execute until a stack frame or variable diverges between the two versions of the system, then drop to the debugger. This may be achieved by introducing new IDD-specific commands. IDD should be able to tab complete the underlying GDB/LLDB commands. The contributor is also expected to set up the necessary CI infrastructure to automate the testing process of IDD.

          Expected Results
          Enable stream capture
          Enable IDD-specific commands to execute until diverging stack or variable value.
          Enable tab completion of commands.
          Set up CI infrastructure to automate testing IDD.
          Present the work at the relevant meetings and conferences.
          Requirements
          Python & C/C++ programming
          Familiarity debugging with GDB/LLDB
          Links
          IDD Repository
          Mentors
          Vipul Cariappa - CompRes
          Martin Vasilev - University of Plovdiv
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          CppInterOp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Julia Interfaces to HepMC3
          Description
          In high-energy physics experiments at CERN it is necessary to simulate physics events in order to compare predicted observations with those that the LHC experiments actually observe. A key piece of the software chain used to do that is the HepMC3 event record library, which encodes the output from physics event generators in a standard way, so that they can be used by downstream detector simulation and analysis codes.

          There is now increasing interest in using Julia as a language for HEP software, as it combines the ease of programming in interactive languages, e.g., Python, with the speed of compiled language, such as C++. As part of building up the ecosystem of supporting packages for Julia in high-energy physics, developing interfaces to read, manipulated and write HepMC3 event records in Julia is the aim of this project.

          Task ideas
          This project would develop a wrapper library for HepMC3 allowing the HepMC3 data objects and methods, in C++, to be called from Julia.

          It would utilise the general underlying wrapper interfaces in CxxWrap and the automated wrapper code generator WrapIt! to allow for as easy and maintainable an interface as possible.

          A key outcome would be a set of unit tests and examples, based on the HepMC3 ones, demonstrating how to use the library and proving that the code is correct.

          Expected results and milestones
          Reading of HepMC3 event files
          Particularly the ASCII format will be targeted first
          Access to event data structures
          Access to particle properties
          Navigation of the event and the vertices between parent and child particles
          Access to run information
          Update of HepMC3 data structures
          Creation of new HepMC3 events
          Re-serialisation of these events to file
          Initially ASCII
          Documentation and examples on how to use the Julia interfaces
          HepMC3.jl package registered in the Julia general registry
          Extension of serialisation to ROOT format (stretch goal)
          Requirements
          Programming experience in C++
          Prior experience in Julia (very advantageous)
          A background understanding of high-energy physics (advantageous)
          Evaluation Exercise
          TBD

          Links
          Julia Programming Language
          JuliaHEP HSF Group
          HepMC3 Repository
          CxxWrap
          WrapIt!
          Mentors
          Graeme Stewart - CERN
          Mateusz Fila - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-July- August
          Corresponding Project
          JuliaHEP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          MCnet/MCviz - graph and 3D-viewer tools for simulated particle collisions
          Description
          Simulations are key to particle-physics research: many modern theoretical models have such complex consequences that we test theory not by comparing measurements of particle collisions to predicted functional forms, but by generating simulated collision-events from the theory and analyzing them identically to the real ones from the particle collider.

          This means that event generators are incredibly important to particle physics, as the most-used link between experiment and theory, and as a crucial data format for exchange of ideas. They are also an excellent way to introduce new researchers and the public to particle-physics concepts. However, the toolset for MC event manipulation and visualisation is less powerful and coherent than it should be, and this project seeks to improve that situation!

          Task ideas
          This project will pick up old ideas and code for MC-event visualisation – both of the interaction graph that illustrates the internal theory computation, and the external appearance of the resulting collision decay-products – and produce a new set of tools useful both to physicists and for public outreach.

          Expected results and milestones
          Extend the mcgraph tool to be usable with both the HepMC and LHE MC-event formats.
          Refactor mcgraph into a library capable of rendering to a web browser in a server app.
          Interface the Phoenix event-viewer library to display 3D events (with and without a dummy detector model) to a web browsers.
          Display interactive particle information and jet clustering in graph and 3D view interfaces.
          Requirements
          Command-line tools
          Python
          Web technologies
          Gitlab CI
          git
          Links
          Phoenix event view library
          Old MCview web-based MC event viewer
          MC event-graph viewer
          Old MCviz event-graph viewer
          HepMC3 event format
          LHE event format
          Mentors
          Andy Buckley - CERN
          Chris Gutschow - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          MCnet
          Participating Organizations
          UofGlasgow


          ~~~~~~~~~~

          MCnet/OpenData - tools and exercises for open-data exploration with MC simulations
          Description
          CERN’s experiments are committed to publishing their data in a form that is accessible to all, both for research purposes and for education. For example, the ATLAS experiment provides Jupyter notebook exercises based on live-analysing reduced forms of the real collider data.

          But particle-physics researchers also use simulations of data as a crucial tool for testing theories and for understanding the background processes that new physics effects have to be isolated from. For this we use Monte Carlo (MC) event-generator codes, which are statistical implementations of the fundamental physics theory that sample real-looking events from the predicted particle types and kinematics. These are not yet represented in open-data exercises.

          Task ideas
          In this project we will develop new tools and exercises for extending open-data analysis resources to include MC event simulations. It will both reduce the entry barriers to outreach with open data and enable more engaging exercises with hypothetical new-physics models.

          Expected results and milestones
          Develop a library of wrapper functions to make open-data analysis more approachable for non-experts.
          Create functions and datasets for loading and analysing MC event samples through Jupyter.
          Develop a new Jupyter+Binder worksheet for outreach-oriented open-data MC analysis.
          Requirements
          Python
          Jupyter
          Binder
          Gitlab CI
          git
          Links
          ATLAS open data
          Example open-data analysis notebook
          Jupyter
          Binder
          Mentors
          Andy Buckley - CERN
          Chris Gutschow - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-October
          Corresponding Project
          MCnet
          Participating Organizations
          UofGlasgow


          ~~~~~~~~~~

          Integrating Support for Google XLS in HLS4ML
          Description
          Google XLS (Accelerated Hardware Synthesis) is an advanced open-source framework for high-level hardware design, offering flexible and efficient synthesis for FPGA and ASIC applications. By integrating XLS into HLS4ML, a framework for translating machine learning models into FPGA-friendly code, we can leverage XLS’s optimizing compiler and domain-specific language to improve resource efficiency, performance, and portability. This integration will enable seamless generation of highly optimized hardware implementations for ML models while maintaining the ease of use that HLS4ML provides.

          HLS4ML currently supports traditional HLS tools like Vivado HLS and Intel HLS, but adding XLS can bring further benefits such as better compilation times, improved hardware efficiency, and wider vendor compatibility. This project will focus on developing an interface between HLS4ML and XLS, allowing ML models to be translated into XLS IR and synthesized efficiently.

          Task Ideas
          Develop a backend in HLS4ML that translates neural network layers into XLS Intermediate Representation (IR).
          Implement the key ML operations (e.g., matrix multiplications, activations, and pooling) via XLS’s DSLX language and map them to HLS4ML operations.
          Benchmark and compare performance, resource utilization, and synthesis results against existing HLS4ML backends.
          Extend HLS4ML’s configuration options to allow selection of XLS as a backend, ensuring ease of integration.
          Expected Results
          A prototype of a backend in HLS4ML supporting XLS-based synthesis.
          Conversion scripts to map ML operations to XLS IR.
          Performance evaluation of XLS and existing HLS backends.
          Documentation and tutorials for using XLS with HLS4ML.
          Requirements
          Proficiency in Python and C++.
          Knowledge of hardware and compiler design.
          Basic familiarity with neural networks.
          Familiarity with version control systems like Git/GitHub.
          Links
          hls4ml documentation
          hls4ml Repository
          Google XLS documentation
          Google XLS repository
          Mentors
          Vladimir Loncar - CERN
          Dimitrios Danopoulos - CERN
          Additional Information
          Difficulty level (low / medium / high): medium/high
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          Optimizing Model Splitting in hls4ml for Efficient Multi-Graph Inference
          Description
          hls4ml is an open-source tool that enables the deployment of machine learning (ML) models on FPGAs using High-Level Synthesis (HLS). It automatically converts pre-trained models from popular deep learning frameworks (e.g., Keras, PyTorch, and ONNX) into optimized firmware for FPGA-based inference.

          Traditionally, the entire ML model is synthesized as a monolithic graph, which can lead to long synthesis times and complicated debugging, especially for large model topologies. Splitting the model graph at specified layers into independent subgraphs allows for parallel synthesis and step-wise optimization. However, finding the ‘optimal’ splitting points and optimizing FIFO buffers in between the subgraphs remains a challenge, especially when dealing with dynamic streaming architectures.

          This project aims to investigate optimal splitting strategies for complex ML models in hls4ml, focusing on efficient FIFO depth optimization across multi-graph designs. The goal is to develop methodologies that can be integrated into hls4ml to enable automated and optimal graph splitting for improved performance.

          Task ideas
          The contributor will start by familiarizing themselves with hls4ml and building ML models using multi-graph designs. They will implement profiling techniques (e.g., VCD logging) to measure FIFO occupancy and backpressure in order to develop a FIFO optimization strategy for multi-graph designs. They will also investigate multi-objective optimization algorithms to determine optimal splitting points based on subgraph resource usage or dataflow patterns. Finally, they will integrate these methodologies with hls4ml and run benchmarks to validate improvements in latency, resource utilization, etc.

          Expected results and milestones
          Familiarization with hls4ml: Understand the hls4ml workflow, including synthesis, and simulation.
          Research and Evaluation: Explore FIFO profiling and optimization strategies along with algorithms to partition the model graph given specific optimization objectives.
          Validation: Benchmark against monolithic implementations and compare differences in latency and resource utilization.
          Requirements
          Proficiency with computer architecture, FPGA design and simulation tools (e.g., Vivado)
          Experience with Python
          Understanding of ML concepts is beneficial.
          Familiarity with version control systems like Git/GitHub.
          Links
          hls4ml documentation
          hls4ml Repository
          Vivado Design Implementation
          Mentors
          Vladimir Loncar - CERN
          Dimitrios Danopoulos - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          RNTuple in JSROOT
          Description
          RNTuple is the next-generation data format for high-energy physics (HEP) data collected from the LHC. It is part of ROOT, a cornerstone software package for the storage, visualization and analysis of scientific data, widely used in the scientific community and particularly in HEP. ROOT is a C++ and Python framework, but it recently became available in the browsers as well through a Javascript implementation of some of its parts: JSROOT. Since RNTuple is still in the experimental phase, it currently lacks a JSROOT interface and its contents cannot be visualized in the browser, a common and desirable property of many ROOT objects. The goal of this project is filling this gap by making JSROOT able to read and display data stored inside an RNTuple.

          Task ideas
          In this project, the student will learn the internals of the RNTuple binary format and use this knowledge to implement a Javascript interface to expose RNTuple to JSROOT.

          Expected results and milestones
          Familiarize with the JSROOT framework, understanding how to integrate new components into it;
          read and implement (a subset of) the RNTuple binary format specifications, in Javascript; this will concretely mean implementing the deserialization code from a binary blob to a RNTuple object that may be used by JSROOT;
          enable the visualization of an RNTuple’s fields in the browser, leveraging the existing framework in JSROOT.
          Requirements
          Knowledge of Javascript / ES6
          Basic knowledge of “low-level” programming (primitive types binary layouts, bit-level manipulations, reinterpreting bytes as different types, …)
          Experience with git / github
          (Bonus): familiarity with any binary format
          Links
          ROOT Project homepage
          ROOT Project repository
          JSROOT homepage
          JSROOT repository
          Introduction to RNTuple
          RNTuple architecture overview
          RNTuple Binary Specification
          Mentors
          Serguei Linev - CERN
          Giacomo Parolini - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          ROOT
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Rucio WebUI Revamp
          Description
          Rucio is an open-source software framework that provides functionality to scientific collaborations to organize, manage, monitor, and access their distributed data and dataflows across heterogeneous infrastructures. Originally developed to meet the requirements of the high-energy physics experiment ATLAS, Rucio has been continuously enhanced to support diverse scientific communities. Since 2016, Rucio has orchestrated multiple exabytes of data access and data transfers globally.

          The Rucio WebUI is a Next.js application utilized by various users within collaborating communities to access, monitor, and manage their distributed data. Key features of the Rucio WebUI include:

          SDK for Streaming: Facilitates seamless data streaming from the Rucio server to page components, ensuring a responsive user interface.
          Typed in TypeScript with Generics: Strict typing ensures code integrity and enhances development efficiency.
          Accessibility and Responsiveness: Designed with accessibility and responsiveness in mind, ensuring usability across various devices.
          Testing and Stability: Extensive testing ensures robustness and reliability in all components.
          Feature Toggles: Dynamic feature toggles provide flexibility in enabling or disabling specific functionalities as needed.
          Component Library: Utilizes Storybook and TailwindCSS to enhance development speed and consistency.
          Tasks
          Upgrade to Next.js 15, React 19, TailwindCSS 4.x:
          Migrate the existing codebase to Next.js 15 to leverage the latest features and performance improvements.
          Utilize Server Side Rendering and React Query in Client Side Components to enhance data-fetching capabilities.
          Migrate tailwind.config.js to new CSS based configuration for TailwindCSS 4.x.
          Enhance User Experience for Site Administrators and Operators:
          Currently the WebUI focuses on List/Get views with the exception of allowing users to Create Rules. Add features to Create/Edit resources for site administrators and operational experts.
          Investigate legacy views in the previous Flask application and migrate them to the new WebUI.
          Redesign these views to be more user-friendly, incorporating feedback from site administrators and operators.
          Migrate Authentication to NextAuth (Auth.js):
          Transition existing x509 and user/password authentication mechanisms to NextAuth.
          Ensure compatibility with various authentication flows, including OAuth and OpenID Connect.
          Develop an RBAC system to ensure users have access only to functionalities relevant to their roles, enhancing security and usability.
          Transition to a Monorepo Structure:
          Migrate the Rucio WebUI to a monorepo structure to improve code organization and facilitate the sharing of common components across different projects.
          Requirements
          Mandatory:

          Proficiency in React.js and Next.js
          Experience with TailwindCSS
          Strong knowledge of JavaScript (ECMAScript 6) and TypeScript
          Familiarity with Python 3 and Flask
          Proficiency with Linux, Git, and Docker
          Good to Have:

          Understanding of NX Monorepos
          Experience with AGGrid Data Tables
          Experience with GitHub Actions
          Knowledge of HTTP REST APIs
          Familiarity with OpenID Connect and x509 protocols
          Expected Results
          By the end of GSoC 2025, we expect to have a revamped Rucio WebUI that:

          Is upgraded to Next.js 15 with integrated React Query.
          Utilizes both client and server-side components as per React 19’s stable features.
          Supports TailwindCSS 4.0 for a modern design system.
          Offers enhanced user experiences tailored for site administrators and operators.
          Employs NextAuth for streamlined authentication processes.
          Implements a robust RBAC system.
          Adopts a monorepo structure for improved code organization and component sharing.
          Links
          Rucio GitHub Repository
          Rucio UI Presentation
          Rucio Documentation
          Rucio System Overview Journal Article (Springer)
          Rucio Operational Experience Article (IEEE Computer Society)
          Mentors
          Mayank Sharma - University of Michigan, Ann Arbor
          Martin Barisits - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-November
          Corresponding Project
          Rucio
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Background Enrichment augmented Anomaly Detection (BEAD) for new physics searches at LHC
          

          Short description of the project
          A long-standing mystery of fundamental physics is the existence of dark matter (DM), a type of matter that has little interaction with ordinary matter but is supported by various astrophysical and cosmological observations and is six times more abundant than ordinary matter in the universe. Several Large Hadron Collider (LHC) experiments are conducting searches aimed at detecting dark matter. Unsupervised and semi-supervised learning outlier detection techniques are advantageous to these searches, for casting a wide net on a variety of possibilities for how dark matter manifests, as they impose minimal constraints from specific physics model details, but rather learn to separate characteristics of rare signals starting from the knowledge of the background they’ve been trained on. Developing innovative search techniques for probing dark matter signatures is crucial for broadening the DM search program at the LHC, and BEAD is a Python package that uses deep learning based methods for anomaly detection in HEP data for such new physics searches. BEAD has been designed with modularity in mind, to enable usage of various unsupervised latent variable models for any task.

          BEAD has five main running modes:

          Data handling: Deals with handling file types, conversions between them and pre-processing the data to feed as inputs to the DL models.

          Training: Trains a model to learn implicit representations of the background data that may come from multiple sources(/generators) to get a single, encriched latent representation of it.

          Inference: Using a model trained on an enriched background, the user can feed in samples where to detect anomalies in.

          Plotting: After running Inference, or Training, one can generate plots. These include performance plots as well as different visualizations of the learned data.

          Diagnostics: Enabling this mode allows running profilers that measure a host of metrics connected to the usage of the compute node to help optimization of the code (using CPU-GPU metrics).

          The package is under active development. The student in this project will work on the machine learning models available in BEAD, and implementing new models to perform anomaly detection, initially on simulated data.

          Task ideas
          Possible projects include:

          New auto-encoder models could be developed, better identifying correlations between data objects in a given particle physics dataset entry (containing event level and/or physics object level information). New models could also improve performance on live / unseen data. These could include transformer, GNN, probabilistic and other tiypes of networks.
          Existing models could be tested on different datasets, potentially identifying distinct latent spaces populated by the different LHC physics processes, that can enable improved anomaly detection.
          Ideas from the student working on this project are also welcome.

          Expected results
          An improved performance of selected models, with documentation and figures of merit that may include:

          Plots made in matplotlib that demonstrate the performance of the new models compared to the old
          Documentation of the design choices made for the improved models
          Documented evaluation of a physics analysis on data before and after compression
          Requirements
          Python
          Linux environment
          ML / unsupervised algorithms key concepts
          PyTorch

          Desired skills: transformers and/or graph neural networks, particle physics theory and experiments, particle physics simulations
          Links
          Paper on unsupervised ML algorithms using HEP datasets
          Review of LHC searches using unsupervised learning
          BEAD GitHub repository (WIP)
          ROOT
          Jupyter
          PyTorch
          Mentors
          Pratik Jawahar - CERN
          Sukanya Sinha - CERN
          Caterina Doglioni - Backup Mentor - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          BEAD
          Participating Organizations
          SMARTHEP
          UManchester

          ~~~~~~~~~~

          Estimating the energy cost of ML scientific software
          Description
          At a time where “energy crisis” is something that we hear daily, we can’t help but wonder whether our research software can be made more sustainable, and more efficient as a byproduct. In particular, this question arises for ML scientific software used in high-throughput scientific computing, where large datasets composed of many similar chunks are analysed with similar operations on each chunk of data. Moreover, CPU/GPU-efficient software algorithms are crucial for the real-time data selection (trigger) systems in LHC experiments, as the initial data analysis necessary to select interesting collision events is executed on a computing farm located at CERN that has finite CPU resources.

          The questions we want to start answering in this work are:

          what is the trade off between performance of a ML algorithm and its energetic efficiency?
          can small efficiency improvements in ML algorithms running on Large Hadron Collider data have a sizable energetic impact?
          how do these energy efficiency improvements vary when using different computing architectures (1) and/or job submission systems (2)?
          Task ideas
          The students in this project will use metrics from the Green Software Foundation and from other selected resources to estimate the energy efficiency of machine learning software from LHC experiments (namely, top tagging using ATLAS Open data) and from machine learning algorithms for data compression (there is another GSoC project developing this code, called Baler). This work will build on previous GSoC / Master’s thesis work, and will expand these results for GPU architectures. If time allows, the student will then have the chance to make small changes to the code to make it more efficient, and evaluate possible savings.

          Expected results and milestones
          Understand and summarise the metrics for software energy consumption, focusing on computing resources at CERN;
          Become familiar with running and debugging the selected software frameworks and algorithms;
          Set up tests and visualisation for applying metrics to the selected software
          Run tests and visualise results (preferably using a Jupyter notebook)
          Vary platforms and job submission systems
          Identify possible improvements, apply them, and run tests again
          Requirements
          Python
          git
          Jupyter notebooks
          PyTorch or equivalent ML toolkit
          Desirable: code profiling experience
          Links
          (1) Green Software Foundation course
          (2) Code by the previous GSoC student
          Mentors
          Caterina Doglioni - CERN
          Tobias Fitschen - Backup Mentor - CERN
          James Smith - Backup Mentor - University of Manchester
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October (with 2-3 weeks mentor vacation where student will work independently with minimal guidance)
          Corresponding Project
          SMARTHEP
          Participating Organizations
          UManchester
          CERN

          ~~~~~~~~~~

          Sustainable Quantum Computing algorithms for particle physics reconstruction
          Description
          Reconstructing the trajectories of charged particles as they traverse several detector layers is a key ingredient for event reconstruction at any LHC experiment. The limited bandwidth available, together with the high rate of tracks per second, makes this problem exceptionally challenging from the computational perspective. With this in mind, Quantum Computing is being explored as a new technology for future detectors, where larger datasets will further complicate this task. Furthermore, when choosing such alternative sustainability will play a crucial role and needs to be studied in detail. This project will consist in the implementation of both Quantum and Classical Machine Learning algorithms for track reconstruction, and using open-source, realistic event simulations to benchmark them from both a physics performance and an energy consumption perspective.

          First steps
          Basic understanding of track reconstruction at LHC using ACTS and/or Allen framework.
          Familiarizing her/himself with trackML simulation datasets https://www.kaggle.com/competitions/trackml-particle-identification/data?select=train_sample.zip.
          Learning how to use the quantum simulator for QML algorithms https://pennylane.ai/.
          Milestones
          Choosing a ML algorithm (or part of) in quantum computing and its classical counterpart for track reconstruction.
          Mapping of track reconstruction problem to Ising-like Hamiltonian.
          Prototype implementation of classical and quantum track reconstruction using trackML simulation inputs.
          Expected results
          Benchmarking physics output and energy consumption of the classical and quantum algorithm.
          Requirements
          CUDA, python, C++
          Evaluation Tasks and Timeline
          To be completed
          Corresponding Project
          QuantumForTracking
          Participating Organizations
          CERN

          ~~~~~~~~~~

          TMVA SOFIE - GPU Support for Machine Learning Inference
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          SOFIE currently supports various Machine Learning operators defined by the ONNX standards, as well as a Graph Neural Network (GNN) implementation. It supports the parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE continues to evolve, there’s a need to enable inference on GPUs. This project aims to explore different GPU stacks (such as CUDA, ROCm, ALPAKA) and implement GPU-based inference functionalities in SOFIE. There is already a SYCL implementation for SOFIE, developed in 2023, which can serve as a reference for future development.

          Task ideas
          In this project, the contributor will gain experience with GPU programming and its role in Machine Learning inference. They will start by understanding SOFIE and running inference on CPUs. After researching GPU stacks and methods of their integration with SOFIE, the contributor will implement GPU support for inference, ensuring the code is efficient and well-integrated with GPU technologies.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understanding the SOFIE architecture, working with its internals, and running inference on CPUs.
          Research and Evaluation: Analyzing various GPU stacks (CUDA, ROCm, ALPAKA, etc.) and determining their alignment with SOFIE.
          Implementation of GPU Inference: Developing functionalities for GPU-based inference in SOFIE.
          [Optional] Benchmarking: Evaluating the performance of the new GPU functionality by benchmarking memory usage, execution time, and comparing results with other frameworks (such as TensorFlow or PyTorch).
          Requirements
          Proficiency in C++ and Python.
          Knowledge of GPU programming (e.g., CUDA).
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          Implementation of SOFIE-SYCL
          Accelerating Machine Learning Inference on GPUs with SYCL
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          TMVA SOFIE - HLS4ML Integration for Machine Learning Inference
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          Currently, SOFIE supports various machine learning operators defined by ONNX standards, as well as a Graph Neural Network implementation. It supports parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE evolves, there is a growing need for inference capabilities on models trained across a variety of frameworks. This project will focus on integrating hls4ml in SOFIE, thereby enabling generation of C++ inference functions on models parsed by hls4ml.

          Task ideas
          In this project, the contributor will gain experience with C++ and Python programming, hls4ml, and their role in machine learning inference. The contributor will start by familiarizing themselves with SOFIE and running inference on CPUs. After researching the possibilities for integration with hls4ml, they will implement functionalities that ensure efficient inference of ML models parsed by hls4ml, which were previously trained in external frameworks like TensorFlow and PyTorch.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understanding the SOFIE architecture, working with its internals, and running inference on CPUs.
          Research and Evaluation: Exploring hls4ml, its support for Keras and PyTorch, and possible integration with SOFIE.
          Integration with hls4ml: Developing functionalities for running inference on models parsed by hls4ml.
          Requirements
          Proficiency in C++ and Python.
          Knowledge of hls4ml
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          hls4ml documentation
          hls4ml Repository
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          TMVA SOFIE - Enhancing Keras Parser and JAX/FLAX Integration
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          SOFIE currently supports various Machine Learning operators defined by the ONNX standards, as well as a Graph Neural Network (GNN) implementation. It supports the parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE continues to evolve, this project aims to:

          Enhance the Keras parser to support models trained in the latest TensorFlow v2.18.0, which introduces NumPy 2.0 compatibility.
          Integrate JAX/FLAX support, enabling SOFIE to generate C++ inference functions for models developed using JAX/FLAX.
          Task ideas
          In this project, the contributor will gain experience with C++ and Python programming, TensorFlow/Keras and its storage formats for trained machine learning models, and JAX/FLAX for accelerated machine learning. They will begin by familiarizing themselves with SOFIE and its Keras parser. After researching the changes required to support the latest TensorFlow version, they will implement functionalities to ensure the successful generation of inference code for the latest Keras models. In the next phase, they will explore the JAX/FLAX library and investigate its potential integration with SOFIE.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understand the SOFIE architecture, run inference using the existing Keras parser, and analyze the current parser’s capabilities.
          Researching latest TensorFlow/Keras: Investigate the latest TensorFlow/Keras developments and assess their alignment with SOFIE.
          Improving the Keras Parser: Implement parser enhancements to support the latest TensorFlow version and validate inference results.
          JAX/FLAX Integration: Design and develop a parsing mechanism for JAX/FLAX models, ensuring compatibility with SOFIE’s IR and further generation of inference code.
          Requirements
          Proficiency in C++ and Python.
          Knowledge of TensorFlow/Keras and JAX/FLAX.
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          Keras: The high-level API for TensorFlow
          JAX Documentation
          FLAX Documentation
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Implementing Debugging Support
          Description
          xeus-cpp is an interactive execution environment for C++ in Jupyter notebooks, built on the Clang-Repl C++ interpreter, provided by CppInterOp. While xeus-cpp enables a seamless workflow for running C++ code interactively, the lack of an integrated debugging experience remains a gap, especially when dealing with code that is dynamically compiled and executed through LLVM’s JIT(Just-In-Time) infrastructure.

          Jupyter’s debugging system follows the Debug Adapter Protocol (DAP), enabling seamless integration of debuggers into interactive kernels. Existing Jupyter kernels, such as the IPython & the xeus-python kernel, have successfully implemented debugging workflows that support breakpoints, variable inspection, and execution control, even in dynamically executed environments. These implementations address challenges such as symbol resolution and source mapping for dynamically generated code, ensuring that debugging within Jupyter remains intuitive and user-friendly.

          However, debugging C++ inside an interactive environment presents unique challenges, particularly due to Clang-Repl’s use of LLVM’s ORC JIT to compile and execute code dynamically. To integrate debugging into xeus-cpp, the project will explore existing solutions for DAP implementations like lldb_dap and debuggers like lldb that can interface with Jupyter while effectively supporting the execution model of Clang-Repl.

          Project Milestones
          Seamless debugging integration, establishing reliable interactions between xeus-cpp, a Debug Adapter Protocol (DAP) implementation, and a debugger.
          Implement a testing framework through xeus-zmq to thoroughly test the debugger. This can be inspired by an existing implementation in xeus-python.
          Present the work at the relevant meetings and conferences.
          Requirements
          C/C++
          Basic understanding of the Debug Adapter Protocol
          Basic understanding of the stack used by xeus-cpp: xeus, cppinterop, clang-repl
          Research on different DAP implementations like lldb_dap and debuggers like lldb/gdb that can be utilized for the project.
          Links
          Repo
          Debug Adaptor Protocol
          Debugging support through Jupyter:
          https://jupyterlab.readthedocs.io/en/stable/user/debugger.html
          https://jupyter-client.readthedocs.io/en/latest/messaging.html#debug-request
          Mentors
          Anutosh Bhat - QuantStack
          Johan Mabille - QuantStack
          Vipul Cariappa - CompRes
          Aaron Jomy - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Xeus-Cpp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable GPU support and Python Interoperability via a Plugin System
          Description
          Xeus-Cpp integrates Clang-Repl with the xeus protocol via CppInterOp, providing a powerful platform for C++ development within Jupyter Notebooks.

          This project aims to introduce a plugin system for magic commands (cell, line, etc.), enabling a more modular and maintainable approach to extend Xeus-Cpp. Traditionally, magic commands introduce additional code and dependencies directly into the Xeus-Cpp kernel, increasing its complexity and maintenance burden. By offloading this functionality to a dedicated plugin library, we can keep the core kernel minimal while ensuring extensibility. This approach allows new magic commands to be developed, packaged, and deployed independently—eliminating the need to rebuild and release Xeus-Cpp for each new addition. Initial groundwork has already been laid with the Xplugin library, and this project will build upon that foundation. The goal is to clearly define magic command compatibility across different platforms while ensuring seamless integration. A key objective is to reimplement existing features, such as the LLM cell magic and the in-development Python magic, as plugins. This will not only improve modularity within Xeus-Cpp but also enable these features to be used in other Jupyter kernels.

          As an extended goal, we aim to develop a new plugin for GPU execution, leveraging CUDA or OpenMP to support high-performance computing workflows within Jupyter.

          Project Milestones
          Move the currently implemented magics and reframe using xplugin
          Complete the on-going work on the Python interoperability magic
          Implement a test suite for the plugins
          Extended: To be able to execute on GPU using CUDA or OpenMP
          Optional: Extend the magics for the wasm use case (xeus-cpp-lite)
          Present the work at the relevant meetings and conferences
          Requirements
          Python
          C/C++
          GPU programming; CUDA/OpenMP
          Links
          Repo
          Related Issues:
          https://github.com/compiler-research/xeus-cpp/issues/4
          https://github.com/compiler-research/xeus-cpp/issues/140
          Mentors
          Anutosh Bhat - QuantStack
          Johan Mabille - QuantStack
          Vipul Cariappa - CompRes
          Aaron Jomy - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Xeus-Cpp
          Participating Organizations
          CompRes

          
    totalCharacters_of_ideas_content_parent: 107186
    totalwords_of_ideas_content_parent: 25550
    totalTokenCount_of_ideas_content_parent: 20259
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cern-hsf/
    idea_list_url: https://hepsoftwarefoundation.org/gsoc/2025/summary.html


  - organization_id: 17
    organization_name: CGAL Project
    no_of_ideas: 11
    ideas_content: |
          Enhancing the 2D Regularized Boolean Operation Demo
          Mentor: Efi Fogel

          Project description: The new demonstration program of the "2D Regularized Boolean Operations" package demonstrates various operations on polygons, such as, union, intersection, and Minkowski sum. It also demonstrates the application of several operations in a pipeline fashion. The demo has not been published yet; it requires a few enhancements, such as the support of Boolean operations on general polygons bounded by non-linear curves.

          Required Skills: Qt6, geometry, code development tools (e.g., git), and C++14 proficiency

          Contact: efifogel@gmail.com

          Duration: 350h

          ~~~~~~~~~~

          Tetrahedral Isotropic Remeshing Parallelization
          Mentor: Jane Tournois

          Project description:

          The goal of this project is to parallelize the code of the Tetrahedral Remeshing algorithm available in CGAL. This multi-material tetrahedral remeshing algorithm [2] is based on local and atomic operations such as edge collapse, edge split and edge flip, that could be performed in parallel to improve the performances of the code. The 3D Triangulations [3] and Tetrahedral Mesh Generation package [4] provide a framework to implement mesh operations concurrently. The same framework will be used to parallelize the remeshing algorithm, with the Intel TBB library [5].

          Resources:

          [1] CGAL Tetrahedral Remeshing package
          [2] The original publication Multi-Material Adaptive Volume Remesher
          [3] CGAL 3D Triangulations
          [4] CGAL Tetrahedral Mesh Generation package
          [5] Intel Threading Building Blocks
          Required Skills: C++17, Mesh Processing, Computational Geometry, Parallelism with TBB

          Contact: jane.tournois@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          New Mesh Subdivision Methods
          Mentor: Mael Rouxel-Labbé

          Project description:

          Subdivision methods are efficient techniques to produce smooth surfaces from polygonal meshes. Within CGAL [1], a handful of classic subdivision techniques already exist (CatmullClark subdivision, Loop subdivision DooSabin subdivision, Sqrt3 subdivision). The goal of this project is two-fold: (a) implement newer subdivision techniques -- such as Interpolatory SQRT(3) Subdivision [2], which builds upon an algorithm that is already found in CGAL -- and compare them to our existing algorithms (b) Investigate the use of these newer techniques as a preprocessing step in some of CGAL's newer remeshing techniques (such as adaptive remeshing).

          Resources:

          [1] CGAL Subdivision package
          [2] Interpolatory SQRT(3) Subdivision
          [3] Gaussian-Product Subdivision Surfaces
          [4] CGAL's upcoming adaptive remeshing algorithms
          Required Skills: C++17, Mesh Processing

          Contact: mael.rouxel.labbe@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Enhanced Dual Contouring
          Mentor: Mael Rouxel-Labbé, Pierre Alliez

          Project description:

          A previous GSoC launched the process of adding classic contouring methods to CGAL: Marching Cubes and Dual Contouring. This package is about to be finalized and will be integrated soon into CGAL (https://github.com/CGAL/cgal/pull/6849). Many enhancements exist for the Dual Contouring method to improve its robustness: placement of the dual point, improved conditioning of the SVD matrices, or on-the-fly refinement of the underlying grid [1]. Another aspect is speed, as a grid structure is well adapted to GPU computation.

          The project will first focus on manifold contouring methods and robustness in standard C++. If there is time and the candidate has the required skills, we can also explore runtime aspects and the conversion to a GPU implementation. If there is time and the candidate does not have the required skills, we shall explore the implementation of other contouring methods such as Dual Marching Cubes [2].

          Resources:

          [1] Manifold Dual Contouring
          [2] Dual Marching Cubes
          Feature-Sensitive Subdivision and Isosurface Reconstruction
          Required Skills: C++17, Dual Contouring, linear algebra / quadric error metrics, possibly GPU algorithms

          Contact: mael.rouxel.labbe@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Topological Filtering of Features in Triangle Meshes
          Mentor: Sebastien Loriot

          Project description:

          Remeshing algorithms in CGAL requires the proper extraction of sharp features so that they can be represented in the output mesh (like here for example). Classical method to detect sharp features are based on collecting edges with sharp dihedral surface angles. However, depending on the quality of the input mesh, some noisy edges might be detected, or some edges might be detected. To workaround these issues, it might be interesting to rely on tools from Topological Data Analysis, like for example persistence. Indeed, extra data or missing data are all related to a notion of scale at which the problem is looked at. The goal of this project is to implement such a strategy for provide curated feature edge graph to the meshing algorithm of CGAL. If time allows, extension to detection of significant handles might also be looked at.

          Resources:

          A Practical Solver for Scalar Data Topological Simplification
          To cut or to fill: a global optimization approach to topological simplification
          Topological Simplification of Nested Shapes
          Gudhi library
          Required Skills: C++17, Mesh Processing, Topological Data Analysis

          Contact: sebastien.loriot@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Improving ARAP in CGAL
          Mentor: Andreas Fabri

          Project description:

          As-Rigid-As-Possible (ARAP) surface modeling is one of the most well known approach for deformation of surfaces. It has been implemented in CGAL, within the Surface Mesh Deformation package (https://doc.cgal.org/latest/Surface_mesh_deformation/index.html#Chapter_SurfaceMeshDeformation). Since the original paper (Sorkine & Alexa, 2007 - As-Rigid-As-Possible Surface Modeling), which is implemented in CGAL, a number of improvements have been proposed. The goal of this project is to investigate these improvements, and enhance the CGAL implementation. Another direction of interest is the extension of the ARAP formulation to the setting of volume deformation of tetrahedral meshes.

          Resources:

          As-Rigid-As-Possible Surface Modeling
          ARAP Revisited Discretizing the Elastic Energy using Intrinsic Voronoi Cells
          Higher Order Continuity for Smooth As-Rigid-As-Possible Shape Modeling
          Required Skills: C++17, linear algebra

          Contact: andreas.fabri@geometryfactory.com

          Duration: 350h

          ~~~~~~~~~~

          Extending 2D Arrangement Drawings
          Mentor: Efi Fogel

          Project description: The "2D Arrangement" package partially supports limited drawing of a 2D arrangements. The goal of this project is extend the capabilities of 2D arrangement drawing. In particular:

          The drawing is limited. An instance of the the Arrangement_2<Traits,Dcel> template can be used to represent 2D arrangements on the plane. The 2D Arrangement package supports ten traits classes that can substitute the Traits parameter. A traits class determines the family of curves that induce the arrangement, e.g., Bezier curves. Currently, arrangement induced by curves of several families cannot be drawn.
          The drawing is inefficient and should be optimized.
          The drawing of arrangements induced by geodesic arcs on the sphere in 3D is deficient. Currently only the curves are drawn (and the faces are not). The Earth demo exhibit some drawing of such arrangements, but it applies a trick that restricts the drawing to faces that do not cross the equator of the sphere. Addressing this item requires knowledge and experience in 3D graphics.
          Required Skills: Qt6 and 3D graphics, geometry, code development tools (e.g., git), and C++17 proficiency

          Contact: efifogel@gmail.com

          Duration: 350h

          ~~~~~~~~~~

          Hexahedral mesh generation
          Mentor: Guillaume Damiand

          Project description:

          The goal of this project is to implement the method of the paper [1] "A template-based approach for parallel hexahedral two-refinement", Steven J. Owen, Ryan M. Shih, Corey D. Ernst; in CGAL. This method allows to generate a locally refined hexahedral mesh, starting from a coarse grid, and using different templates for refinement. It will be implemented using a 3D linear cell complex [2] as underlying data-structure. To implement the different templates, we can use the volumic Query-replace operation [3]. The project was started last year and a preliminary version of the method already exists. The goal of this project is to finish this development, and to propose an integration in CGAL. To do so, the work to do is: (1) finish the sequential version, adding displacement of new vertices in order to obtain smooth meshes; (2) validate results on many different input meshes; (3) write the doc and the examples; (4) finish the parallel version.

          Resources:

          [1] The paper to be implemented: "A template-based approach for parallel hexahedral two-refinement"
          [2] CGAL Linear cell complex package
          [3] Query-replace operations for topologically controlled 3D mesh editing and the Gitlab repository
          Required Skills: C++17, Geometry Processing, Mesh Processing, Computational Geometry

          Contact: guillaume.damiand@cnrs.fr

          Duration: 175h

          ~~~~~~~~~~

          Cut by plane a volumetric mesh
          Mentor: Guillaume Damiand and Sebastien Loriot

          Project description:

          The goal of this project is to implement a method allowing to cut a 3D volumetric mesh (represented by a 3D linear cell complex) by a plane. There are some code available for the two first steps of the method (insert vertices on the cut edges, and insert edges between the new vertices to split faces); it remains the last step which consists in inserting new faces along path of edges. The method must be robust, i.e. deal with any configuration of volumetric mesh. To do so, the work to do is: (1) implement the 3 steps in CGAL; (2) validate results on many different input meshes; (3) write the doc and the examples.

          Required Skills: C++17, Geometry Processing, Mesh Processing, Computational Geometry

          Contact: guillaume.damiand@cnrs.fr sloriot.ml@gmail.com

          Duration: 175h

          ~~~~~~~~~~

          Improvement of Named Parameters
          Mentor: Sebastien Loriot and Laurent Rineau

          Project description:

          The goal of this project is to continue the work started in the pull-request https://github.com/CGAL/cgal/pull/7966. This change proposal implements a mechanism that allows the user to check at compile time that the options passed are used by the function (currently a flow of our mechanism). The proof of concept is there, but now we need to apply it globally in CGAL to all the functions using named parameters. As the function are documented, one way to tackle this project is to write a (python?) script that will collect for all the function the expected named parameters and add the macro calls in the function. There are also other improvements that can be implemented during this project if time allows (automatic extraction of a subset of options, more friendly developer interface, ...)

          Required Skills: C++17, Scritping Language such as Python, with knowledge in parsing

          Contact: sloriot.ml@gmail.com

          Duration: 175h

          ~~~~~~~~~~

          Adding Support for New File Formats for Meshes
          Mentor: Sebastien Loriot and Mael Rouxel-Labbé

          Project description:

          The CGAL library provides several functions to read and write meshes (surface and volume) in the Stream Support package. The list of currently supported file format is available here. The goal of this project is to add support to more file formats. We could for example add support for glTF, gmsh format, 3mf v2, ... The duration of the project will depend on the file format proposed for addition.

          Required Skills: C++17

          Contact: sloriot.ml@gmail.com

          Duration: 90h, 175h, or 350h

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://github.com/CGAL/cgal/wiki/Project-Ideas
    idea_list_url: https://github.com/CGAL/cgal/wiki/Project-Ideas


  - organization_id: 18
    organization_name: CHAOSS
    no_of_ideas: 
    ideas_content: |
      Idea: Enhance Conversational Topic Modelling Capabilities in CHAOSS Software
      Hours: 350

      Micro-tasks and place for questions

      This project will add GenSIM logic, and other capabilities to the Clustering Worker inside of Augur Software, and be extended into a generalized Open Source Software Conversational Topic Modeling Instrument.

      CHOASS/augur has several workers that store machine learning information derived from computational linguistic analysis of data in the message table. The message table includes messages from issue, pull request, pull request review, and email messages. They are related to their origin with bridge tables like pull_request_message_ref. The ML/CL workers are all run against all the messages, regardless of origin.

      Clustering Worker (clusters created and topics modeled)
      message analysis worker (sentiment and novelty analysis)
      discourse analysis worker (speech act classification (question, answer, approval, etc.)
      Clustering Worker Notes:

      Clustering Worker: 2 Models.

      Models:
      Topic modeling, but it needs a better way of estimating number of topics.
      Tables - repo_topic - topic_words
      Computational linguistic clustering
      Tables - repo_cluster_messages
      Key Needs
      Add GenSim algorithms to topic modeling section #1199
      The topics, and associated topic words need to be persisted after each run. At the moment, the topic words get overwritten for each topic modeling run.
      Description/optimization of the parameters used to create the computational linguistic clusters.
      Periodic deletion of models (heuristic: If 3 months pass, OR there’s a 10% increase in the messages, issues, or PRs in a repo, rebuild the models)
      Establish some kind of model archiving with appropriate metadata (lower priority)
      Discourse Analysis Worker Notes:

      discourse_insights table (select max(data_collection_date) for each msg_id)

      sequence is reassembled from the timestamp in the message table (look at msg_timestamp)
      issues_msg_ref, pull_request_message_ref, pull_request_review_msg_ref
      Message Analysis Worker

      message_analysis
      message_analysis_summary
      augur-tech

      The aims of the project are as follows:

      Advance topic modeling of open source software conversations captured in GitHub.
      Integrate this information into clearer, more parsimonious CHAOSS metrics.
      Automate the management machine learning insights, and topic models over time.
      (Stretch Goal) Improve the operation of the overall machine learning insights pipeline in CHAOSS/augur, and generalize these capabilities.
      
      ~~~~~~~~~~
      IDEA: Implement Conversion Rate Metric in CHAOSS Software
      Hours: 350

      Micro-tasks and place for questions

      Conversion Rate
      Question: What are the rates at which new contributors become more sustained contributors?

      Description
      The conversion rate metric is primarily aimed at identifying how new community members become more sustained contributors over time. However, the conversion rate metric can also help understand the changing roles of contributors, how a community is growing or declining, and paths to maintainership within an open source community.

      Objectives (why)
      Observe if new members are becoming more involved with an open source project
      Observe if new members are taking on leadership roles within an open source project
      Observe if outreach efforts are generating new contributors to an open source project
      Observe if outreach efforts are impacting roles of existing community members
      Observe if community conflict results in changing roles within an open source community
      Identify casual, regular, and core contributors
      Implementation
      This project could be implemented using either the CHAOSS/Augur, or CHAOSS/Grimoirelab (including stack components noted in references) technology stacks.

      The aims of the project are as follows:

      Implement the Conversion Rate Metric in CHAOSS Software
      After discussion, consider which CHAOSS Software Stack you wish to work with
      In collaboration with mentors, define the technology framework, and initial path to a "hello world" version of the metric
      Iterative development of the metric
      Assist in the deployment of this metric for a pre-determined collection of repositories in a publicly viewable website linked to the CHAOSS project.
      Advance the work of the chaoss metrics models working group.
      Difficulty: Medium
      Requirements: Knowledge of Python is desired. Some knowledge of Javascript or twitter/bootstrap is also desired. Key requirement is a keenness to dig into this challenge!
      Recommended: Python experience.
      Mentors: Sean Goggins
      Filters (optional)
      Commits
      Issue creation
      Issue comments
      Change request creation
      Change request comments
      Merged change requests
      Code Reviews
      Code Review Comments
      Reactions (emoji)
      Chat platform messages
      Maillist messages
      Meetup attendance
      Visualizations


      Source: https://chaoss.github.io/grimoirelab-sigils/assets/images/screenshots/sigils/overall-community-structure.png



      Source: https://opensource.com/sites/default/files/uploads/2021-09-15-developer-level-02.png

      Tools Providing the Metric
      Augur
      openEuler Infra
      Data Collection Strategies
      The following is an example from the openEuler community:

      A group of people who attended an offline event A held by the community, can be identified as Group A. Demographic information of Group A could be fetched from an on-line survey when people register for the event. To identify the conversation rate of these participants:
      Some people from Group A started watching and forking the repos, indicating they have shown some interest in this community. We marked them as subgroup D0 (Developer Level 0) as a subset of Group A.
      Conversion rate from the total number of people in Group A to the number of people in subgroup D0 is: D0/Group A
      Some people from subgroup D0 make more contributions beyond just watching or forking, including creating issues, making comments on an issue, or performed a code review. We marked them as subgroup D1 (Developer Level 1) as a subset of D0.
      Conversion rate from the total number of people in Subgroup D0 to the number of people in subgroup D1 is: D1/D0.
      Some people from subgroup D1 continue to make more contributions, like code contributions, to the project. This could include creating merge requests and merging new project code. We marked them as subgroup D2 (Developer Level 2) as a subset of D1.
      Conversion rate from the total number of people in subgroup D1 to the number of people in subgroup D2 is: D2/D1.


      Definition:

      Developer Level 0 (D0) example: Contributors who have given the project a star, or are watching or have forked the repository
      Developer Level 1 (D1): Contributors who have created issues, made comments on an issue, or performed a code review
      Developer Level 2 (D2): Contributors who have created a merge request and successfully merged code
      Conversion Rate (Group A -> D0): CR (Group A -> D2) = D0/Group A
      Conversion Rate (D0 -> D1): CR (D0 -> D1) = D1/D0
      Conversion Rate (D1 -> D2): CR (D1 -> D2) = D2/D1
      References
      https://opensource.com/article/21/11/data-open-source-contributors
      https://github.com/chaoss/augur
      https://gitee.com/openeuler/website-v2/blob/master/web-ui/docs/en/blog/zhongjun/2021-09-15-developer-level.md
      https://chaoss.github.io/grimoirelab-sigils/common/onion_analysis/
      https://mikemcquaid.com/2018/08/14/the-open-source-contributor-funnel-why-people-dont-contribute-to-your-open-source-project/
      Contributors
      Sean Goggins
      Andrew Brain
      John McGinness

      ~~~~~~~~~~
      IDEA: Open Source Software Health Metrics Visualization Exploration
      Hours: 350

      Micro-tasks and place for questions

      The CHAOSS Community currently delivers pre-packaged visualizations of open source software health data through Augur APIs (https://github.com/chaoss/augur/blob/main/augur/routes/pull_request_reports.py and https://github.com/chaoss/augur/blob/main/augur/routes/contributor_reports.py), and the https://github.com/chaoss/augur-community-reports repository. This project seeks to expand, refine, and standardize the visualization of different classes of community health metrics data. Specifically, some analyses are temporal, others are anomaly driven, and in some cases contrasts across repositories and communities are required. In each case, the visualization of data is an essential component for metrics, and what we are now referring to as metrics models (https://github.com/chaoss/wg-metrics-models).

      Additional resources include: http://new.augurlabs.io/ && https://github.com/augurlabs/augur_view which demonsrate the updated twitter/bootstrap Augur frontend.

      The aims of the project are as follows:

      Experiment with standard metrics visualizations using direct Augur database connections, or through the Augur API.
      Refine metrics, and metrics model visualizations using Jupyter Notebooks are similar technology.
      Transform visualizations, as they are completed, into Augur API endpoints, following the pull request, and contributor reports examples.
      Difficulty: Medium
      Requirements: Strong interest in data visualization.
      Recommended: Experience with Python is desirable, and experience designing, or developing visualizations is desirable.
      Mentors: Isaac Milarsky, Andrew Brain

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/chaoss/
    idea_list_url: https://github.com/chaoss/augur/blob/main/gsoc-ideas.md


  - organization_id: 19
    organization_name: CNCF
    no_of_ideas: 22
    ideas_content: |
        etcd
        etcd cache
        Description: Develop a generic, high-performance caching library for etcd, inspired by the Kubernetes watch cache, to facilitate building scalable and efficient etcd-based applications.
        Expected Outcome:
        A well-tested and performant library providing core caching primitives similar to Kubernetes' watch cache, significantly reducing etcd load and latency for generic etcd use cases.
        The library will offer feature parity with Kubernetes watch cache, including support for:
        Caching watch events and demultiplexing requests.
        Caching non-consistent list requests using a B-tree structure, updated via watch events.
        Handling requests during cache initialization and re-initialization.
        Custom encoders/decoders for data serialization.
        Custom indexing for optimized lookups.
        Consistent reads.
        Exact stale reads via B-tree snapshots.
        Comprehensive documentation, examples, benchmarks, and metrics to enable easy adoption and monitoring. This includes e2e and robustness tests.
        Recommended Skills: Go, Distributes Systems
        Expected project size: small
        Mentor(s):
        Marek Siarkowicz (@serathius, siarkowicz@google.com) - primary
        Madhav Jivrajani (@MadhavJivrajani, madhav.jiv@gmail.com)
        Upstream Issue (URL): etcd-io/etcd#19371

        ~~~~~~~~~~
        Harbor
        Enhance Harbor Satellite for Artifact Replication from Remote Registry to Edge
        Description: The Harbor Satellite project aims to enable decentralized artifact replication in edge computing environments. This project currently focuses on Use Case #1, where Harbor Satellite will pull images from a central Harbor registry and store them in a local OCI-compliant registry for use by edge devices. The solution is designed for environments with limited or intermittent internet connectivity, ensuring continuous access to required artifacts by local edge devices even when connectivity is unavailable.
        Expected Outcome:
        Enhance Harbor Satellite to enable reliable artifact replication from a central Harbor registry to a local OCI-compliant registry at the edge.
        Implement secure synchronization between central and local registries, especially in air-gapped environments.
        Optimize configuration management for edge container runtimes to pull images from the local registry.
        Provide clear documentation and setup guides for deploying Harbor Satellite in edge environments.
        Recommended Skills: Go, OCI-Registries, Distribution-spec
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Vadim Bauer (@vad1mo, vb@container-registry.com) - primary
        Orlin Vasilev (@OrlinVasilev, orlin@orlix.org)
        Prasanth Baskar (@bupd, bupdprasanth@gmail.com)
        Upstream Issue (URL): goharbor/harbor#21605

        ~~~~~~~~~~
        Jaeger
        Service performance analysis on top of Elasticsearch / OpenSearch data
        Description: Jaeger is an open-source, distributed tracing platform designed to monitor and troubleshoot transactions in distributed systems. In its basic deployment it allows collecting tracing data, storing it in a database, and querying & analyzing individual traces in the UI. This workflow is great for deep-diving into individual requests, but it does not answer some higher level questions like "which endpoints in my service are the slowest?" To address those questions Jaeger has a special feature called SPM (Service Performance Management), which allows the user to see the trends of services' and endpoints' performance and to drill down into the outliers. However, this feature requires a more complicated deployment where a special real-time processor is running and extracting metrics from the traces and storing those metrics in a Prometheus-compatible remote storage. Some of the storage backends supported by Jaeger, such as Elasticsearch & OpenSearch, can provide the same aggregate answers directly from the trace data, which can significantly simplify the deployment. This project aims to enable this integration.
        Expected Outcome:
        Support SPM functionality directly in Elasticsearch / OpenSearch backends by implementing the metrics query API
        Enhance existing e2e integration tests to continuously test this new capability
        Recommended Skills: Go, basic familiarity with Elasticsearch
        Expected project size: large (~350 hour projects)
        Mentors:
        Yuri Shkuro (@yurishkuro, github@ysh.us) - primary
        Jonah Kowall (@jkowall, jkowall@kowall.net)
        Upstream Issue (URL): jaegertracing/jaeger#6641

        ~~~~~~~~~~
        KCL
        KCL OCI third-party dependency management enhancement
        Description: KCL is an open-source constraint-based record & functional language mainly used in configuration and policy scenarios. KPM is a package management tool for the KCL language that supports the management of KCL packages in the OCI registry and Git Repo. This topic only applies to third-party dependencies from the OCI registry. Use the layering mechanism in OCI to help KPM implement dependency management of KCL third-party dependencies.

        Expected Outcome:

        Refactor the current KPM dependency management module with the OCI's layered mechanism.
        Recommended Skills: Go, OCI

        Expected project size: medium (~175 hour projects)

        Mentor(s):

        Zhe Zong (@zong-zhe, zongzhe1024@163.com)
        Heipa (@He1pa, he1pa404@gmail.com)
        Upstream Issue (URL): kcl-lang/kpm#598

        ~~~~~~~~~~
        Knative Functions
        Dynamic AI Agent Callbacks
        Description: Knative Functions is well-suited for AI agent integration. The serverless nature and isolated runtime environment of Functions make them ideal for creating lightweight, purpose-built services that can be dynamically invoked and even created by agents.
        Expected Outcome: This project would be a combination of research and practicum. First, an analysis of current AI agent interaction patterns, including emergent protocols and available frameworks. Second, the development of a Proof-of-concept integration between Functions and AI agents. This would involve at a minimum invocation, with a stretch goal of implementation and deployment by the agent based on a human prompt.
        Recommended Skills:
        Strong language and communication skills, with the ability to both research deeply and communicate clearly.
        Experience with AI/ML agents and desire to learn about programmatic LLM integrations.
        Familiarity with the Go programming language (ideal) or Python (secondarily), and web services.
        Familiarity with kubernetes, serveless, and microservices a plus.
        Expected project size: Large
        Mentor(s):
        Luke Kingland @lkingland (kingland AT redhat DOT com) - primary
        Aleksander Slominski @aslom (aslomins AT redhat DOT com)
        Upstream Issue (URL): knative/func#2690
        
        ~~~~~~~~~~
        Konveyor
        Extend usage of Konveyor AI to detect and update deprecated Kubernetes API usage in golang applications
        Description: Konveyor is an application modernization platform that helps organizations migrate legacy applications to Kubernetes at scale. As part of this effort, you will contribute to Konveyor AI (Kai), an intelligent code assistant that automates source code updates using data from static code analysis and changelog histories. Your work will focus on applying Generative AI techniques to detect and update deprecated Kubernetes APIs in Golang applications. You’ll build a tool that uses a LLM to generate Konveyor static code analysis rules from published documentation such as the Kubernetes deprecated API guide. Additionally, you’ll create workflows to identify deprecated API usage in legacy applications and automate code suggestions for updates — all powered by Konveyor AI.

        Expected Outcome:

        Develop a prototype tool to convert Kubernetes API deprecation documentation into static code analysis rules.
        Collaborate with the Konveyor AI team to extend support for Golang applications, identify issues, and contribute improvements.
        Demonstrate Konveyor AI’s ability to detect and suggest fixes for deprecated API usage in Golang projects.
        Recommended Skills: Golang, Python, Kubernetes, Generative AI

        Expected project size: # Large (~350 hours)

        Mentor(s):

        John Matthews (@jwmatthews, jwmatthews@gmail.com) - primary
        Savitha Raghunathan (@savitharaghunathan, saveetha13@gmail.com)
        Upstream Issue (URL): konveyor/kai#644

        ~~~~~~~~~~
        KubeArmor
        Improve KubeArmor Observability Spectrum
        Description: KubeArmor is a security enforcement system that provides runtime protection for Kubernetes workloads. To enhance observability, this task involves exposing key Prometheus metrics related to KubeArmor’s policy enforcement. These metrics will provide insights into security policy activity and alerting within a Kubernetes cluster.

        For starters, the following metrics can be started with:

        Number of Policies Applied
        Number of Alerts Triggered
        List of Active Policies
        Policy Status (Active/Inactive)
        Expected Outcome: Prometheus metrics are successfully integrated into KubeArmor, allowing users to monitor policy enforcement and security events effectively. The metrics should be accessible via a Prometheus endpoint and conform to best practices for Prometheus metric exposition.

        Recommended Skills: Go, Prometheus, Kubernetes.

        Expected project size: 175 hrs

        Mentor(s):

        Rishabh Soni (@rootxrishabh, risrock02@gmail.com)
        Prateek Nandle (@Prateeknandle, prateeknandle@gmail.com)
        Barun Acharya (@daemon1024, barun1024@gmail.com)
        Nishant Singh (@tesla59, talktonishantsingh.ns@gmail.com)
        Upstream Issue (URL): kubearmor/KubeArmor#1902

        ~~~~~~~~~~
        KubeBuilder
        Automating Operator Maintenance: Driving Better Results with Less Overhead
        Description:
        Code-generation tools like Kubebuilder and Operator-SDK have transformed cloud-native application development by providing scalable, community-driven frameworks. These tools simplify complexity, accelerate results, and enable developers to create tailored solutions while avoiding common pitfalls, laying a strong foundation for innovation.
        However, as these tools evolve with ecosystem changes and new features, projects risk becoming outdated. Manual updates are time-consuming, error-prone, and make it challenging to maintain security, adopt advancements, and stay aligned with modern standards.
        This project introduces an automated solution for Kubebuilder, with potential applications for similar tools or those built on its foundation. By streamlining maintenance, projects remain aligned with modern standards, improve security, and adopt the latest advancements. It fosters growth and innovation across the ecosystem, letting developers focus on what matters most: building great solutions.
        Note that the initial idea is to solve this with 3-way Git merges. However, users will face conflicts, and in the first phase, we want to study whether AI could help resolve these conflicts in a future phase to achieve this goal.

        Expected Outcome

        Conduct research on 3-Way Merge & Advanced Merge Options in Git.
        Conduct research on how AI could help resolve conflicts. If open-source solutions are available and align with the proposal, include them for consideration in a second phase.
        Develop a Proof of Concept (POC) implementing a GitHub Action that automatically creates a Pull Request (PR) in a mock repository, demonstrating the feasibility of the proposed solution.
        Successfully complete the proposal for PEP.
        Introduce a new Kubebuilder Plugin that scaffolds the GitHub Action based on the POC. This plugin will be released as an alpha feature, allowing users to opt-in for automated updates. The initial solution does not need to have AI, but AI integration could be a future enhancement if feasible.
        Recommended Skills

        Golang
        GitHub Actions
        Software Automation
        CI/CD
        Git
        IA
        Expected project size: Large (~350 hour projects)

        Mentor(s)

        Camila Macedo (@camilamacedo86, camilamacedo86@gmail.com) - Primary
        Varsha Prasad (@varshaprasad96, varshaprasad1507@gmail.com)
        TianYi(Tony) (@Kavinjsir)
        Upstream Issue: WIP - Proposal: Automating Operator Maintenance: Driving Better Results with Less Overhead
        ~~~~~~~~~~
        KubeStellar
        AI/ML Model Monitoring and Drift Detection in Disconnected Clusters using KubeStellar
        Description: AI/ML models deployed in disconnected environments, such as edge clusters and air-gapped systems, often suffer from model drift—a degradation in model performance due to changes in input data distributions. Without continuous monitoring, models may become inaccurate, leading to unreliable predictions.

        This project aims to integrate model monitoring and drift detection into KubeStellar, enabling Kubernetes-based AI workloads to detect data drift locally and sync monitoring metrics when connectivity is restored. The solution will use lightweight monitoring agents deployed alongside ML models to track data distribution changes and alert mechanisms to trigger model retraining when necessary.

        The system will also include policies for efficient metric storage and synchronization between disconnected and central clusters while minimizing bandwidth usage.

        Expected Outcome:

        A KubeStellar-compatible AI/ML monitoring component that tracks model drift in disconnected clusters.
        Efficient local storage and synchronization of monitoring metrics when connectivity is restored.
        Policies for adaptive model retraining triggers based on drift detection signals.
        Integration with existing ML tools (e.g., Prometheus, TensorFlow Extended, OpenTelemetry).
        Open-source documentation and example workflows demonstrating how KubeStellar manages AI model monitoring across disconnected clusters.
        Recommended Skills:

        Kubernetes and container orchestration
        AI/ML model deployment & monitoring
        Python, Go (for Kubernetes integrations)
        Experience with logging/monitoring tools (Prometheus, OpenTelemetry)
        Familiarity with KubeStellar (preferred but not required)
        Expected Project Size: Large (~350 hours) This project requires implementing multiple components: local monitoring, drift detection, synchronization, and integration with KubeStellar. It also involves research into efficient data synchronization strategies for low-bandwidth environments.

        Mentor(s):

        Andy Anderson (@clubanderson, andy@clubanderson.com) - Primary Mentor
        [Second Mentor's Name] (@second-mentor-github, second-mentor-email)
        Upstream Issue (URL): kubestellar/kubestellar#2791

        ~~~~~~~~~~

        Kubewarden
        Allow policies to be written using JavaScript
        Description: Kubewarden is a Policy Engine powered by WebAssembly policies that enforces security and compliance in Kubernetes clusters. Its policies can be written in CEL, Rego (OPA & Gatekeeper flavours), Rust, Go, YAML, and others.

        Kubewarden does not have a JavaScript SDK yet. Recent work done inside of the Bytecode Alliance made possible to compile Javascript code into WebAssembly . This means It's now possible to create such a SDK. This task consists on writing a JavaScript SDK that provides an idiomatic way to write Kubewarden policies.

        Expected Outcome: A new JavaScript SDK is created. The SDK API is documented, and the policy tutorial as well.

        Recommended Skills: JavaScript, Kubernetes.

        Expected project size: Large

        Mentor(s):

        Victor Cuadrado (@viccuad, vcuadradojuan@suse.com) - primary
        Flavio Castelli (@flavio, fcastelli@suse.com)
        José Guilherme Vanz (@jvanz, jguilhermevanz@suse.com)
        Fabrizio Sestito (@fabriziosestito, fabrizio.sestito@suse.com)
        Upstream Issue (URL): kubewarden/community#37

        ~~~~~~~~~~

        Elevate our .NET SDK into a first class citizen
        Description: Kubewarden is a Policy Engine powered by WebAssembly policies that enforces security and compliance in Kubernetes clusters. Its policies can be written in CEL, Rego (OPA & Gatekeeper flavours), Rust, Go, YAML, and others.

        Kubewarden has a .NET SDK that allows policy authors to write policies in C#. Starting with .NET 8, a big chunk of the work from https://github.com/dotnet/dotnet-wasi-sdk made its way upstream. This means it's a good time to revisit Kubewarden's .NET SDK for policies. This task consists on bringing our .NET SDK up to standard with the rest of our SDKs such as the Go or Rust ones.

        Expected Outcome: Our .NET SDK has been ported to .NET 9, and supports the same capabilities as our other SDKs. The SDK API is documented, and the policy tutorial as well.

        Recommended Skills: C#, .NET, Kubernetes.

        Expected project size: medium

        Mentor(s):

        Victor Cuadrado (@viccuad, vcuadradojuan@suse.com) - primary
        Flavio Castelli (@flavio, fcastelli@suse.com)
        José Guilherme Vanz (@jvanz, jguilhermevanz@suse.com)
        Fabrizio Sestito (@fabriziosestito, fabrizio.sestito@suse.com)
        Upstream Issue (URL): kubewarden/policy-sdk-dotnet#47

        ~~~~~~~~~~
        Lima
        VM plugin subsystem
        Description: Lima (https://lima-vm.io) is a project that provides Linux virtual machines with a focus on running containers. Lima supports several VM backends via built-in drivers: qemu, vz (Apple Virtualization.framework), and wsl2 (see lima/pkg/driverutil/instance.go). The idea for GSoC is to make a plugin subsystem that decouples the built-in VM drivers into separate binaries that communicate with the main Lima binary via some RPC (probably gRPC). This idea will improve the maintainability of the code base, and also help supporting additional VM backends (e.g., vfkit and cloud-based drivers).
        Expected Outcome:
        Design the plugin subsystem and its RPC (probably gRPC)
        Migrate the existing built-in VM drivers to the new plugin subsystem
        Implement additional VM plugins if the time allows
        Recommended Skills: Go, gRPC, QEMU, macOS
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Akihiro Suda (@AkihiroSuda, suda.kyoto@gmail.com) - primary
        Anders Björklund (@afbjorklund, anders.f.bjorklund@gmail.com)
        Upstream Issue (URL): lima-vm/lima#2007
        
        ~~~~~~~~~~
        LitmusChaos
        Terraform Support for LitmusChaos
        Description: LitmusChaos is an open-source Chaos Engineering platform that helps teams uncover weaknesses and potential outages in their applications by running controlled chaos experiments. However, before injecting chaos, several prerequisite steps must be completed, including user and project creation, connecting target infrastructure, and setting up experiments. To streamline this process, developers and SREs often seek automation, especially when integrating chaos testing into CI/CD pipelines. This Google Summer of Code (GSoC) project proposes developing a Terraform provider for LitmusChaos, enabling users to automate these essential setup steps and seamlessly manage chaos experiments through Terraform.

        Expected Outcome:

        LitmusChaos will have a terraform provider supporting user, project, infrastructure, and experiment resource operations along with proper documentation and usage scripts.
        A stretch goal for the mentee would be to become an official maintainer of the Litmus Terraform provider project.
        Recommended Skills: Golang, Terraform

        Expected project size: large (~175 hour projects)

        Mentor(s):

        Saranya Jena (@Saranya-jena, saranya.jena@harness.io)
        Sarthak Jain (@SarthakJain26, sarthak.jain@harness.io)
        Upstream Issue (URL): litmuschaos/litmus#5042

        ~~~~~~~~~~
        Meshery
        Multi-player Collaboration: Resilient Websockets and GraphQL Subscriptions
        Description: Meshery's current implementation of websockets and GraphQL subscriptions is in need of refactoring for increased reliability and resiliency. This client and server-side refactoring includes use of webworkers and separation of concerns for the client-side, and the use of a message broker for the server-side. The project has implications on Meshery's implementation of multi-player collaboration functionality.

        Expected Outcome: Resilient websockets and GraphQL subscriptions for Meshery, enabling multi-player collaboration functionality.

        Recommended Skills: Golang, Kubernetes, Azure, well-written and well-spoken English

        Expected project size: large (~175 hour project)

        Mentor(s):

        Lee Calcote (@leecalcote, leecalcote@gmail.com)
        Aabid Sofi (@aabidsofi19, mailtoaabid01@gmail.com)
        Upstream Issue: meshery/meshery#13554

        ~~~~~~~~~~

        Support for Azure in Meshery
        Description: Enhance Meshery's existing orchestration capabilities to include support for Azure. The Azure Service OperatorAzure Service Operator (ASO) provides a wide variety of Azure Resources via Kubernetes custom resources. as first-class Meshery Models. This involves enabling Meshery to manage and orchestrate Azure services and their resources, similar to how it handles other Kubernetes resources. The project will also include generating support for Azure services and their resources in Meshery's Model generator.

        Expected Outcome: Meshery will be able to orchestrate and manage all Azure services supported by ASO. This includes the ability to discover, configure, deploy, and operate the lifecycle of Azure services through Meshery. The Meshery Model generator will be updated to automatically generate models for Azure services, simplifying their integration and management within Meshery. This will be an officially supported feature of Meshery.

        Recommended Skills: Golang, Kubernetes, Azure, well-written and well-spoken English

        Expected project size: large (~175 hour project)

        Mentor(s):

        Lee Calcote (@leecalcote, leecalcote@gmail.com)
        Mia Grenell (@miacycle, mia.grenell2337@gmail.com)
        Upstream Issue: meshery/meshery#11244

        ~~~~~~~~~~

        Distributed client-side inference (policy evaluation) with WebAssembly (WASM) and OPA in Meshery
        Description: Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https://github.com/open-policy-agent/golang-opa-wasm project into Meshery UI, so that a powerful, real-time user experience is possible.

        Expected Outcome: The goal of this project is to enhance Meshery's infrastructure configuration capabilities by incorporating real-time policy evaluation using the golang-opa-wasm project. This project will integrate the capabilities of golang-opa-wasm into the Meshery UI, enabling users to experience the existing, powerful, server-side policy evaluation client-side.

        Recommended Skills: WebAssembly, Golang, Open Policy Agent, well-written and well-spoken English

        Expected project size: large (~175 hour project)

        Mentor(s): Lee Calcote (@leecalcote, leecalcote@gmail.com), James Horton (@hortison, james.horton2337@gmail.com)

        Upstream Issue: meshery/meshery#13555

        ~~~~~~~~~~
        Open Cluster Management
        Privacy-preserving and efficient AI model training across multi-cluster
        Description: Open Cluster Management (OCM) streamlines multi-cluster workload management through APIs that align with SIG-Multicluster standards. Beyond traditional workload orchestration, OCM enables scalable AI training and inference across distributed environments.

        As machine learning (ML) expands across clusters, data privacy becomes a critical concern. ML models rely on vast datasets, making it essential to safeguard sensitive information across clusters without compromising model performance.

        This project integrates Federated Learning (FL) into OCM, enabling privacy-preserving, collaborative model training without transferring raw data between clusters. Instead, training occurs locally where the data resides, ensuring compliance, enhancing efficiency, and reducing bandwidth and storage costs.

        By leveraging OCM's Placement, ManifestWork, and other APIs. we standardize FL workflows and seamlessly integrate frameworks like Flower and OpenFL through a unified interface. This approach harnesses OCM's capabilities to deliver scalable, cost-efficient, and privacy-preserving AI solutions in multi-cluster environments.

        Expected Outcome:

        Comprehensive Documentation:
        Define the scenarios addressed by the prototype, highlighting its purpose and value.
        Provide an intuitive and architectural comparison between Federated Learning (FL) and OCM, mapping FL terminology to OCM APIs to showcase OCM’s native support for FL.
        Illustrate the complete Federated Learning workflow within Open Cluster Management.
        Extended Prototype (or CRD) Support:
        Enable model aggregation persistence in AWS S3 (currently supports only native PVC).
        Extend compatibility to support additional Federated Learning frameworks like OpenFL (currently supports Flower). This requires understanding how OpenFL works, containerizing it, and integrating it into the prototype.
        Recommended Skills: Golang, Kubernetes, Federated Learning, Open Cluster Management, Scheduling

        Expected project size: medium (~175 hour projects)

        Mentor(s):

        Meng Yan (@yanmxa, myan@redhat.com) - primary
        Qing Hao (@haoqing0110, qhao@redhat.com)
        Upstream Issue (URL): open-cluster-management-io/ocm#825

        ~~~~~~~~~~    
        ORAS
        Enhance Java ORAS SDK
        Description: The ORAS project aims to enhance its Java SDK to support a broader range of features from the OCI Distribution spec. This involves implementing missing functionality, improving existing features, and expanding the SDK’s overall capabilities.
        Expected Outcome:
        Implement missing features from the OCI Distribution and Image Specifications, such as chunked uploads and the Referrers API
        Improve existing features, robustness and tests to ensure full compatibility with the OCI Distribution and Image Specifications.
        Enhance documentation and provide more comprehensive examples.
        Add support for additional authentication methods, including using credentials from docker config.json
        Recommended Skills: java, oci
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Valentin Delaye (@jonesbusy, jonesbusy@gmail.com) - primary
        Feynman Zhou (@FeynmanZhou, zpf0610@gmail.com)
        Upstream Issues: https://github.com/oras-project/oras-java/issues
        The Update Framework (TUF)
        Snapshot Merkle trees
        Description: The TUF snapshot role is responsible for consistency proofs in a TUF repository. In certain high-volume repositories, the related snapshot metadata file can become prohibitively large, and thus impose a significant overhead for TUF clients. TAP 16 proposes a method for reducing the size of snapshot metadata a client must download without significantly weakening the security properties of TUF. In this project you will add TAP 16 support to python-tuf.
        Expected Outcome: Snapshot Merkle trees are implemented in python-tuf Metadata API and ngclient
        Recommended Skills: Python, data structures (merkle trees)
        Expected project size: medium (~175 hour projects)
        Mentor(s):
        Lukas Pühringer (@lukpueh) - primary
        Justin Cappos (@JustinCappos)
        Upstream Issue (URL): theupdateframework/taps#134

        ~~~~~~~~~~
        Vitess
        Enhancements for FAQ Chatbot for Vitess
        Description: Vitess is a distributed database system built on MySQL. Developers often need to search through documentation, Slack discussions, and GitHub issues to find answers. We are starting a project to implement an AI-powered FAQ chatbot using Retrieval-Augmented Generation, integrating vector search with an LLM (such as OpenAI, DeepSeek, GPT-4, Mistral, Llama 3). The chatbot will be available via a CLI and Slack bot for developer support.

        In the next phase, which will be implemented in this Summer Of Code (SOC) project, we will be adding more features like:

        Content filtering for chatbot safety and response validation
        Fine-tuning the model for improved accuracy
        Pipelines for refreshing data from new/updated docs
        Caching previous replies to reduce LLM costs
        Rate-limiting
        Better benchmarking for iterative improvements
        User feedback integration to improve relevancy
        Expected Outcome: Improved chatbot that provides accurate Vitess-related answers via CLI and Slack, using indexed documentation and discussions for retrieval.

        Recommended Skills: golang, python, LLM APIs, vector databases

        Expected project size: large (~350 hour projects)

        Mentor(s):

        Rohit Nayak (@rohit-nayak-ps, rohit@planetscale.com)
        Manan Gupta (@GuptaManan100, manan@planetscale.com)
        Upstream Issue: vitessio/vitess#17690

        ~~~~~~~~~~

        WasmEdge
        Virtual filesystem security for WasmEdge plug-ins with exporting WASI APIs
        Description: The WASI proposal defines the variety of rules to guarantee the virtual filesystem security and isolation when executing WASM binaries. However, besides using WASI directly in WASM, developers can also implement the host functions to access the filesystem in their guest programming language. This will break the sandbox of WebAssembly. In this program, our goal is to export the WASI APIs in WasmEdge, and use the APIs in WasmEdge plug-ins to ensure the filesystem security and WebAssembly isolation.
        Expected Outcome:
        Export needed WASI APIs in WasmEdge internal to provide the functions of checking and accessing host filesystem.
        Apply the APIs in some WasmEdge plug-ins which accessing the filesystem, such as WASI-NN.
        Implement test suites to verify the above behaviors.
        Recommended Skills:
        C++
        WebAssembly
        Expected project size: Large (~350 hour projects)
        Mentor(s):
        YiYing He (@q82419 , yiying@secondstate.io) - Primary
        Shen-Ta Hsieh (@ibmibmibm , beststeve@secondstate.io)
        Upstream Issue (URL): WasmEdge/WasmEdge#4012

        ~~~~~~~~~~
        Port WasmEdge and the WASI-NN ggml backend to the s390x platform
        Description: WasmEdge provides cross-platform support for amd64 and arm64 for executing AI/LLM applications. We would like to support as many new hardware platforms as possible, so developers and users will no longer need to worry about the actual hardware. All they need to do is develop their AI agent or LLM applications once and deploy their services anywhere. For more information, please check the upstream issue.
        Expected Outcome:
        Make the WasmEdge toolchain support the s390x platform, including the interpreter and the AOT mode.
        Ensure the WASI-NN ggml plugin can execute without any issues on the s390x platform.
        Implement test suites to verify the above behaviors.
        Write a document discussing the compilation, installation, execution, and verification of the work.
        Recommended Skills:
        C++
        s390x
        LLVM
        Expected project size: Large (~350 hour projects)
        Mentor(s):
        Hung-Ying Tai (@hydai, hydai@secondstate.io) - Primary
        dm4 (@dm4, dm4@secondstate.io)
        Upstream Issue (URL): WasmEdge/WasmEdge#4010

        ~~~~~~~~~~

        Use Runwasi with WasmEdge runtime to test multiple WASM apps as cloud services
        Description: With WasmEdge serving as one of Runwasi’s standard runtimes, and as our C++-implemented library continues to evolve, we also need a verification process integrated into Runwasi to streamline and validate the stability of both container and cloud environments.
        Expected Outcome:
        A concise GitHub workflow demonstrates Runwasi end-to-end testing on Kubernetes.
        Need to design an interactive application scenario that supports multiple nodes
        Try to incorporate the use of the WasmEdge plugin into this scenario
        Document
        Recommended Skills:
        Rust
        C++
        GDB
        git / github workflow
        shell script
        Expected project size: Large (~350 hour projects)
        Mentor(s):
        Vincent (@CaptainVincent, vincent@secondstate.io) - Primary
        yi (@0yi0 yi@secondstate.io)
        Upstream Issue (URL): WasmEdge/WasmEdge#4011

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cncf/
    idea_list_url: https://github.com/cncf/mentoring/blob/main/programs/summerofcode/2025.md


  - organization_id: 20
    organization_name: CRIU 
    no_of_ideas: 5
    ideas_content: |
        Project ideas
        Add support for memory compression
        Summary: Support compression for page images

        We would like to support memory page files compression in CRIU using one of the fastest algorithms (it's matter of discussion which one to choose!).

        This task does not require any Linux kernel modifications and scope is limited to CRIU itself. At the same time it's complex enough as we need to touch memory dump/restore codepath in CRIU and also handle many corner cases like page-server and stuff.

        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Suggested by: Andrei Vagin <avagin@gmail.com>
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Alexander Mikhalitsyn <alexander@mihalicyn.com>, Andrei Vagin <avagin@gmail.com>

        ~~~~~~~~~~
        Use eBPF to lock and unlock the network
        Summary: Use eBPF instead of external iptables-restore tool for network lock and unlock.

        During checkpointing and restoring CRIU locks the network to make sure no network packets are accepted by the network stack during the time the process is checkpointed. Currently CRIU calls out to iptables-restore to create and delete the corresponding iptables rules. Another approach which avoids calling out to the external binary iptables-restore would be to directly inject eBPF rules. There have been reports from users that iptables-restore fails in some way and eBPF could avoid this external dependency.

        Links:

        https://www.criu.org/TCP_connection#Checkpoint_and_restore_TCP_connection
        https://github.com/systemd/systemd/blob/master/src/core/bpf-firewall.c
        https://blog.zeyady.com/2021-08-16/gsoc-criu
        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Prajwal S N <prajwalnadig21@gmail.com>
        Suggested by: Adrian Reber <areber@redhat.com>

        ~~~~~~~~~~
        Files on detached mounts
        Summary: Initial support of open files on "detached" mounts

        When criu dumps a process with an open fd on a file, it gets the mount identifier (mnt_id) via /proc/<pid>/fdinfo/<fd>, so that criu knows from which exact mount the file was initially opened. This way criu can restore this fd by opening the same exact file from topologically the same mount in restored mount tree.

        Restoring fd from the right mount can be important in different cases, for instance if the process would later want to resolve paths relative to the fd, and obviously resolving from the same file on different mount can lead to different resolved paths, or if the process wants to check path to the file via /proc/<pid>/fd/<fd>.

        But we have a problem finding on which mount we need to reopen the file at restore if we only know mnt_id but can't find this mnt_id in /proc/<pid>/mountinfo.

        Mountinfo file shows the mount tree topology of current mntns: parent - child relations, sharing group information, mountpoint and fs root information. And if we don't see mnt_id in it we don't know anything about this mount.

        This can happen in two cases

        1) external mount or file - if file was opened from e.g. host it's mount would not be visible in container mountinfo
        2) mount was lazily unmounted
        In case of 1) we have criu options to help criu handle external dependencies.

        In case of 2) or no options provided criu can't resolve mnt_id in mountinfo and criu fails.

        Solution: We can handle 2) with: resolving major/minor via fstat, using name_to_handle_at and open_by_handle_at to open same file on any other available mount from same superblock (same major/minor) in container. Now we have fd2 of the same file as fd, but on existing mount we can dump it as usual instead, and mark it as "detached" in image, now criu on restore knows where to find this file, but instead of just opening fd2 from actually restored mount, we create a temporary bindmount which is lazy unmounted just after open making the file appear as a file on detached mount.

        Known problems with this approach:

        Stat on btrfs gives wrong major/minor
        file handles does not work everywhere
        file handles can return fd2 on deleted file or on other hardlink, this needs special handling.
        Additionally (optional part): We can export real major/minor in fdinfo (kernel). We can think of new kernel interface to get mount's major/minor and root (shift from fsroot) for detached mounts, if we have it we don't need file handle hack to find file on other mount (see fsinfo or getvalues kernel patches in LKML, can we add this info there?).

        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Mentor: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
        Suggested by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
        Checkpointing of POSIX message queues
        Summary: Add support for checkpoint/restore of POSIX message queues

        POSIX message queues are a widely used inter-process communication mechanism. Message queues are implemented as files on a virtual filesystem (mqueue), where a file descriptor (message queue descriptor) is used to perform operations such as sending or receiving messages. To support checkpoint/restore of POSIX message queues, we need a kernel interface (similar to MSG_PEEK) that would enable the retrieval of messages from a queue without removing them. This project aims to implement such an interface that allows retrieving all messages and their priorities from a POSIX message queue.

        Links:

        https://github.com/checkpoint-restore/criu/issues/2285
        https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/ipc/mqueue.c
        https://www.man7.org/tlpi/download/TLPI-52-POSIX_Message_Queues.pdf
        Details:

        Skill level: intermediate
        Language: C
        Expected size: 350 hours
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Pavel Tikhomirov <ptikhomirov@virtuozzo.com>, Prajwal S N <prajwalnadig21@gmail.com>
        Suggested by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>

        ~~~~~~~~~~


        Add support for arm64 Guarded Control Stack (GCS)
        Summary: Support arm64 Guarded Control Stack (GCS)

        The arm64 Guarded Control Stack (GCS) feature provides support for hardware protected stacks of return addresses, intended to provide hardening against return oriented programming (ROP) attacks and to make it easier to gather call stacks for applications such as profiling (taken from [1]). We would like to support arm64 Guarded Control Stack (GCS) in CRIU, which means that CRIU should be able to Checkpoint/Restore applications using GCS.

        This task should not require any Linux kernel modifications but will require a lot of effort to understand Linux kernel and glibc support patches. We have a good example of support for x86 shadow stack [4].

        Links:

        [1] kernel support https://lore.kernel.org/all/20241001-arm64-gcs-v13-0-222b78d87eee@kernel.org
        [2] libc support https://inbox.sourceware.org/libc-alpha/20250117174119.3254972-1-yury.khrustalev@arm.com
        [3] libc tests https://inbox.sourceware.org/libc-alpha/20250210114538.1723249-1-yury.khrustalev@arm.com
        [4] x86 support https://github.com/checkpoint-restore/criu/pull/2306
        Details:

        Skill level: expert (a lot of moving parts: Linux kernel / libc / CRIU)
        Language: C
        Expected size: 350 hours
        Suggested by: Mike Rapoport <rppt@kernel.org>
        Mentors: Mike Rapoport <rppt@kernel.org>, Andrei Vagin <avagin@gmail.com>, Alexander Mikhalitsyn <alexander@mihalicyn.com>

        ~~~~~~~~~~
        Coordinated checkpointing of distributed applications
        Summary: Enable coordinated container checkpointing with Kubernetes.

        Checkpointing support has been recently introduced in Kubernetes, where the smallest deployable unit is a Pod (a group of containers). Kubernetes is often used to deploy applications that are distributed across multiple nodes. However, checkpointing such distributed applications requires a coordination mechanism to synchronize the checkpoint and restore operations. To address this challenge, we have developed a new tool called criu-coordinator that relies on the action-script functionality of CRIU to enable synchronization in distributed environments. This project aims to extend this tool to enable seamless integration with the checkpointing functionality of Kubernetes.

        Links:

        https://github.com/checkpoint-restore/criu-coordinator
        https://lpc.events/event/18/contributions/1803/
        https://sched.co/1YeT4
        https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/
        Details:

        Skill level: intermediate
        Language: Rust / Go / C
        Expected size: 350 hours
        Mentors: Radostin Stoyanov <rstoyanov@fedoraproject.org>, Prajwal S N <prajwalnadig21@gmail.com>
        Suggested by: Radostin Stoyanov <rstoyanov@fedoraproject.org>

        

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/criu/
    idea_list_url: https://criu.org/Google_Summer_of_Code_Ideas
  

  - organization_id: 21
    organization_name: Center for Translational Data Science
    no_of_ideas: 7
    ideas_content: |
        Google Summer of Code - Project Ideas 2025
        #1 Project Name: Towards Personalized Medicine with FHIR Integration and Data Driven Discovery
        Mentor(s): Alex VanTol, Kyle Burton
        Project Description
        FHIR (Fast Healthcare Interoperability Resources, pronounced “fire”) is the standard for ensuring smooth data exchange and interoperability in electronic health records (EHR) and widely used in hospital settings. This project will focus on integrating a FHIR server into an open-source data platform used for managing and analyzing distributed biomedical data and supporting federated AI for data driven discovery.
        As part of this endeavor, you will work on enabling this platform to support FHIR standards, using synthetic data to ensure the functionality of the solutions developed. You will gain valuable experience at the intersection of healthcare and technology, working on tasks that include cloud infrastructure and modern data standards.
        Gain a deep understanding of the FHIR standard and how it supports interoperability of EHR systems.
        Learn about cloud-based architectures for managing and sharing biomedical data.
        Hone your ability to write, translate, and estimate business and technical requirements through user stories, tickets, and epics.
        Develop and deploy code, as well as create integration tests for proof of concept scenarios.
        Contribute to the enhancement of healthcare data interoperability.
        Expand your technical skills by working with cloud infrastructure and standardized data protocols.
        Acquire hands-on experience contributing to an impactful open-source software project.
        Expected Outcomes
        Automated deployment and configuration of a FHIR server within the data platform.
        Capability for the data platform to import and export FHIR data seamlessly.
        Skills Required/Preferred
        Required: 
        Python programming experience
        Some experience with cloud infrastructure

        Preferred:
        Some familiarity with healthcare data
        Java programming experience
        Javascript programming experience
        Expected Size and Difficulty
        350 hours
        Difficulty - Medium

        ~~~~~~~~~~

        #2 Project Name: Supporting an Open-Source Data Lake Architecture with Graph-like Data
        Mentor(s): Alex VanTol, Michael Lukowski
        Project Description
        This project focuses on enhancing a data platform by implementing a versatile solution for data ingestion and storage using a data lakehouse architecture. The aim is to prototype a method for converting graph-structured data into Avro-based files, specifically utilizing a file format called the Portable Format for Biomedical data (PFB). This format supports exporting and importing bulk clinical and other structured data within healthcare data platforms.
        You will engage in designing and developing tooling to convert data from original sources, representing graph-like models, into serialized Avro files for seamless ingestion into a data lake. This project offers a unique opportunity to delve into data lakehouse architectures, graph data models, and serialization techniques, enriching your expertise in these areas.
        Understand and work with data lakehouse architectures for managing diverse datasets.
        Learn to convert data from graph models into an Avro-based serialized format.
        Develop skills in creating extensible methodologies and tooling for data ingestion.
        Gain familiarity with querying and visualizing graph models contained within serialized files.
        Explore the intersection of data management, bioinformatics, and healthcare data platforms.
        Contribute to the development of advanced data ingestion and storage solutions.
        Enhance your understanding of data lakehouses and serialization formats like Avro.
        Acquire hands-on experience with graph data models and their applications.
        Participate in an open-source project that has real-world impact in the healthcare field.
        Expected Outcomes
        Develop methodology and tools for converting graph-structured data into Avro-based serialized files.
        Potentially include tools for querying and visualizing graph models within the serialized format, depending on project scope.
        Skills Required/Preferred
        Required:
        Python programming experience
        Preferred:
        Knowledge of graph models
        Some familiarity with serialization formats (specifically, Avro)
        Knowledge of UX and/or client side tooling
        Expected Size and Difficulty
        175 hours 
        Difficulty - Medium

        ~~~~~~~~~~

        #3 Project Name: Improve Scalable Data Download Functionality Using Globus and an Open-Source Python SDK & CLI
        Mentor(s): Alex VanTol, Pauline Ribeyre
        Project Description
        This project aims to enhance the data download capabilities of a Python SDK & CLI used to interact with a large-scale data management platform. The goal is to create a consistent and efficient tool for researchers to download and stream large biomedical data sets. This involves integrating with existing RESTful APIs and Globus, a service for secure and reliable data transfer.
        You will work on implementing data download functionality within the Python SDK & CLI, writing unit tests to ensure robust code, and optimizing the download process using asynchronous capabilities in Python. This tool will be extensively used by researchers and scientists to stream large data sets to virtual workspaces for analysis.
        Gain experience in enhancing data download functionalities in a scalable manner.
        Learn to work with RESTful APIs and how to interact with them programmatically.
        Get hands-on experience with Globus for secure data transfer.
        Develop skills in Python programming, particularly in creating efficient and asynchronous code.
        Learn to write thorough unit tests to ensure high code quality.
        Understand the challenges and requirements of streaming large biomedical data sets.
        Contribute to improving tools that support large-scale data management and research.
        Enhance your programming skills, particularly in Python and Golang.
        Work on real-world projects that benefit researchers and scientists in the biomedical field.
        Gain experience in using modern data transfer technologies and optimizing performance.
        Expected Outcomes
        Implemented data download functionality within the Python SDK & CLI, integrated with Globus.
        Achieve 100% unit test coverage for the new code.
        Optimized data download process for improved performance.
        Skills Required/Preferred
        Required:
        Ability to read and understand Golang
        Ability to code in Python
        Preferred:
        Familiarity with RESTful APIs
        Familiarity with testing
        Familiarity with command line interfaces and/or UX
        Familiarity with concepts around concurrency
        Expected Size and Difficulty
        175 hours
        Difficulty - Medium

        ~~~~~~~~~~

        #4 Project Name: Towards a Data Commons Operations Center with Observability and Monitoring
        Mentor(s): Jawad Qureshi
        Project Description
        The goal of this project is to develop an Operations Center dashboard (CSOC) to manage multiple standalone and interconnected data commons running Gen3, a well-established open source platform for biomedical research. This dashboard will streamline the deployment, monitoring, and management of Gen3 data commons and meshes through a unified interface. The project will involve creating a distributed system with a Go-based backend and Next.js frontend, integrating with Kubernetes, Grafana, Helm, and Terraform, and ensuring secure server-agent communication.

        Expected Outcomes
        Production-ready Operations Center: A functional dashboard with a Go backend and Next.js frontend.
        RBAC and Security Policy Administration: Implementation of role-based access control (RBAC) and security policies.
        Observability Platform: Complete observability integrated with Prometheus/Grafana for monitoring.
        User-Friendly Dashboard: An intuitive interface for Gen3 commons deployment and management.
        Secure Server-Agent Communication: Infrastructure to ensure secure communication between server and agents.
        Comprehensive Documentation: Detailed system documentation and deployment guides.

        Skills Required/Preferred
        Go programming experience
        React/Next.js development skills / Javascript
        Understanding of Kubernetes, Helm, and Terraform
        Understanding of Cloud Infrastructure
        Experience with gRPC
        Understanding the pillars of observability (metrics, logs, tracing)
        Expected Size and Difficulty
        Number of hours 1000
        Difficulty - (Medium - Difficult)

        ~~~~~~~~~~

        #5 Project Name: Enhance Data Solutions with Native Graph Database Integration
        Mentor(s): Craig Barnes, Andrew Prokhorenkov, Alex VanTol
        Project Description
        This project aims to significantly improve the efficiency and capabilities of our data platform by transitioning from a custom PostgreSQL backend to a native graph database solution. With the increasing reliability and performance of graph databases like neo4j, this transition will enhance how we manage, analyze, and query complex biomedical data.
        Your role will involve developing a new Python-based microservice that supports the same RESTful APIs as our current submission and query services. Additionally, the microservice will dynamically generate GraphQL (or GraphQL-like) APIs based on a configured data schema, facilitating advanced search and query capabilities.
        Expected Outcomes
        Evaluate and analyze the best native graph database solutions for integration.
        Implement the selected graph database to replace the current PostgreSQL backend.
        Develop a new microservice in Python to support the existing RESTful and GraphQL (or GraphQL-like) APIs.
        Ensure seamless data submission, access, and querying within our data platform.
        This project will enhance our data platform's performance and flexibility, offering more robust solutions for managing biomedical data. Your contribution will provide significant improvements in data querying and management capabilities, benefiting the broader biomedical research community.
        Skills Required/Preferred
        Required: 
        Proficiency in Python
        Basic understanding of GitHub or other version control platforms
        Familiarity with RESTful APIs
        Understanding of graph databases
        Knowledge of user authorization and security principles

        Preferred:
        Familiarity with relational databases
        Understanding of microservice architecture, Docker, and containerization
        Expected Size and Difficulty
        Number of hours: 350 hours
        Difficulty - Hard

        ~~~~~~~~~~

        #6 Project Name: GPU Cluster Orchestration and Observability
        Mentors: Salman Sikandar and Bilal Baqar
        Project Description
        This project focuses on developing a CPU-based control plane to orchestrate jobs on an existing GPU cluster used by ML/AI researchers. The current setup is rudimentary, and the goal is to implement an automated job scheduling system along with comprehensive observability. The intern will design and implement a solution using technologies like Slurm for orchestration, and Prometheus/Grafana for monitoring and alerting.
        Expected Outcomes:
        Implement a job scheduler with priority queues and preemption capabilities for GPU workloads
        Develop a centralized dashboard displaying GPU/CPU utilization, job queue status, and thermal metrics
        Create Terraform/Ansible playbooks for reproducible cluster provisioning
        Design and implement automated alerting based on predefined thresholds and anomaly detection
        Skills Required/Preferred
        Required:
        Python or Go programming
        Linux systems administration
        Basic networking concepts
        Containerization (Docker)
        Preferred:
        Experience with Slurm
        Familiarity with Prometheus and Grafana
        GPU architecture knowledge (CUDA/NCCL)
        Experience with distributed systems

        Expected Size and Difficulty
        Number of Hours: 300-350 hours (8-10 weeks)
        Difficulty: Intermediate to Advanced

        ~~~~~~~~~~


        #7 Project Name: Staging Inference Cluster with CI/CD
        Mentors:
        Salman Sikandar and Bilal Baqar
        Project Description
        This project involves building a staging inference cluster to host production versions of ML models with auto scaling capabilities and CI/CD pipelines. The intern will set up an environment that allows for seamless promotion of trained models to production, implementing canary deployments and rollback mechanisms. The project will utilize technologies such as KEDA or Vertical Pod Autoscaler for autoscaling, and ArgoCD or Flux for GitOps-driven CI/CD.
        Expected Outcomes:

        Establish a staging inference cluster with autoscaling triggered by request latency/throughput thresholds
        Implement a GitHub Actions pipeline for model validation, containerization, and deployment
        Develop GitOps-driven CI/CD pipelines using ArgoCD or Flux for automated deployments
        Create a system for canary deployments and automated rollbacks based on performance metrics

        Skills Required/Preferred
        Required:
        Python or Go programming
        CI/CD concepts and tools
        Containerization (Docker)
        Basic understanding of ML workflows
        Preferred:
        Knowledge of ML model serving frameworks
        Familiarity with GitOps principles

        Expected Size and Difficulty
        Number of Hours: 300-350 hours (8-10 weeks)
        Difficulty: Intermediate to Advanced


          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/center-for-translational-data-science/
    idea_list_url: https://docs.google.com/document/d/1kiEDB6tw2xD8Qj3uxpSELjazW35llHM1a9Nt_LQKwHw/edit?usp=sharing

  - organization_id: 22
    organization_name: Ceph Foundation
    no_of_ideas: 7
    ideas_content: |
      
      Teuthology on Podman ¶
      Mentor name(s): Zack Cerza, Kamoltat (Junior) Sirivadhna Aishwarya Mathuria, Vallari Agrawal
      Mentor email(s): zack1@ibm.com, ksirivad@ibm.com, aishwarya.mathuria@ibm.com, vallari.agrawal@ibm.com
      Difficulty: Hard
      Project Hours: 175
      Skills needed: python, containerisation, linux
      Subcomponent of Ceph: Ceph Integration Test Framework
      Description of project:
      ceph-devstack is an in-development tool that uses rootless podman containers to deploy a scaled-down teuthology lab. It has proven useful for testing changes to teuthology and its related services, allowing us to more easily and flexibly make changes to components without worrying about causing outages.
      It has some basic ability to run Ceph tests, but could benefit significantly from more investment in that area.
      Improve and extend ceph-devstack's ability to perform teuthology tests against Ceph builds. This project will involve writing Python code and tests to orchestrate podman containers, and working with security systems like SELinux, CGroups, and Linux capabilities.
      Standup/weekly call mentee could attend?: Teuthology weekly meeting
      Steps to evaluate an applicant for the project: TBD
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD
      Expected Outcome:
      Extend ceph-devstack's ability to perform teuthology tests

      ~~~~~~~~~~
      smartmontools drivedb.h postprocessor ¶
      Mentor name(s): Anthony D'Atri, Sunil Angadi
      Mentor email(s): anthony.datri@ibm.com, sunil.angadi@ibm.com
      Difficulty: Intermediate
      Project Hours: 90
      Skills needed: c++, maybe python or golang
      Subcomponent of Ceph: Observability
      Description of project:
      smartmontools (smartctl) is pretty much the only game in town for harvesting metrics and counters from storage devices: SMART for SATA, a few things for SAS, and passthrough to nvme-cli for NVMe. It leverages a runtime file named drivedb.h that directs what attributes are to be found with what numeric IDs, and how to interpret them. drivedb.h is a mess, and upstream smartmontools would likely resist wholesale refactoring. For example, SSD wear might be labeled as "lifetime remaining" or "wear level" or multiple other strings. Some devices also report wear used, others wear remaining.
      One task would be to add an interpretation primitive to the c++ code so that a drivedb.h entry can specify that the result should be subtracted from 100.
      The larger task would be to write a postprocessor for drivedb.h that more or less is a sequence of regex invocations that converges the existing freeform attribute label names into a normalized, defined set. Many tools just pass through the text labels, so doing meaningful analysis or queries is difficult; often only a fraction of the data is actually captured as a result. The output also includes numeric attribute IDs, which are less varied, but relying on them instead of the text labels is fraught because these numeric IDs are not strictly standardized either. I have seen drives that report a metric on a different numeric ID than most others, and/or that report a different metric on a specific numeric than most others report on that ID.
      For extra credit, interface with the central telemetry DB as described in project "Public telemetry slice/dice of SMART data".
      Standup/weekly call mentee could attend?: TBD
      Steps to evaluate an applicant for the project: Ability to leverage code libraries and write the glue code.
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD

      ~~~~~~~~~~
      The More The Merrier ¶
      Mentor name(s): Yuval Lifshitz
      Mentor email(s): ylifshit@ibm.com
      Difficulty: Hard
      Project Hours: 350
      Skills needed: C++, Python
      Subcomponent of Ceph: RGW
      Description of project:
      Detailed description of the project and evalution steps can be found here.
      Persistent bucket notifications are a very useful and powerful feature
      tech talk: https://www.youtube.com/watch?v=57Ejl6R-L20
      usecase example: https://www.youtube.com/watch?v=57Ejl6R-L20
      However, they can pose a performance issue, since the notifications regarding a specific bucket are written to a single RADOS queue (unlike the writes to the bucket which are distributed across multiple bucket shards. So, in case that small objects are written to the bucket, the overhead of the notifications is considerable. In this project, our goal would be to create a sharded bucket notifications queue, to allow for better performance of sending persistent bucket notifications.
      Standup/weekly call mentee could attend?: RGW daily Standup, RGW weekly refactoring meeting
      Steps to evaluate an applicant for the project:
      build ceph from source and run basic bucket notification tests
      fix low-hanging-fruit issues in bucket notifications
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD
      Expected outcome:
      sharded implementation of persistent topic queue

      ~~~~~~~~~~
      stretch goal: perf test proving performance improvement
      Public telemetry slice/dice of SMART data ¶
      Mentor name(s): Anthony D'Atri
      Mentor email(s): anthony.datri@ibm.com
      Difficulty: Medium
      Project Hours: 175
      Skills needed: Some coding language, Python or Go, jq or JSON parsing or other text library.
      Subcomponent of Ceph: telemetry
      Description of project:
      Public telemetry today offers a few Grafana panels and downloadable archives of anonymized data. One field is a JSON blob of smartctl output. Parse this, apply a normalization layer, deduplicate, and present in one or more formats that facilitate analysis:
      CSV file containing attributes for only the latest report found for a given device
      The number of data points might be too high, but possibly a Grafana dashboard or even spreadsheet with template variables for manufacturer/model, interface type, etc. with various panes:
      Histograms of power_on hours, normalized endurance used or remaining, etc
      histogram or table of endurance remaining vs power on hours or TBW, i.e. allowing one to predict drive lifetime and inform purchase decisions, vs. assuming that SSDs especially QLC lack endurance or that high-endurance SKUs are required.
      reallocated sectors over time, etc.
      Standup/weekly call mentee could attend?: TBD
      Steps to evaluate an applicant for the project: Coding experience beyond Karel
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship:
      Gain familiarity with the data format, including JSON. Discuss input filtering: skip over invalid entries, handle submissions from older smartmontools, uniqify, learn about SMART -- and how dumb it is, the need for nomalization of counters.
      Expected outcome:
      Described above under Description. More specifically, deriving the rate of wear over time for each specific SSD for which we have more than say a month of data: capture the delta between earliest and latest wear levels reported for each given serial number, and the time delta between those samples. Divide the wear delta by the time delta for rate of wear over time.
      
      ~~~~~~~~~~
      Warm and Fuzzy ¶
      Mentor name(s): Yuval Lifshitz, Pritha Srivastava
      Mentor email(s): ylifshit@ibm.com, Pritha.Srivastava@ibm.com
      Difficulty: Medium
      Project Hours: 175
      Skills needed: C++, Python and also depending with the tool
      Subcomponent of Ceph: RGW
      Description of project:
      The RGW's frontend is an S3 REST API server, and in this project we would like to use a REST API fuzzer to test the RGW for security issues (and other bugs). First step of the project would be to select the right tool (e.g. https://github.com/microsoft/restler-fuzzer), feed it with the AWS S3 OpenAPI spec, and see what happens when we let it connect to the RGW. Fixing issues the fuzzer finds would nice, but the real stretch goal would be to integrate these tests into teuthology.
      Standup/weekly call mentee could attend: RGW daily Standup, RGW weekly refactoring meeting
      Steps to evaluate an applicant for the project:
      Detailed description of the project and evalution steps can be found here.
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship: TBD
      Expected outcome:
      find and fix security issues in the RGW found by the fuzzing tool
      stretch goal: integrate tool into automated teuthology runs

      ~~~~~~~~~~
      Ceph Dashboard Usability Improvements ¶
      Mentor name(s): Afreen Misbah
      Mentor email(s): afreen@ibm.com
      Difficulty: Easy
      Project Hours: 175
      Skills needed: Typescript, Angular, and basic understanding of HTML & CSS.
      Subcomponent of Ceph: Dashboard
      Description of project:
      Ceph Dashboard is Ceph's management and monitoring tool. It's a web application tool with Angular/Typescript on frontend side and Python as backend.
      We are in an effort to provide more usability workflows and solve UX issues to make management and monitoring easy for Ceph users.
      The task includes improving the notification system and creating a workflow for managing NVMe-oF devices from dashboard.
      Standup/weekly call mentee could attend?: Dashboard daily sync
      Steps to evaluate an applicant for the project:
      Build ceph dashboard locally via docker-compose and kcli both
      Able to understand issues and ask useful questions
      Eagerness to learn and contribute
      1-2 short paragraphs about what first 2 weeks of work would look like during the internship:
      Learning about ceph and storage and gradually contributing to the dashboard.
      Expected Outcome:
      Improve dashboard usability.

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ceph-foundation/
    idea_list_url: https://ceph.io/en/developers/google-summer-of-code/

  - organization_id: 23
    organization_name: Checker Framework
    no_of_ideas: 4
    ideas_content: |
      Evaluate a type system or a Checker Framework feature
      These projects evaluate a recently-written type system or a feature used by multiple type systems. Using the type systems on real code is our most important source of new ideas and improvements. Many people have started out “just” doing a case study but have ended up making deep, fundamental contributions and even publishing scientific papers about their discoveries.

      One possible outcome is to identify weaknesses in the type-checker so that we can improve it. Another possible outcome is to provide evidence that the type-checker is effective and convince more users to adopt it. You will probably also discover defects (bugs) in the codebase being type-checked.

      Signature strings
      Determine whether the ASM library, or some other library, properly handles signature strings.

      Some challenging aspects of this case study are:

      Some libraries define their own new signature string formats (!), which you need to define in the Signature String Checker.
      Sometimes the library's documentation is incorrect, and in other cases the string format is not defined.
      Preventing mixed signed/unsigned computations
      An unsigned integer's bits are interpreted differently than a signed integer's bits. It is meaningless to add a signed and an unsigned integer — the result will be nonsense bits. The same is true of printing and of other numeric operators such as multiplication and comparison.

      We have a prototype compile-time verification tool that detects and prevents these errors. The goal of this project is to perform case studies to determine how often programmers make signedness errors (our initial investigation suggests that this is common!) and to improve the verification tool.

      The research questions are:

      How often do programmers make signedness errors?
      Is it feasible to automatically detect signedness errors? What techniques are useful?
      What is the false positive rate of a signedness verification tool — that is, false alarms from the tool?
      How much effort is required from a programmer?
      The methodology is:

      find open-source projects that use unsigned arithmetic
      run the verification tool on them
      for each tool warning, determine whether it is a defect in the project or a limitation of the verification tool. For example, the Signedness Checker does not currently handle boxed integers and BigInteger; these haven't yet come up in case studies but could be worthwhile enhancements. You may also need to write more annotations for libraries such as the JDK.
      submit bug reports against the project, or improve the verification tool
      A good way to find projects that use unsigned arithmetic is to find a library that supports unsigned arithmetic, then search on GitHub for projects that use that library.

      Here are some relevant libraries.

      In the JDK's Integer and Long, these include compareUnsigned, divideUnsigned, parseUnsignedInt, remainderUnsigned, and toUnsignedLong.
      Classes like DataInputStream, ObjectInputStream, and RandomAccessFile have readUnsignedByte.
      Arrays has compareUnsigned. The JDK is already annotated; search for @Unsigned within https://github.com/typetools/jdk.
      In Guava, see its unsigned support, such as UnsignedBytes, UnsignedLong, UnsignedLongs, etc. Guava is already annotated; search for @Unsigned within https://github.com/typetools/guava.
      The jOOU library consists of support for unsigned integers.
      Another possibility is to find Java projects that could use an unsigned arithmetic library but do not. For example, bc-java defines its own unsigned libraries, and some other programs might do direct bit manipulation.

      Whole-program type inference
      A type system is useful because it prevents certain errors. The downside of a type system is the effort required to write the types. Type inference is the process of writing the types for a program.

      The Checker Framework includes a whole-program inference that inserts type qualifiers in the user's program. It works well on some programs, but needs more enhancements to work well on all programs.

      Sound checking by default
      By default, the Checker Framework is unsound in several circumstances. “Unsound” means that the Checker Framework may report no warning even though the program can misbehave at run time.

      The reason that the Checker Framework is unsound is that we believe that enabling these checks would cause too many false positive warnings: warnings that the Checker Framework issues because it cannot prove that the code is safe (even though a human can see that the code is safe). Having too many false positive warnings would irritate users and lead them not to use the checker at all, or would force them to simply disable those checks.

      We would like to do studies of these command-line options to see whether our concern is justified. Is it prohibitive to enable sound checking? Or can we think of enhancements that would let us turn on those checks that are currently disabled by default?

      There is no need to annotate new code for this project. Just use existing annotated codebases, such as those that are type-checked as part of the Checker Framework's Azure Pipeline. In other words, you can start by enabling Azure Pipelines for your fork and then changing the default behavior in a branch. The Azure Pipelines job will show you what new warnings appear.

      Comparison to other tools
      Many other tools exist for prevention of programming errors, such as Error Prone, NullAway, FindBugs, JLint, PMD, and IDEs such as Eclipse and IntelliJ. These tools are not as powerful as the Checker Framework (some are bug finders rather than verification tools, and some perform a shallower analysis), but they may be easier to use. Programmers who use these tools wonder, "Is it worth my time to switch to using the Checker Framework?"

      The goal of this project is to perform a head-to-head comparison of as many different tools as possible. You will quantify:

      the number of annotations that need to be written
      the number of bugs detected
      the number of bugs missed
      the number of false positive warnings
      This project will help programmers to choose among the different tools — it will show when a programmer should or should not use the Checker Framework. This project will also indicate how each tool should be improved.

      One place to start would be with an old version of a program that is known to contain bugs. Or, start with the latest version of the program and re-introduce fixed bugs. (Either of these is more realistic than introducing artificial bugs into the program.) A possibility would be to use the Lookup program that has been used in previous case studies.

      Android support annotations
      Android uses its own annotations that are similar to some in the Checker Framework. Examples include the Android Studio support annotations, including @NonNull, @IntRange, @IntDef, and others.

      The goal of this project is to implement support for these annotations. That is probably as simple as creating aliased annotations by calling method addAliasedTypeAnnotation() in AnnotatedTypeFactory.

      Then, do a case study to show the utility (or not) of pluggable type-checking, by comparison with how Android Studio currently checks the annotations.
      
      ~~~~~~~~~~

      Annotate a library
      These projects annotate a library, so that it is easier to type-check clients of the library. Another benefit is that this may find bugs in the library. It can also give evidence for the usefulness of pluggable type-checking, or point out ways to improve the Checker Framework.
      When type-checking a method call, the Checker Framework uses the method declaration's annotations. This means that in order to type-check code that uses a library, the Checker Framework needs an annotated version of the library.
      The Checker Framework comes with a few annotated libraries. Increasing this number will make the Checker Framework even more useful, and easier to use.
      After you have chosen a library, fork the library's source code, adjust its build system to run the Checker Framework, and add annotations to it until the type-checker issues no warnings.
      Before you get started, be sure to read How to get started annotating legacy code. More generally, read the relevant sections of the Checker Framework manual.
      Choosing a library to annotate
      There are several ways to choose a library to annotate:
      The best way to choose a library is to try to annotate a program and notice that library annotations are needed in order to type-check the program.
      Alternately, you can choose a popular Java library.
      When annotating a library, it is important to type-check both the library and at least one client that uses it. Type-checking the client will ensure that the library annotations are accurate.
      Whatever library you choose, you will need to deeply understand its source code. You will find it easier to work with a library that is well-designed and well-documented.
      You should choose a library that is not already annotated. There are two exceptions to this.
      A library might be annotated for one type system, but you add annotations for a different type system. One advantage of this is that the library's build system is already set up to run the Checker Framework. You can tell which type systems a library is annotated for by examining its source code.
      A library might be annotated, but the annotations have not been verified by running the type-checker on the library source code. You would verify that the annotations in the library are correct.
      Guava library
      Guava is already partially annotated with nullness annotations — in part by Guava's developers, and in part by the Checker Framework team. However, Guava does not yet type-check without errors. Doing so could find more errors (the Checker Framework has found nullness and indexing errors in Guava in the past) and would be a good case study to learn the limitations of the Nullness Checker.
      
      ~~~~~~~~~~
      Create a new type system
      The Checker Framework is shipped with about 20 type-checkers. Users can create a new checker of their own. However, some users don't want to go to that trouble. They would like to have more type-checkers packaged with the Checker Framework for easy use.
      Each of these projects requires you to design a new type system, implement it, and perform case studies to demonstrate that it is both usable and effective in finding/preventing bugs.
      Ownership type system
      The lightweight ownership mechanism of the Resource Leak Checker is not implemented as a type system, but it should be. That would enable writing ownership annotations on generic type arguments, like List<@Owning Socket>. It would also enable changing the Resource Leak Checker so that non-@Owning formal parameters do not have their @MustCall annotation erased.
      We have some notes on possible implementation strategies.
      Non-Empty Checker for precise handling of Queue.peek() and poll()
      The Nullness Checker issues a false positive warning for this code:
      import java.util.PriorityQueue;
      import org.checkerframework.checker.nullness.qual.NonNull;
      
      public class MyClass {
          public static void usePriorityQueue(PriorityQueue<@NonNull Object> active) {
              while (!(active.isEmpty())) {
                  @NonNull Object queueMinPathNode = active.peek();
              }
          }
      }
      The Checker Framework does not determine that active.peek() returns a non-null value in this context.
      The contract of peek() is that it returns a non-null value if the queue is not empty and the queue contains no null values.
      To handle this code precisely, the Nullness Checker needs to know, for each queue, whether it is empty. This is analogous to how the Nullness Checker tracks whether a particular value is a key in a map.
      It should be handled the same way: by adding a new subchecker, called the Nonempty Checker, to the Nullness Checker. Its types are:
      @UnknownNonEmpty — the queue might or might not be empty
      @NonEmpty — the queue is definitely non-empty
      There is a start at this type-checker in branch nonempty-checker. It:
      defines the annotations
      creates the integration into the Nullness Checker
      However, it is not done. (In fact, it doesn't even compile.) For information about what needs to be done, see issue #399.
      When you are done, the Nullness Checker should issue only the // :: diagnostics from checker/tests/nullness/IsEmptyPoll.java — no more and no fewer. You can test that by running the Nullness Checker on the file, and when you are done you should delete the // @skip-test line so that the file is run as part of the Checker Framework test suite.
      Iteration Checker to prevent NoSuchElementException
      A Java program that uses an Iterator can throw NoSuchElementException if the program calls next() on the Iterator but the Iterator has no more elements to iterate over. Such exceptions even occur in production code (for example, in Eclipse's rdf4j).
      We would like a compile-time guarantee that this run-time error will never happen. Our analysis will statically determine whether the hasNext() method would return true. The basic type system has two type qualifiers: @HasNext is a subtype of @UnknownHasNext.
      A variable's type is @HasNext if the program calls hasNext() and it returns true. Implementing this is easy (see the dataflow section in the "How to create a new checker" chapter). The analysis can also permit some calls to next() even if the programmer has not called hasNext(). For example, a call to next() is permitted on a newly-constructed iterator that is made from a non-empty collection. (This special case could build upon the Non-Empty Checker mentioned above.) There are probably other special cases, which experimentation will reveal.
      Parts of this are already implemented, but it needs to be enhanced. Once case studies have demonstrated its effectiveness, then it can be released to the world, and a scientific paper can be written.
      Preventing injection vulnerabilities via specialized taint analysis
      Many security vulnerabilities result from use of untrusted data without sanitizing it first. Examples include SQL injection, cross-site scripting, command injection, and many more. Other vulnerabilities result from leaking private data, such as credit card numbers.
      We have built a generalized taint analysis that can address any of these problems. However, because it is so general, it is not very useful. A user must customize it for each particular problem.
      The goal of this project is to make those customizations, and to evaluate their usefulness. A specific research question is: "To what extent is a general taint analysis useful in eliminating a wide variety of security vulnerabilities? How much customization, if any, is needed?"
      The generalized taint analysis is the Checker Framework's a Tainting Checker. It requires customization to a particular domain:
      rename the @Tainted and @Untainted qualifiers to something more specific (such as @Private or @PaymentDetails or @HtmlQuoted), and
      annotate libraries.
      The first part of this project is to make this customization easier to do — preferably, a user will not have to change any code in the Checker Framework (the Subtyping Checker already works this way). As part of making customization easier, a user should be able to specify multiple levels of taint — many information classification hierarchies have more than two levels. For example, the US government separates information into four categories: Unclassified, Confidential, Secret, and Top Secret.
      The second part of this project is to provide several examples, and do case studies showing the utility of compile-time taint checking.
      Possible examples include:
      SQL injection
      OS command injection
      the @PrivacySource and @PrivacySink annotations used by the Meta Infer static analyzer.
      information flow
      many of the CWE/SANS most dangerous software programming errors (and the "on the cusp" ones too)
      For some microbenchmarks, see the Juliette test suite for Java from CWE.
      Warn about unsupported operations
      In Java, some objects do not fully implement their interface; they throw UnsupportedOperationException for some operations. One example is unmodifiable collections. They throw the exception when a mutating operation is called, such as add, addAll, put, remove, etc.
      The goal of this project is to design a compile-time verification tool to track which operations might not be supported. This tool will issue a warning whenever an UnsupportedOperationException might occur at run time. This helps programmers to avoid run-time exceptions (crashes) in their Java programs.
      The research questions include:
      Is it is possible to build a verification tool to prevent UnsupportedOperationException? What design is effective?
      How difficult is such a tool to use, in terms of programmer effort and number of false alarms?
      Are potential UnsupportedOperationException exceptions pervasive in Java programs? Is it possible to eliminate them?
      The methodology is:
      design a static (compile-time) analysis
      implement it
      evaluate it on open-source projects
      report bugs in the projects, and improve the tool
      Here is a possible design, as a pluggable type system.
        @Unmodifiable
             |
        @Modifiable
      In other words, the @Unmodifiable type qualifier is a supertype of @Modifiable. This means that a @Modifiable List can be used where an @Unmodifiable List is expected, but not vice versa.
      @Modifable is the default, and methods such as Arrays.asList and Collections.emptyList must be annotated to return the less-capable supertype.
      Overflow checking
      Overflow is when 32-bit arithmetic differs from ideal arithmetic. For example, in Java the int computation 2,147,483,647 + 1 yields a negative number, -2,147,483,648. The goal of this project is to detect and prevent problems such as these.
      One way to write this is as an extension of the Constant Value Checker, which already keeps track of integer ranges. It even already checks for overflow, but it never issues a warning when it discovers possible overflow. Your variant would do so.
      This problem is so challenging that there has been almost no previous research on static approaches to the problem. (Two relevant papers are IntScope: Automatically Detecting Integer Overflow Vulnerability in x86 Binary Using Symbolic Execution and Integer Overflow Vulnerabilities Detection in Software Binary Code.) Researchers are concerned that users will have to write a lot of annotations indicating the possible ranges of variables, and that even so there will be a lot of false positive warnings due to approximations in the conservative analysis. For example, will every loop that contains i++ cause a warning that i might overflow? That would not be acceptable: users would just disable the check.
      You can convince yourself of the difficulty by manually analyzing programs to see how clever the analysis has to be, or manually simulating your proposed analysis on a selection of real-world code to learn its weaknesses. You might also try it on good and bad binary search code.
      One way to make the problem tractable is to limit its scope: instead of being concerned with all possible arithmetic overflow, focus on a specific use case. As one concrete application, the Index Checker is currently unsound in the presence of integer overflow. If an integer i is known to be @Positive, and 1 is added to it, then the Index Checker believes that its type remains @Positive. If i was already Integer.MAX_VALUE, then the result is negative — that is, the Index Checker's approximation to it is unsound.
      This project involves removing this unsoundness by implementing a type system to track when an integer value might overflow — but this only matters for values that are used as an array index. That is, checking can be restricted to computations that involve an operand of type @IntRange). Implementing such an analysis would permit the Index Checker to extend its guarantees even to programs that might overflow.
      This analysis is important for some indexing bugs in practice. Using the Index Checker, we found 5 bugs in Google Guava related to overflow. Google marked these as high priority and fixed them immediately. In practice, there would be a run-time exception only for an array of size approximately Integer.MAX_INT.
      You could write an extension of the Constant Value Checker, which already keeps track of integer ranges and even determines when overflow is possible. It doesn't issue a warning, but your checker could record whether overflow was possible (this could be a two-element type system) and then issue a warning, if the value is used as an array index. Other implementation strategies may be possible.
      Here are some ideas for how to avoid the specific problem of issuing a warning about potential overflow for every i++ in a loop (but maybe other approaches are possible):
      The loop checks whether i == Integer.MAX_VALUE before incrementing. This wide-scale, disruptive code change is not acceptable.
      Make the default array size (the length of an unannotated array) be @ArrayLenRange(0, Integer.MAX_VALUE-1) rather than @UnknownVal, which is equivalent to @ArrayLenRange(0, Integer.MAX_VALUE-1). Now, every array construction requires the client to establish that the length is not Integer.MAX_VALUE. I don't have a feel for whether this would be unduly burdensome to users.
      Index checking for mutable length data structures
      The Index Checker is currently restricted to fixed-size data structures. A fixed-size data structure is one whose length cannot be changed once it is created, such as arrays and Strings. This limitation prevents the Index Checker from verifying indexing operations on mutable-size data structures, like Lists, that have add or remove methods. Since these kind of collections are common in practice, this is a severe limitation for the Index Checker.
      The limitation is caused by the Index Checker's use of types that are dependent on the length of data structures, like @LTLengthOf("data_structure"). If data_structure's length could change, then the correctness of this type might change.
      A naive solution would be to invalidate these types any time a method is called on data_structure. Unfortunately, aliasing makes this still unsound. Even more, a great solution to this problem would keep the information in the type when a method like add or remove is called on data_structure. A more complete solution might involve some special annotations on List that permit the information to be persisted.
      Another approach would be to run a pointer analysis before type-checking, then use that information for precise information about what lists might be changed by each call to add or remove. One possible pointer analysis would be that of Doop.
      This project would involve designing and implementing a solution to this problem.
      Nullness bug detector
      Verifying a program to be free of errors can be a daunting task. When starting out, a user may be more interested in bug-finding than verification. The goal of this project is to create a nullness bug detector that uses the powerful analysis of the Checker Framework and its Nullness Checker, but omits some of its more confusing or expensive features. The goal is to create a fast, easy-to-use bug detector. It would enable users to start small and advance to full verification in the future, rather than having to start out doing full verification.
      This could be structured as a new NullnessLight Checker, or as a command-line argument to the current Nullness Checker. Here are some differences from the real Nullness checker:
      No initialization analysis; the checker assumes that every value is initialized.
      No map key analysis; assume that, at every call to Map.get, the given key appears in the map.
      No invalidation of dataflow facts. Assume all method calls are pure, so method calls do not invalidate dataflow facts. Assume there is no aliasing, so field updates do not invalidate dataflow facts.
      Assume that boxing of primitives is @Pure: it returns the same value on every call.
      If the Checker Framework cannot infer a type argument, assume that the type argument is @NonNull.
      Each of these behaviors should be controlled by its own command-line argument, as well as being enabled in the NullnessLight Checker.
      The implementation may be relatively straightforward, since in most cases the behavior is just to disable some functionality of existing checkers.
      Tools such as FindBugs, NullAway, NullnessLight, and the Nullness Checker form a spectrum from easy-to-use bug detectors to sound verification. NullnessLight represents a new point in the design space. It will be interesting to compare these checkers:
      How much easier is it to use? For example, how many fewer annotations need to be written?
      How many more fewer true positives does it report — in other words, how many more false negatives does it suffer?
      How many fewer false positives does it report?
      Uber's NullAway tool is also an implementation of this idea (that is, a fast, but incomplete and unsound, nullness checker). NullAway doesn't let the user specify Java Generics: it assumes that every type parameter is @NonNull. Does Uber's tool provide users a good introduction to the ideas that a user can use to transition to a nullness type system later?
      
      ~~~~~~~~~~
      Enhance the toolset
      Indicate library methods that should be used instead
      Sometimes, the best way to avoid a checker warning is to use an annotated library method. Consider this code:
      @FqBinaryName String fqBinaryName = ...;
      @ClassGetName String componentType = fqBinaryName.substring(0, fqBinaryName.indexOf('['));
      The Signature String Checker issues a warning, because it does not reason about arbitrary string manipulations. The code is correct, but it is in bad style. It is confusing to perform string manipulations to convert between different string representations. It is clearer and less error-prone (the above code is buggy when fqBinaryName is not an array type!) to use a library method, and the checker accepts this code because the library method is appropriately annotated:
      import org.plumelib.reflection.Signatures;
      ...
      @ClassGetName String componentType = Signatures.getArrayElementType(fqBinaryName);
      However, users may not know about the library method. Therefore, the Checker Framework should issue a warning message, along with the error message, notifying users of the library method. For example, the Signature String Checker would heuristically mention the Signatures.getArrayElementType() method when it issues an error about string manipulation where some input is a FqBinaryName and the output is annotated as ClassGetName. It would behave similarly for other library methods.
      Improving error messages
      Compiler writers have come to realize that clarity of error messages is as important as the speed of the executable (1, 2, 3, 4). This is especially true when the language or type system has rich features.
      The goal of this project is to improve a compiler's error messages. Here are some distinct challenges:
      Some type errors can be more concisely or clearly expressed than the standard "found type A, expected type B" message.
      Some types are complex. The error message could explain them, or link to the manual, or give suggested fixes.
      Compiler messages currently show the effective type, which may be different than what the user wrote due to defaulting, inference, and syntactic sugar. For example, a user-written @IndexFor("a") annotation is syntactic sugar for @NonNegative @LTLengthOf("a"), and those types are the ones that currently appear in error messages. It might be good to show simpler types or ones that the user wrote.
      Some checkers combine multiple cooperating type systems; the Nullness Checker and the Index Checker are examples. If there is a problem with a variable's lower bound type, then its upper bound type should not be shown in the error message. This will make the message shorter and more specific, and avoid distracting the user with irrelevant information.
      When a checker has multiple type systems, a type error or the lack of one may depend on facts from multiple type systems, and this should be expressed to the user.
      Replace JavaParser by javac
      The Checker Framework uses JavaParser to parse a Java expressions. However, JavaParser is buggy and poorly maintained. The goal of this project is to replace every use of JavaParser by a use of javac-parse.
      Java expression parser
      A number of type annotations take, as an argument, a Java expression. The representation for these is as a JavaExpression. The goal of this project is to remove it.
      The JavaExpression class represents an AST. There is no need for the Checker Framework to define its own AST when the javac AST already exists and is maintained.
      The goals for the project include:
      Replace every use of JavaExpression by a use of the javac class class com.sun.tools.javac.tree.JCTree.JCExpression.html.
      Replace every use of a subclass of JavaExpression (listed in the "Direct Known Subclasses" section of the JavaExpression API documentation) by a use of a subclass of JCTree.JCExpression.html. For example, replace every use of MethodCall by JCTree.JCMethodInvocation.
      Replace the JavaExpressionParseUtil class and delete ExpressionToReceiverVisitor.
      Direct replacement of the classes is not possible, or we would have done it already. For example, JavaExpression contains some methods that javac lacks, such as isUnassignableByOtherCode. As a first step before doing the tasks listed above, you may want to convert these methods from instance methods of JavaExpression into static methods in JavaExpressions, making JavaExpression more like a standard AST that can be replaced by JavaParser classes. You also need to decide how to store the type field of JavaExpression, when JavaExpression is eliminated. An alternate design (or a partial step in the refactoring process) would be to retain the JavaExpression class, but make it a thin wrapper around javac classes that do most of the real work.
      Another aspect of this project is fixing the issues that are labeled "JavaExpression".
      Dataflow enhancements
      The Checker Framework's dataflow framework (manual here) implements flow-sensitive type refinement (local type inference) and other features. It is used in the Checker Framework and also in Error Prone, NullAway, and elsewhere.
      There are a number of open issues — both bugs and feature requests — related to the dataflow framework. The goal of this project is to address as many of those issues as possible, which will directly improve all the tools that use it.
      Side effect analysis, also known as purity analysis
      A side effect analysis reports what side effects a procedure may perform, such as what variable values it may modify. A side effect analysis is essential to other program analyses. A program analysis technique makes estimates about the current values of expressions. When a method call occurs, the analysis has to throw away most of its estimates, because the method call might change any variable. However, if the method is known to have no side effects, then the analysis doesn't need to throw away its estimates, and the analysis is more precise. Thus, an improvement to the foundational side effect analysis can improve many other program analyses.
      The goal of this project is to evaluate existing side effect analysis algorithms and implementations, in order to determine what is most effective and to improve them. The research questions include:
      What side effect analysis algorithms are most effective? What are their limitations?
      Can the most effective algorithms be combined to become even effective? Or can their limitations be overcome?
      How much does accurate side effect analysis improve other programming tasks?
      The methodology is to collect existing side effect analysis tools (two examples are Soot and Geffken); run them on open-source projects; examine the result; and then improve them.
      Javadoc support
      Currently, type annotations are only displayed in Javadoc if they are explicitly written by the programmer. However, the Checker Framework provides flexible defaulting mechanisms, reducing the annotation overhead. This project will integrate the Checker Framework defaulting phase with Javadoc, showing the signatures after applying defaulting rules.
      There are other type-annotation-related improvements to Javadoc that can be explored, e.g. using JavaScript to show or hide only the type annotations currently of interest.
      
      

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/checker-framework/
    idea_list_url: https://rawgit.com/typetools/checker-framework/master/docs/developer/new-contributor-projects.html

  - organization_id: 24
    organization_name: Chromium
    no_of_ideas:
    ideas_content: |
      1. Interaction to Next Paint (INP) "subparts" (Medium Size)




      Project Description
      The web performance specifications define how browsers are supposed to be behaving when it comes to the various web performance APIs, and the Core Web Vitals (CWV) program uses those apis to automatically measure the performance and UX of most websites in the world, and shares the data publicly via the Chrome User Experience Report (CrUX). Last year a new metric, Interaction to Next Paint (INP), was added to the CWV. This year, we would like your help to give developers additional insights into INP latency issues by also reporting INP "subparts", similar to what we did for LCP last year.  For example: a developer should know if any INP responsiveness issues are caused by: long input delay (maybe the page as blocked and busy)
      event processing times (maybe the interaction event listeners were too complex) rendering/pixel presentation delays (maybe the page content is too complex/bloated) …or other task scheduling issues. We’ve recently instrumented the measurement code (i.e. the Event Timing API) to measure these time points, but we need your help to finish the job: moving the data from Renderer code to Browser code (using Mojo IPC), adding it to field metrics reporting (UKM), and eventually helping teammates integrate into CrUX experimental data – while also writing tests for the above. This is an open source project and some of the contributions will be useful to developers for local lab testing, as well (as for other downstream browsers). Finally, there are also several stretch technical opportunities related to the Event Timing API / INP metric.
      Proposal Doc for more information: Link
      Location to ask project specific questions not answered above: here (preferred), or chromium-gsoc-project1@chromium.org
      Beginner Bugs: https://issues.chromium.org/issues/40858679 
      Requirements: C++, JavaScript, Performance tooling (DevTools performance panel). Time zone flexible but hosts are in Eastern time zone.

      ~~~~~~~~~~


      2. Android Virtual Printer Application (Large Size)


      Project Description
      An Android application which can pretend to be a printer (act as a virtual printer), which can handle interactions between Android devices, software and printer itself. Once this virtual printer Android application is installed, the Android device can detect a new printer, set up the printer via network connection, users can use it to print and change settings based on its capability, and the virtual printer can respond to print jobs. This virtual printer application should allow configuration on printer capabilities, and print job response, in this way it can be used to test different functionalities and validate handling errors.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project2@chromium.org
      Requirements: Android application development, Kotlin, Rust, Java, C++, C

      ~~~~~~~~~~

      3. Chrome Extension APIs (Large Size)

      Project Description
      We have a number of new APIs and features that we would like to add to the Chrome extensions platform but don't currently have prioritized (for example, improvements to the declarative network request API used for network filtering, and new features in the `chrome.sidePanel` API). Many of these involve work in the W3C WebExtensions Community Group. We would like to offer a contributor the opportunity to own a work item from this list.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here  (preferred), chromium-gsoc-project3@chromium.org
      Beginner Bugs: 
      Requirements: C++ for the Chromium codebase.  JavaScript needed for writing extensions.


      ~~~~~~~~~~


      4. Structured DNS Errors (Small Size)

      Project Description
      Chrome should implement enough of the emerging Public DNS Errors standards to allow public DNS servers to indicate to clients when certain DNS resolutions are blocked for legal reasons.
      https://github.com/mnot/public-resolver-errors
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project4@chromium.org
      Beginner Bugs: https://issues.chromium.org/hotlists/6689006
      Requirements: C++, Chromium Net Stack. Must overlap with US East Coast timezone.

      ~~~~~~~~~~


      5. FedCM API Test Coverage and Flakiness (Medium Size)

      Project Description
      The FedCM API comprises of multiple tests which require the bot to go through a federated authentication process. The project consists of documenting the existing test coverage, improving it, and fixing test flakiness.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project5@chromium.org
      Beginner Bugs: https://issues.chromium.org/issues/41482163
      Requirements: C++, as well as a little bit of HTML, JS, Python. US East is preferred but not required


      ~~~~~~~~~~


      6. Add 3rd Party Theme Support for Tab Groups (Small Size)

      Project Description
      Add an API to allow 3rd party theme developers to customize the colors used for tab groups.

      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project6@chromium.org
      Beginner Bugs: https://issues.chromium.org/issues/40711397, https://issues.chromium.org/issues/370416945 https://issues.chromium.org/issues/40929354
      Requirements: C++ prior experience is requested, ability to work in PST is requested.

      ~~~~~~~~~~

      7. ChromeOS Platform Input Device Quality Monitoring (Medium Size)

      Project Description
      ChromeOS devices generally contain user-input devices (touchscreens, touchpads, etc.) that contain firmware and communicate over internal buses. This firmware can have bugs, sometimes needs to be updated in the field, and the communication buses can show errors.

      Improved testing during development, and logging for production devices, can reduce shipped issues and reduction of in-field failures, and improved analysis leading to better OEM response to such issues.

      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project7@chromium.org
      Beginner Bugs: https://issuetracker.google.com/397520373, https://issuetracker.google.com/397521060, https://issuetracker.google.com/397520674
      Requirements: ChromeOS development tech stack (C++, shell scripts), experience or patience to learn ChromeOS build system and loading custom software onto a ChromeBook, moderate overlap with US-Pacific timezone.


      ~~~~~~~~~~


      8. Farfetchd: tracing/replay (Medium Size)

      Project Description
      Preloading files into memory (or prefetch) is a useful mechanism for improving application “cold start” performance. On ChromiumOS, this technique is used via ureadahead to speed up boot times by up to 20%. Farfetchd is a general purpose D-Bus service that allows prefetch of application binaries and associated resources and allows the optimization of cold start times.
      This proposal aims to add support to farfetchd for:
      1) Tracing: via tracefs, tracing file pages that are fetched for an application during a given workload and standardized tracepoints.
      2) Replay: Using the collected trace, preload the specific pages from disk and measure improvements in application performance.

      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project8@chromium.org
      Beginner Bugs: https://issuetracker.google.com/issues/397539111, https://issuetracker.google.com/issues/397539767
      Requirements: C++, linux kernel, tracing, no TZ requirements

      ~~~~~~~~~~


      9. Develop fwupd plugin to handle touch firmware updates (Large Size)

      Project Description
      A number of components like storage devices use fwupd to handle firmware updates in ChromeOS. The platform inputs team would like to start using fwupd to manage and handle firmware updates for touch controllers. This project aims to build out a handful of fwupd plugins for touch controllers.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project9@chromium.org
      Beginner Bugs: https://issuetracker.google.com/issues/172340186, https://issuetracker.google.com/issues/397567795
      Requirements: C, C++

      ~~~~~~~~~~


      10. Debug WebUI For Tabstrip states (Medium Size)

      Project Description
      Tabstrip state and session states for browsers are complicated and rely on correct ordering of tabs in the tabstrip model, groups and sessions to restore tabs in the right position and selection state. As a result this has resulted in numerous bugs and it is often a pain point to figure out if it is a tabstrip model issue or a client issue. A webUI that captures the live state of the backend of the browser and tabstrip models will be really helpful in finding issues and edge cases.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project10@chromium.org
      Beginner Bugs: WebUI TabStrip Bug, Print Preview + Tab Dragging Bug, Keyboard Shortcut Bug
      Requirements: Familiarity with HTML, CSS and JS, Proficiency in C++ codebases and concepts, Working from PST 9:00 AM - 5:00 PM hours

      ~~~~~~~~~~

      11. Improve Chromium Web Audio Testing (Medium Size)

      Project Description
      The audit.js WebAudio test helper library was introduced in 2016 and is currently used by over 300 test files in Chromium’s blink/web_test directory. This library was initially created to run tests sequentially with callback/promise support and to provide utility functions for audio-specific assertions. However, the W3C Test Harness (testharness.js) offers superior support for test runners and assertions, rendering many features in audit.js redundant. Removing audit.js from Chromium would reduce resource consumption in the Chrome team’s Continuous Integration Infrastructure and allow us to leverage the well-supported and well-maintained W3C Test Harness library.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project11@chromium.org
      Beginner Bugs: crbug.com/396477778
      Requirements: Proficiency in JavaScript (reading, writing, debugging), Familiarity with HTML and CSS

      ~~~~~~~~~~
      12. ChromeStatus Search UI Enhancement Project (Medium Size)

      Project Description
      Enhance the search functionality on chromestatus.com and webstatus.dev with several new auto-complete details.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project12@chromium.org
      Beginner Bugs: https://github.com/GoogleChrome/chromium-dashboard/issues?q=is%3Aissue%20state%3Aopen%20label%3Agoodfirstbug
      Requirements: HTML, CSS, TypeScript, Python, Go, Web components, UX / Usability

      ~~~~~~~~~~


      13. Enhancing the webstatus.dev User Experience (Medium Size)


      Project Description
      webstatus.dev is a valuable resource for web developers, providing up-to-date information on the browser compatibility of various web technologies. This project aims to enhance the user experience of webstatus.dev by making it more accessible and user-friendly across a wider range of devices and user preferences, with a particular focus on mobile devices and dark mode support. The project will involve redesigning key pages (overview, feature detail, and stats) for optimal mobile viewing, implementing a comprehensive dark theme, and ensuring compatibility with the Lit framework, Google Charts, and Shoelace components.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project13@chromium.org
      Beginner Bugs: https://github.com/GoogleChrome/chromium-dashboard/issues?q=is%3Aissue%20state%3Aopen%20label%3Agoodfirstbug
      Requirements: Lit, TypeScript, HTML, CSS Google Charts, Shoelace Web Test Runner (for unit tests) Playwright (for E2E testing), Docker (for local environment provisioning)

      ~~~~~~~~~~


      14. WebGPU Texel Buffers (Medium Size)


      Project Description
      WebGPU is a new graphics API, shipped in Chromium in 2023, that brings modern GPU capabilities to the web. The goal of this project is to implement new functionality in Chromium to further advance the capabilities of WebGPU, allowing a broader range of applications to target the API. Specifically, the "texel buffers" feature has already been proposed at the W3C standards committee, and the next steps are to prototype the functionality so that we can get feedback from real users and push the standardization process forwards.
      Proposal Doc for more information: Link
      Location to ask project specific questions: here (preferred), chromium-gsoc-project14@chromium.org
      Beginner Bugs: crbug.com/42250870 and crbug.com/42250968
      Requirements: C++ proficiency, timezone compatible with North American mentors


          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/chromium/
    idea_list_url: https://docs.google.com/presentation/d/1ozDiULkf2Gi4HH1XA_Ad9O7rQpP9SP0yPEs_dpcBZKM/edit?usp=sharing

  - organization_id: 25
    organization_name: CircuitVerse.org
    no_of_ideas: 7
    ideas_content: |
      
      Project 1: Circuit Management & Performance Enhancement
      Duration: 175 hours
      Difficulty: Medium
      Technologies: Ruby on Rails, JavaScript
      This project focuses on refining the organizational structure and performance of the CircuitVerse platform. The objective is to enhance user efficiency by introducing a systematic approach to circuit management and optimizing backend operations. Participants will implement a folder-based system for subcircuits, enabling users to categorize their work effectively. Additionally, the project includes developing a feature for circuits with group-specific visibility, ensuring accessibility within designated teams while maintaining privacy. Performance improvements will involve optimizing Ruby on Rails N+1 queries. The scope also extends to creating a comprehensive circuits explore page with search functionality, integrating a leaderboard for weekly contests, and enabling locale switching from the homepage to improve accessibility.
      Learning Path:
      Acquire proficiency in Ruby on Rails through The Odin Project’s Full-Stack Ruby on Rails Course, covering foundational and advanced concepts.
      Enhance Rails expertise with Pragmatic Studio’s Rails Course for structured, practical training.
      Develop JavaScript skills using JavaScript.info to support frontend enhancements.
      Possible Mentors: Vaibhav Upreti, Smriti Garg

      ~~~~~~~~~~
      Project 2: Desktop Application & Vue Frontend Updates
      Duration: 175 hours
      Difficulty: Medium
      Technologies: VueJS, Ruby on Rails, TypeScript, JavaScript
      This project aims to extend CircuitVerse’s reach by improving our lightweight desktop application using Tauri, ensuring efficient performance across operating systems, and establishing an automated release pipeline for streamlined deployment. Participants will refactor web-based components into reusable, platform-agnostic units suitable for both web and desktop environments, optimizing resource usage for desktop contexts. A reliable update mechanism will be implemented to deliver future improvements seamlessly. The project also entails modernizing the frontend by converting the JavaScript codebase to TypeScript, replacing jQuery with Vue’s reactivity system, and addressing issues in the Verilog module and layout rendering. Enhancements to the TestBench will improve usability and output compatibility, supported by thorough testing with Vitest. Synchronization with the main CircuitVerse repository will ensure consistency across platforms.
      Learning Path:
      Gain expertise in VueJS via the Vue.js Official Guide for reactive frontend development.
      Study TypeScript fundamentals at TypeScript Documentation to facilitate codebase migration.
      Learn Tauri application development through its Official Docs.
      Prepare for testing with Vitest Documentation covering unit, integration, and end-to-end tests.
      Possible Mentors: Vedant Jain, Aryann Dwivedi, Niladri Adhikary

      ~~~~~~~~~~
      Project 3: Migrate to View Components & Improve Search Experience
      Duration: 175 hours
      Difficulty: Easy
      Technologies: HTML, CSS, JavaScript, Figma, Ruby on Rails
      This project focuses on modernizing CircuitVerse’s technical foundation and enhancing the user experience. Participants will migrate UI elements to ViewComponents for better maintainability and scalability. Responsive design principles will be applied to ensure compatibility across devices. Candidates will also utilize Figma to design and implement UI improvements while refactoring CSS to reduce redundancy and effectively use grid and flexbox with Bootstrap utility classes.
      Additionally, the project includes an initiative to improve the search experience within CircuitVerse. This will involve analyzing existing search functionality, optimizing query performance, and enhancing the UI/UX to make search results more intuitive and accessible.
      Learning Path:
      Build foundational skills in HTML, CSS, and JavaScript with FreeCodeCamp’s Responsive Web Design.
      Learn UI/UX design with Figma through Figma’s Official Tutorials.
      Study Rails ViewComponents at ViewComponent Docs and Hotwire via Hotwire Docs.
      Possible Mentors: Aman Asrani, Siddhant-K-code

      ~~~~~~~~~~
      Project 4: Assignment Suite Enhancement
      Duration: 175 hours
      Difficulty: Easy
      Technologies: Ruby on Rails, JavaScript
      This project seeks to advance CircuitVerse’s educational tools by enhancing classroom and assignment management capabilities. The scope includes developing a multi-level classroom structure, allowing students to form subgroups for collaborative projects, and creating a flexible assignment management system for both individual and group submissions. Participants will introduce features such as pre-built circuit submissions with integrated test cases, incorporate auto-verification from practice sessions, and refine the assignment submission process for efficiency. Integration with Canvas LMS will be improved to strengthen CircuitVerse’s utility in academic settings, supporting educators and students with robust, user-friendly tools.
      Learning Path:
      Develop Rails proficiency with The Odin Project’s Ruby on Rails Course.
      Enhance JavaScript knowledge at MDN Web Docs for dynamic functionality.
      Review Canvas LMS integration through its Developer Docs.
      Learning tools interoperability (LTI)[https://en.wikipedia.org/wiki/Learning_Tools_Interoperability]
      Possible Mentors: Aman Asrani, Siddharth Asthana, Yashika Jotwani

      ~~~~~~~~~~
      Project 5: Enhanced Verilog Support & Stability
      Duration: 175 hours
      Difficulty: Hard
      Technologies: JavaScript, Canvas
      This project targets the enhancement of CircuitVerse’s Verilog module and overall simulator stability. The goal is to refine the Verilog interface, making it more intuitive and enabling users to generate, view, edit, and test circuits comprehensively. Detailed documentation will accompany these improvements to assist users. Additional enhancements include adding play/pause functionality to simulations, implementing a full-screen view for the Boolean Logic Table This project requires a strong focus on technical precision to strengthen a critical component of the platform.
      Learning Path:
      Master JavaScript for simulation logic with Eloquent JavaScript.
      Learn Canvas rendering techniques at MDN Canvas Tutorial.
      Possible Mentors: Vedant Jain, Niladri Adhikary, Josh Varga

      ~~~~~~~~~~
      Project 6: Open Hardware Component Library
      Duration: 90 hours
      Difficulty: Hard
      Technologies: JavaScript, Ruby on Rails
      This project aims to expand CircuitVerse’s digital component library and introduce hardware integration capabilities. Participants will enrich the platform by adding components such as shift registers, sensors, and counters, broadening its appeal to hardware-focused users. The initiative also involves enabling serial device connectivity to facilitate interaction with physical hardware. Though shorter in duration, this project demands a high level of technical skill and innovation to advance CircuitVerse’s utility in hardware education.
      Learning Path:
      Strengthen Rails knowledge with Rails Guides.
      Study hardware integration concepts via Arduino Tutorials.
      Deepen JavaScript proficiency at You Don’t Know JS.
      Possible Mentors: Smriti Garg
      ~~~~~~~~~~
      Project 7: Flutter Upgrade
      Duration: 90 hours
      Difficulty: Easy
      Technologies: Dart, JavaScript
      This project focuses on updating the CircuitVerse mobile application to ensure compatibility with the latest Flutter framework. Participants will upgrade the app to the current Flutter version, optimizing its performance, and implement circuit embedding functionality to enhance mobile usability. This concise project offers an opportunity to contribute to the platform’s mobile presence with a focus on modern development practices.
      Learning Path:
      Learn Flutter development at Flutter Official Docs.
      Study Dart essentials with the Dart Language Tour.
      Review JavaScript fundamentals at MDN JavaScript Guide.
      

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/circuitverse.org/
    idea_list_url: https://github.com/CircuitVerse/CircuitVerse/wiki/GSoC'25-Project-List


  - organization_id: 26
    organization_name: CloudCV
    no_of_ideas: 3
    ideas_content: |
     
      
      Enhanced Test Suite and Improved User Experience
      SQL Django AngularJS AWS
      This project is focused on significantly improving EvalAI’s usability by enhancing exsiting comprehensive test suite alongside a series of user experience enhancements. By increasing our test coverage, automating critical workflows, and refining the platform’s interface and documentation, this initiative aims to create a more robust, user-friendly, and resilient environment for both challenge hosts and participants.
      The enhanced test suite will ensure that all core functionalities, from challenge creation to submission processing are verified, reducing bugs and increasing system reliability. In parallel, targeted user experience improvements will simplify navigation, enhance error reporting, and streamline user interactions, leading to a more intuitive and supportive EvalAI ecosystem.
      Project Size: Medium (175 hours)
      Difficulty Rating: Medium
      Participate in the issue corresponding to this project to get started
      Enhanced Test Suite and Improved User Experience together with gautamjajoo Yes, let's do it

      ~~~~~~~~~~
      Mitigating Biases & Prompt Effects in Vision-Language Models
      Python PyTorch/TensorFlow NLP Vision-Language Models LLMs Bias and Fairness in AI
      This research project aims to investigate how prompt engineering influences the behaviour and outputs of Vision-Language Models (VLLMs), with a particular focus on the emergence and amplification of biases. By systematically studying the relationship between prompt formulations and model responses, the project seeks to uncover the mechanisms through which biases are introduced and propose effective mitigation strategies.
      Project Size: Medium (175 hours)
      Difficulty Rating: Medium to Advanced
      Sorry but currently we don't have any mentoring project ...

      ~~~~~~~~~~
      RAG-Based Chat Bot for Enhanced Challenge Support
      Python Natural Language Processing (NLP) Machine Learning Retrieval Augmented Generation (RAG) Django SQL
      This project aims to enhance the user experience for both challenge hosts and participants by developing an intelligent, RAG (Retrieval Augmented Generation) based chatbot. The chatbot will efficiently address queries related to challenge hosting, guidelines, troubleshooting, and FAQs. By integrating state-of-the-art NLP techniques with robust retrieval mechanisms, the solution will ensure prompt, accurate, and context-aware responses that reduce support overhead and streamline communication.
      Using the RAG approach, the chatbot will retrieve relevant information from challenge documentation and combine it with generative models to create coherent and helpful answers. This will empower hosts to manage challenges more effectively and assist participants in resolving queries, ultimately contributing to a smoother and more interactive challenge experience.
      Project Size: Medium (175 hours)
      Difficulty Rating: Medium
      Participate in the issue corresponding to this project to get started
      RAG-Based Chat Bot for Enhanced Challenge Support together with gautamjajoo Yes, let's do it
      

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cloudcv/
    idea_list_url: https://gsoc.cloudcv.org/


  - organization_id: 27
    organization_name: D Language Foundation
    no_of_ideas: 4
    ideas_content: |
      Translate DRuntime Hooks to Templates
      Mentor: Teodor Duțu

      Spec	Details
      Difficulty Level	hard
      Project Duration	350 hours
      Number of Contributors	2
      Prerequisites	Familiarity with compilers, build systems; low-level programming
      Description
      High-level language constructs are often compiled into lower-level function calls that implement the same functionality. This process is called lowering It simplifies compiler design by offloading complex code into simpler function calls. These functions are implemented in D’s runtime library (called DRuntime) and are commonly called runtime hooks.

      Below is an example of such a runtime hook:

      struct S { ... }

      S[3] a, b;

      // Original code
      b = a;  // b is a copy of a 

      // Resulting DRuntime hook
      _d_arrayassign_l(b, a)  // copies a into b

      _d_arrayassign_l handles array (re)allocation and assignment operators according to the types of a and b (S[3]) to simplify the compiler’s work.

      As shown in the example above, runtime hooks require type information, such as which assignment operator to call and the arrays’ sizes. D supports templates, but runtime hooks pre-date the introduction of templates to D. Therefore, they retrieve the necessary type information at runtime by receiving an extra argument, whose type is TypeInfo. This object contains the size, constructors, destructors and overloaded operators of a given type. However, because this information is processed at run time, this approach is slower than the alternative of having the compiler send it to the runtime hook via template arguments. Due to D’s high flexibility regarding metaprogramming, translating each hook to a template function would allow its code to specialise according to the types of its arguments at compile time.

      In general, these are the steps required to convert a runtime hook to a template:

      Implement a new template version of the hook in DRuntime.
      Change the lowering in the compiler to use the new hook.
      Run a benchmark to measure the increase in performance generated by using the new hook.
      Remove the old hook from DRuntime.
      The hooks that are yet to be templated can be split into 2 categories:

      rt/aa.d implements associative arrays as language builtins. This module is made up of multiple hooks, all of them using TypeInfo. Therefore, this module is to be reimplemented using templates. Associative arrays are defined as an opaque structure called Impl. It receives a TypeInfo argument in its constructor. This structure is accessed via the runtime hooks listed below. The plan for this direction is the following:
      Template the hooks below and temporarily extract the TypeInfo structure required by Impl from the template arguments using typeid. Each hook will require changes to DRuntime and the compiler.
      Template the Impl structure and modify the previously templated hooks to use the template arguments itself instead of TypeInfo.
      The list of hooks for associative arrays is:

      _aaApply
      _aaApply2
      _aaDelX
      _aaEqual
      _aaGetRvalueX
      _aaGetX
      _aaInX
      _aaLen
      _d_assocarrayliteralTX

      Somewhat more independent hooks, which still have interdependencies. They are mostly implemented in rt/lifetime.d. A goal of this project is to remove this file and replace all its code with templated implementations. Each hook can be handled individually and separately. There was previous work on _d_arrayliteralTX so it might be a good starting point. Another promising starting point are _d_arrayset{capacity,lengthT,lengthiT} or _d_arrayappendcTX. The latter three already have wrapper template hooks that call the functions from rt/lifetime.d. What is needed in their case is to fully move the underlying implementation to the template hooks. The full list of hooks is below:

      _d_arraysetcapacity
      _d_arraysetlengthiT
      _d_arraysetlengthT
      _d_arrayshrinkfit
      _d_arrayappendcTX
      _d_arrayliteralTX
      _d_interface_cast
      _d_isbaseof
      _d_isbaseof2
      _adEq2

      Resources
      Initial project proposal and discussion
      PRs converting some of the DRuntime hooks to templates
      Weekly reports regarding the earlier work
      DConf presentations from 2022 and 2024 on the work on DRuntime hooks
      Instructions on how to build the reference compiler - DMD

      ~~~~~~~~~~

      Separate Semantic Routines From AST Nodes
      Mentor: Razvan Nitu

      Spec	Details
      Difficulty Level	easy-medium
      Project Duration	175 hours
      Number of Contributors	1
      Prerequisites	Familiarity with compiler organization, visitor pattern, object oriented programming
      Description
      In the DMD compiler codebase, AST nodes are defined as classes within various files. The ideal structure for these nodes is to have minimal fields and methods focused solely on field queries. However, the current state of the DMD frontend deviates from this ideal. AST nodes are laden with numerous methods that either perform or are dependent on semantic analysis. Furthermore, many AST node files contain free functions related to semantic analysis. Our objective is to decouple AST nodes from these functions.

      How to start working on this project
      Clone the compile repository - check this guideline.
      Choose an AST node file: start by selecting a file from this list of AST node definition files.
      Examine Imports: open your chosen file and scrutinize the top-level imports.
      Isolate semantic imports: temporarily comment out one of the imports that includes semantic routines, particularly those ending in sem (e.g., dsymbolsem, expressionsem, etc.).
      Build and identify dependencies: compile DMD and observe any unresolved symbols that emerge.
      Relocate functions: shift the functions reliant on the unresolved symbols to the semantic file where the import was commented out.
      Move and test a function: select a function for relocation and ensure it functions correctly in its new location.
      Submit a Pull Request: Once you’re satisfied with the changes, create a PR that follows the guidelines.
      Check this PR for an illustration of the above steps.

      Sometimes, more intricate solutions are required. For instance, if an overridden method in an AST node calls a semantic function, it can’t be simply relocated. In these cases, using a visitor to collate all overrides, along with the original method, into the appropriate semantic file is the way forward. A notable instance of this approach is detailed in this pull request.

      Other complex scenarios may arise, especially when dealing with AST nodes that interact with the backend. Finding solutions to those will be the fun part of the project.

      This project helps advance the development of the compiler library by creating a clear separation between compilation phases.

      This project is ideal for someone that has no prior experience with real-life compilers but wants to start by doing valuable work.

      Resources
      Compiler codebase
      List of files that need to have semantic separated out of them
      How to start
      Building the compiler

      ~~~~~~~~~~

      Performance Regression Publisher
      Mentor: Dennis Korpel

      Spec	Details
      Difficulty Level	easy-medium
      Project Duration	175 hours
      Number of Contributors	1
      Prerequisites	Github actions, webdev, performance testing
      Description
      The D compiler currently does not have an automated performance regression test. Oftentimes pull requests that claim to improve compiler performance are being made (be it spatial or temporal). However, it’s up to the reviewer to actually believe the committer or to test things on their own. To make things simpler and more transparent, we want to implement a bot that monitors all the pull requests made to the compiler codebase and analyzes the compiler’s performance with and without the pull request.

      The list of stats should include, but not be limited to:

      size of some predefined binaries (like a “hello world” program)
      compile time of popular projects
      compiler size
      runtime of test suite
      Adding more performance tests, such as stress tests also falls under the scope of this project. Ideally the bot could also store a history of performance regressions within a web page.

      Project milestones:
      Analyze the best way to publish the results: it could be a GitHub action, a bot that sends data to a website. Depending on your skills and preferences, we expect you propose something and toghether we will decide what’s best.
      Implement the initial part that simply collects how long running the testing pipeline took and publishes the result.
      Decide on other metrics that need to be collected and implement them.
      Add more stress tests to the compiler testing suite.
      Resources
      Compiler pull request queue

      ~~~~~~~~~~
      Json Library
      Mentor: Adam Wilson

      Spec	Details
      Difficulty Level	medium
      Project Duration	175 hours
      Number of Contributors	1
      Prerequisites	json, parsers, object oriented programming
      Description
      D currently does not have a json parser integrated in the standard library and it is the year 2025. There are 3rd party libraries that implement some pieces to a certain extent, however they are not at industry level requirements. The purpose of this project is to create a json library that offers all the facilities required for working with json objects. The goal is to eventually integrate it in the standard library, however, an initial step is to publish as a dub (D package manager) package.

      Although there is no json library in the D ecosystem, there are multiple json parser implementations. We can start by picking up a notable json parser as a basis for our implementation. Next, we can built higher level functionalities on top of the existing parser.

      This project is of high priority and impact, as json objects essentially rule the world.

      Project Milestones
      Initial Package Setup and Porting from jsoniopipe.
      Object Model Design/Implementation.
      Object Model Serialization/Deserialization
      Streaming Serialization/Deserialization
      Stretch Goal: Object Serialization/Deserialization (Direct serialization/deserialization from a D object instead of using the object model.)
      Resources
      jsoniopipe

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/d-language-foundation/
    idea_list_url: https://dlang.github.io/GSoC/gsoc-2025/project-ideas.html


  - organization_id: 28
    organization_name: DBpedia
    no_of_ideas: 5
    ideas_content: |
      Towards Amharic DBpedia
      Description
      DBpedia is a collaborative initiative focused on extracting structured information from Wikipedia and presenting it as Linked Open Data. This is a continuation of GSoC 2024. In GSoC 2024, we successfully integrated Amharic parsers and extractors into the DBpedia chapter. However, due to time constraints, we could not add sufficient mappings to extract data from Wikipedia. This year, we plan to add more new mappings, build a robust landing page for presentation, and clean the existing data.

      Goal
      The primary goal of this project is to enhance the Amharic DBpedia chapter 3:

      Extend the existing Amharic DBpedia chapter in the DBpedia knowledge graph with data from Amharic Wikipedia.
      Add additional mappings.
      Extend the DBpedia extraction framework to extract citations, disambiguation, personal data, topical concepts, anchor text, and shared resources from Amharic Wikipedia.
      Create an automatic extraction framework and mapping.
      Make the knowledge graph available to end users via a web page.
      Create documentation for processes, tools, and techniques used for sustainable development, following FAIR principles.
      Impact
      Enable users to access and utilize structured data in Amharic DBpedia more effectively.
      This will promote linguistic diversity and support research, education, and applications that rely on multilingual knowledge graphs.
      NLP downstream tasks: Apply knowledge graphs from DBpedia to NLP applications such as machine translation and sentiment analysis.
      Community engagement: Encourage the community to contribute and collaborate in sustaining and expanding Amharic DBpedia.
      Warm-up tasks
      Please read the following papers:
      GitHub Repository 6

      Amharic Wikipedia 1
      Arabic DBpedia 1
      Korean DBpedia
      German DBpedia 1
      Skills Required
      A good understanding of Java and Python
      Optionally, good knowledge of SPARQL, RDF, and other Semantic Web technologies
      Good documentation and communication skills
      Project Size
      350 hours

      Mentors
      Hizkiel Alemayehu 3

      Tilahun Tafa 2

      Ricardo Usbeck 1

      Keywords
      Amharic DBpedia, Semantic Web, Extraction Framework

      ~~~~~~~~~~

      DBpedia Hindi Chapter - GSoC 2025

      DBpedia is an open knowledge graph in continuous evolution. Unlike Wikidata, where the RDF content is directly edited as a wiki, DBpedia relies strictly on Wikipedia, meaning that every single triple in DBpedia — except for ontology statements — can be traced back to some infobox, sentence or table cell in Wikipedia.

      The graph exposed at the root domain of DBpedia is derived solely from English Wikipedia (e.g. https://dbpedia.org/page/India 4). Purpose of this project is to create a graph derived solely from Hindi Wikipedia. Methods to generate triples rely on the Extraction Framework 15 6 for infobox extraction or through novel NLP-based approaches such as the Neural Extraction Framework 2. Unfortunately, the latter approach only supports the English language. We thus welcome NLP and/or LLM-based solutions to target multilingual text. We have proposed the first edition on DBpedia Hindi Chapter in 2024 GSoC proposal for Configuring different extractors for hindi in DBpedia extraction framework and included a neural pipeline for extracting tuples directly from hindi wiki. In this proposal we are extending the first edition of DBpedia Hindi Chapter with extended goals.

      Goal
      Extending the DBpedia Chapter in Hindi language to be reached at hi.dbpedia.org 4. In particular:

      Create the knowledge graph with data from Hindi Wikipedia 6by including more indic neural extractor. It aims for extracting information in the form of relational triples(subject → predicate → object) from unstructured text in hindi Wikipedia articles that can be added to the DBpedia knowledge base
      Automating the knowledge graph with the availability of new LLMs by creating Indic embeddings, so that missing links can be generated automatically.
      Create a SPARQL endpoint to make it queryable.
      Material
      See Warm-up tasks.

      Project size
      This project is medium-sized (175 hours).

      Impact
      Cultural and Educational Enrichment: Empower Hindi-speaking users with culturally relevant and easily accessible knowledge, fostering educational enrichment and linguistic inclusivity.
      Semantic Search and NLP Applications: Enable advanced semantic search and natural language processing (NLP) applications in Hindi, opening avenues for innovation in information retrieval and analysis.
      Community Engagement: Encourage community contributions, feedback, and collaboration in maintaining and expanding the Hindi ontology, ensuring continuous improvement and relevance.
      In summary, this project seeks to contribute significantly to linguistic diversity in the semantic web domain by extending the DBpedia ontology to Hindi, promoting a more inclusive and accessible knowledge landscape
      for Hindi-speaking users.

      Warm-up tasks
      Please read carefully our overview on creating new DBpedia Chapters 11.
      Read the paper Internationalization of Linked Data: The case of the Greek DBpedia edition 7 by Kontokostas et al.
      Learn about the DBpedia Extraction Framework 6, the software used to transform Wikipedia infobox data into RDF triples.
      Check the mapping in Hindi of the DBpedia ontology 8 and Indic embeddings.
      Go through the list of current chapters can be found at this address 7 to get an idea of how they are structured.
      Get familiar with SPARQL on the DBpedia endpoint 7.
      Run a local DBpedia Virtuoso endpoint 5.
      Mentors
      Sanju Tiwari (@tiwarisanju18), Debarghya Dutta, Ananya, Ronak Panchal


      ~~~~~~~~~~

      This project started in 2021 and is looking to its 5th participation in DBpedia’s GSoC.

      Description
      Every Wikipedia article links to a number of other articles. In DBpedia, we keep track of these links through the dbo:wikiPageWikiLink property. Thanks to them, we know that the :Berlin_Wall 3 entity (at the time of writing this) is semantically connected to 299 base entities.

      However, only 9 out of 299 base entities are linked from :Berlin_Wall via also another predicate. This suggests that in the large majority of cases, it is not clear what kind of relationship exists between the entities. In other words, DBpedia does not know what specific RDF predicate links the subject (in our case, :Berlin_Wall) to any of the objects above.

      Currently, such relationships are extracted from tables and the infobox (usually found top right of a Wikipedia article) via the Extraction Framework 4. Instead of extracting RDF triples from semi-structured data only, we want to leverage information found in the entirety of a Wikipedia article, including page text.

      wiki-meme
      wiki-meme
      500×683 108 KB
      The repository where all source code will be stored is the following:


      GitHub

      GitHub - dbpedia/neural-extraction-framework: Repository for the GSoC project... 32
      Repository for the GSoC project 'Towards a Neural Extraction Framework' - dbpedia/neural-extraction-framework

      Goal
      The goal of this project is to develop a framework for predicate resolution of wiki links among entities.

      During GSoC 2022, we employed a suite of machine-learning models 7 to perform joint entity-relation extraction on open-domain text.
      During GSoC 2023, we implemented an end-to-end system 4 that translates any English sentence into triples using the DBpedia vocabulary.
      Last year, we improved the quality of output triples using a chain-of-thought approach powered by a large language model.
      However, the current algorithm still has the following issues. Now, we want to devise a method that can solve as many of them as possible.

      When an RDF property representing the predicate is not found, our algorithm cannot make any suggestions for the creation of a new property.
      The current models are not efficient enough to scale to millions of entities.
      The extracted relations are not categorised with respect to their semantics (e.g. reflexive/irreflexive, symmetric/antisymmetric/asymmetric, transitive, equivalence).
      The generated triples were not validated against the DBpedia ontology and may thus lead to inconsistencies in data.
      Our algorithm should be able to adapt its output not only to the DBpedia vocabulary but to any specified one (e.g., SKOS, schema.org, Wikidata, RDFS, or even a combination of many).
      Extraction examples
      The current pipeline targets relationships that are explicitly mentioned in the text. The contributor may also choose to extract complex relationships, such as:

      Causality. (Addressed during GSoC 2021, but not completed.) The direct cause-effect between events, e.g., from the text
      The Peaceful Revolution (German: Friedliche Revolution) was the process of sociopolitical change that led to the opening of East Germany’s borders with the west, the end of the Socialist Unity Party of Germany (SED) in the German Democratic Republic (GDR or East Germany) and the transition to a parliamentary democracy, which enabled the reunification of Germany in October 1990.

      extract: :Peaceful_Revolution –––dbo:effect––> :German_reunification

      Issuance. An abstract entity assigned to some agent, e.g., from the text
      Messi won the award, his second consecutive Ballon d’Or victory.

      extract: :2010_FIFA_Ballon_d'Or –––dbo:recipient––> :Lionel_Messi 1

      Material
      The contributor may use any Python deep learning framework and/or existing tool. The following resources are recommended (but not compulsory) for use in the project.

      The project repository linked above and the machine-learning models mentioned in the readme files found in each GSoC folder.
      Last year’s 9 and the 2023 blog 4 to understand the project status quo.
      Python Wikipedia 1 makes it easy to access and parse data from Wikipedia.
      Huggingface Transformers for Natural Language Inference 1 can be extremely useful to extract structured knowledge from text or perform zero-shot classification.
      DBpedia Lookup is a service available both online and offline (e.g., given a string, list all entities that may refer to it).
      DBpedia Anchor text is a dataset containing the text and the URL of all links in Wikipedia; the indexed dataset will be available to the student (e.g., given an entity, list all strings that point to it).
      An example of an excellent proposal 11 that was accepted a few years ago.
      Project size
      The size of this project can be either medium or large. Please state in your proposal the number of total project hours you intend to dedicate to it (175 or 350).

      Impact
      This project will potentially generate millions of new statements. This new information could be released by DBpedia to the public as part of a new dataset. The creation of a neural extraction framework could introduce the use of robust parsers for a more accurate extraction of Wikipedia content.

      Warm-up tasks
      Get familiar with SPARQL on the DBpedia endpoint 8.
      Understand the science behind relation extraction 4.
      Run and understand the pipeline implemented last year 8.
      Mentors
      @tsoru, @zoelevert, TBD

      ~~~~~~~~~~

      Containerized Installers for Data-centric Services using Databus Collections
      Project Description:
      This GSoC project aims to develop containerized installers for data-centric services utilizing Databus collections. Databus collections provide a framework for managing and sharing datasets across distributed systems, offering versioning, replication, and access control features.

      One exemplary application of this project is integrating Databus collections with the Virtuoso Open-Source triple store, a widely used RDF service. This integration enables seamless deployment and loading of RDF datasets into Virtuoso instances within containerized environments.

      Additionally, the project entails both designing and documenting best practices for deploying other Databus-driven services, along with implementing more deployment-ready containers. These containers will encapsulate the necessary components for pulling data from Databus collections and installing them with associated services, ensuring ease of deployment and scalability.

      Furthermore, the project may explore integration options with the Databus frontend or even metadata, enhancing discoverability and interoperability of the deployed services within the Databus ecosystem.

      Key Objectives:
      Integrate Databus collections with the Virtuoso Open-Source Triple Store as a first use case. This can be done by building upon the Virtuoso Quickstarter repository (GitHub - dbpedia/virtuoso-sparql-endpoint-quickstart: creates a docker image with Virtuoso preloaded with the latest DBpedia dataset)

      Design and document best practices for deploying Databus-driven services.

      Implement 4-5 deployment-ready containers for data-centric services utilizing Databus collections. Services could, for instance, be chosen from a list of Semantic Web applications and services here: GitHub - semantalytics/awesome-semantic-web: A curated list of various semantic web and linked data resources.

      Explore integration possibilities with the Databus frontend or metadata systems for enhanced functionality and interoperability.

      Expected Outcome:
      A well-documented Databus-driven Virtuoso Quickstarter container that focuses on ease of deployment.

      Documentation outlining best practices and guidelines for implementing, deploying and managing Databus-driven services.

      4-5 Containerized installers for deploying data-centric services leveraging Databus collections.

      Design proposal for integration of these services with the Databus frontend.

      [Optional] integration with Databus frontend or even metadata for improved discoverability and usability.

      Skills Required:
      A good understanding of SPARQL, RDF and other Semantic Web technologies

      Some proficiency in containerization technologies (e.g., Docker, Kubernetes).

      Knowledge of the core concepts of the DBpedia Databus (see Overview | Databus Gitbook 4)

      Good documentation and communication skills

      Project Size:
      Estimated anywhere between 90 to 180 hours, depending on expertise and number of tackled tasks.

      ~~~~~~~~~~

      Automatically adding Wikimedia Dumps on the Databus — GSoC 2025


      Project Description:
      Wikimedia publishes their dumps via https://dumps.wikimedia.org 6 . At the moment, these dumps are described via HTML, so the HTML serves as the metadata and it is required to parse to identify whether new dumps are available. To automate retrieval of data, the Databus (and also MOSS as its extension Databus and MOSS – Search over a flexible, multi-domain, multi-repository metadata catalog 5 ) has a metadata knowledge graph, where one can do queries like “check whether a new version of x is available”. Since DBpedia uses the dumps to create knowledge graphs, it would be good to put the download links for the dumps and the metadata on the Databus.

      Key Objectives

      Build a docker image that we can run daily on our infrastructure to crawl dumps.wikimedia.org 3 and identify all new finished dumps, then add a new record on the Databus.
      Goal is to allow checking for new dumps via SPARQL. go to OIDC Form_Post Response 2 , then example queries, then “Latest version of artifact”.
      this would help us to 1. track new releases from wikimedia, so the core team and the community can more systematically convert them to RDF as well as to 2. build more solid applications on top, i.e. DIEF or other
      process wise I would think that having an early prototype is necessary and then plan iterations from this.
      Skills
      The task is not very complex per se, but requires some experience in executing a software project. This includes a clean project setup, good code, tests and also a simple, but well-thought out process. (simple because it will be more robust and maintainable than sth. complicated). We would prefer coding in scala, but python or other would also be ok. Some devOp skills are required to produce good docker (swarm), but they can be learned during the project as well.

      Size
      120 to 180 hours to do it properly. I would estimate that the final deployment also takes a week to make it effective and thoroughly evaluate that the final result is working well.









          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/dbpedia/
    idea_list_url: https://forum.dbpedia.org/tag/gsoc2025-ideas

  

  - organization_id: 29
    organization_name: Dart
    no_of_ideas: 5
    ideas_content: |
      
      Idea: Exception testing for package:webcrypto
      Possible Mentor(s): jonasfj@google.com,
      Difficulty: Hard
      Project size: Large (350 hours)
      Skills: Dart, FFI, JS
      Description: package:webcrypto (github.com/google/webcrypto.dart) is a cross-platform implementation of the Web Cryptography API. It is important that it behaves the same way whether it's running on Windows, Linux, Mac, Android, iOS, Chrome, Firefox, or Safari. Towards that end, it has a lot of test cases. We could and should probably make more test cases. But we should also test that it throws the types of exceptions when given incorrect parameters. This probably needs a small test framework to ensure good test coverage.
      We expect a proposal for this project to include:
      A sample showing how to test exceptions for RsaPssPrivateKey.generateKey. Ideally, the sample project includes parts of a generalized framework for testing exceptions.
      An outline of what kind of exceptions should be tested?
      A design for extending TestRunner, or creating a new framework, to test exceptions thrown by all methods.
      Illustrative code for how test cases would be configured
      Pros and cons of the design (especially when multiple choices are available)
      Timeline for the project
      Good Sample Project: Write a test cases that tests the different kinds of errors and exceptions that can be thrown by RsaPssPrivateKey.generateKey, run the tests across desktop, Chrome and Firefox. Consider extending the tests to cover all members of RsaPssPrivateKey. Try to generalize these test cases to avoid repetitive code, see the existing TestRunner for inspiration.
      Expected outcome: PRs that land in package:webcrypto and increases our confidence in correctness cross-platforms.
      
      
      ~~~~~~~~~~
      Idea: Use an LLM to translate Java/Kotlin tutorial snippets into Dart JNIgen code
      Possible Mentor(s): dacoharkes@google.com, yousefi@google.com
      Difficulty: Hard
      Project size: Large (350 hours)
      Skills: Dart, FFI, Java
      Description: This project will be very exploratory. We’ll explore how much is needed to make an LLM generate Dart snippets that call JNIgen-generated code. The snippets should be the equivalent of the original native code. How much will be needed? Is a single shot prompt enough? Or do we need to teach an AI how to run JNIgen and make it generate code that is subsequently analyzed with the Dart analyzer and the errors are fed back in to the AI to improve its answer.
      If we get this working, we’ll want to explore how to make such a tool useful to users. For example, we could make a browser extension that automatically adds the generated code snippets to documentation websites.
      Inspired by this issue: dart-lang/native#1240
      Good Sample Project:
      Get a Gemini API key https://ai.google.dev/gemini-api/docs/api-key
      Follow https://developers.google.com/learn/pathways/solution-ai-gemini-getting-started-dart-flutter
      Write a Dart script that invokes the API with a prompt containing a Java snippet (for example from https://developer.android.com/media/camera/camerax/take-photo#take_a_picture) and try to come up with a prompt that will make it generate code that would work on the Dart API generated with JNIgen for this Java/Kotlin API.
      Expected outcome: A tool for translating code samples usable by users.
      
      
      ~~~~~~~~~~
      Idea: package:coverage + LLM = test generation
      Possible Mentor(s): liama@google.com
      Difficulty: Medium
      Project size: Medium (175 hours)
      Skills: Dart, LLMs
      Description: This is a very experimental project. The idea is to use package:coverage to identify uncovered code, use an LLM to decide if that code needs a test (not all code actually needs to be tested), then use an LLM to write tests that hit those cases, and then use package:coverage to verify that those lines are covered.
      Good Sample Project:
      Get a Gemini API key https://ai.google.dev/gemini-api/docs/api-key
      Follow https://developers.google.com/learn/pathways/solution-ai-gemini-getting-started-dart-flutter
      Try generating tests for any old Dart API. Don't try to integrate package:coverage yet.
      Expected outcome: A package on pub.dev for increasing test coverage.
      
      
      ~~~~~~~~~~
      Idea: Secure Paste Custom Actions on iOS
      Possible Mentor(s): huanlin@google.com, jmccandless@google.com
      Difficulty: Hard
      Project size: Large (350 hours)
      Skills: Dart, Objective-C
      Description: Support custom action items for native edit menu on iOS. It's a pretty impactful project requested by many developers (main issue here: flutter/flutter#103163). This project is one of the key milestones: flutter/flutter#140184.
      Project:
      Prepare: Learn basic git commands; Setup flutter engine dev environment; Read style guide, etc;
      Design new dart API for custom items in context menu (Related API: https://api.flutter.dev/flutter/widgets/SystemContextMenu-class.html)
      Design engine <-> framework communication API using method channel
      Implement both framework part (in Dart) and engine part (in Objective-C)
      Go through code review process and land the solution
      The final product should allow developers to add custom items to the iOS native edit menu.
      Good Sample Project: ...
      Build a sample project in Flutter with a text field that shows custom actions in the context menu. (Hint: use https://docs.flutter.dev/release/breaking-changes/context-menus).
      Build a sample project in UIKit that shows custom actions in the native edit menu (Hint: use https://developer.apple.com/documentation/uikit/uieditmenuinteraction?language=objc). You can either use ObjC or Swift, but ObjC is preferred.
      Expected outcome: A PR merged in Flutter
      
      
      ~~~~~~~~~~
      Idea: TUI framework for dart
      Possible Mentor(s): mudit.somani00@gmail.com
      Difficulty: Medium
      Project size: Medium (175 hours)
      Skills: Dart, CLIs
      Description: Dart is already used to create GUI applications through Flutter, it would be great if it can also be used to develop good looking TUI applications. Currently the language of choice for TUI development would be either Golang or Python due to their developed package ecosystems (like charm or textual) so a package that makes TUI development easier and faster on dart would increase its adoption in that space.
      Project:
      Design composable methods to render components and text on the terminal
      Include popular components like inputs, checkboxes and tables by default
      Intuitive way to create your own custom components for the terminal
      Ensure library works with popular state management libraries in dart
      Good Sample Project:
      Composable methods to style text on the terminal (kinda like libgloss).
      Component based model to render and interact with terminal based text inputs and checkboxes (kinda like bubbles).
      Expected outcome: A package on pub.dev with terminal primitives like text styling, inputs, checkboxes, tables, layouts, spinners etc.
      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/dart/
    idea_list_url: https://github.com/dart-lang/sdk/blob/main/docs/gsoc/Dart-GSoC-2025-Project-Ideas.md


  - organization_id: 30
    organization_name: Data for the Common Good
    no_of_ideas: 9
    ideas_content: |
      
      Title Enhancing the SMART on FHIR Backend for Patient-Controlled Data Sharing
      Description The goal of this project is to improve and extend the backend of a SMART on FHIR application that empowers patients to autonomously access, share, and manage their electronic health records (EHR). The application serves as a data interoperability layer, allowing users to seamlessly transfer their healthcare data to meet various needs, such as research participation, second opinions, or personal health tracking.
      Expected Outcomes One or both of the following: Improving the integration with diverse FHIR servers and refining data transformation capabilities. Building APIs or plugins to support additional healthcare applications and third-party services.
      Skills Java, python
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating medium


      ~~~~~~~~~~
     
      Title Enhancing the SMART on FHIR Frontend for Patient-Controlled Data Sharing
      Description This project aims to improve the frontend user interface for a SMART on FHIR application that enables patients to autonomously access, manage, and share their electronic health records (EHR). The focus is on making the UI more intuitive, accessible, and user-friendly, ensuring a seamless experience for users connecting to the backend service.
      Expected Outcomes Improved User Experience (UX)
      Skills Javascript, Node
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating medium

      ~~~~~~~~~~
     
      Title Developing a FHIR Resource Tabular Viewer for Efficient Data Exploration
      Description This project aims to build a FHIR Resource Tabular Viewer, an application that transforms complex, nested FHIR data structures into an easy-to-navigate tabular format. This tool will allow users—such as researchers, clinicians, and developers—to efficiently search, filter, and analyze FHIR resources, improving accessibility and usability of healthcare data.
      Expected Outcomes Tabular Representation of FHIR Data and Search & Filtering Capabilities.
      Skills Python, Javascript
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating medium

      ~~~~~~~~~~
     
      Title Developing Custom Jupyter Notebooks for AVRO File Processing and QA/QC Analysis
      Description This project aims to create custom Jupyter notebooks that help users efficiently unpack AVRO files, perform quality assurance (QA) and quality control (QC) checks, and run basic data analyses. The goal is to provide a user-friendly, interactive environment where users can explore, validate, and analyze AVRO-formatted data without requiring deep expertise in data engineering.
      Expected Outcomes AVRO File Handling on startup, QA/QC Checks, Basic Data Analysis.
      Skills Python, networking
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating medium

      ~~~~~~~~~~
     
      Title Extending the HAPI FHIR Server for Enhanced Functionality and Interoperability
      Description This project aims to extend the HAPI FHIR Server, a leading open-source implementation of the FHIR standard, to improve its functionality, scalability, and interoperability. The enhancements will support advanced healthcare use cases, making it easier for developers and organizations to manage and exchange FHIR-compliant health data efficiently.
      Expected Outcomes Custom FHIR Operations & Extensions
      Skills Java, FHIR
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating medium

      ~~~~~~~~~~
     
      Title Developing a Translation Service to Connect GEARBOx API with mCODE Trial Matching Service
      Description This project aims to build a translation service that connects the GEARBOx API with the mCODE (Minimal Common Oncology Data Elements) trial matching service. The goal is to enable seamless translation of oncology data between GEARBOx and mCODE, allowing healthcare providers, researchers, and clinical trial platforms to effectively match patients to relevant clinical trials based on their mCODE-compliant health data.
      Expected Outcomes Data Mapping & Transformation, Interoperability & Validation
      Skills Python, Typescript
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating medium

      ~~~~~~~~~~
     
      Title Building a Chatbot for Generating GraphQL and Custom Queries for Cohort Descriptions
      Description This project aims to develop a chatbot powered by ChatGPT or another large language model (LLM) that allows users to describe a cohort of patients and automatically generates GraphQL queries or custom queries based on the input for the PCDC. The goal is to simplify the process of building complex queries for patient data by allowing users to interact with the chatbot in natural language, rather than navigating through a UI or manually searching for filters.
      Expected Outcomes GraphQL Query Generation
      Skills LLM, Javascript
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating hard

      ~~~~~~~~~~
     
      Title Developing a Cross-Platform App for User Consent and Data Sharing from Apple Health and CommonHealth Using React Native
      Description This project focuses on creating a cross-platform mobile app (iOS and Android) using React Native that allows users to consent and share their health data from both Apple Health and CommonHealth. The app will enable users to manage their data sharing preferences, securely transmit health information, and empower them to participate in research or share data with healthcare providers.
      Expected Outcomes Initial App version
      Skills React Native, Android, iOS
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating hard

      ~~~~~~~~~~
     
      Title Enhancing the Cohort Discovery Chatbot
      Description This project aims to enhance a cohort discovery chatbot by improving its accuracy, usability, and query generation capabilities. Enhancements will focus on refining natural language understanding (NLU), improving query accuracy, supporting more complex filters, and integrating feedback mechanisms to learn from user interactions.
      Expected Outcomes Improved chatbot accuracy in understanding and generating cohort queries
      Skills LLM, Javascript
      Mentors TBD - One of the Senior Developer in the team.
      Project Size 350 hours
      Rating hard
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/data-for-the-common-good/
    idea_list_url: https://docs.pedscommons.org/GSoC/ideas
  

  - organization_id: 31
    organization_name: Debian
    no_of_ideas: 11
    ideas_content: |
     
 
      Quality assurance and continuous integration for biological and medical applications inside Debian
      Description of the project: The Debian Med Blend has packaged a lot of applications used in medicine and biology for Debian. To enhance and continuously ensure the quality of the packaged software we try to implement Continuous Integration tests for all our packages. This was accomplished thanks to several past interns. These tests are of specific importance since only a very small share of the developers inside the Debian Med project are actual users of the software and thus automated testing is required to provide our users with the quality we like to approach. Interns are also not necessarily comfortable with the topic of medicine and biology - reading documentation or publications or directly contact the authors of the software frequently gives sensible ideas how to write a test for the software.
      Confirmed Mentor: Andreas Tille
      How to contact the mentor: tille@debian.org
      Confirmed co-mentors: Emmanuel Arias <eamanu@debian.org>, Étienne Mollier <emollier@debian.org>
      Difficulty level: medium
      Project size: Depending from students availability this project can be medium or large. The advantage of the project is it can be split into small pieces
      Deliverables of the project:Continuous integration tests for Debian Med applications lacking a test, Quality Assurance review and bug fixing if issues might be uncovered
      Desirable skills: Background in bioinformatics, medical imaging could be an advantage, but interest in scientific software and reading relevant documentation and papers might be sufficient. Debian packaging skills are an extra plus but can be taught in the project run.
      What the intern will learn: Detailed insight into the software maintained by the Debian Med team, bug triaging in scientific software, Debian packaging skills, optimising bioinformatics and other scientific tools
      Application tasks: Pick bugs like 1035121, 1035175, 1035178, 1035182, 1035188, 1035200, 1035277, 1036500, 1036506 and try fixing it - asking the mentor for help is perfectly fine and actually recommended. This is on one hand proof that the student is able to understand Debian packaging and understands the actual topic at a sufficient level.
      Related projects: SummerOfCode2016/Projects/BioToolsTesting, SummerOfCode2017/Projects/QA_BiologyApps, ?Continuous_Integration_for_biological_applications_inside_Debian, SummerOfCode2019/ApprovedProjects/CIforDebianMed SummerOfCode2020/ApprovedProjects/DebianMedQAGSoC and Outreachy Project Proposal: Quality Assurance and Continuous integration for applications in life sciences and medicine
      [SummerOfCode2025/ApprovedProjects/DebianMedQA][edit]

      ~~~~~~~~~~
      Device-specific Tweaks Management
      Description of the project: A significant number of non-x86 Linux-capable devices made it to the market in the past few years, especially ARM64 laptops, mobile phones and tablets. Most of those, inheriting their design from embedded systems (and therefore lacking support for software interfaces such as ACPI/UEFI), need device-specific "tweaks" (configuration files/fragments, shell scripts...). As the number of supported devices grows rapidly, providing device-specific Debian packages containing those tweaks doesn't scale, and limits our ability to provide a generic system image/rootfs.
      This project aims at researching and implementing a more flexible way of managing those tweaks, by creating a service capable of identifying the exact device it's running on, selecting the appropriate tweaks based on its configuration file, and installing them to the system. Reaching this goal will ultimately ease supporting new devices in Debian.
      Confirmed Mentor: Arnaud Ferraris (UTC+2)
      How to contact the mentor: Matrix (a-wai on mobian-dev:matrix.org or email (aferraris@debian.org)
      Difficulty level: Mostly medium, although difficulty level is expected to increase over the project's course
      Project size: The whole project is a large (350 hours) one, although its scope could be reduced to fit either a 90 hours or a 175 hours project.
      Deliverables of the project:
      Analysis and discussion of the current state of device tweaks management in Debian and Mobian
      Proposal for a unified, run-time approach
      Initial implementation of a "tweaks management" service
      Packaging of this service and tweaks data/configuration for at least one device
      Desirable skills:
      Familiarity with ARM64 devices (RPi 3+, Pine{Book,Phone,Tab} etc)
      Basic understanding of Linux systems (common services and middleware, user/admin/distro-specific configuration...)
      Shell scripting and basic programming skills
      (optional) basic Rust knowledge
      What the intern will learn: Through this project, the intern will improve their analysis and project management skils, gain a better understanding of Linux systems from a low-level perspective along with basic embedded software skills. They will also learn about Debian development (on both technical and philosophical levels) and likely learn/improve their Rust knowledge.
      Application tasks:
      Locate current tweaks packages (in both Debian and Mobian) targeting mobile devices and their source code
      Briefly analyze one or several of those tweaks and either:
      Explain their use
      Rework them, ideally making them more generic
      Offer to remove them, explaining the reasoning
      Related projects:
      Mobian
      DebianOnMobile team
      mobile-tweaks
      tweakster
      [SummerOfCode2025/ApprovedProjects/DeviceTweaksManagement][edit]

      ~~~~~~~~~~
      Enhancing Debian packages with ROCm GPU acceleration
      Description of the project: There now exists a solid foundation of AMD ROCm components packaged within Debian, so it is time to start making use of them! This project would consist of enhancements to existing packages that have AMD GPU support available upstream but not enabled in Debian, or the packaging of new tools and libraries that would be useful for AMD GPU users. A (non-exhaustive) list of potential packages include: adios2, blaspp, cp2k, cupy, dbcsr, elpa, gloo, hpx, hypre, jax, kokkos, lammps, lapackpp, magma, mfem, mpich, onnxruntime, papi, paraview, petsc, pyfr, pytorch, slepc, spfft, sundials, superlu-dist, or trilinos. There are a lot of options of varying difficulty, so it may be possible to tune the project to the skills and time available to the contributor.
      Confirmed Mentor: Cordell Bloor
      How to contact the mentor: cgmb@slerp.xyz
      Difficulty level: Medium
      Project size: Large (350 hours) if attempting to enhance as many packages as possible, but the scope could be reduced to fit a Medium (175 hour) or Small (90 hour) project
      Deliverables of the project:
      New Debian packages with GPU support
      Enhanced GPU support within existing Debian packages
      More autopackagetests running on the Debian ROCm CI
      Desirable skills:
      Strong familiarity with Debian and/or Ubuntu
      Proficiency with CLIs
      Some experience with build systems (e.g. CMake)
      What the intern will learn:
      Debian packaging (.deb) and maintenance within the Debian ecosystem
      Interacting with a broad variety of other groups within Debian, for example the Release Team and ftp-master
      How to work with ROCm (the AMD alternative to CUDA)
      Application tasks:
      Read the Debian New Maintainer's Guide and the Developer's reference
      Analyze which packages you would target
      Try to enhance one Debian package with AMD ROCm support
      Related projects:
      AMD ROCm GitHub
      [SummerOfCode2025/ApprovedProjects/EnhancingPackagesWithROCm][edit]

      ~~~~~~~~~~
      Make Debian for Raspberry Build Again
      Description of the project: There is an available set of images for running Debian in Raspberry Pi computers (all models below the 5 series)! However, I (the maintainer) am severely lacking time to take care for them; I called for help for somebody to adopt them, but have not been successful. The image generation scripts might have bitrotted a bit, but it is mostly all done. And there is a lot of interest and use still in having the images freshly generated and decently tested! This GSoC project is about getting the [[https://raspi.debian.net/ | Raspberry Pi Debian images] site working reliably again, and ideally making it easily deployable to be run in project machines.
      Confirmed Mentor: Gunnar Wolf
      How to contact the mentor: gwolf@debian.org, IRC: gwolf on OFTC
      Difficulty level: Easy
      Project size: Medium
      Deliverables of the project:
      Refreshing the set of daily-built images
      Having the set of daily-built images become automatic again — that is, go back to the promise of having it daily-built
      Write an Ansible playbook / Chef recipe / Puppet whatsitsname to define a virtual serve and have it build daily
      Do the (very basic!) hardware testing on several Raspberry computers. Do note, naturally, this will require having access to the relevant hardware.
      Desirable skills:
      Understanding the early-boot process of a single-board computer
      Declarative configuration (for vmdb2 as well as for Ansible/Chef/Puppet)
      Writing systemd units and timers
      What the intern will learn: The Raspberry Pi family of computers are ARM-based computers, which have a boot process quite different from “traditional” UEFI-based PCs. You will get acquinted with how a different architecture (that is growing in importance!) boots, how Device Tree maps the hardware for the operating system to use it (and maybe even how to work with overlays). You will also learn how deployment of production-level code is done to servers so they run reliably.
      Application tasks:
      We try to diverge the least possible from regular Debian installs with these images, but the RPi's way of working forces us to take some decisions.
      How much do we differ?
      Do you think all of our modifications make sense, or we might be carrying over some cruft that could be removed?
      We use the vmdb2 image building system. It is not much known outside Debian. How do you compare it with other image building tools?
      Related projects: We are filling approximately the same role as our debian-installer tool, but generating for a series of computers where users often flash ready-to-use images instead of doing an explicit install. Of course, the images we provide could be compared to what Raspberry Pi OS offers, but giving the quality and free-software guarantees that Debian has.
      [SummerOfCode2025/ApprovedProjects/MakeDebianForRaspberryBuildAgain][edit]

      ~~~~~~~~~~
      Package LLM Inference Libraries
      Description of the project: Package Large Language Model (LLM) inference libraries, in particular vLLM. It is needless to explain how LLMs are important. Currently, in the Debian archive, we only have ?PyTorch, but downstream applications are still missing. One of the most promising downstream applications is LLM inference. There are already people working on llama.cpp and Ollama, but vLLM still lacks lots of dependencies to land onto Debian. For multi-GPU inference and concurrency, vLLM has its advantages over llama.cpp. The missing packages are, for instance, transformers, huggingface-hub, etc. We would like to trim the dependency tree a little bit at the beginning until we get a minimum working instance of vLLM. Such, this project involves the Debian packaging work for vLLM and its dependencies that are missing from Debian, as well as fixing issues (if there is any) in existing packages to make vLLM work.
      Confirmed Mentor: Mo Zhou
      How to contact the mentor: lumin@debian.org
      Confirmed co-mentors: Christian Kastner (ckk@debian.org), Xuanteng Huang (xuanteng.huang@outlook.com). On the other hand, Debian Deep Learning Team (debian-ai@lists.debian.org) could offer help.
      Difficulty level: Medium (There might be some hard bits. Some packages that we are going to deal with have a clearly above-average difficulty than general Debian packages.
      Project size: 350 hour (large). I get this rough estimate by looking at the pipdeptree of the vllm package. The tree is a little deep.
      Deliverables of the project: Eventually I hope we can make vLLM into Debian archive, based on which we can deliver something for LLM inference out-of-the-box. If the amount of work eventually turns to be beyond my expectation, I'm still happy to see how far we can go towards this goal. If the amount of work required for vLLM is less than I expected, we can also look at something else like SGLang, another open source LLM inference library.
      Desirable skills: Long term Linux user (familiarity with Debian family is preferred), Python, ?PyTorch, and experience of running Large Language Models locally.
      What the intern will learn: Through this project, the intern will learn about the Debian development process, and gain more experience of running LLMs locally, including the inference performance tuning.
      Application tasks: Analyze how ?PyTorch is packaged in Debian, including how the CUDA variant of ?PyTorch is prepared. Those details are very important for the whole reverse dependency tree. And, the intern also needs to setup vLLM locally using pip or uv, and run the LLM inference locally for reference.
      Related projects: The ?PyTorch packaging repository is here: https://salsa.debian.org/deeplearning-team/pytorch
      [SummerOfCode2025/ApprovedProjects/PackageLLMInferenceLibraries][edit]
      
      ~~~~~~~~~~
      Autopkgtests for the rsync package
      Description of the project: A recent series of breakages caused in the rsync package as part of CVE fixes exposed the lack of testing coverage on Debian, e.g.: https://github.com/RsyncProject/rsync/issues/702. The rsync package on Debian has no autopkgtest. This project is for adding these tests to the rsync package, covering as many usecases as possible, making impossible for regressions to go unnoticed. These tests will also be submitted to stable through the proposed-updates mechanism.
      Confirmed Mentor: SamuelHenrique
      How to contact the mentor: samueloph@d.o, @samueloph:matrix.org, samueloph @ OFTC.
      Confirmed co-mentors: N/A
      Difficulty level: Easy
      Project size: 90 hour (small project)
      Deliverables of the project: Autopkgtests for the rsync package
      Desirable skills: Debian packaging, autopkgtest, shell scripting, rsync.
      What the intern will learn: How the Debian project does CI, how to write CI tests for the rsync package.
      Application tasks: Debian packaging contributions. It is required to have a non-virtualized machine running Debian Stable or Testing (no WSL, no containers, no VMs).
      Related projects: N/A
      More Resources: https://salsa.debian.org/ci-team/autopkgtest/-/blob/master/doc/README.package-tests.rst
      [SummerOfCode2025/ApprovedProjects/RsyncAutopkgtests][edit]

      ~~~~~~~~~~
      Salsa CI in Debian
      Description of the project: Salsa CI is a custom-built continuous integration framework that is used in the Debian Gitlab instance (Salsa) and helps Debian maintainers manage roughly 9,000 projects. The Salsa CI pipeline emulates the Debian build process and runs several Debian quality tests, helping to increase the probability that packages can migrate from Debian Unstable to Testing reliably, quickly, and without issue. When new source code triggers a Salsa CI pipeline, 17 different jobs run to build and test it automatically. Salsa CI checks to see whether the to-be-uploaded packages build on multiple architectures (at the moment, amd64 and i386, and optionally on Arm), runs autopkgtest test suites to try to identify potential regressions, and checks for common errors with our custom linter, lintian, among other tests.
      Confirmed Mentor: Otto Kekäläinen
      How to contact the mentor: otto@debian.org
      Confirmed co-mentors: Emmanuel Arias <eamanu@debian.org>
      Difficulty level: Medium
      Project size: Medium sized (175 hours). Depending on the student's availability, this project can be medium or large. The advantage of the project is it can be split into small pieces.
      Deliverables of the project: Fix and discuss issues reported to Salsa CI. Specially Labels "Nice-to-have", "Accepting MRs".
      Desirable skills: Awareness of GitLab CI. Working with git. Basic knowledge of Debian packaging.
      What the intern will learn: Debian Release process, Debian package building, Debian CI process, Basic QA of Debian packages.
      Application tasks: Pick issues from here, discuss with the team and try to fix them.
      More resources:
      https://debconf20.debconf.org/talks/47-where-is-salsa-ci-right-now/
      https://about.gitlab.com/blog/2023/09/19/debian-customizes-ci-tooling-with-gitlab/
      https://debconf19.debconf.org/talks/148-salsa-ci-debian-pipeline-for-developers/
      [SummerOfCode2025/ApprovedProjects/SalsaCI][edit]

      ~~~~~~~~~~
      
      Unapproved Projects with confirmed mentors
      findutils: Finish Support
      Description of the project: Complete the Rust implementation of GNU Findutils, ensuring full compatibility with all options and passing GNU tests. This project focuses on refining and finalizing the Rust-based reimplementation of key utilities from the Findutils package, which are essential for file searching and manipulation in Unix-like systems. The goal is to achieve full feature parity with the GNU versions while maintaining performance and correctness.
      To improve your chances of being selected, please contribute a few changes to the project to demonstrate your commitment and understanding.
      Confirmed Mentor: Sylvestre Ledru
      How to contact the mentor: sylvestre@debian.org
      Confirmed co-mentors: Daniel Hofstetter <daniel.hofstetter@42dh.com>
      Difficulty level: Large
      Project size: 350 hours
      Deliverables of the project:
      A fully functional Rust implementation of the Findutils suite, including:
      /usr/bin/find - search for files in a directory hierarchy
      /usr/bin/locate - find files by name in a prebuilt index
      /usr/bin/updatedb - update the locate database
      /usr/bin/xargs - build and execute command lines from input
      Full compatibility with GNU Findutils
      Passing all relevant GNU tests
      Desirable skills:
      Rust expertise
      Knowledge of file systems and directory traversal
      Understanding of command-line utilities and Unix system interactions
      What the intern will learn:
      How file search utilities work
      Efficient directory traversal and filtering techniques
      Optimization strategies for large-scale file searches
      Application tasks:
      Implement or improve one of the Findutils utilities from the uutils/findutils project: https://github.com/uutils/findutils
      [SummerOfCode2025/PendingProjects/rust-bsdutils][edit]

      ~~~~~~~~~~
      login: Reimplementation of Login Infrastructure Tools in Rust
      Description of the project: Create Rust versions of login infrastructure tools, with a focus on full option compatibility and passing GNU tests. This project involves the Rust-based reimplementation of essential login infrastructure tools that provide functionality for logins and changing effective user or group IDs. The objective is to implement these tools as drop-in replacements for the original shadow-utils suite, ensuring full compatibility with all options and passing all relevant tests. To improve your chances to be selected, please contribute a few changes to the project to demonstrate your commitment and understanding of the project.
      Confirmed Mentor: Sylvestre Ledru
      How to contact the mentor: sylvestre@debian.org
      Confirmed co-mentors: Daniel Hofstetter <daniel.hofstetter@42dh.com>
      Difficulty level: Large
      Project size: 350 hours
      Deliverables of the project: Robust login infrastructure tools, including:
      /bin/login - the program that invokes a user shell on a virtual terminal
      /usr/bin/faillog - tool for displaying and maintaining failure records
      /usr/bin/lastlog - examine the last login record
      /usr/bin/newgrp - change to a new group
      /usr/sbin/nologin - a dummy shell for disabled user accounts
      /usr/bin/sg - execute command with different group ID
      Desirable skills: Rust expertise, knowledge of Linux authentication systems, user/group management, and security considerations.
      What the intern will learn: How login infrastructure works, system security concepts, authentication mechanisms, and privilege management.
      Application tasks: Implement or improve one of the login tools from the shadow-utils project: https://github.com/shadow-maint/shadow
      [SummerOfCode2025/PendingProjects/rust-login][edit]

      ~~~~~~~~~~
      procps: Development of System Monitoring, Statistics and Information Tools in Rust
      Description of the project: Create Rust versions of system monitoring and statistics tools, with a focus on full option compatibility and passing GNU tests. This project involves the Rust-based development of system monitoring and statistics tools: top, vmstat, tload, w, and watch. And process management and information tools: ps, pgrep, pidwait, pkill, skill, and snice. The objective is to achieve full compatibility with all options and to pass GNU tests, ensuring these tools provide accurate and reliable system insights. To improve your chances to be selected, please contribute a few changes to the project to demonstrate your commitment and understanding of the project. Debian can lead in this space with security and Rust!
      Confirmed Mentor: Sylvestre Ledru
      How to contact the mentor: sylvestre@debian.org
      Confirmed co-mentors:Daniel Hofstetter <daniel.hofstetter@42dh.com>
      Difficulty level: Large
      Project size: 350 hours
      Deliverables of the project: Robust tools for system monitoring and statistics, fully compatible with existing options and verified by GNU tests.
      Desirable skills: Rust expertise, knowledge of system performance metrics, familiarity with GNU testing frameworks.
      What the intern will learn: How the Coreutils work, the low level part of the OS
      Application tasks: Fix one or more GNU test listed on: https://uutils.github.io/coreutils/book/test_coverage.html
      [SummerOfCode2025/PendingProjects/rust-procps][edit]

      ~~~~~~~~~~
      util-linux: Development of System Utilities in Rust
      Description of the project: Create Rust versions of util-linux tools, with a focus on full option compatibility and passing GNU tests. This project involves the Rust-based reimplementation of various util-linux tools, including system information tools (dmesg, lscpu), filesystem tools (mountpoint, fsfreeze), partition management tools, process management tools, and utility tools. The objective is to achieve full compatibility with all options and to pass GNU tests, ensuring these tools function as drop-in replacements for the original util-linux suite. To improve your chances to be selected, please contribute a few changes to the project to demonstrate your commitment and understanding of the project.
      Confirmed Mentor: Sylvestre Ledru
      How to contact the mentor: sylvestre@debian.org
      Confirmed co-mentors: Daniel Hofstetter <daniel.hofstetter@42dh.com>
      Difficulty level: Large
      Project size: 350 hours
      Deliverables of the project: Robust tools for system utilities, fully compatible with existing options and verified by GNU tests.
      Desirable skills: Rust expertise, knowledge of system utilities and Linux internals, familiarity with GNU testing frameworks.
      What the intern will learn: How util-linux tools work, the low level part of the OS, system management, and filesystem operations
      Application tasks: Implement or improve one of the tools listed in the util-linux repository: https://github.com/uutils/util-linux
      [SummerOfCode2025/PendingProjects/rust-util-linux][edit]
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/debian/
    idea_list_url: https://wiki.debian.org/SummerOfCode2025/Projects

  - organization_id: 32
    organization_name: DeepChem
    no_of_ideas: 10
    ideas_content: |
      
      Length: Small (90 hours)
      Description: DeepChem has been moving towards first class layers and now has a collection of general layers. We still need to improve the documentation for existing layers to make them more useful for the community. This project should add tutorials for using existing layers to the DeepChem tutorial series, and should plan to add a few new layers that would be useful to the community.
      Educational Value: Students will learn to improve their technical communication skills and learn how to construct useful Jupyter/Colab tutorials. Layers are easier to add than full models since they are effectively functions.
      Potential Mentors: Aryan, Jose, Riya, Maithili, Nimisha, Shreyas
      Note: This was also a 2024 project, but there remains more work to be done for 2025 expanding tutorials/ideas.
      
      ~~~~~~~~~~
      Improving New Drug Modality Support
      Length: Small (90 hours)
      Description: DeepChem at present doesn’t have much tooling or support for working with emerging drug modalities. These include PROTACs, Antibody-drug-conjugates, macrocycles, oligonucleotides and more. This project would add new tutorials introducing these new drug modalities and provides examples of how to work with them with deepchem. It would also be useful to identify and process relevant datasets.
      Educational Value: New drug modalities drive many emerging startups in the space. Improving DeepChem’s support for these new modalities of therapeutics could help drive discovery of new medicine at the cutting edge. It would prepare students to potentially find jobs at these up-and-coming biotech firms as well.
      Potential Mentors: Jose, David, Bharath
      Note: This was also a 2024 project, but there remains more work to be done for 2025 expanding support for new modalities.
      
      ~~~~~~~~~~
      Improving support for drug formulations
      Length: Small (90 hours)
      Description: Drug formulations are a rich area of industrial study that is often critical for actually bringing a drug to patients. See https://drughunter.com/resource/the-modern-medicinal-chemist-s-guide-to-formulations/ 65 for example for a guide. In this project, you will build a tutorial introducing readers to the study of drug formulations along with DeepChem examples of how you can computationally help design a potential formulation.
      Educational Value: Formulations are critical for bringing drugs to patients. Improving DeepChem’s support for these new modalities of therapeutics could help drive discovery of new medicine at the cutting edge. It will prepare students to find jobs at large biotech/pharma firms as well.
      Potential Mentors: Jose, David, Bharath
      Intermediate Projects
      These projects require some degree of hacking, but likely won’t raise challenging engineering difficulties.
      
      ~~~~~~~~~~
      Improve Equivariance Support
      Length: Medium (175 hours)
      Description: DeepChem has limited support for equivariant models. This project would extend support for equivariance to DeepChem and add additional equivariant model such as tensor field networks to DeepChem
      Educational Value: Equivariance is one of the most interesting ideas in modern machine learning and underpins powerful systems like AlphaFold2. Contributors will learn more about this field and could potentially write a research paper about their work on this project.
      Potential Mentors: Aryan, Riya, Nimisha, Bharath, Shreyas
      Note: This was also a 2024 project, but this project was not taken up by a student last year.
      
      ~~~~~~~~~~
      Numpy 2.0 Upgrade
      Length: Large (175 hours)
      Description: DeepChem is currently on Numpy < 2.0. The upgrade to 2.0 is not backwards compatible. We need to fix any broken compatibilities.
      Educational Value: Complex version upgrades take a lot of sophistication and will teach students challenging debugging skills.
      Potential Mentors: Bharath
      
      ~~~~~~~~~~
      Conversion of Smiles to IUPAC and IUPAC to smiles
      Length: Large(300 hours)/Medium(175 hours)
      Description: This project focuses on developing tools within DeepChem to enable accurate, bidirectional conversion between SMILES (Simplified Molecular Input Line Entry System) strings and IUPAC (International Union of Pure and Applied Chemistry) names. The final deliverables will include user-friendly APIs, thorough documentation, and comprehensive testing to facilitate reliable molecular representation transformations.
      Educational Value: Deepening of understanding of chemical data structures, algorithm optimization for molecular conversions, and contributing to the Deepchem ecosystem.
      Potential Mentors: Shreyas, Bharath
      
      ~~~~~~~~~~
      Advanced Projects
      These projects raise considerable technical and engineering challenges. We recommend that students who want to tackle these projects have past experience working in large codebases and tackling code reviews for complex code.
      Implement a Wishlist Model
      Length: Large (300 hours)
      Description: DeepChem has an extensive wishlist of models (https://github.com/deepchem/deepchem/issues/2680 193). Pick a model from the wishlist and implement it in DeepChem. We suggest tackling a model such as Hamiltonian or Lagrangian Neural networks or Physics Inspired Neural Operators (PINO) that will improve DeepChem’s physics support.
      Educational Value: Implementing a machine learning model from scratch or from an academic reference into a production grade library like DeepChem is a challenging task. Doing so requires understanding the base model, dealing with numerical issues in implementation, and benchmarking the model correctly. Multiple past GSoC contributors have leveraged their implementations to write papers on their work and have gained skills that they have used subsequently in industry or in academia.
      Potential Mentors: Depends on model.

      ~~~~~~~~~~
      PyTorch Porting
      Length: Medium (175 hours)
      Description: DeepChem has mostly shifted to PyTorch as its primary backend, but a couple models are still implemented in TensorFlow, in particular our Chemception implementation. This project would port Chemception and do final testing to fix issues with PyTorch/DeepChem compatibility on implementations. https://github.com/deepchem/deepchem/issues/2863. 77
      Educational Value: Porting models while preserving numerical properties requires a strong understanding of deep learning implementations. It serves as a test of machine learning know-how that will serve students well in future machine learning positions in academia or industry.
      Potential Mentors: Aryan, Jose, Riya, Nimisha, Bharath, Shreyas
      
      ~~~~~~~~~~
      HuggingFace-style easy pretrained-model Load
      Length: Large (300 hours)
      Description: DeepChem requires you to know the parameters used to train a model in order to reload it from disk. This is unfriendly for distributing pretrained models. In this project, you will implement an easy 
      HuggingFace-style function call to load weights from disk without having to know training parameters. To do this, you will set a standard metadata format for saving model parameters that can be used behind the scene to autoload models from disk.
      Educational Value: This is a technically challenging project which will require understanding metadata formats and changing saving/reloading for existing models.
      Potential Mentors: Aryan, Bharath
      
      ~~~~~~~~~~
      Model-Parallel DeepChem Model Training
      Length: Large (300 hours)
      Description: DeepChem now has good support for training LLM models through huggingface. At present though, these models cannot be too large and must fit on a single GPU. In this project, you will implement basic support for model parallel training to train models with weights that don’t fit on a single GPU.
      Educational Value: This is a technically challenging project which will require understanding multi-GPU training methods. You may need to explore existing PyTorch frameworks for model-parallel training and adapt them to DeepChem.
      Potential Mentors: Aryan, Bharath
      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/deepchem/
    idea_list_url: https://forum.deepchem.io/t/deepchem-gsoc-2025-project-ideas/1568

  - organization_id: 33
    organization_name: Department of Biomedical Informatics, Emory University
    no_of_ideas: 5
    ideas_content: |
      
      [1] Title: Securing Linux Storage with ACLs: An Open-Source Web Management Interface for Enhanced Data Protection
      Mentor(s): Robert Tweedy and Mahmoud Zeydabadinezhad, PhD (mzeydab -at- emory.edu)
      Overview:
      While the traditional POSIX permissions used by nearly all common Linux filesystems allow for simple data permissions management based on group membership, this can become complicated to manage in a large research environment where a project's directory tree may not be efficiently structured to permit access via the simple group management achievable via POSIX permissions, especially when a research PI would like to grant different levels of access to a file to users who are otherwise in the same group. Linux ACLs, while a de-facto standard rather than a formally defined one like POSIX, can be used to resolve these types of situations but have a higher learning curve & far fewer management tools available for a large-scale storage system than tools managing POSIX permissions, relying mainly on the command line tools "setfacl" and "getfacl" to view any details. While there is a GUI tool known as "eiciel" that can adjust Linux ACLs, its audience & feature set is aimed towards an individual user on their personal machine & thus is not suitable for large-scale storage system management. This project aims to develop an application with a web-based GUI that can be deployed on a research network's storage system to provide PIs with a graphical overview of all their research data & allow management of the Linux ACLs to grant appropriate levels of access to the PI's research team members.
      Current Status: New project
      Expected Outcomes:
      A self-contained application (ie. the application should not require external APIs/javascript/etc. to be queried by the end-user's web browser at runtime & should have any relevant scripts packaged with it; the underlying back-end should only access local network resources & not need external Internet connectivity) that allows an end-user to view and manage the access permissions of a large research storage network at a fine-grained level via a web GUI. Both the back-end logic & front-end web interface are in scope of this project.
      The application should support authentication via different modules, with a minimum of LDAP-based authentication.
      The application should support deployment behind a reverse proxy running on Apache or Nginx
      The application must be Linux distribution agnostic as much as possible, but at a minimum should support both running on both RedHat Enterprise Linux (or free derivatives like Rocky Linux) and Debian Linux (or other derivatives like Ubuntu Server) as a SystemD service.
      The application must support running under a limited service account & not as the root user; if special permissions are required for the service account (ie. AmbientCapabilities defined in the SystemD service script) these should be stated in the application's documentation.
      Documentation explaining a basic overview of application usage & installation steps for use by research network system administrators.
      The code and documentation for the application will be made publicly available on a platform such as GitHub & licensed under an Open-Source license such as GPLv3.
      Required Skills:
      Strong web application/frontend development skills using lightweight web frameworks (ie. this should not be a Java application that requires Tomcat/Glassfish/etc.). Strong backend/logic development skills in any standard language commonly available by default on Linux systems (Python, C, C++, Rust, etc.) A good understanding of POSIX and Linux ACL file permissions. A security-focused mindset to ensure that the application's coded to only permit users to view/modify permissions on their own files & not those of others. Familiarity with databases (PostgreSQL and/or MySQL) may be beneficial depending on the approach used to develop the application and track user permissions for the application itself. Familiarity with standard web servers like Apache or Nginx, especially with the concept of reverse proxies.
      Source Code: New Project
      Discussion Forum: https://github.com/NISYSLAB/Emory-BMI-GSoC/discussions
      Effort: 350 Hours
      Difficulty Level: Hard

      ~~~~~~~~~~
      [2] Title: Open-Source Framework for Advanced EEG Data Analysis Using Pre-trained Foundation Models
      Mentor(s): Babak Mahmoudi, PhD and Mahmoud Zeydabadinezhad, PhD (mzeydab -at- emory.edu)
      Overview:
      A foundation model refers to a large-scale model that is pre-trained on extensive, often unlabeled data, capturing a broad understanding of that data. Recent studies have indicated that foundation models could potentially offer enhanced robustness and versatility in the analysis of complex patterns within Electroencephalography (EEG) data. This is particularly relevant in scenarios where EEG data for specific downstream tasks is limited in quantity. This project aims to create an open-source foundation model for EEG data analysis. It will involve developing algorithms for EEG signal processing, automatic feature extraction, and implementing deep learning-based algorithms for pre-training a foundation model on publicly available EEG datasets.
      Current Status: In Progress.
      Expected Outcomes:
      Literature Review: Research and review existing open-source foundation models for medical data, specifically EEG data. Identify the current limitations and challenges in this field.
      A robust, open-source EEG foundation model.
      Documentation and examples demonstrating the model's usage.
      A report detailing the methodologies used and the performance of the model.
      The model weights and code will be made publicly available on a platform such as GitHub.
      Required Skills:
      Strong programming skills, preferably in Python.
      Experience with deep learning frameworks, preferably PyTorch.
      Knowledge of self-supervised learning and large language models
      Knowledge in signal processing, neuroscience, or related fields.
      Source Code: In Progress
      Discussion Forum: https://github.com/NISYSLAB/Emory-BMI-GSoC/discussions
      Effort: 350 Hours
      Difficulty Level: Hard

      ~~~~~~~~~~
      [3] Python Expansion of the Open Source Electrophysiological Toolbox
      Mentor(s): Reza Sameni, PhD
      Overview:
      Standardized, open-source codes are indispensable in advancing biomedical engineering and biomedical informatics. The Open-Source Electrophysiological Toolbox (OSET) [https://github.com/alphanumericslab/OSET] was conceived in 2006 with this perspective, aiming to offer researchers an open-source codebase for biomedical signal processing. The toolbox has incrementally evolved and expanded over the years. Many researchers have utilized this toolbox for their research. Notably, some modules of OSET have been translated to C/C++ and Python and integrated into medical devices and cloud-based automatic diagnostic systems for large data. It operates under a permissive open license, encouraging community-driven development. The project aims to continue the Python expansion of OSET to convert it into a standard Python package, broadening access and maintaining consistency across platforms. The planned upgrades include comprehensive cross-language unit tests (between MATLAB and Python), documentation, example codes, installation, and maintenance mechanisms, with a modern software-engineered architecture and objective microbenchmarks. Through this expansion, we aim to extend OSET's benefits to a broader community of AI researchers, also contributing to the training of the next generation of biomedical engineers in the AI era. OSET will be maintained under the 3-Clause BSD License, a permissive open-source license that allows the redistribution and use of software in any form with adequate disclaimer clauses, offering flexibility for both open-source and commercial use, while minimizing legal complexities.
      Current Status: Ongoing project.
      Expected Outcomes:
      A Python package for OSET, with standard unit tests, installation guidelines and documentation. The codebase should operate exactly identical to the current MATLAB implementation.
      Key features:
      Open-source code development for biomedical engineering and biomedical informatics.
      Identical cross-language performance
      Standardized unit tests
      Required Skills:
      Proficiency in Python and MATLAB.
      A background in signal processing
      Experience in biomedical signal processing
      Source Code: https://github.com/alphanumericslab/OSET
      Discussion Forum: https://github.com/NISYSLAB/Emory-BMI-GSoC/discussions
      Effort: 350 Hours
      Difficulty Level: Medium

      ~~~~~~~~~~
      [4] Health-AI Ethics Atlas
      Mentor(s): Selen Bozkurt, PhD
      Overview:
      This project aims to develop an interactive global map that visualizes the application and development of ethical principles in medical AI across different countries. It will highlight the diversity in ethical standards, regulatory approaches, and implementation practices in healthcare AI worldwide.##
      Key features:
      Interactive world map displaying medical AI ethics initiatives.
      Filters for different ethical principles and AI applications.
      Country-specific data on AI healthcare policies and ethics.
      Time-lapse feature to observe changes over time.
      Case studies and detailed reports linked to map locations.
      Dynamic interface allowing users to explore technological ethical dimensions.
      Current Status: New project.
      Expected Outcomes:
      A comprehensive resource for understanding global trends in medical AI ethics.
      Enhanced awareness of ethical diversity in medical AI applications.
      Tool for researchers and policymakers to identify global best practices and gaps.
      Required Skills:
      Proficiency in web development (e.g., HTML, CSS, JavaScript).
      Experience with data visualization tools and libraries (e.g., D3.js, Leaflet).
      Understanding of GIS and mapping software.
      Knowledge in data analysis and handling large datasets.
      Interest or background in AI ethics, particularly in healthcare.
      Source Code: New Project.
      Discussion Forum: https://github.com/NISYSLAB/Emory-BMI-GSoC/discussions
      Effort: 350 Hours
      Difficulty Level: Medium

      ~~~~~~~~~~
      [5] Advancing Brain Decoding and Cognitive Analysis: Leveraging Diffusion Models for Spatiotemporal Pattern Recognition in fMRI Data
      Mentor(s): Babak Mahmoudi, PhD and Ozgur Kara
      Overview:
      Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging technique that measures brain activity by detecting changes in blood flow, providing high-dimensional, time-series data representing brain function. By leveraging diffusion models—probabilistic generative models originally designed for image synthesis—this project aims to learn complex spatiotemporal patterns in fMRI data, enabling downstream tasks such as brain decoding, disease prediction, and cognitive state classification.
      Current Status: New project
      Expected Outcomes:
      Data Preprocessing:
      Preprocess data using standard pipelines to normalize, denoise, and align brain scans.
      Convert the data into a structured representation suitable for diffusion training, such as voxel-wise time series or connectivity matrices.
      Model Architecture & Training:
      Adapt diffusion models to the temporal and spatial characteristics of fMRI data.
      Use a U-Net or transformer-based architecture to model the evolution of brain signals.
      Train the model with a denoising diffusion probabilistic approach, learning to reconstruct fMRI signals from noise.
      Explore conditioning techniques, such as using behavioral or task labels, to guide model learning.
      Required Skills:
      Strong programming skills, preferably in Python.
      Experience with deep learning frameworks, preferably PyTorch.
      Knowledge in signal processing, neuroscience, or related fields.
      Source Code: New Project
      Discussion Forum: https://github.com/NISYSLAB/Emory-BMI-GSoC/discussions
      Effort: 350 Hours
      Difficulty Level: Hard
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/department-of-biomedical-informatics-emory-university/
    idea_list_url: https://github.com/NISYSLAB/Emory-BMI-GSoC/?tab=readme-ov-file#list-of-ideas

  - organization_id: 34
    organization_name: Django Software Foundation
    no_of_ideas: 4
    ideas_content: |
      
      Django Templates: Bring django-template-partials into core
      Difficulty Medium
      Size 350hrs
      Potential Mentors Carlton Gibson
      Key Skills Django Template Language, template tags
      The third-party app django-template-partials allows for reusable named inline partials for the Django Template Language. These named inline partials can be used multiple times within a template or included in other templates. They are particularly useful when using libraries such as HTMX. It would be good to have support built-in to core.
      Outcome would be a PR adding this into Django. In addition there would be preparatory work on the Django-template-partials repo to smooth the migration for existing users. There is a tracking issue on the repo that can be used for guidance.
      
      ~~~~~~~~~~
      Automate processes within Django contribution workflow
      Difficulty Medium
      Size 350hrs
      Potential Mentors Lily Foote
      Key Skills GitHub actions, scripting, Jenkins
      The contribution workflow within Django has several manual processes. These are error prone and take up valuable time from contributors. This project would seek to automate these processes, such as automating releases, identifying active PRs and managing aging PR queue.
      Part of this project will involve working with the Fellows to determine which of their tasks and the community's tasks are the best candidates for automation.
      Outcome would be one to several PRs automating these workflows, depending on how many are accomplished.
      ~~~~~~~~~~
      Expand django-stubs coverage
      Difficulty Hard.
      Size Variable
      Potential Mentors 2024 mentor: Adam Johnson
      Key Skills Python typing and Django-stubs
      django-stubs is an external project that adds type hints to Django. It may be possible to work on it under GSoC if you can show experience with Python’s type hints and Mypy.
      django-stubs uses Mypy’s stubtest tool to check that its type hints align with Django’s source, per its contributing documentation. The “todo” list contains ~1600 functions and classes missing type hints. A proposal targeting a specific, significant subset of the missing types is likely to be accepted.
      
      
      ~~~~~~~~~~
      Django Admin: Add Command palette
      Difficulty Medium
      Size Variable
      Potentia mentors Tom Carrick
      Key Skills UI/UX
      Many dashboards nowadays have command palettes, often associated with the CTL+K keyboard shortcut. This allows for quicker, easier and more accessible navigation. This project is about adding such a feature to the Django admin.
      This would allow power users to e.g. very quickly find a particular object in the database to edit. The overall goal is to improve the user-experience for keyboard and keyboard-only users.
      This feature already has an accepted ticket. However, this work needs to be done carefully to ensure that all shortcuts are useful, documented in the admin, and work across browsers without shadowing browser shortcuts. In addition, shortcuts should be easily extensible by developers.
      Some initial work has been done to add keyboard shortcuts to core. The 175 hour version of this project would be to get this project finished and merged into Django. The 350 hour version would be to build this into a full command palette.
      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/django-software-foundation/
    idea_list_url: https://code.djangoproject.com/wiki/SummerOfCode2025

  - organization_id: 35
    organization_name: Drupal Association
    no_of_ideas: 9
    ideas_content: |

    Proposal 2025: Augmentor DALL·E 3 module
    Project Description
    This project aims to ship the Augmentor plugin with seamless integration to the DALL·E 3 service, enabling users to generate high-quality images directly from text prompts. By combining Augmentor's flexible plugin architecture with DALL·E 3’s advanced AI capabilities, the solution will empower content creators to effortlessly transform ideas into compelling visual content. The integration will focus on delivering a user-friendly interface, efficient performance, and robust error handling to cater to both technical and non-technical users.

    Project Goal
    Functional Changes:
    Integrate the DALL·E 3 API into the Augmentor plugin for real-time image generation from text inputs.
    Develop an intuitive interface for configuring image generation parameters and previewing results.
    Implement error handling and optimization strategies to ensure reliability and performance.
    Provide comprehensive documentation and usage guides to facilitate seamless adoption.
    Mentor Details
    Name: Naveen Valecha & Murrayw
    Email / Slack: @naveenvalecha, @murrayw

    Project Size
    175 hours

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Expertise in plugin development within the Augmentor ecosystem.
    Proficiency in JavaScript, Python, or other relevant programming languages.
    Familiarity with RESTful APIs and integrating third-party AI services.
    Experience with DALL·E 3 or similar AI image generation APIs is a plus.
    Understanding of UI/UX design principles to create an engaging and intuitive user interface.
    Project Resources
    DALL·E 3 Documentation
    Augmentor Plugin Developer Guide
    OpenAI API Reference
    Web API Documentation
    R&D Tasks
    Research the DALL·E 3 API, including authentication methods, rate limits, and usage policies.
    Design and develop a modular integration layer within the Augmentor plugin.
    Create a user-friendly interface for inputting text prompts and previewing generated images.
    Implement error handling, logging, and performance optimizations to ensure a seamless user experience.
    Document the integration process, including setup instructions and usage tutorials.

    ~~~~~~~~~~

    Proposal 2025: Open Source AI Content Generation
    
    Project Description
    This project focuses on advancing the integration of open-source AI models into existing Drupal modules, streamlining the creation of structured content based on user prompts. By embedding AI-powered content generation directly into Drupal, the solution aims to automate and simplify the content creation process. Site builders and content creators will benefit from a seamless workflow that transforms text inputs into well-organized, coherent content.

    Project Goal
    Functional Changes:
    Develop an integration layer that connects Drupal content creation modules with open-source AI models.
    Create an intuitive interface where users can input prompts or guidelines to generate structured content.
    Implement mechanisms to format and validate the generated content according to predefined templates.
    Optimize the integration for performance and reliability, including robust error handling and logging.
    Document the integration process and provide comprehensive usage guides for end users.
    Mentor Details
    Name & Slack: Gaurav & Anushri Kumari
    Email / Slack: @gauravvvv, @anushrikumari

    Project Size
    350 hours

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Proficiency in PHP and Symfony for backend development.
    Experience in JavaScript for front-end interface development.
    Familiarity with AI models and APIs, including experience working with open-source AI frameworks.
    Understanding of Drupal module development and integration patterns is a plus.
    Project Resources
    Drupal Documentation
    Symfony Documentation
    Hugging Face – Open Source AI Models
    JavaScript Guide
    R&D Tasks
    Research and evaluate suitable open-source AI models that can generate structured content from text prompts.
    Identify the optimal integration points within existing Drupal content creation modules.
    Develop the backend integration using PHP and Symfony, ensuring smooth communication with the AI APIs.
    Create a user-friendly front-end interface using JavaScript that allows users to input prompts and preview generated content.
    Implement error handling, logging, and performance optimizations to enhance the overall user experience.
    Document the integration process and produce detailed guides for installation, configuration, and usage.
    Issue fork gsoc-3419059
    Command icon Show commands
    About issue forks

    ~~~~~~~~~~

    Proposal 2025: Assisting Modules in Drupal 11 Compatibility


    Project Description
    Taking on a project idea from 2023. Many important Drupal modules are still not ready for Drupal 11. This makes it harder for developers and site owners to upgrade smoothly.

    This project aims to assist in making Drupal modules Drupal 11-ready by identifying top-used but outdated modules, categorizing them, and streamlining their migration process. The project will include:

    Prioritization of the top most-used yet incompatible modules.
    Categorization based on functionality (e.g., e-commerce, security, SEO).
    Automated compatibility checks and migration guides.
    By ensuring these modules are compatible, this project will significantly ease the transition to Drupal 11, benefiting developers and site owners alike.

    Project Goal
    Functional Changes:
    Analyze and prioritize high-impact modules needing Drupal 11 updates.
    Ensure code consistency and adherence to modern Drupal standards.
    Provide clear documentation for developers migrating modules.
    User Experience Improvements:
    Enhance documentation for easier adoption.
    Provide a clear roadmap for Drupal 11 compatibility efforts.
    Code & Performance Enhancements:
    Ensure PSR standards compliance.
    Mentor Details
    Name : [To be assigned]

    Email / Slack: [To be assigned]

    Project Size
    350 hours

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Proficiency in PHP and JavaScript
    Experience in Drupal module development
    Familiarity with upgrading Drupal modules to newer versions
    Understanding of Drupal's API and best practices
    Experience with Git and version control
    Project Resources
    Drupal Contributor Guide
    R&D Tasks
    Identify top-used modules that need compatibility updates.
    Analyze and categorize modules based on their functionality.
    Document compatibility issues and solutions for developers.
    Test migrated modules for performance and stability.

    ~~~~~~~~~~

    Proposal 2025: FAISS Vector Search Integration for Drupal

    Project Description
    Drupal currently lacks support for FAISS (Facebook AI Similarity Search), a high-performance library designed for efficient vector-based similarity search. This project aims to develop a FAISS provider module that enables advanced content discovery, documentation deduplication, and AI-powered search capabilities within Drupal.

    The solution will integrate with existing Drupal AI modules and Search API, providing a robust foundation for vector-based similarity search. By implementing FAISS as a local vector database, we can significantly improve content discovery while reducing dependency on external APIs.

    Current Scenario / Pain Points
    No native FAISS support in Drupal for vector similarity search
    External API calls causing latency in embedding generation
    Cost implications with third-party embedding services
    Growing documentation duplication without automated detection
    Performance bottlenecks in large-scale content similarity matching
    Why This Matters
    This integration will revolutionize how Drupal handles content discovery and similarity detection:

    Cost Efficiency: Local vector operations eliminate expensive API calls
    Performance: FAISS is optimized for rapid similarity search at scale
    Independence: Reduced reliance on external services
    Scalability: Handles millions of vectors efficiently
    Future Impact
    The FAISS provider will enable next-generation features in Drupal:

    Intelligent Documentation Management
    Automated duplicate detection
    Content relationship mapping
    Smart content suggestions
    Enhanced Module Discovery
    Similar module detection
    Functionality matching
    Better developer experience
    AI-Powered Search
    Semantic search capabilities
    Context-aware results
    Improved content relevance
    Project Goal
    Develop a FAISS provider module (ai_provider_faiss) for vector storage and similarity search
    Create an embedding pipeline using HuggingFace model
    Integrate with Search API for enhanced content discovery
    Implement a dashboard for visualizing similarity scores and content relationships
    Mentor Details
    Name: [To be assigned]
    Email / Slack: [To be assigned]

    Project Size
    350 Hours

    Project Difficulty
    INTERMEDIATE

    Project Skills/Prerequisites
    Strong PHP and Drupal module development experience
    Understanding of vector embeddings and similarity search concepts
    Familiarity with Search API and HuggingFace integration
    Experience with performance optimization and scalable solutions
    Project Resources
    FAISS GitHub Repository
    HuggingFace Documentation
    Original Feature Request

    ~~~~~~~~~~

    Proposal 2025: Appwrite Integration Module for Drupal

    Project Description
    Appwrite is an open-source Backend-as-a-Service (BaaS) that offers a comprehensive suite of tools—ranging from authentication and storage to databases and serverless functions—through easy-to-use APIs. This project aims to develop a Drupal module that integrates Appwrite’s services, giving Drupal developers a robust alternative for handling key backend functionalities. The module will primarily focus on connecting Drupal with Appwrite’s authentication system, object storage, and document database, while also laying the groundwork for future integrations like serverless functions, messaging, and real-time event handling.

    Many Drupal sites currently rely on local media storage or traditional cloud services, but with Appwrite’s flexible architecture, we can streamline media management and content handling. The solution will include a dedicated configuration interface within Drupal, making it simple for site administrators to set up and manage Appwrite settings.

    Project Goal
    Functional Changes:
    Develop a Drupal module that connects with Appwrite’s OAuth-based authentication system (supporting providers like Google, GitHub, Apple, etc.).
    Enable media file management by integrating with Appwrite's object storage APIs, reducing the dependency on traditional cloud storage solutions.
    Integrate Appwrite’s document database to store and retrieve structured Drupal content efficiently.
    Create an intuitive configuration interface within Drupal for managing all Appwrite-related settings.
    Mentor Details
    Name: stanzin
    Email / Slack: @stan

    Project Size
    TBD

    Project Difficulty
    INTERMEDIATE

    Project Skills/Prerequisite
    Experience with PHP and Symfony for Drupal backend development.
    Understanding of Drupal module development, including hooks, services, and dependency injection.
    Knowledge of JavaScript for interacting with the Appwrite SDK and APIs.
    Familiarity with REST APIs to facilitate seamless integration of external services with Drupal.
    Understanding of OAuth and authentication protocols for implementing secure Appwrite authentication.
    Project Resources
    Appwrite Official Website
    Drupal Module Development Guide
    R&D Tasks
    Research and evaluate the Appwrite API, focusing on authentication, storage, and document database functionalities.
    Design a modular integration approach to connect Drupal with Appwrite’s services.
    Develop and test the OAuth-based authentication workflow within Drupal using Appwrite.
    Implement media storage integration to manage file uploads and retrievals via Appwrite’s object storage.
    Integrate Appwrite’s document database to support Drupal’s content management and retrieval.
    Create a user-friendly configuration interface in Drupal to manage Appwrite settings and integrations.
    Document the integration process and provide comprehensive user guides and technical documentation.
    
    ~~~~~~~~~~

    

    Proposal 2025: AI-Powered Media Caption Generator

    Project Description
    Currently, Drupal lacks a built-in AI-powered solution for generating media captions, alt tags, and metadata. Manual captioning and tagging are time-consuming, inconsistent, and impact accessibility, SEO, and media discoverability.

    The AI-Powered Media Enhancement Module will integrate with AI APIs for NLP and computer vision to automatically generate captions, alt tags, and relevant media tags based on content analysis. This module will include:

    Adaptive feedback learning: Continuously improves caption accuracy based on user interactions.
    Batch processing: Allows handling multiple media files efficiently.
    Customizable AI model selection: Users can choose different AI services (OpenAI, Hugging Face, Google Vision, etc.).
    By automating captioning and metadata generation, this project will improve accessibility, SEO, and user engagement, reducing manual effort and making media content more discoverable.

    Project Goal
    Functional Changes:
    Develop a Drupal module capable of generating AI-powered captions, alt tags, and media metadata.
    Integrate NLP models for text-based caption generation.
    Implement computer vision models for object detection and image recognition.
    Enable an adaptive feedback loop where user edits refine AI-generated captions over time.
    Support batch processing to handle multiple media files at once.
    Allow users to choose and configure AI models based on their needs (e.g., OpenAI, Hugging Face, etc.).
    User Interface Changes:
    AI settings page: Configure AI model selection and preferences.
    Caption preview & edit section: Users can review and modify AI-generated captions.
    Bulk processing interface: Manage and process multiple media files at once.
    API Changes:
    Integration with AI APIs for NLP, image recognition, and metadata generation.
    Implement an adaptive feedback system where user corrections improve AI results.
    Data Model Changes:
    New database fields to store generated captions, alt tags, media tags, and user feedback for adaptive learning.
    Mentor Details
    Name : Binal Patel, [Senior / Experienced Drupal Developer - To be assigned]

    Email / Slack: @Binal Patel, [To be assigned]

    Project Size
    350 hours

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Proficiency in PHP and JavaScript
    Familiarity with Drupal module development
    Experience with AI/ML models and API integration (OpenAI, Hugging Face, Google Vision, etc.)
    Experience with Git, containerization (Docker), and RESTful APIs
    Good understanding of UI/UX principles and responsive design
    Project Resources
    Hugging Face Transformers
    OpenAI API Docs
    Google Cloud Vision API
    R&D Tasks
    Research AI models for caption generation, image recognition, and metadata extraction.
    Develop backend integration with AI APIs.
    Design and implement a user-friendly UI for media caption management.
    Ensure compliance with accessibility standards (WCAG, ARIA, etc.).
    Write module documentation and conduct testing.


    ~~~~~~~~~~

    Proposal 2025: AI-Powered Unified Accessibility Compliance Suite for Drupal

    Project Description
    This project aims to create an AI-driven Drupal module that audits and remediates accessibility issues across text, layout, and media content. Currently, Drupal administrators struggle with time-consuming manual accessibility audits that often miss context-dependent issues. The module will leverage computer vision, OCR, and NLP models to automatically scan pages for ADA/WCAG compliance issues, generate actionable remediation reports, and provide a dynamic dashboard to monitor issues, prioritize fixes based on impact severity, and implement one-click remediations where possible.

    Project Goal
    Dynamic Dashboard:
    Create a user-friendly dashboard to monitor website accessibility status and track remediation progress.
    Provide visualizations and reports for compliance documentation.
    Implement severity-based issue prioritization with configurable thresholds.
    Functional Changes / User Interface Changes:
    Integrate AI and accessibility APIs (e.g., Deque Axe) for automated scans of text, layout, and media.
    Use NLP models to suggest context-aware alt-text for media as part of remediation.
    Create an inline editor for quick fixes to identified accessibility issues.
    DB Changes / API Changes:
    Design database schema for storing accessibility audit results and remediation history.
    Create RESTful APIs for integration with external accessibility testing tools.
    Implement caching mechanisms to improve performance of repeated scans.
    Workflow Integration:
    Allow for the export of reports in common formats such as CSV, PDF, and WCAG-compatible documentation.
    Create a feedback system for users to rate AI-generated remediation suggestions.
    Integrate with Drupal's content workflow to include accessibility checks in the publishing process.
    Mentor Details
    Name: [To be assigned]
    Email / Slack: [To be assigned]

    Project Size
    350 Hours (Large)

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Expertise in Drupal module development and Drupal APIs.
    Proficiency in PHP, JavaScript, and Python for AI integration.
    Familiarity with AI frameworks (e.g., Hugging Face Transformers, OpenAI APIs).
    Experience with accessibility standards (WCAG 2.1/ADA) and tools like Axe.
    Understanding of semantic HTML and ARIA attributes.
    Technologies: PHP, JavaScript, Python, React, AI Frameworks, Axe Core, Drupal theme system.
    Project Resources
    Drupal Accessibility Handbook
    Deque Axe Accessibility Tools
    Drupal Module Development Guide
    GPTBot – AI Chatbot Integration for Drupal
    WCAG Guidelines and Resources
    Accessibility Checker for Drupal
    Axe Core GitHub Repository
    R&D Tasks
    Research AI models for accessibility analysis (e.g., Deque Axe, Hugging Face).
    Implement backend integrations with accessibility-focused AI providers.
    Design a UI for audit reporting and remediation, aligned with Drupal's accessibility standards.
    Evaluate existing accessibility modules like Siteimprove and Monsido for potential integration.
    Research automated testing frameworks that can validate accessibility fixes.
    Investigate machine learning approaches to recognize patterns in accessibility issues specific to Drupal sites.
    Similar Projects for Inspiration
    Editoria11y - In-CMS Accessibility Checker
    Site Audit - Framework for analyzing Drupal sites
    Microsoft Accessibility Insights
    Tota11y for Drupal

    ~~~~~~~~~~

    Proposal 2025: SARIF Integration for Error Reporting

    Project Description
    This project aims to enhance Drupal’s automated testing and error reporting ecosystem by integrating support for the Static Analysis Results Interchange Format (SARIF). SARIF is a standardized format for static analysis tool output, and by incorporating it into Drupal, developers will have a more consistent and efficient way to debug and analyze issues in both Drupal core and contributed modules. The proposed integration will enable Drupal’s CI pipeline to export error messages, linting reports, and test failures in SARIF format, making these results easily consumable by tools like GitHub Code Scanning, VS Code’s SARIF Viewer, and GitLab Security Dashboards.

    By developing a SARIF-compatible module or integration layer, the project will streamline the debugging process and facilitate better collaboration among developers, ultimately improving code quality and reducing turnaround time for fixes.

    Project Goal
    Functional Changes:
    Design and implement a conversion layer to translate Drupal’s CI output (errors, linting reports, test failures) into SARIF format.
    Develop a Drupal module or integration that automates the export of SARIF reports from the CI pipeline.
    Integrate the solution with popular CI/CD platforms such as GitHub Actions and GitLab CI for seamless workflow integration.
    Ensure compatibility with external tools including GitHub Code Scanning, VS Code’s SARIF Viewer, and GitLab Security Dashboards.
    Provide comprehensive documentation and usage guides to support adoption and future enhancements.
    Mentor Details
    Name & Slack: Royal Pinto
    Email / Slack: @royalpinto007

    Project Size
    350 hours

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Strong proficiency in PHP and JavaScript with hands-on experience in Drupal module development.
    Experience using Git for version control.
    Knowledge of compiler concepts and static analysis techniques, including ASTs and tokenization.
    Familiarity with CI/CD platforms, particularly GitHub Actions and GitLab CI.
    Understanding of standardized reporting formats and integration strategies.
    Project Resources
    SARIF Specification on GitHub
    Drupal Official Website
    Drupal Module Development Guide
    GitHub Actions Documentation
    GitLab CI Documentation
    R&D Tasks
    Research the SARIF specification and analyze Drupal’s existing CI output formats.
    Design a conversion layer that maps error messages, linting reports, and test failures to SARIF.
    Develop and integrate the SARIF module with Drupal’s CI pipeline to automate report generation.
    Establish integrations with GitHub Actions and GitLab CI to ensure seamless usage of SARIF reports.
    Conduct thorough testing, optimize performance, and prepare detailed documentation and tutorials for end users.


    ~~~~~~~~~~

    Proposal 2025: Entity Display Manager Module

    Project Description
    This project is focused on developing the Entity Display Manager Module for Drupal, designed to enhance field display customization by offering extended control over HTML structures, field output rewrites, and entity view mode configurations. The module addresses common development pain points by enabling site builders to manage field rendering without resorting to custom theming or complex preprocess hooks. By streamlining the display management process, the module aims to significantly speed up development workflows and improve the overall flexibility of Drupal's presentation layer.

    Project Goal
    Functional Changes:
    Introduce custom field display settings with options to enable or disable the Field HTML wrapper, Label HTML wrapper, and default field classes.
    Implement a template override mechanism for advanced customization of field output.
    Provide robust field rewrite options to allow custom text overrides, transformation of fields into clickable links, character limit trimming, HTML tag stripping, and removal of extraneous whitespace.
    Extend entity view modes by allowing bundle-specific configurations and custom display settings per view mode.
    Seamlessly integrate the new settings into Drupal’s UI under the Manage Display section, utilizing field formatters or preprocess hooks as needed.
    Mentor Details
    Name: stanzin

    Email / Slack: @stan

    Project Size
    350 hours

    Project Difficulty
    Intermediate

    Project Skills/Prerequisite
    Strong proficiency in Drupal module development and a deep understanding of Drupal's entity and field systems.
    Expertise in PHP and familiarity with Drupal’s theming system, including template overrides and preprocess hooks.
    Experience with front-end technologies such as HTML, CSS, and JavaScript for effective UI integration.
    Understanding of configurable display settings and the nuances of entity view mode configurations.
    Knowledge of best practices in module design and development to ensure maintainability and performance.
    Project Resources
    Drupal Documentation
    Understanding Drupal’s Entity API
    Drupal Theming Guide
    R&D Tasks
    Analyze current limitations in Drupal's field display customization and identify key areas for enhancement.
    Design the module architecture to introduce extended control over HTML wrappers, field classes, and output rewrites.
    Develop custom configuration settings within the Manage Display interface, including checkboxes and template override options.
    Implement comprehensive field rewrite functionalities such as custom text, link transformations, character limit trimming, HTML stripping, and whitespace normalization.
    Extend entity view mode configurations to support bundle-specific custom display settings.
    Conduct thorough testing across different Drupal environments and document configuration guidelines and usage instructions.
    Key Features
    Custom Field Display Settings:
    Enable/disable field and label HTML wrappers.
    Remove default field classes.
    Provide template override options for advanced customization.
    Field Rewrite Options:
    Override field output with custom text.
    Transform fields into custom links.
    Implement character limit trimming for field values.
    Strip HTML tags and remove unnecessary whitespace from field output.
    Entity View Mode Configuration:
    Extend entity view modes to support custom display settings.
    Allow bundle-specific configurations per entity view mode.
    Integration with UI:
    Add settings under "Manage Display" in the entity form display.
    Implement a field formatter or use preprocess hooks for modifications.















      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/drupal-association/
    idea_list_url: https://www.drupal.org/project/issues/gsoc?text=2025&status=All&priorities=All&categories=All&version=All&component=All


  - organization_id: 36
    organization_name: Eclipse Foundation
    no_of_ideas: 4
    ideas_content: |
        Agentic UI Framework for Eclipse LMOS
        Description 
        Eclipse LMOS has established a strong foundation for multi-agent backend architecture, enabling true backend agents that operate independently. However, the user interface (UI) has not yet been designed with the same agentic principles. This project aims to create an Agentic UI Framework specifically for agent builders using LMOS. The framework will provide a unified, configurable interface for deploying, managing, and configuring backend agents, allowing agent builders to efficiently oversee agent operations.

        The goal is to design a UI system where agent builders interact with their deployed agents in a natural, goal-driven manner, rather than manually configuring settings or navigating complex menus. This would involve:

        UI agents that dynamically emerge based on the context and operational needs of agent builders.
        A single, centralized interface for managing all LMOS agent deployments and configurations.
        Extensible UI components that align with agentic principles, enabling customization for different agent use cases.
        Benefits to the Community

        Empowers agent builders with an intuitive UI for configuring and managing LMOS agents.
        Simplifies interactions for non-technical users by abstracting complexity while providing advanced controls for experienced developers.
        Creates a specialized UI framework that aligns with LMOS’s multi-agent backend and enhances agent deployment workflows.
        Enables flexible, adaptive user interfaces that evolve based on agent builder requirements.
        Links to Eclipse Project
        Eclipse LMOS @ EF PMI
        Eclipse LMOS Website
        Expected outcomes 
        Initial Agentic UI Framework – A set of UI components that dynamically instantiate based on agentic needs.
        Prototype Interface for Agent Control – A single UI where users can control, configure, and observe agent activity.
        Integration with ARC View – Extending the ARC View to function as a local dev platform for UI-driven agent workflows.
        Extensibility Support – Ensuring that new agents can seamlessly introduce new UI components.
        Skills required/preferred 
        React & Modern Frontend Expertise: Strong skills in React, JavaScript, HTML, and CSS for building the UI.
        Dynamic & Adaptive UI Design: Ability to create UIs that change based on context and user needs.
        Agent System Understanding: Basic knowledge of agent concepts to build an effective agent-focused UI.
        Extensible Component Architecture: Skill in designing modular UI components that can be easily expanded.
        Project size 
        175 hours or 350 hours (scope can be adjusted)

        Possible mentors: 
        Arun Joseph
        Robert Winkler
        Patrick Whelan
        Kai Kreuzer

        ~~~~~~~~~~


        [Eclipse 4diac] Improving the Usability of the 4diac IDE's State Machine Editor


        Description 
        Over the last years, the several graphical editors in 4diac IDE were reworked to make them more usable and to reduce the number of user interactions. These are, for example, the FB network editor or the FB interface editor. One editor that didn't get this treatment is the state machine editor. With this project, we would like to explore what issues we have in this editor and how we can improve the usability of this editor. Topics of a potential project are:

        identify usability issues
        group them regarding implementation effort
        fix usability issues
        document results


        Links to Eclipse Project

        Both to the Eclipse 4diac and Project Repository.

        Expected outcomes 

        List of usability issues
        List of potential improvements
        Improved editor
        Documentation of results


        Skills required/preferred 
        Strong programming skills in Java and knowledge of Eclipse GEF Classic are required.

        Project size 
        175 hours or 350 hours (scope can be adjusted)

        Possible mentors: 
        Alois Zoitl

        Rating 
        Hard

        ~~~~~~~~~~

        GlitchWitcher: AI-assisted Bug Prediction



        Description 
        Defects exist in source code.  Some defects are easily found during code reviews, some are revealed by proper unit or integration testing.  Before code is reviewed and the code is executed during testing, there are other ways to detect where bugs may be lurking.
        The intent of this project is to trial and compare 2 approaches to predicting where defects live in source code.  There are a lot of different research papers that exist that discuss different algorithms that assist in finding defects.  This project will focus on implementation of 2 approaches.

        Approach 1: Predicting Faults from Cached History
        This first approach is a relatively simple, inexpensive technique for predicting where bugs live.  It is outlined in this research paper [1].  We would revive an earlier prototype of BugTools [2], which is a utility that applies the BugCache/FixCache algorithm to selected Github repositories to return scores against the different files in the repositories, as per the algorithm outlined in the paper.  Those with the highest hit rates are the most likely to contain defects, so are the more important to cover thoroughly with tests.
        This phase of the project is not expected to take very long, it is really to warm up to the idea of analyzing source code and integrating a new verification ‘check’ into a workflow of a Github repository.  This utility would report top 10 files that are most likely to contain defects.

        Approach 2: Reconstruction Error Probability Distribution (REPD) model
        The 2nd approach utilizes a supervised anomaly detection/classification model outlined in this research paper [3] to categorize defective and non-defective code.  This approach is much more involved.  Section 3 of the paper describes the model in use, while section 4 describes the methodology.  They train against datasets from NASA ESDS Data Metrics project [4].
        As part of this project, participants are asked to try and reproduce the REPD model described in this paper and apply it to both the data used by the researchers to see if similar results are found and then apply it to a separate C/C++ code base such as OpenJ9 [5] or OpenJDK [6] (or both).
        Ideally, one of the outcomes of this project would be to compare the results found with Approach A versus Approach B when applied against the same codebase.  A second outcome of this work would be to incorporate an interim verification check against a source code repository, perhaps on the cadence of every time a new tag is applied.

        Reference Links
        [1] https://web.cs.ucdavis.edu/~devanbu/teaching/289/Schedule_files/Kim-Predicting.pdf
        [2] https://github.com/adoptium/aqa-test-tools/tree/master/BugPredict/BugTool
        [3] https://www.sciencedirect.com/science/article/abs/pii/S0164121220301138
        [4] https://www.earthdata.nasa.gov/about/data-metrics
        [5] https://github.com/eclipse-openj9/openj9
        [6] https://github.com/adoptium/jdk

        Links to Eclipse Projects / Repositories

        https://projects.eclipse.org/projects/adoptium.aqavit
        https://projects.eclipse.org/projects/adoptium.temurin
        https://projects.eclipse.org/projects/technology.openj9
        https://github.com/adoptium/aqa-tests
        https://github.com/adoptium/aqa-test-tools
        https://github.com/eclipse-openj9/openj9
        https://github.com/adoptium/jdk (mirror of upstream repository)

        Expected outcomes 


        Trialing 2 different approaches (implemented as static analysis 'utilities’) to predict source code defects in a given source code base


        A comparison of the 2 approaches (do they identify the same files in a code base as ‘most likely’ containing bugs)


        An additional way to flag areas of code that need more scrutiny during code reviews and a greater emphasis during testing


        A verification check (or workflow, a.k.a. GlitchWitcher) that runs these static analysis utilities against pull requests in a repository



        Skills required/preferred 


        Languages & Frameworks: Python (for ML and automation), Git APIs, NLP libraries (e.g., SpaCy, BERT, GPT-based models).  Awareness of different classifiers (Gaussian Naive Bayes, logistic regression, k-nearest-neighbors, decision tree, and Hybrid SMOTE-Ensemble) and statistical analysis will be helpful.


        CI/CD Integration: GitHub Actions, Jenkins


        Database & Storage: MongoDB (or PostgreSQL/MySQL) for storing historical build data and test results.


        Deployment: integration with current development workflow and pipelines



        Project size 
        350 hours

        Possible mentors: 

        Lan Xia lan_xia@ca.ibm.com

        Longyu Zhang longyu.zhang@ibm.com

        Shelley Lambert slambert@redhat.com



        Rating 
        medium - hard


        ~~~~~~~~~~

        CommitHunter: AI-Powered Commit Debugger

        Description 
        The goal of this project is to develop an automated system that identifies problematic Git commits causing test failures in both performance and non-performance test scenarios. Given a "Good" build (where tests pass) and a "Bad" build (where tests fail), the system will analyze all intermediate commits to pinpoint the problematic commit(s). The approach will evolve from rule-based methods to AI-driven models for higher accuracy.
        Phase 1: Rule-Based Approach
        Use Rule based approach and below are some examples:

        String Matching: Identify test failure messages and correlate them with commit messages, logs, or diffs.
        Binary Search for Performance Tests: Implement a binary search approach to efficiently narrow down the commit range in performance test failures.

        Phase 2: Machine Learning Model

        Train an ML model using historical build data, commit logs, and failure reports.
        Utilize supervised learning techniques to classify commits as "Likely Problematic" or "Safe."
        Use natural language processing (NLP) to analyze commit messages and correlate them with test failures.

        Phase 3: Automation and Integration

        Integrate the approach with Git repositories (e.g., GitHub) via APIs.
        Develop a bot to automatically comment on Git issues with the identified problematic commit(s) if confidence levels meet a reliability threshold.
        Provide a dashboard for tracking identified problematic commits and their validation over time.


        Links to Eclipse Project


        https://github.com/eclipse/openj9
        https://github.com/adoptium/aqa-tests
        https://github.com/adoptium/aqa-test-tools
        https://projects.eclipse.org/projects/technology.openj9
        https://projects.eclipse.org/projects/adoptium.aqavit


        Expected outcomes 

        Reduction in manual effort needed to debug and triage test failures.
        Faster identification of problematic commits leading to improved development efficiency.
        A scalable system that can be expanded with improved AI models that can apply to other project and team.


        Skills required/preferred 

        Languages & Frameworks: Python (for ML and automation), Git APIs, NLP libraries (e.g., SpaCy, BERT, GPT-based models).
        CI/CD Integration: GitHub Workflow, Jenkins
        Database & Storage: MongoDB (or PostgreSQL/MySQL) for storing historical build data and test results.
        Deployment: integration with current development workflow and pipelines.


        Project size 
        350 hours

        Possible mentors: 

        Shelley Lambert slambert@redhat.com

        Lan Xia lan_xia@ca.ibm.com

        Longyu Zhang longyu.zhang@ibm.com



        Rating 
        Medium - hard







      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/eclipse-foundation/
    idea_list_url: https://gitlab.eclipse.org/eclipsefdn/emo-team/gsoc-at-the-ef/-/issues/?sort=due_date&state=opened&label_name%5B%5D=Project%20Idea&label_name%5B%5D=GSoC%202025&first_page_size=50


  - organization_id: 37
    organization_name: Electron
    no_of_ideas: 7
    ideas_content: |
    
       Electron Documentation PR Previews
      Size: 175 Hours (Medium)
      Difficulty: Medium
      Tags: JavaScript, Static Site Generators, MDX, Automation, GitHub Actions
      Scope: Electron maintains its documentation as MDX files in the main electron/electron repository, but the website lives in a separate repo that pulls in the docs as content via webhook.
      There’s a disconnect in our system between when docs get merged and when they appear on the website, so it’s easy for formatting issues to appear. This project would aim to create automation tooling that would create an ephemeral deployment of the website for each PR containing doc changes which previews the website with the changes, and link it as a comment on the PR.
      A previous version of this project idea referenced Heroku Review Apps as the technology of choice for implementation. While we still have access to Heroku deployments, the website has migrated its PR deploys to use Cloudflare Pages instead. Either technology would be available for a GSoC project.
      Success Criteria: A successful project will produce documentation PR previews and create a comment on PRs linking that preview. Ideally, previews from PRs coming from forks would still work.
      Skills:
      TypeScript/JavaScript (required)
      Resources:
      Website Repo: https://github.com/electron/website
      Heroku Review App APIs: Platform API Reference | Heroku Dev Center
      Cloudflare Preview Deployments
      Mentors:
      @dsanders11
      @erickzhao
      @yangannyx

      ~~~~~~~~~~
       Save/Restore Window State API
      Size: 175 Hours (Medium)
      Difficulty: Hard
      Tags: Electron Core, Feature, Chromium, C++, API development
      Scope: Currently Electron does not have any built-in mechanism for saving and restoring the state of BrowserWindows, but this is a very common need for apps that want to feel more native. Create an API in Electron to save, restore, and clear window state: position, size, maximized state, etc.
      Success Criteria:
      API spec is accepted by Electron’s API working group
      New window state API is added to the BrowserWindow module and merged into the main branch for a future Electron release
      Proper API documentation is added
      Skills:
      C++ (required)
      Resources:
      GitHub issue tracker discussion: https://github.com/electron/electron/issues/526
      Popular userland module for this feature: GitHub - mawie81/electron-window-state: A library to store and restore window sizes and positions for your Electron app https://github.com/mawie81/electron-window-state
      Electron’s BrowserWindow API docs
      Mentors:
      @dsanders11
      @vertedinde
      @georgexu99

      ~~~~~~~~~~
       Electron Forge CLI UX Improvements
      Size: 175 Hours (Medium)
      Difficulty: Medium
      Tags: Electron Forge, TypeScript, Node.js, Bash, CLI tools
      Scope: Electron Forge is a complete tool for building and publishing Electron applications. In 2022, Forge became the official packaging tool for Electron. However, Forge’s CLI is not as user friendly and modern as other tools in the JS ecosystem. This project would update the UX for the Forge CLI to make it easier for developers to create new Electron apps entirely from the command line.
      Success Criteria:
      The Electron Forge CLI is updated to include accurate flows from init, start, package and other major commands and expanded to allow configuring the options most commonly used by users.
      A new interactive mode to allow for a more user friendly experience.
      Tested cross-platform on Mac, Windows and Linux
      Stretch goals would include adding new features to the CLI
      Skills:
      Typescript/Javascript (required)
      Bash (recommended)
      Node.js (recommended)
      Resources:
      Electron Forge Documentation: https://www.electronforge.io/
      Forge Docs Repo: https://github.com/electron-forge/electron-forge-docs
      Mentors:
      @dsanders11
      @erickzhao
      @blackhole1

      ~~~~~~~~~~
       Electron Fiddle in VS Code
      Size: 175 Hours (Medium)
      Difficulty: Medium
      Tags: Electron Fiddle, TypeScript, Node.js, VS Code
      Scope: Electron Fiddle is a standalone tool used daily by Electron maintainers to help debug and fix issues reported in Electron, and is the best way for developers to provide minimal repro cases for their bug reports. This project would involve taking the internals of Electron Fiddle and integrating them as a VS Code extension, to allow using Fiddle’s logic without leaving the IDE, and more easily use Fiddle with a local build of Electron. This would require a reimagining of the Electron Fiddle UX to work within the constraints of the UI/UX available to VS Code extensions, rather than a 1:1 port of the UI from the standalone Electron Fiddle.
      Success Criteria:
      A working VS Code extension which integrates @electron/fiddle-core to provide an Electron Fiddle experience, with emphasis on more easily working with local builds of Electron.
      Ability to import/export GitHub Gists, integrating with VS Code’s existing GitHub auth.
      Skills:
      TypeScript/JavaScript (required)
      Node.js (preferred)
      Resources:
      Electron Fiddle: https://electronjs.org/fiddle
      fiddle-core repo, with Fiddle’s core logic: https://github.com/electron/fiddle-core
      Visual Studio Code Extension API
      Mentors:
      @dsanders11
      @blackhole1
      @georgexu99

      ~~~~~~~~~~
       Releases Working Group Calendar Website
      Size: 175 Hours (Medium)
      Difficulty: Easy/Medium
      Tags: Electron, web, APIs
      Scope: Electron’s Releases Working Group currently have several tracks of automation that help us track and communicate details about Electron’s major stable releases. However, the various tracks of automation are currently disjointed, and don’t always communicate with each other. Additionally, we depend on an upstream Chromium release schedule that we update manually, a process we would like to automate.
      Success Criteria: This project asks the contributor to 1) add new automation to allow Electron to pull dates from Chromium’s upstream API and 2) make a barebones Release calendar website that the Electron team can view, based on Chromium's alpha, beta and stable release dates.
      Going above and beyond could include:
      Creating a page on releases.electronjs.org with the information
      Setting up a bot that allows the Electron team to update the calendar automatically or add additional dates and details
      Alerting #wg-releases and updating the calendar automatically if upstream dates change
      Skills:
      TypeScript/JavaScript (required)
      Express (preferred)
      HTML/CSS (preferred)
      Resources:
      Chromium’s releases API - https://chromiumdash.appspot.com/schedule
      Releases website - https://releases.electronjs.org/
      Release website repo - https://github.com/electron/release-status
      Mentors:
      @dsanders11
      @vertedinde
      @yangannyx

      ~~~~~~~~~~
       Electron Fiddle Support for Fuses
      Size: 175 Hours (Medium)
      Difficulty: Easy/Medium
      Tags: Electron Fiddle, TypeScript, Node.js
      Scope: Electron Fiddle is a standalone tool used daily by Electron maintainers to help debug and fix issues reported in Electron, and is the best way for developers to provide minimal repro cases for their bug reports. Electron fuses are feature toggles that are set when packaging an Electron app to enable / disable certain features / restrictions. Currently Electron Fiddle has no support for setting these fuses when running a fiddle, so it cannot be used to to provide repro cases for bug reports involving specific fuse settings. This project would add support to Fiddle for setting fuses before running the fiddle, and include the fuse values in any fiddles uploaded to GitHub Gists.
      Success Criteria:
      An intuitive UI is added to Electron Fiddle which allows users to view and toggle values for Electron fuses to be set on the next run.
      Fuse values are included in fiddles uploaded to a GitHub gist, and fuse values are set when loading a fiddle from a GitHub gist.
      Skills:
      TypeScript/JavaScript (required)
      Node.js (preferred)
      React (preferred)
      HTML/CSS (preferred)
      Resources:
      Electron Fiddle: https://electronjs.org/fiddle
      fiddle-core repo, with Fiddle’s core logic: https://github.com/electron/fiddle-core
      Electron fuses repo: https://github.com/electron/fuses
      Electron’s fuses docs
      Mentors:
      @dsanders11
      @erickzhao  
      @vertedinde

      ~~~~~~~~~~
       Restore Electron Devtron Extension
      Size: 175 Hours (Medium)
      Difficulty: Medium
      Tags: JavaScript, TypeScript, Chrome Extension, IPC, Developer Tool
      Scope: Inter-process communication (IPC) is a key part of building feature-rich desktop applications in Electron. App developers getting started with Electron often struggle in understanding IPC concepts and how to work with them; multiple processes and finding where IPC is sent and received isn’t straightforward. Devtron was a Chrome extension available for Electron which brought visibility to IPC events by adding a new tab under Chrome DevTools where each IPC event was logged. It was eventually deprecated due to a lack of available maintainers, and an aging codebase. This project would modernize the existing Devtron codebase or create a new Chrome extension from scratch (many APIs in Chrome and Electron have changed in the 10 years since Devtron was created) to fill this continuing need for app developers.
      Success Criteria:
      A successful project will produce a Chrome extension that, when installed to Electron, displays IPC events sent between processes.
      Skills:
      TypeScript/JavaScript (required)
      Chrome Extensions (preferred)
      Resources:
      Deprecated Devtron: 
      Chrome Extensions: Extend DevTools
      Electron’s IPC docs
      Mentors:
      @dsanders11
      @samuelmaddock
      @yangannyx
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/electron/
    idea_list_url: https://electronhq.notion.site/Electron-Google-Summer-of-Code-2025-Ideas-List-1851459d1bd1811894dad8b48a68596d

  - organization_id: 38
    organization_name: FFmpeg
    no_of_ideas: 6
    ideas_content: |
      
      WebRTC-HTTP ingestion protocol (WHIP)
      Description: The WHIP task only requires the implementation of basic streaming capabilities as outlined in https://www.ietf.org/archive/id/draft-ietf-wish-whip-16.txt. Given that WebRTC itself is extremely large and complex, FFmpeg has no plans to fully implement WebRTC capabilities in the short term. Therefore, it will only support the most basic ICE interaction capabilities, the most basic DTLS capabilities, and minimal modifications to FFmpeg's RTP/RTCP workflow to enable WHIP support.
      Expected results: This will allow FFmpeg users to quickly utilize FFmpeg for WHIP streaming.
      Prerequisites: Good C code, basic familiarity with Git, and basic knowledge of network transport protocols.
      Difficulty: Medium to Hard
      Qualification Task: Fix a random bug in an existing muxer, demuxer or protocol.
      Mentor: Steven Liu (lingjiujianke at gmail dot com)
      Backup Mentor: Zhao Jun (barryjzhao at tencent dot com)
      Duration: 350 hours

      ~~~~~~~~~~
      ProRes Vulkan decoder
      Description: Decoding of ProRes is quite important, as its the default mezzanine codec used in production. The bandwidth and CPU requirements for 4k and 8k streams are high, therefore, having a Vulkan-based implementation would be able to speed up the workflow of users, particularly those who do editing.
      Expected results: Write a ProRes decoder in Vulkan, specifically supporting the ap4h and ap4x profiles.
      Prerequisites: Good C, GLSL, and Vulkan knowledge.
      Difficulty: Hard
      Qualification Task: Write and validate the ProRes DCT transform in GLSL.
      Mentor: Lynne (Lynne in #ffmpeg-devel on Libera.Chat IRC)
      Backup Mentor: Niklas Haas (haasn in #ffmpeg-devel on Libera.Chat IRC)
      Duration: 350 hours

      ~~~~~~~~~~
      VVC wasm simd optimization
      Description: Using WASM SIMD to optimize VVC decoding performance in WASM environment (web browser or wasi)
      Expected results: Add ALF, inter, and SAO implementation. More is better.
      Prerequisites: Good C knowledge, basic shell script skills and understanding of compilation and build processes.
      Difficulty: Medium.
      Qualification Task: Fix a random bug in FFmpeg. Build and run ffmpeg checkasm in wasm. It's easy to be done with wasi runtime like wasmtime.
      Mentor: Zhao Zhili (zhilizhao at tencent dot com)
      Backup Mentor: Nuo Mi (nuomi2021 at gmail dot com)
      Duration: 350 hours

      ~~~~~~~~~~
      VVC ARM simd optimization
      Description: Using ARM simd to optimize VVC decoding performance in arm64 environment.
      Expected results: Implement inverse transform and intra prediction using arm64 instructions.
      Prerequisites: Good C knowledge. Basic ARM assembly programming skills.
      Difficulty: Hard.
      Qualification Task: Fix a random bug in FFmpeg. Build and run ffmpeg checkasm in arm64.
      Mentor: Zhao Zhili (zhilizhao at tencent dot com)
      Backup Mentor: Nuo Mi (nuomi2021 at gmail dot com)
      Duration: 350 hours

      ~~~~~~~~~~
      VVC x86 simd optimization
      Description: Using x86 simd to optimize VVC decoding performance in x86 environment.
      Expected results: Implement inverse transform or intra prediction using x86 instructions.
      Prerequisites: Good C knowledge. Basic x86 assembly programming skills.
      Difficulty: Hard.
      Qualification Task: Any patch merged by ffmpeg
      Mentor: Nuo Mi (nuomi2021 at gmail dot com)
      Backup Mentor: Lynne (Lynne in #ffmpeg-devel on Libera.Chat IRC)
      Duration: 350 hours
      
      ~~~~~~~~~~
      VP6 encoder
      Description: Write a basic non-performant progressive VP6 encoder, starting with keys frames and adding motion compensation frames later.
      Expected results: Bitstreams produced by the encoder are decodable using FFmpeg VP6 decoder and original On2 VP6 decoder binary.
      Prerequisites: Good C knowledge, some understanding of MPEG-type video compression would be useful.
      Difficulty: Medium.
      Qualification Task: Fix a random bug in FFmpeg _or_ extend libavcodec/vpx_rac.h to support encoding
      Mentor: Peter Ross (pross at xvid dot org)
      Backup Mentor: will be choosen before project begin, the admins will serve as backup before that
      Duration: 350 hours
     
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ffmpeg/
    idea_list_url: https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2025

  - organization_id: 39
    organization_name: FLARE
    no_of_ideas: 10
    ideas_content: |
      


      capa: add Binary Ninja Explorer plugin
      size: medium, estimated 175 hours
      difficulty: medium
      mentors: @williballenthin
      link: mandiant/capa#169
      capa is the FLARE team's open-source tool to identify program capabilities using an extensible rule set.
      Binary Ninja (Binja) is a modern disassembler and reverse engineering tool with a robust Python API that facilitates plugin development. A capa Explorer plugin for Binary Ninja would significantly enhance the workflow of reverse engineers who use Binja, allowing them to seamlessly identify and analyze program capabilities within their preferred environment. This project would not only benefit Binja users but also expand the reach and adoption of capa within the reverse engineering community.
      The core functionality of the plugin would be to:
      Use capa's existing Binary Ninja backend to find capabilities in the currently open binary.
      Display the capa results in a user-friendly manner within Binary Ninja. This includes displaying matching rules, the locations of matched features, and potentially the associated source code (if debug information is available).
      Allow users to navigate from the capa results to the corresponding locations in the disassembly view. This is crucial for efficient analysis, enabling users to quickly jump to the code responsible for a detected capability.
      Deliverables:
      Results Display:
      Implement a custom dock widget (view) in Binary Ninja to display the capa results.
      Display a hierarchical tree view of matching rules, grouped by namespace (e.g., "anti-analysis", "communication").
      Show the rule name, description (short summary), and match status.
      Display the locations (addresses) of matched features within each rule.
      Implement filtering and searching capabilities within the results view. Allow users to filter rules by namespace, ATT&CK technique, or keyword.
      Highlight matched features directly in the disassembly view using Binary Ninja's highlighting API.
      Navigation:
      Enable double-clicking on a rule or feature location in the results view to navigate to the corresponding address in the Binary Ninja disassembly view. Highlight the relevant instruction(s).
      Add tags/bookmarks for the matches.
      Rule Selection:
      Basic UI for user to select a file path that contains the rulesets they'd like to use.
      Testing and Documentation:
      Write basic unit tests for the plugin's core functionality.
      Create user documentation explaining how to install and use the plugin.
      Blog Post:
      Document the development process and findings in a blog post suitable for publication on the Mandiant blog or a similar platform.
      Required Skills:
      Solid knowledge of Python 3.
      Experience with Binary Ninja's API (or strong willingness to learn).
      Basic understanding of reverse engineering concepts (disassembly, assembly language, executable file formats).
      Experience with Git and GitHub.
      Potential Challenges and Mitigation Strategies:
      Binary Ninja API Learning Curve: Binary Ninja's API is extensive, but well-documented. The contributor should allocate time for learning the API and exploring existing plugins. The mentors can provide guidance and point to relevant examples.
      Performance Optimization: Running capa on large binaries can be time-consuming. The plugin should be designed to handle large analysis results efficiently and provide progress feedback to the user. Asynchronous execution and caching strategies can be employed.
      UI Design: Provide the user an intuitive way to interact with the plugin.
      
      ~~~~~~~~~~
      capa: add Ghidra Explorer plugin
      size: medium, estimated 175 hours
      difficulty: medium
      mentors: @mike-hunhoff
      link: mandiant/capa#1980
      capa is the FLARE team's open-source tool to identify program capabilities using an extensible rule set. Currently, analysts often invoke capa as a command-line tool or via the capa Explorer plugin for IDA Pro. This project aims to bring the interactive rule exploration experience of capa Explorer to Ghidra, a powerful and extensible reverse engineering platform developed by the NSA.
      Ghidra is a free and open-source software reverse engineering (SRE) framework. It includes a suite of tools for analyzing compiled code on a variety of platforms. Ghidra's extensibility is a key feature, and recently, the PyGhidra project has provided Python bindings for the Ghidra API, enabling plugin development in Python. A capa Explorer plugin for Ghidra would greatly enhance the workflow of reverse engineers who rely on Ghidra, allowing them to seamlessly integrate capa's capability detection into their analysis process. This project would benefit both Ghidra users and expand the user base of capa.
      The core functionality of the plugin would be to:
      Use capa's existing Ghidra backend to find capabilities in the currently open binary.
      Display the capa results in a user-friendly manner within Ghidra. This includes showing matching rules, the locations of matched features (addresses, function names, etc.), and potentially linking to the relevant decompiler output.
      Allow users to navigate from the capa results to the corresponding locations in the Ghidra disassembly listing and decompiler views. This is critical for efficient analysis, enabling users to quickly jump to the code associated with a detected capability.
      Deliverables:
      Results Display:
      Implement a custom Ghidra Tool window or panel to display the capa results.
      Display a hierarchical tree view of matching rules, grouped by namespace (e.g., "anti-analysis", "communication").
      Show the rule name, description, and match status.
      Display the locations of matched features within each rule.
      Implement filtering and searching capabilities within the results view. Allow users to filter rules by namespace, ATT&CK technique, or keyword.
      Highlight matched features directly in the Ghidra listing view using Ghidra's highlighting API.
      Navigation:
      Enable double-clicking on a rule or feature location in the results view to navigate to the corresponding address in the Ghidra disassembly listing view.
      Highlight the relevant instructions.
      Rule Selection:
      Provide a basic UI for the user to select the file path containing the desired rulesets.
      Testing and Documentation:
      Write basic unit tests for the plugin's core functionality.
      Create user documentation explaining how to install and use the plugin.
      Blog Post:
      Document the development process, challenges and finding in a blog post.
      Required Skills:
      Solid knowledge of Python 3.
      Experience with Ghidra and PyGhidra (or strong willingness to learn). Familiarity with Java is a plus, but not strictly required due to PyGhidra.
      Basic understanding of reverse engineering concepts (disassembly, assembly language, executable file formats).
      Experience with Git and GitHub.
      Potential Challenges and Mitigation Strategies:
      PyGhidra Learning Curve: While PyGhidra simplifies Ghidra plugin development, the student will still need to learn the PyGhidra API and how it interacts with Ghidra's underlying Java API. The mentors can provide guidance and point to relevant examples.
      Performance Optimization: Running capa on large binaries can be time-consuming. The plugin should handle large results efficiently and provide feedback to the user. Asynchronous execution and caching can help.
      UI Design: Design the user interface to be intuitive within the Ghidra environment.

      ~~~~~~~~~~
      capa: add Frida dynamic analysis for Android
      size: large, estimated 350 hours
      difficulty: hard
      mentors: @larchchen
      capa is the FLARE team's open-source tool to identify program capabilities using an extensible rule set.
      Frida is a popular dynamic instrumentation toolkit for developers, reverse-engineers, and security researchers, allowing custom scripts injected into black box processes thus monitoring program behaviors. Frida is a particularly preferred option to analyze Android Apps by launching Apps in Android Emulator and intercepting certain function calls.
      In addition to the capa's dependencies on CAPE sandbox during dynamic capabilities detection, Frida is a more friendly alternative for mobile App analysis. With the possibilities of using existing Frida scripts and/or developing new Frida scripts, extending capa's dynamic detection upon logs generated from Frida logs would be a good start. Integrating capa rule matching engine with Frida scripts could be another bonus approach. The goal of this project is to support capa rule matching capabilities in Android via Frida instrumentation framework.
      Deliverables
      Research
      Review capa's existing support of dynamic capabilities detection
      Review Frida's instrumentation framework
      Identify Additions, Changes, and Improvements
      Suggest technical roadmaps to support Frida-capa detection
      Discuss ideas with mentors and capa user community
      Implementation
      Implement ideas aligned with finalized roadmaps
      Evaluation and Knowledge Sharing
      Test deliverables and gather feedback from users
      Write blog post about experience and project achievements
      Required Skills
      Solid knowledge of Python 3
      Solid knowledge of one of JavaScript/C/Go
      Basic understanding of reverse engineering / malware analysis
      Basic understanding of Git
      Basic understanding of Android App analysis using Android Emulator
      Basic understanding of Frida

      ~~~~~~~~~~
      capa: migrate to PyGhidra
      size: small, estimated 90 hours
      difficulty: low
      mentors: @mike-hunhoff
      link: mandiant/capa#2600
      This project aims to modernize the existing capa Ghidra backend by migrating it from the third-party Ghidrathon Python bindings to the officially supported PyGhidra bindings, released with Ghidra 11.3. Since PyGhidra is distributed with Ghidra, we expect this to have better long term support and be easier for users to access. This migration will ensure the long-term maintainability and compatibility of the capa plugin with future Ghidra releases.
      Deliverables:
      Port Existing Functionality: Migrate the existing capa Ghidra backend's code to use the PyGhidra API. This primarily involves updating API calls and adapting to any differences in how PyGhidra interacts with Ghidra.
      Testing: Thoroughly test the migrated plugin to ensure that all existing features function correctly with PyGhidra.
      Documentation Updates: Update the plugin's documentation to reflect the change to PyGhidra and provide installation instructions for users.
      Required Skills:
      Basic Python programming skills.
      Familiarity with Ghidra and its scripting capabilities, or willingness to learn.
      Experience with Git and GitHub.
      Understanding of capa is a plus, but not required for this project.

      ~~~~~~~~~~
      capa: add ARM support to IDA Pro, Ghidra, and/or Binary Ninja backends
      size: small to large
      difficulty: medium
      mentors: @mr-tz
      link: mandiant/capa#1774
      This project aims to extend capa's support for analyzing programs targeting the ARM architecture across its major analysis backends: IDA Pro, Ghidra, and Binary Ninja. While capa's core analysis engine (via the BinExport2 backend) already supports ARM, the backends for these popular disassemblers currently lack direct feature extraction for this architecture. This project will bridge that gap, enabling users to analyze ARM binaries seamlessly within their preferred reverse engineering environments.
      The core task involves extending the existing backends to extract relevant features (instructions, API calls, constants, etc.) from ARM binaries loaded in IDA Pro, Ghidra, and Binary Ninja. This will leverage the respective disassembler APIs to access the disassembled code and program information. The extracted features will then be formatted and passed to capa's core analysis engine.
      Deliverables:
      update capa IDA Pro backend (optional, pick 1-3)
      update capa Ghidra backend (optional, pick 1-3)
      update capa Binary Ninja backend (optional, pick 1-3)
      Testing: Develop test cases (ARM binaries with known capabilities) and verify that capa correctly identifies capabilities in these binaries through each of the extended plugins.
      Documentation: Update the documentation for each plugin to reflect the added ARM support.
      Required Skills:
      Solid Python programming skills.
      Familiarity with at least one of: IDA Pro, Ghidra, or Binary Ninja, and their respective plugin APIs (or willingness to learn quickly).
      Basic understanding of the ARM architecture and assembly language.
      Experience with Git and GitHub.

      ~~~~~~~~~~
      FLOSS: extract language specific strings (.NET, Swift, Zig, ...)
      size: large, estimated 350 hours
      difficulty: medium
      mentors: @mr-tz
      link: mandiant/flare-floss#718
      Various programming languages embed the constant data, like strings, used within executables in different ways. Most tools, like strings.exe, just look for printable character sequences. This doesn't work well for files compiled from Go or Rust.
      Here we propose to extend FLOSS to include a framework to extract language specific strings from executables. After identifying the language, a specific extractor can use specialized logic to pull out the strings embedded into a program by the author. When possible, the extractor should indicate library and runtime-related strings. For example, the extractor may parse debug information to recognize popular third party libraries and annotate the related strings appropriately.
      Today, FLOSS automatically deobfuscates protected strings found in malware. Better categorization of its output would make its users more efficient. Extracting language-specific strings would make FLOSS more useful and manifest success as the default tool used by security analysts.
      Deliverables
      Develop language identification module
      Initial focus on .NET
      Consider also Swift, Zig, …
      Research language string embeddings and create extractor code
      We can share existing knowledge and code to bootstrap this
      Identify strings related to runtime and library code for targeted programming languages
      Extend standard output format and render results
      Required Skills
      Medium knowledge of Python 3
      Basic understanding of reverse engineering (focus: Windows PE files)
      Experience with .NET or Swift (internals) is a plus, but not required
      Interest in malware analysis with focus on static analysis
      Basic understanding of Git

      ~~~~~~~~~~
      FLOSS: QUANTUMSTRAND
      size: large, estimated 350 hours
      difficulty: medium
      mentors: @williballenthin
      link: mandiant/flare-floss#943
      Extend FLOSS to use the rendering techniques pioneered by QUANTUMSTRAND.
      QUANTUMSTRAND is an experiment that augments traditional strings.exe output with context to aid in malware analysis and reverse engineering. For example, we show the structure of a file alongside its strings and mute/highlight entries based on their global prevalence, library association, expert rules, and more.
      FLOSS is a tool that automatically extracts obfuscated strings from malware, rendering the human-readable data in a way that enables rapid reverse engineering.
      We propose to extend FLOSS to use the techniques pioneered by QUANTUMSTRAND to highlight important information while muting common and/or analytically irrelevant noise. The project will provide an opportunity to dig into the PE, ELF, and/or Mach-O file formats, finding ways to make technical details digestible. If successful, FLOSS will continue to be the tool that malware analysts turn to when triaging unknown files.
      Deliverables
      Brand new output format released as part of FLOSS v4 in late 2025.
      Research
      Review Quantumstrand functionality
      Evaluate most useful features for integration into FLOSS
      Identify and Propose Improvements
      Suggest improvements for the user interface and experience
      Discuss ideas with mentors and FLOSS user community
      Implementation
      Implement improved functionality
      [stretch goal]: Work on a GUI to interactively display FLOSS results
      Evaluation and Knowledge Sharing
      Test improvements and gather feedback from users
      Write blog post about experience and project achievements
      Required Skills
      Solid knowledge of Python 3
      Basic understanding of reverse engineering / malware analysis
      Basic understanding of Git
      Experience or interest with file formats such as PE, ELF, and/or Mach-O
      Experience or interest in user interface and/or user experience design

      ~~~~~~~~~~
      BinDiff: rearchitect Binary Diff Server and port to PyQt
      size: large, estimated 350 hours
      difficulty: hard
      mentors: @cblichmann
      link: google/bindiff#17
      This project aims to modernize BinDiff by re-architecting it as a cross-platform "diffing service" with a unified UI layer. The core idea is to separate the diffing engine from the user interface. A "diff server," implemented (likely in C++ or Rust for performance), will handle the core diffing logic. This server will load BinExport files and perform the diffing computations. It will communicate with client plugins via a protocol like gRPC.
      Client plugins will be developed for IDA Pro and Binary Ninja, using a shared Python codebase and PyQt for the UI. Each disassembler will have a small, platform-specific module to handle tasks like symbol porting. This architecture promotes code reuse and simplifies maintenance. Keeping the diff server running in the background allows for dynamic re-diffing as binaries are modified, and opens up possibilities for improved flow graph visualization by combining data from multiple functions.
      The project scope is intentionally flexible, allowing the student and mentors to collaboratively define the specific features and implementation details. The focus will be on establishing a solid foundation for the new architecture and demonstrating its feasibility.
      Deliverables (Flexible, to be refined during the project):
      Diff Server Prototype:
      Design and implement a basic "diff server" that can load BinExport files and perform a simple diffing algorithm.
      Implement a communication protocol (e.g., gRPC) for interaction with client plugins.
      Shared UI Library (Python/PyQt):
      Develop a shared Python library using PyQt that provides the core UI components for displaying diffing results. This includes views for function lists, matched/unmatched functions, and potentially basic flow graph comparisons.
      IDA Pro and Binary Ninja Plugins:
      Create basic plugins for IDA Pro and Binary Ninja that utilize the shared UI library and communicate with the diff server.
      Implement symbol porting.
      Demonstrate basic diffing functionality within each disassembler.
      Proof of Concept:
      Demonstrate the ability to load two BinExport files, perform a diff, and display the results in both IDA Pro and Binary Ninja.
      Documentation:
      Document the design, architecture, and API of the diff server and client plugins.
      Required Skills:
      Solid knowledge of Python 3 and C++ and/or Rust.
      Experience with or willingness to learn PyQt.
      Experience with or willingness to learn gRPC.
      Basic understanding of binary diffing concepts.
      Familiarity with IDA Pro and Binary Ninja APIs (or strong willingness to learn).
      Experience with Git and GitHub.
      Potential Challenges:
      Defining the Scope: The open-ended nature of the project requires careful planning and communication between the student and mentors to define achievable goals.
      Inter-process Communication: Choosing and implementing an efficient and reliable communication protocol between the diff server and client plugins will be crucial.

      ~~~~~~~~~~
      XRefer: Build a Multi-Backend Abstraction Layer with Binary Ninja Support
      size: large, estimated 360 hours
      difficulty: medium
      mentors: @m-umairx
      XRefer is tightly coupled with IDA Pro, making it challenging to adapt for use with other popular reverse-engineering platforms like Ghidra or Binary Ninja. This project aims to refactor XRefer's core analyzer component, creating a new backend abstraction layer that standardizes how different platforms interact with the plugin's logic. Additionally, the project aims to aid support for Binary Ninja by implementing a new PoC backend.
      Note: This project focuses on creating and demonstrating an abstraction layer for XRefer's underlying analysis engine only. The user interface is not included in the project scope.
      Deliverables:
      Code Review
      Identify and document all places where IDA-specific APIs or data structures are used within the analyzer and lang components.
      Assess the feasibility and scope of decoupling those calls into a new abstraction layer.
      Design a Backend Interface
      Specify the APIs needed for core tasks (e.g., disassembly, cross-references, function discovery, flow analysis) that different backends must implement.
      Draft an interface or set of classes that each supported platform (IDA, Ghidra, Binary Ninja, etc.) can plug into with minimal friction.
      Refactor XRefer
      Migrate IDA-specific logic into a separate module or wrapper.
      Adapt XRefer's main codebase to use the newly created backend interface rather than direct IDA calls.
      Proof-of-Concept for Additional Backends
      Implement a PoC backend using Binary Ninja's API.
      Demonstrate how XRefer can run independently of IDA using the newly defined backend interface to generate a .xrefer analysis file.
      Outline best practices for future contributors to add and maintain backends.
      Required Skills
      Proficiency in Python programming language.
      Experience with (or strong willingness to learn) IDA's Python API.
      Experience with (or strong willingness to learn) Binary Ninja's API.
      Basic understanding of reverse engineering and underlying concepts (disassembly, functions, cross-references) and executable file formats.
      Basic knowledge of Git/Github.

      ~~~~~~~~~~
      XRefer: HTML Exporter and Visualizer for XRefer's Cluster Analysis
      size: medium, estimated 160 hours
      difficulty: low
      mentors: @m-umairx
      The goal of this project is to design and implement an HTML export module for XRefer. The module will convert XRefer's internal cluster analysis data into a dynamic HTML visualization. This interactive output should allow users to:
      View Cluster Graphs: Render detailed graphs illustrating the relationships between clusters.
      Read Semantic Descriptions: Provide natural language explanations for each cluster and its contained functions.
      Interact with Data: Offer interactive controls (e.g., zoom, pan, node selection, filtering) to explore and analyze clusters in depth.
      Deliverables:
      Design and Architecture
      Develop an intuitive UI/UX design that outlines how clusters and their semantic descriptions will be presented. Consider interactive elements such as zoomable graphs, clickable nodes, and filtering options.
      Evaluate and choose suitable front-end libraries or frameworks (e.g., D3.js, Cytoscape.js) for rendering graphs and managing interactivity.
      Develop the HTML Export Module
      Create a Python module to convert XRefer's cluster analysis data into a format consumable by the front-end (e.g., JSON).
      Develop a responsive HTML template that integrates the chosen visualization libraries. The template should include placeholders for cluster graphs, semantic descriptions, and interactive controls.
      Implement features such as zoom, pan, node highlighting, and tooltips to enhance the user's exploratory experience.
      Integrate the export module into the existing XRefer workflow so that a .html file is generated as part of the analysis process.
      Documentation
      Document the design decisions, data transformation process, and integration steps to help future contributors extend or maintain the module.
      Required Skills
      Proficiency in Python programming language.
      Familiarity with HTML, CSS, and JavaScript for building interactive web interfaces.
      Experience with visualization libraries (e.g., D3.js, Cytoscape.js) or willingness to learn how to implement interactive graphs.
      Ability to conceptualize and design an intuitive user interface that effectively presents complex data.
      Basic knowledge of Git/Github.
      GoReSym: project in scope
      mentors: @stevemk14ebr
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/flare/
    idea_list_url: https://github.com/mandiant/flare-gsoc/blob/2025/doc/project-ideas.md


  - organization_id: 40
    organization_name: FOSSASIA
    no_of_ideas: 15
    ideas_content: |

        1. Enhance Usability of the eventyay Badge Creator
        Description: The goal of this project is to streamline the process of generating and printing badges for eventyay. Organizers often need to print badges on the spot for attendees and speakers during check-in. The project will focus on adding a user-friendly badge customization feature in the frontend, ensuring seamless badge generation in the backend, and integrating with the check-in system to allow for on-site printing. Specifically, the tasks include:
        Badge Design and Customization
        Backend API Enhancements
        Check-In App Integration for On-Site Printing
        Print Service and Configuration
        Containerization & Deployment
        Performance and Scalability
        Documentation and Testing
        Repository URLs: 
        1. https://github.com/fossasia/eventyay-tickets
        2.https://github.com/fossasia/eventyay-tickets-exhibitors 


        Expected outcome: 
        Organizers can create custom badges for different user groups (speakers, attendees, volunteers, etc.).
        Attendees are able to check in quickly and receive a printed badge on the spot via the integrated check-in app.
        The system is fully documented and containerized, making it easy to set up for events of varying sizes.
        Comprehensive tests ensure reliability and maintainability of the new badge-related features.


        Skills required/preferred: 
        Python (Django backend)
        JavaScript (Ember.js for frontend in open-event-frontend)
        Docker & Kubernetes (containerization and deployment)
        CI/CD (to automate builds, tests, and deployments)
        Familiarity with printing workflows and/or label printing software (preferred)
        Possible mentors: hongquan, marcoag, norbusan, cweitat, DonChiaQE, shaunBoahCodes
        Expected Size: 350 hours
        Difficulty level: Intermediate

        ~~~~~~~~~~
        2. Convert eventyay-tickets VueJS implementation to Single Page application
        Description: The current eventyay-tickets system, built with VueJS, uses a traditional multi-page approach. While functional, this architecture can lead to slower page transitions, higher latency, and a less responsive user experience. The proposed project aims to refactor the existing implementation into a full Single Page Application (SPA) that leverages modern VueJS techniques for client-side routing, state management, and lazy loading.
        Key components of the project include:
        Routing and Navigation
        Integrate Vue Router to handle all in-app navigation without full page reloads.
        Design a smooth and intuitive navigation experience that includes dynamic routing for ticket details, user profiles, and event overviews.
        State Management
        Utilize Vuex (or an equivalent state management solution) to manage application-wide state.
        Ensure that ticket data, user sessions, and event information are synchronized across all components.
        Performance Optimization
        Implement code splitting and lazy loading to minimize initial load times and improve overall performance.
        Optimize API interactions to efficiently fetch and update data without redundant network requests.
        UI/UX Enhancements
        Redesign critical user interfaces to align with SPA principles, offering a more fluid and interactive experience.
        Enhance error handling, loading indicators, and overall responsiveness to create a seamless experience for users on various devices.
        Progressive Enhancement and Backward Compatibility
        Ensure that the new SPA implementation gracefully degrades for users with JavaScript disabled or limited browser support.
        Provide fallbacks and clear communication for any features that depend on modern browser capabilities.
        Testing and Documentation
        Develop a suite of unit, integration, and end-to-end tests to ensure robust functionality throughout the transition.
        Update documentation to reflect the new SPA architecture and guide future contributors in maintaining the codebase.
        Repository URL
        eventyay-tickets Repository
        Expected Outcome
        At the end of the project, the eventyay-tickets system will:
        Operate as a fully functional Single Page Application, resulting in faster navigation and a more engaging user experience.
        Offer a robust and maintainable codebase with modern state management and routing practices.
        Include comprehensive testing and clear documentation to facilitate future development and onboarding of contributors.
        Seamlessly integrate with existing eventyay services while ensuring backward compatibility where needed.
        Skills Required/Preferred
        JavaScript & VueJS: Deep understanding of VueJS, including Vue Router and Vuex (or similar state management libraries).
        Front-End Development: Strong grasp of modern front-end development best practices, including component-based architecture and performance optimization.
        Testing Frameworks: Experience with testing tools (e.g., Jest, Cypress) for ensuring code reliability and robustness.
        API Integration: Familiarity with RESTful APIs and efficient data handling in client-side applications.
        UI/UX Design: Ability to create intuitive and responsive interfaces, with attention to detail for user experience improvements.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 350 hours
        Difficulty level: Advanced

        ~~~~~~~~~~
        3. Implement eventyay features as a plugin in eventyay-talk
        Description: The goal of this project is to enhance the design and functionality of eventyay-talk and to integrate it more closely with eventyay-tickets and eventyay-video. While in the first version we already implemented some integration features, this project aims to expand on that work by decoupling the custom functionalities from the core eventyay-talk component. The approach will be to create a modular, admin-manageable plugin that provides seamless integrations without compromising upstream updates.
        Objectives:
        Modular Plugin Architecture:
        Develop an independent admin plugin that encapsulates all eventyay-specific integrations. This ensures that custom features remain separate from the core eventyay-talk codebase, facilitating easier updates and maintenance.
        Enhanced Integration with eventyay-tickets and eventyay-video:
        Integrate ticketing data to display attendee information, session details, and speaker profiles within the discussion threads of eventyay-talk.
        Integrate video content so that related event sessions or live streams are readily accessible from within the talk platform.
        Synchronize user profiles and session metadata across these services for a consistent experience.
        Improved Design and Usability:
        Enhance the overall UI/UX of eventyay-talk to match the branding and design language of the eventyay ecosystem.
        Provide a centralized admin interface where event organizers can configure and control integrations, toggle features, and manage settings without modifying core code.
        Future-Proof and Maintainable Code:
        Keep the integration functionality in-line with upstream eventyay-talk releases, ensuring compatibility and ease of updates.
        Write comprehensive documentation and tests to support long-term maintenance and community contributions.
        Repositories:
        eventyay-talk Repository
        eventyay-tickets Repository
        eventyay-video Repository
        Expected Outcome:
        At the end of the project, the eventyay-talk platform will have a standalone, well-documented plugin that:
        Integrates ticketing and video functionalities directly within the environment.
        Offers a seamless and consistent user experience across eventyay components.
        Is easily configurable through an admin interface, reducing the need to modify core eventyay-talk code.
        Maintains alignment with upstream changes, ensuring long-term compatibility and simplified maintenance.
        Includes robust testing and clear documentation to facilitate future enhancements and community contributions.
        Skills Required/Preferred:
        JavaScript & Frameworks: Strong knowledge of JavaScript and experience with the framework used in eventyay-talk using Vue.js).
        Plugin Architecture: Familiarity with creating modular plugins and extension points in a larger application ecosystem.
        API Integration: Experience with RESTful APIs and backend integrations.
        UI/UX Design: Ability to enhance user interfaces for better usability and consistency across platforms.
        DevOps: Familiarity with Docker, CI/CD, and automated testing to ensure reliable deployment and maintenance.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 175 hours
        Difficulty level: Intermediate

        ~~~~~~~~~~
        4. Upgrade eventyay-video to a Single Page Application with Dashboard, Settings, and Configuration Options for Organisers and Admins
        Description: The current eventyay-video implementation provides basic video functionalities for event streaming and management. However, it lacks an integrated, user-friendly dashboard and administrative interface. This project aims to transform eventyay-video into a modern Single Page Application (SPA) that offers comprehensive dashboards and configuration options for organisers and admins. By upgrading the UI/UX and incorporating advanced settings, the platform will allow seamless management of video content and live sessions, as well as integration with the broader eventyay ecosystem.
        Objectives:
        SPA Conversion and Modernization:
        Refactor the existing eventyay-video codebase to a Single Page VueJS Application.
        Implement client-side routing, lazy loading, and state management to ensure fast, smooth interactions and a responsive user experience.
        Dashboard and Administrative Interface:
        Develop intuitive and fully functional dashboards for organisers and admins to manage video sessions, live streams, and related content.
        Create comprehensive settings and configuration panels that allow users to customize video parameters, manage integrations, and monitor streaming analytics.
        Ensure that the dashboards are secure, scalable, and aligned with eventyay’s overall design language.
        Integration with eventyay Ecosystem:
        Seamlessly integrate with eventyay-tickets and other eventyay modules to synchronize data (e.g., event schedules, session details, user profiles).
        Enable automated configuration and updates from central eventyay administration systems, reducing manual maintenance.
        Enhanced UI/UX and Performance Optimization:
        Redesign the video interface to be more interactive and user-friendly, with emphasis on accessibility and responsiveness across devices.
        Optimize performance using code splitting, efficient API calls, and caching strategies to support large-scale live events and high traffic.
        Testing, Documentation, and Deployment:
        Write comprehensive unit, integration, and end-to-end tests to ensure reliability and performance.
        Document the new features, installation procedures, and administration guides to support future development and community onboarding.
        Leverage Docker and CI/CD pipelines to automate the build, testing, and deployment process, ensuring a robust development workflow.
        Repositories:
        eventyay-video Repository
        Expected Outcome:
        By the end of the project, eventyay-video will:
        Be transformed into a Single Page Application.
        Offer fully functional dashboards for organisers and admins, including robust settings and configuration options.
        Seamlessly integrate with the broader eventyay ecosystem to synchronize event data and manage video content efficiently.
        Include comprehensive testing and documentation, ensuring ease of maintenance and further development.
        Skills Required/Preferred:
        JavaScript & SPA Frameworks: Expertise in VueJS for SPA development, including client-side routing and state management.
        Front-End Development: Strong experience with modern UI/UX design principles and performance optimization.
        Backend Integration: Familiarity with RESTful APIs and integrating front-end applications with backend services.
        DevOps & CI/CD: Knowledge of Docker, Kubernetes, and continuous integration/deployment pipelines.
        Testing & Documentation: Experience writing comprehensive tests (unit, integration, e2e) and maintaining technical documentation.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 350 hours
        Difficulty level: Intermediate

        ~~~~~~~~~~
        5. Add New Integrations of Talks and Speakers component with eventyay Video
        Description: The goal of this project is to seamlessly integrate eventyay-talk with the eventyay-video component, ensuring a unified experience for event organisers and attendees. The project focuses on automating the synchronization between talks and video rooms, as well as enhancing speaker integration. For instance, when rooms are created in eventyay-talk, they should automatically appear in eventyay-video. Additionally, event organisers will have the ability to assign team members as MCs (Masters of Ceremonies) for different rooms, streamlining room management and improving live event coordination. This integration will reduce manual work, ensure data consistency, and provide a coherent user experience across eventyay components.
        Objectives:
        Automatic Room Synchronization:
        Develop functionality to automatically sync room creation and updates from eventyay-talk to eventyay-video.
        Ensure that any changes in the talk component (e.g., room details, schedule updates) reflect in real time on the video side.
        Speaker and Talk Data Integration:
        Integrate detailed talk and speaker information within the eventyay-video interface.
        Display session titles, descriptions, and speaker profiles alongside corresponding video rooms, providing context for viewers.
        MC Assignment Feature:
        Create an admin dashboard interface that enables organisers to assign team members as MCs for various video rooms.
        Implement notifications and role-based permissions to ensure that MCs have the tools and access needed to manage live sessions effectively.
        Unified User Experience:
        Design a consistent and intuitive UI/UX that bridges the gap between eventyay-talk and eventyay-video.
        Include visual cues and real-time status updates to help users navigate between talk sessions and their video counterparts.
        Backend Synchronization & API Enhancements:
        Enhance existing APIs or develop new endpoints to facilitate seamless data exchange between eventyay-talk and eventyay-video.
        Ensure robust error handling and data validation to maintain system reliability.
        Testing, Documentation, and Maintenance:
        Develop comprehensive unit, integration, and end-to-end tests covering all new integration functionalities.
        Provide clear documentation for developers and administrators to ensure ease of future maintenance and onboarding.
        Repositories:
        eventyay-talk Repository
        eventyay-video Repository
        Expected Outcome:
        At the end of the project, the eventyay ecosystem will benefit from:
        Automatic synchronization of rooms between eventyay-talk and eventyay-video, reducing manual intervention.
        Enhanced display of talk and speaker information in the video component.
        A dedicated admin interface for assigning MCs to rooms, ensuring better event management.
        A unified and consistent experience across the eventyay platforms, bolstered by robust API integrations, thorough testing, and clear documentation.
        Skills Required/Preferred:
        JavaScript & Front-End Frameworks: Experience with modern JavaScript frameworks (e.g., Vue.js) used in eventyay components.
        Backend Development: Experience in RESTful API design and integration.
        Integration Development: Experience in connecting disparate systems and ensuring smooth data synchronization.
        UI/UX Design: Strong skills in creating intuitive, responsive, and consistent user interfaces.
        Testing & Documentation: Ability to write comprehensive tests (unit, integration, and end-to-end) and maintain clear technical documentation.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 175 hours (medium project)
        Difficulty level: Intermediate
        
        ~~~~~~~~~~  
        6. Extend Features of the Exhibition Plugin for eventyay
        Description: eventyay currently includes an exhibition system designed to showcase exhibitors during events. This project aims to significantly enhance the existing exhibition plugin by expanding its feature set and improving its usability for both exhibitors and organizers. The new features will empower exhibitors to register and manage their own profiles, add detailed content, embed video links, and link to their social media channels. At the same time, event organizers will gain better control over exhibitor signups, payments, and the publication process. Additionally, the plugin will support integration with event sessions, allowing related sessions to be showcased on the exhibitor’s page, along with the ability to feature an exhibitor video directly on their profile.
        Objectives:
        Enhance the Exhibitor Registration & Profile Management:
        Develop a user-friendly registration process for exhibitors, allowing them to submit detailed profiles, including company information, content, and social media links.
        Allow exhibitors to edit and update their profiles through a dedicated dashboard.
        Integration of Multimedia Content:
        Enable exhibitors to embed video links (e.g., from YouTube or Vimeo) directly on their public exhibitor page.
        Support the addition of rich content (images, descriptions, documents) to provide a comprehensive overview of each exhibitor.
        Organizers’ Administrative Tools:
        Create an admin interface that enables organizers to review, approve, and manage exhibitor signups.
        Integrate payment processing options to manage exhibitor fees, if applicable, ensuring a secure and streamlined financial workflow.
        Provide tools for organizers to publish approved exhibitor profiles to the public website.
        Integration with Event Sessions:
        Implement a feature to associate related event sessions with exhibitor profiles. This will allow attendees to see relevant sessions alongside exhibitor details, fostering deeper engagement.
        Design intuitive navigation between exhibitor pages and session details.
        Responsive & Modern UI/UX:
        Redesign the exhibitor plugin interface to align with eventyay’s overall design language, ensuring a modern, accessible, and responsive user experience across all devices.
        Focus on usability for both exhibitors and event organizers through clear workflows and consistent design elements.
        Robust Testing & Documentation:
        Develop comprehensive unit, integration, and end-to-end tests to ensure reliability of all new features.
        Write detailed documentation for both developers and end-users to support future maintenance and community contributions.
        Repository:
        eventyay-tickets-exhibitors
        Expected Outcome:
        Upon completion of the project, the extended exhibition plugin will provide:
        A complete exhibitor registration system with rich profile management capabilities.
        An administrative interface for organizers to manage exhibitor applications, process payments, and publish profiles.
        Enhanced exhibitor pages that integrate multimedia content, social media links, and related event sessions.
        A modern, responsive UI that offers a seamless experience for both exhibitors and event attendees.
        Comprehensive testing and documentation that ensure maintainability and facilitate future enhancements.
        Skills Required/Preferred:
        Web Development: Strong experience with front-end technologies (JavaScript frameworks, HTML, CSS) and back-end development.
        Frameworks & Libraries: Familiarity with frameworks used in eventyay (e.g., Ember.js, Vue.js, or similar) and RESTful API integration.
        Payment Integration: Knowledge of integrating payment systems (e.g., Stripe, PayPal) for secure transaction processing.
        UI/UX Design: Ability to create intuitive and responsive interfaces.
        Testing & Documentation: Experience with writing unit, integration, and end-to-end tests as well as technical documentation.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 175 hours (medium size)
        SUSI Interpretation Project
        
        ~~~~~~~~~~
        1. SUSI Interpretation Project: Develop a Prototype for Real-Time AI Live-Interpretation of Event Videos
        Description: The SUSI AI Translator project aims to build an interpretation assistant that leverages large language models (LLMs) to enable automated, real-time live interpretation on top of the eventyay video component. The system integrates advanced speech-to-text technology with AI-powered language translation, allowing for the seamless display of interpreted text (e.g., translated subtitles) during live event video sessions. By bridging the gap between live audio streams and real-time translation, this project will significantly enhance accessibility and engagement for multilingual audiences at events.
        Scope: 
        The scope of your specific work on this project should be discussed with your mentor.
        Objectives:
        Real-Time Speech Recognition:
        Integration: Evaluate and integrate state-of-the-art speech-to-text engines (such as OpenAI’s Whisper or other open-source alternatives) to capture live audio streams from event videos with minimal latency.
        Optimization: Optimize for high accuracy and performance under varying noise and speech conditions.
        AI-Powered Translation & Interpretation:
        LLM Integration: Leverage large language models or dedicated translation APIs to interpret and translate the transcribed text in near real-time.
        Context Preservation: Ensure that the translation maintains context and clarity, offering accurate subtitles that enhance audience understanding.
        Subtitle Overlay & UI Integration:
        Eventyay Video Integration: Develop a module that overlays interpreted subtitles onto the live video stream from the eventyay video platform.
        Customization: Provide configurable options (e.g., font size, color, position, language selection, and toggle controls) to optimize subtitle display for diverse event settings.
        System Optimization and Latency Reduction:
        Pipeline Efficiency: Optimize data pipelines and processing workflows to minimize end-to-end latency, ensuring a smooth live interpretation experience.
        Caching & Asynchronous Processing: Implement techniques to handle network delays and processing loads effectively.
        Testing, Documentation, and Community Engagement:
        Comprehensive Testing: Develop unit, integration, and end-to-end tests to ensure system reliability and performance across various scenarios.
        Documentation: Create detailed guides on system setup, configuration, troubleshooting, and customization.
        Tutorials & Demos: Produce step-by-step tutorials and demo videos showcasing how to deploy and use the live interpretation feature.
        Additional Features (Exploratory):
        Admin Controls: Design an admin panel for event organizers to manage language pairs, thresholds, and real-time translation settings.
        Multi-Language Support: Expand the system to support multiple target languages and easy language switching during live events.
        Repositories:
        SUSI Translator Repository
        eventyay-video Repository
        (Additional modules or repositories may be created as the project evolves.)
        Expected Outcome:
        An improved prototype fully integrated, real-time AI live interpretation system that displays translated subtitles on the eventyay video platform.
        Enhanced accessibility and engagement for multilingual audiences through automated, low-latency speech recognition and translation.
        A robust, well-documented codebase accompanied by comprehensive tests and tutorials, fostering further community contributions and improvements.
        Skills Required/Preferred:
        Speech Recognition & Audio Processing: Familiarity with integrating and optimizing speech-to-text systems.
        AI & Machine Learning: Experience working with large language models, translation APIs, or similar AI-driven solutions.
        Programming & API Integration: Proficiency in Python and/or other relevant languages for backend processing and API development.
        Front-End Development: Skills in integrating real-time UI overlays within video platforms.
        DevOps & Testing: Experience with CI/CD pipelines and automated testing frameworks.
        Open Source Collaboration: Prior experience contributing to or managing open-source projects.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 350 hours

        ~~~~~~~~~~
        Scrum Helper
        1. Extend Features of the Scrum Helper Chrome Extension
        The Scrum Helper Chrome extension currently assists users in writing daily scrums within Google Groups by automatically fetching data from the GitHub API. It retrieves information such as pull requests, issues, and reviewed PRs based on the user’s GitHub username and pre-fills scrum reports accordingly. The proposed project aims to expand the extension's usability by:
        Adding support for popular web email clients like Gmail, Yahoo, and Outlook.
        Implementing a standalone pop-up interface that can be used independently of any email provider.
        This will allow users who do not utilize Google Groups or specific web email services to seamlessly generate and edit their scrum reports. The project focuses on extending API integrations, enhancing user interfaces, and ensuring compatibility across multiple platforms and environments.
        Objectives:
        Multi-Email Client Integration:
        Gmail, Yahoo, and Outlook Support:
        Integrate the extension with these popular web email clients by detecting the email interface and injecting necessary UI components to allow scrum report generation.
        Authentication & Permissions:
        Adapt OAuth flows or other authentication methods where necessary to securely connect to these platforms while preserving user privacy and data security.
        Standalone Pop-Up Interface:
        Development of a Standalone UI:
        Build a responsive, easy-to-use pop-up interface that operates independently of any specific email provider.
        User Experience Enhancements:
        Provide options to input GitHub username, dates, and other relevant details directly in the pop-up, pre-fill scrum data via the GitHub API, and allow users to edit the content before copying or exporting it.
        Customization Options:
        Allow users to configure which sections appear in their scrum reports and to save templates for future use.
        Enhanced GitHub Integration:
        Robust Data Fetching:
        Optimize API calls to fetch PRs, Issues, and reviewed PRs with error handling and caching mechanisms for a smoother user experience.
        User Customization:
        Offer additional filtering or sorting options for the fetched data, enabling users to fine-tune the information that populates their scrum reports.
        Cross-Platform Compatibility & Performance:
        Responsive Design:
        Ensure the extension’s UI adapts to different screen sizes and resolutions across various email clients.
        Performance Optimization:
        Optimize the extension for minimal load times and resource usage, ensuring a smooth experience even when processing large amounts of data.
        Testing, Documentation, and Community Engagement:
        Comprehensive Testing:
        Write unit, integration, and end-to-end tests to cover new functionalities, ensuring robustness and ease of maintenance.
        Documentation:
        Update technical and user documentation to guide future contributors and end users.
        Community Feedback:
        Engage with the community to gather feedback and iterate on the features, ensuring that the extension meets the evolving needs of its users.
        Repository:
        Scrum Helper Repository
        Expected Outcome:
        By the end of the project, the Scrum Helper Chrome extension will:
        Support integration with Gmail, Yahoo, and Outlook, enabling users to generate and send scrum reports directly from these platforms.
        Include a fully functional standalone pop-up interface for users who prefer not to rely on specific email providers.
        Enhance GitHub integration to provide robust, customizable data fetching and pre-filling of scrum reports.
        Feature an intuitive, responsive, and optimized UI that works seamlessly across different platforms.
        Be accompanied by comprehensive testing and detailed documentation, facilitating future contributions and maintenance.
        Skills Required/Preferred:
        Chrome Extension Development: Experience building and maintaining Chrome extensions.
        Front-End Development: Experience in HTML, CSS, JavaScript (and frameworks/libraries as needed) for building responsive UIs.
        API Integration: Knowledge of integrating third-party APIs (especially GitHub and web email clients) and handling OAuth authentication.
        Back-End Integration & Optimization: Ability to design robust data-fetching mechanisms and implement caching/error handling strategies.
        Testing & Documentation: Experience with writing automated tests and maintaining clear technical documentation.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 90 hours (small project)
        Difficulty level: Easy

        ~~~~~~~~~~
        FOSSASIA.ORG
        1. Revamp FOSSASIA.org with WordPress for a Modern, Maintainable Website
        Description: The goal of this project is to develop a new version of the FOSSASIA.org website using WordPress. The new website will adhere to the existing FOSSASIA brand guidelines, utilizing the core colors of red, gray, and white to maintain brand identity. By leveraging standard WordPress plugins and best practices, the project aims to create a website that is easy to update and maintain, ensuring a future-proof and scalable online presence for the community. The revamped site will showcase FOSSASIA's projects, events, news, and community initiatives in an engaging, responsive, and user-friendly manner.
        Objectives:
        Design & Branding:
        Develop a modern, responsive design for FOSSASIA.org that respects the current color scheme (red, gray, white).
        Create custom layouts and a theme that reflect the FOSSASIA identity, ensuring visual consistency across all pages.
        Plugin & Functionality Integration:
        Utilize standard, well-supported WordPress plugins to handle essential functionalities such as:
        Blog and news feeds.
        Contact forms and newsletter sign-ups.
        Social media integration and sharing.
        Keep customization minimal to simplify future updates and maintenance.
        Content & Information Architecture:
        Structure the site to clearly present FOSSASIA’s projects, events, community news, and resources following the current structure.
        Ensure easy navigation and a seamless user experience across devices.
        Performance, SEO, and Security:
        Optimize the site for speed and responsiveness.
        Implement SEO best practices and integrate tools for analytics.
        Ensure the website adheres to current security standards and best practices for WordPress.
        Documentation & Future-Proofing:
        Provide comprehensive documentation on theme customization, plugin management, and content updates.
        Create guidelines for future developers and maintainers to ensure the website remains current and easy to update.
        Expected Outcome:
        By the end of the project, the new FOSSASIA.org website will:
        Feature a fresh, modern design that maintains brand consistency with red, gray, and white as the primary colors.
        Leverage standard WordPress plugins to deliver a robust, feature-rich experience including event management, blog updates, contact forms, and social media integration.
        Be optimized for performance, SEO, and security, providing a seamless experience across all devices.
        Include detailed documentation to facilitate ongoing maintenance and future enhancements, ensuring longevity and scalability.
        Skills Required/Preferred:
        WordPress Development: Proficiency in WordPress theme development, PHP, and customizing plugins.
        Front-End Technologies: Experience with HTML, CSS, JavaScript, and responsive design principles.
        UI/UX Design: Ability to translate branding guidelines into a modern, user-friendly design.
        SEO & Performance Optimization: Understanding of web performance best practices and SEO fundamentals.
        Documentation: Strong technical writing skills for creating comprehensive developer and user guides.
        Repository:
        A new repository for the FOSSASIA.org WordPress theme and related assets will be maintained under the FOSSASIA GitHub organization.
        Possible mentors: hongquan, marcoag, norbusan, cweitat, mohit
        Expected Size: 175 hours (intermediate project)
        Difficulty level: Intermediate

        ~~~~~~~~~~  
        LED Badge Magic
        The Badge Magic Android app lets you create moving text and draw clipart for LED name badges. The app provides options to portray names, cliparts, and simple animations on the badges. For the data transfer from the smartphone to the LED badge it uses Bluetooth.
        Project Ideas
        1. Enhance the feature set of the Flutter Badge Magic Application 
        Description: Badge Magic is a Flutter-based application that controls LED name badges via Bluetooth, allowing users to display names, graphics, and simple animations. Currently available on GitHub and major app stores, the app provides basic functionalities to customize LED displays. The goal of this project is to significantly expand its feature set to offer a richer, more customizable experience. New features include support for various fonts, games, and additional LED badge screen sizes and color options. Users will also benefit from advanced badge configuration options (such as Bluetooth always-on mode) and the ability to manage up to 8 storage slots for text and animations, including making animations editable and storable. Additionally, the project will streamline deployment by integrating automated support for additional app stores (e.g., Aptoide, App Gallery) and extend the app’s availability across all platforms supported by Flutter (Windows, Linux, MacOS, and Websites). Furthermore, existing bugs should be solved and feature enhancements made to ensure robust functionality and an improved user experience.
        Objectives:
        Multi-Font, Color, and Content Support:
        Integrate a diverse range of fonts and color palettes to allow users to customize the appearance of text, graphics, and animations on their LED badges.
        Enhance content management by enabling editable storage for animations—allowing users to store, edit, and reapply animations to 8 storage slots of the badge.
        Enable Interactive Games Integration:
        Provide an option to integrate simple, engaging mini-games that can be displayed on the LED badges.
        Support for Multiple LED Badge Screen Sizes:
        Enhance the app to automatically adapt content for various LED badge screen sizes and resolutions.
        Advanced Badge Configuration Options supported by the firmware:
        Implement detailed configuration settings, such as enabling/disabling Bluetooth always-on mode, and other badge-specific options to provide greater control over the device behavior.
        Solve existing bugs and improve feature enhancements to refine overall functionality.
        Automated Multi-Platform Deployment:
        Improve the automated deployment pipeline that facilitates the submission of the app to additional app stores (e.g., Aptoide, App Gallery) as well as all platforms supported by Flutter.
        Ensure that installation files for Windows, Linux, MacOS, and Websites are autogenerated and maintained in the project's app branch.
        Enhanced UI/UX Improvements:
        Provide clear visual feedback and customization options across all supported platforms.
        Comprehensive Testing and Documentation:
        Implement unit, integration, and end-to-end tests to ensure all new features work as expected and existing bugs are resolved.
        Update technical and user documentation to assist future contributors and end users, ensuring long-term maintainability.
        Repository:
        https://github.com/fossasia/badgemagic-app 
        Expected Outcome:
        At the end of the project, the Flutter Badge Magic Application will:
        Support a wide range of fonts, colors, and display options for highly customizable badge presentations.
        Feature interactive games and advanced configuration settings, including a “Bluetooth always on” mode.
        Allow users to store, edit, and manage animations in up to 8 dedicated storage slots.
        Automatically adapt to various LED badge screen sizes, ensuring optimal content display.
        Support automated multi-platform deployment, generating installation files for Windows, Linux, MacOS, and Websites on the app branch.
        Provide a robust, bug-free experience with a modern, user-friendly interface and comprehensive documentation.
        Skills Required/Preferred:
        Flutter/Dart: Advanced knowledge in Flutter development and Dart programming.
        Bluetooth Communication: Experience with Bluetooth integration in mobile applications.
        UI/UX Design: Strong design skills for building intuitive and responsive mobile interfaces.
        Mobile and Multi-Platform App Development: Proven experience in Android, iOS, and cross-platform development (Windows, Linux, MacOS, and Web).
        CI/CD & Deployment: Familiarity with automated deployment pipelines and multi-app store submissions.
        Testing & Documentation: Experience in writing unit, integration, and end-to-end tests, as well as maintaining detailed project documentation.
        Possible mentors: Aditya, MarioB, cweitat
        Expected Size: 175 hours (median project)
        Difficulty level: Easy
        
        ~~~~~~~~~~
        Magic ePaper Badge
        1. Improve the Magic ePaper Badge app into a User friendly Functional App
        Description: The Magic ePaper Badge is a unique hardware platform featuring a tri-color ePaper display, NFC capabilities, and battery-free operation. The goal of this project is to build an open source Flutter application that allows users to interact with and control the badge with rich, customizable content. The app will enable users to create and edit content through drawing tools, text inputs, emojis, and image import functionalities—with various effects (none, semi-transparent, block, portrait). Additional features include adjusting contrast, colors, image rotation, and text customization (font and size). Finally, the app will support transferring the composed content to the badge via NFC. This project is especially suited for contributors with an electronics background who are also interested in modern mobile app development using Flutter.
        Objectives:
        User Interface & Drawing Tools:
        Develop an intuitive Flutter UI for creating badge content.
        Implement drawing functionalities, text input, and emoji insertion.
        Integrate a color picker to easily change colors by tapping on a color button.
        Image Import and Editing:
        Enable users to import images from their devices.
        Support multiple image display effects: none, semi-transparent, block, and portrait.
        Provide features for rotating images and adjusting brightness, contrast, and overall color balance.
        Text Customization:
        Allow users to change fonts and text sizes to match their design needs.
        Ensure text overlays are compatible with the tri-color ePaper display.
        NFC-Based Content Transfer:
        Integrate NFC functionalities to enable the transfer of created content from an NFC-enabled phone directly to the Magic ePaper Badge.
        Ensure a smooth, secure, and reliable connection between the mobile device and the badge hardware.
        Support for Tri-Color ePaper Displays:
        Ensure that all drawing, text, image, and animation functionalities are fully optimized for the unique constraints of tri-color ePaper displays.
        Testing, Bug Fixes & Feature Enhancements:
        Identify and resolve existing bugs, particularly issues around storing and editing animations.
        Continuously refine and enhance features based on user feedback and testing results.
        Multi-Platform Deployment & Automation:
        Adapt the app to run on all platforms supported by Flutter (Android, iOS, Windows, Linux, MacOS, and Web).
        Set up an automated deployment pipeline to generate installation files in the app branch for all target platforms.
        Ensure that the deployment process is smooth and maintainable for future updates.
        Documentation & Community Engagement:
        Produce comprehensive documentation covering installation, usage, and NFC transfer processes.
        Provide guidelines for future contributions and ensure robust testing strategies are in place.
        Repository:
        Magic ePaper App Repository
        Resources:
        Waveshare ePaper Sample App
        Expected Outcome:
        By the end of the project, the Magic ePaper Badge Flutter App will:
        Offer a complete suite of content creation tools (drawing, text, emoji, image editing) tailored for the tri-color ePaper display.
        Provide robust customization options including color adjustments, image effects, and text formatting.
        Seamlessly transfer content to the badge via NFC.
        Run across all major platforms supported by Flutter, with autogenerated installation files maintained in the app branch.
        Include comprehensive documentation, a solid testing framework, and a refined UI/UX for an engaging user experience.
        Skills Required/Preferred:
        Flutter & Dart Development: Knowledge in building cross-platform apps with Flutter.
        Mobile & Multi-Platform App Development: Experience in deploying applications on Android, iOS, Windows, Linux, MacOS, and Web.
        NFC Integration: Understanding of NFC communication and its implementation on mobile devices.
        UI/UX Design: Skills in creating intuitive and responsive user interfaces.
        Image Processing & Drawing: Experience with image editing, drawing functionalities, and handling various display effects.
        Electronics & ePaper Displays: Basic knowledge of ePaper technology and hardware integration is a plus.
        Testing & Documentation: Proven ability to write comprehensive tests and maintain clear, detailed documentation. 
        Possible mentors: Aditya, MarioB, cweitat
        Expected Size: 350 hours (large project)
        Difficulty level: Intermediate
        
        ~~~~~~~~~~
        2. Enhance the Firmware of the Magic epaper Badge 
        Description: The Magic Epaper Firmware is a fully customizable and efficient firmware designed to control FOSSASIA's ePaper displays. It serves as the backbone for smart signage, dashboards, IoT devices, and various applications that require low-power, high-performance e-ink display control. The firmware is optimized for FOSSASIA ePaper displays, ensuring smooth rendering while consuming minimal power, and features a modular codebase that allows easy adaptation to different e-paper sizes and types. With flexible connectivity options supporting SPI, I2C, or UART interfaces, and compatibility with various microcontrollers, the firmware is capable of rich content rendering including text, images, and custom graphics—with support for grayscale and partial refresh (depending on display capabilities). Fully open-source under the Apache2 license, the firmware invites contributors to extend its capabilities and enhance its reliability.
        Objectives:
        Reliability and Stability Enhancements:
        Audit the current firmware codebase to identify and resolve bugs affecting rendering, connectivity, and power consumption.
        Optimize the code for smoother rendering and enhanced stability across various hardware configurations.
        Modular Codebase Improvements:
        Refactor and document the modular components to simplify customization for different e-paper display sizes and types.
        Enhance configurability to allow easier integration with new microcontrollers and communication protocols (SPI, I2C, UART).
        Low-Power Optimization:
        Fine-tune power management routines to further reduce energy consumption, making the firmware even more suitable for battery-powered devices.
        Implement advanced sleep and wake-up cycles tailored for e-ink display operations.
        Advanced Content Rendering:
        Improve support for rich content rendering including text, images, and custom graphics.
        Enhance grayscale rendering capabilities and optimize partial refresh functionality to reduce ghosting and improve update speed.
        Connectivity and Integration Enhancements:
        Ensure robust communication over SPI, I2C, or UART, adding diagnostics or fallback mechanisms to handle connection interruptions.
        Develop test cases and debugging tools to facilitate easier firmware integration with different hardware setups.
        Documentation and Community Support:
        Update and expand technical documentation, providing detailed guides on customizing and extending the firmware.
        Create a set of example projects and usage scenarios for smart signage, dashboards, and IoT devices to encourage community contributions.
        Repository:
        Magic ePaper Firmware Repository
        Expected Outcome:
        By the end of the project, the enhanced Magic Epaper Firmware will:
        Deliver a more stable and reliable performance with optimized rendering and low-power operation.
        Feature a thoroughly documented and modular codebase that simplifies customization for various e-paper displays and microcontrollers.
        Offer improved content rendering capabilities, including refined support for grayscale and partial refresh.
        Provide robust connectivity with diagnostics and enhanced support for SPI, I2C, and UART interfaces.
        Include comprehensive documentation and example projects, fostering further development and community engagement.
        Skills Required/Preferred:
        Embedded Systems & Firmware Development: C/C++ or similar languages used in microcontroller programming.
        Electronics & Hardware Integration: Experience with ePaper displays, low-power device design, and understanding of communication protocols (SPI, I2C, UART).
        Optimization & Debugging: Skills in debugging firmware and optimizing performance for resource-constrained devices.
        Documentation & Open-Source Collaboration: Ability to write clear technical documentation and collaborate with a global open-source community.
        Possible mentors: fcartegnie, Benjamin Henrion, Simon Budig, danielm
        Expected Size: 350 hours
        Difficulty level: Difficult

        ~~~~~~~~~~
        Pocket Science Lab
        In the Pocket Science Lab Project we create phone and desktop applications to collect measurements and data to solve global problems with science and build a sustainable world. With the PSLab mobile and desktop apps it is possible to use sensors of a phone or desktop PC to collect measurements and data. The app comes with a built-in Oscilloscope, Multimeter, Wave Generator, Logic Analyzer, Power Source, and we are constantly adding more digital instruments or even robotic controls. With PSLab applications your phone or PC becomes like many devices in one. 
        Project Ideas
        1. Complete the Port of the PSLab app to Flutter
        The Pocket Science Lab (PSLab) project has empowered countless students to explore science hands-on with affordable, versatile instruments. Currently, the PSLab mobile app is available only for Android, limiting its accessibility. In response to feedback from educational institutions and schools, this project aims to develop a cross-platform Flutter application that works seamlessly on Android, iOS, desktop (Windows, macOS, Linux), and web. The new app will provide support for popular scientific instruments such as the Oscilloscope, Logic Analyzer, Multimeter, and robot controls. Additionally, due to Apple's restrictions on USB connections, the project will include robust configuration options and WiFi connectivity to enable data transmission on Apple devices.
        Objectives:
        Cross-Platform Development:
        Porting/Implementation: Rebuild the existing Android app using Flutter to support iOS, desktop, and web platforms.
        Optimization: Ensure the app performs smoothly across different devices and screen sizes.
        Core Instrument Functionality:
        Oscilloscope, Logic Analyzer, Multimeter, and Robot Controls: Implement and integrate these instrument features with consistent behavior and accuracy across all platforms.
        Data Visualization: Develop clear, interactive visualizations and controls for each instrument, enabling real-time monitoring and analysis.
        Enhanced Connectivity:
        USB and WiFi Support:
        Retain USB connectivity for Android while implementing configuration options for WiFi-based data transmission.
        Address Apple’s limitations by enabling WiFi connectivity, ensuring seamless communication between the PSLab hardware and the app on iOS devices.
        Robust Communication Protocols: Ensure secure and reliable data transfer through efficient connection management.
        User Interface & Experience:
        Responsive UI: Design an intuitive, responsive interface that adapts to mobile, desktop, and web environments.
        User-Centric Design: Prioritize ease of use, especially for educational settings, with clear navigation and instrument control layouts.
        Testing, Documentation, and Deployment:
        Comprehensive Testing: Develop unit, integration, and end-to-end tests to validate functionality across all supported platforms.
        Automated Deployment: Set up CI/CD pipelines to automate building and deployment for mobile (App Store/Google Play), desktop installers, and a web version.
        Documentation: Create detailed user and developer documentation, including setup guides and usage examples, to foster community engagement and future contributions.
        Repository:
        PSLab Android Repository (Check the flutter branch)
        Expected Outcome:
        By the end of the project, PSLab will have a robust, cross-platform Flutter application that:
        Runs seamlessly on Android, iOS, desktop (Windows, macOS, Linux), and web.
        Provides essential scientific instrument functionalities (Oscilloscope, Logic Analyzer, Multimeter, Robot Controls) with intuitive, real-time visualizations.
        Supports both USB (for Android) and WiFi connectivity (for iOS and other platforms) to overcome hardware limitations.
        Offers a user-friendly interface tailored for educational use, complete with comprehensive testing, documentation, and automated multi-platform deployment.
        Skills Required/Preferred:
        Flutter/Dart Development: Knowledge in building cross-platform applications.
        Mobile, Desktop & Web Development: Experience in deploying apps across various platforms.
        Hardware & IoT Integration: Understanding of connectivity protocols (USB, WiFi) and interfacing with scientific instruments.
        UI/UX Design: Ability to design responsive and intuitive interfaces.
        Testing & CI/CD: Experience with automated testing frameworks and deployment pipelines.
        Open Source Collaboration: Familiarity with contributing to and managing open-source projects.
        Possible mentors: padmal, bessman
        Expected Size: 350 hours
        Difficulty level: Difficult

        ~~~~~~~~~~

        2. Advanced Robot Control and Python Integration for the PSLab Android App
        Description: The PSLab Android app currently allows users to control a robotic manipulator with 4 degrees of freedom via connected servo motors. In parallel, similar control functionalities are available using Python, offering flexibility for custom automation and scripting. This project aims to enhance the robot control features within the PSLab Android app while seamlessly bridging the gap between mobile and Python environments. Key enhancements include advanced control options, import/export capabilities for Python scripts, and storage of user-defined robot controls. Additionally, comprehensive documentation and a web-based UI guide will be developed to showcase how to transfer control data and operate the robot through a web interface.
        Objectives:
        Enhanced Robot Control Features:
        Upgrade the existing robot control module within the PSLab Android app to support advanced manipulation options.
        Improve the user interface for intuitive control over the 4 degrees of freedom, including real-time feedback and fine-tuning capabilities.
        Implement additional control modes, presets, and customizable parameters for servo motor movements.
        Python Script Integration:
        Develop functionality for exporting the current robot control configurations and commands from the Android app as a Python script.
        Implement an import feature that allows users to load Python scripts into the Android app, enabling them to execute predefined control sequences.
        Ensure that the Python integration maintains consistency with the PSLab Python library and adheres to best practices.
        Data Storage & Management:
        Introduce a storage mechanism for users to save and manage their custom robot control profiles and scripts within the app.
        Provide a clear and accessible user interface to review, edit, and organize stored controls.
        Documentation & Web UI Integration:
        Create comprehensive, step-by-step documentation detailing the transfer of control data between the Android app and Python scripts.
        Develop a web-based UI guide that demonstrates how to control the robot via a browser, highlighting the integration process and potential use cases.
        Include troubleshooting guides and best practices to help users optimize their control setups.
        Repository:
        PSLab Android App
        PSLab Python
        Expected Outcome:
        At the end of the project, the PSLab Android app will feature:
        Enhanced and more intuitive robot control capabilities with advanced options.
        Seamless import and export functionalities for Python scripts, allowing bi-directional control and integration.
        Robust storage of user-defined control profiles for quick access and customization.
        Comprehensive documentation and a web UI guide that illustrate the control data transfer process and provide additional insights on operating the robot.
        Skills Required/Preferred:
        Android Development: Android app development with a strong understanding of Java/Kotlin and Flutter (if applicable).
        Python Programming: Experience with Python scripting and integrating Python functionalities within mobile applications.
        Robotics & IoT: Familiarity with robotic control systems, servo motors, and interfacing with hardware.
        UI/UX Design: Ability to design intuitive user interfaces that enhance user interaction and control.
        Documentation & Testing: Experience with writing clear documentation and performing robust testing to ensure a stable release.
        Possible mentors: padmal, orangecms, bessman
        Expected Size: 90 hours
        Difficulty level: Intermediate

        ~~~~~~~~~~
        3. Create a Firmware Prototype for PSLab Mini: A Compact Oscilloscope & Multimeter
        The PSLab Mini aims to be a streamlined, cost-effective version of the Pocket Science Lab, specifically designed to offer dedicated Oscilloscope and Multimeter functionalities in a compact form factor. This project involves developing a firmware prototype based on the existing PSLab firmware. The prototype will be optimized to run on resource-constrained hardware while delivering precise data acquisition, processing, and visualization for oscilloscope and multimeter measurements. As an alternative, if a dedicated PSLab Mini hardware is not available, an ARM development board can be used to simulate the target environment. This project is ideal for students with embedded systems and firmware development experience, and it will help extend PSLab’s reach into classrooms and educational settings.
        Objectives:
        Firmware Analysis and Requirements Definition:
        Review the existing PSLab firmware to understand its architecture and core functionalities.
        Identify the essential modules for Oscilloscope and Multimeter operations.
        Define the performance and resource requirements for the PSLab Mini.
        Prototype Development:
        Refactor and streamline the current firmware to create a lightweight version that focuses on Oscilloscope and Multimeter features.
        Implement core functionalities such as high-speed data acquisition, signal processing, and measurement display.
        Integrate a basic user interface for real-time visualization of measurements (using an onboard display or external interface).
        Hardware Optimization & Alternative Platform Integration:
        Optimize the firmware for low-power consumption and efficient operation on minimal hardware.
        If a dedicated PSLab Mini device is unavailable, port and test the firmware on an ARM development board as a substitute platform.
        Testing and Validation:
        Develop comprehensive test cases to ensure the accuracy and reliability of oscilloscope and multimeter functions.
        Benchmark the firmware’s performance against standard measurement devices.
        Iterate based on testing feedback to fine-tune responsiveness and accuracy.
        Documentation and Community Engagement:
        Document the firmware architecture, development process, and hardware requirements.
        Provide step-by-step guides for setting up the firmware on the PSLab Mini (or ARM development board).
        Engage with the community for feedback and further enhancements.
        Repository:
        PSLab Firmware Repository: [Insert Repository URL here]
        (If a dedicated repository for the PSLab Mini firmware prototype is created, it will be added to the FOSSASIA GitHub organization.)
        Expected Outcome:
        A fully functional firmware prototype for the PSLab Mini that supports oscilloscope and multimeter features.
        Optimized firmware tailored for resource-limited hardware with low power consumption and reliable performance.
        Comprehensive documentation and test suites for further development and community contributions.
        Optionally, a demonstration of the firmware running on an ARM development board if dedicated hardware is unavailable.
        Skills Required/Preferred:
        Embedded Systems & Firmware Development: Proficiency in C/C++ and experience with ARM Cortex or similar microcontrollers.
        Hardware Interfacing: Familiarity with analog-to-digital conversion, sensor interfacing, and low-level hardware communication protocols.
        Optimization Techniques: Ability to optimize firmware for performance and low power consumption on resource-constrained devices.
        Testing & Debugging: Experience in developing unit tests and debugging firmware in embedded environments.
        Documentation: Strong technical writing skills to produce clear, comprehensive documentation.
        Possible mentors: bessman
        Expected size: 350 h (large project)
        Difficulty level: Intermediate

      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fossasia/
    idea_list_url: https://docs.google.com/document/d/1Tz1KxYefreKzBr98C4vbCv9UwnchZoyTwO8rBrz_lmg/edit?tab=t.0#heading=h.9sjk3ie7l2o2
  

  - organization_id: 41
    organization_name: FOSSology
    no_of_ideas: 13
    ideas_content: |
      
      Data pipelining for safaa project
      Goal: Automate the process of model training using pipelining.
      Currently in Safaa Project data was manually curated And we see that most of the things are manual here. the project should concentrate on creating a pipeline, Utilizing LLMS if required to increase the accuracy, use deep learning techniques to improve.
      Scripts to copy copyright data automatically(group's data or some users data) from fossology instance to train the model.
      
      Test cases needs to be provided as well.
      Category Rating
      Low Hanging Fruit **
      Risk/Exploratory *
      Fun/Peripheral **
      Core Development *
      Project Infrastructure **
      Project size Large
      Preferred contributor Student/professional
      Skills needed Python, ML And Data
      Contact @Kaushl2208 @GMishx @shaheemazmalmmd

      ~~~~~~~~~~
      License Detection Using Large Language Models
      Goal: To automate license detection using license dataset and ensure accurate and up-to-date results by leveraging a Retrieval-Augmented Generation (RAG) approach.
      We have previously tried semantic similarity approach for license detection #104-Atarashi. Which used text processing and prompt engineering. We have tried multiple LLM models for license statement types. Visit Weekly Reports for more performance details
      What we want to achieve?
      Utilize SPDX or SAFAA Database for licenses.
      To create RAG knowledge Base, For model to understand specifics of licenses.
      High Accuracy on Random license texts(Input provided need not to be a full fledged statement). Confidence score if necessary
      Needs to be a Language Agnostic Solution.
      Pipeline to Fetch New License Data (If available) from SPDX Database or SAFAA so RAG Knowledge Base should always be up to date.
      Category Rating
      Low Hanging Fruit **
      Risk/Exploratory *
      Fun/Peripheral **
      Core Development *
      Project Infrastructure **
      Project size Medium/Large
      Preferred contributor Student/professional
      Skills needed Python, LLMs, Fine-tuning, Documentation
      Contact @Kaushl2208 @GMishx @shaheemazmalmmd

      ~~~~~~~~~~
      Transforming Nirjas into a Technical Documentation tool Using Large Language Models (LLMs)
      Goal: To transform Nirjas into a comprehensive technical documentation tool using LLMs by automatically generating, improving, and structuring documentation for source code files. This will include comments, function documentation, and metadata extracted using Nirjas, ensuring consistency, clarity, and quality in technical documentation.
      We have previously worked on extracting metadata and comments using regex-based approaches in Nirjas. While this method provided structured results, it can also be used to generate high-quality documentation. Leveraging LLMs with metadata extraction from Nirjas.
      What we want to achieve?
      Integrate LLMs for Documentation Generation
      Use Existing Knowledge Sources for Training
      Implement a Retrieval-Augmented Generation (RAG) Approach
      Automatic Summarization and Quality Scoring
      Seamless Integration with Existing Tools
      Category Rating
      Low Hanging Fruit **
      Risk/Exploratory *
      Fun/Peripheral **
      Core Development **
      Project Infrastructure **
      Project Size Medium/Large
      Preferred Contributor Student/Professional
      Skills Needed Python, LLMs, Fine-tuning, Data Engineering, Documentation Standards
      Contact @hastagAB @GMishx @Kaushl2208

      ~~~~~~~~~~
      Overhauling scheduler design
      Goal: Improving FOSSology scheduler or replacing with OTS solution
      The existing scheduler design is causing new issues which need to be addressed. Moreover, existing scheduler design is not touched in years.
      Concerning points
      The scheduler is written in C which makes it next to impossible to find cause of a failure.
      The C language does not support exception handling out of the box. It makes code less readable and prone to errors.
      The linear queue design causes issue when there should be only one instance of an agent running for an upload, but overall the agent is not mutually exclusive.
      For example, if the monkbulk has a limit set to 1, it should be implied for only single upload. But with linear queue, this monkbulk job will block all other agents from executing even when they are not effected by the results of monkbulk.
      This essentially makes the agent mutually exclusive even though, there is a special flag EXCLUSIVE for the very same purpose: https://github.com/fossology/fossology/wiki/Job-Scheduler#agentconfs
      One idea on redesigning the queue, it can be broken into buckets per upload each maintaining its own priority queue. There can be another queue for global operations like maintenance, delagent, etc.
      Doing so, each bucket can be traversed in round-robin and pick first pending job and check against host limit. This will eliminate the scenario mentioned in point 3. Also, exclusive agents can be sent to global queue.
        upload specific queue
      |-<upload_2> -> nomos, copyright, ojo, keyword
      |-<upload_3> -> monkbulk, decider, monkbulk, decider
      |-<upload_4> -> reuser, decider
      
      global queue
      -> delagent,
      Since the FOSSology is released, there can be number of new scheduling libraries being released which needs to be explored. They can be a nice addition to the project.
      There have been some work already done in GSoC 2024, Can be visited here
      Category Rating
      Low Hanging Fruit -
      Risk/Exploratory **
      Fun/Peripheral ***
      Core Development ***
      Project Infrastructure *
      Project size Large
      Preferred contributor Professional
      Skills needed Go
      Contact @GMishx @Kaushl2208 @avinal @shaheemazmalmmd

      ~~~~~~~~~~
      Debian packaging for Debian repository
      Goal: Improve Debian packaging and make it acceptable for APT
      The existing effort to put FOSSology under Debian packaging list needs to be taken forward. A repository under Debian Salsa was setup initially but not maintained any more: https://salsa.debian.org/fossology-team/fossology It is configured to use gbp.
      Blockers
      The Debian building mechanism does not allow installation from sources other than apt. The Composer packages need to be packed as Debian packages and shipped with FOSSology.
      Packaging and shipping other tools needs to satisfy their licensing terms.
      The versions of packages in APT and actual versions used are different.
      APT also provides JS libraries like JQuery and DataTables but RHL does not.
      See also
      https://github.com/fossology/fossology/pull/2075
      https://wiki.debian.org/PackagingWithGit
      https://wiki.debian.org/SimplePackagingTutorial
      https://wiki.debian.org/Diagrams
      https://wiki.debian.org/PHP
      https://peertube.debian.social/videos/watch/0fb2dbc4-f43d-477e-8b14-20c426f970de
      Category Rating
      Low Hanging Fruit *
      Risk/Exploratory **
      Fun/Peripheral ***
      Core Development *
      Project Infrastructure ***
      Project size Small
      Preferred contributor Student/Professional
      Skills needed Debian, APT, CMake
      Contact @GMishx @shaheemazmalmmd @Kaushl2208

      ~~~~~~~~~~
      User & Developer Assistant Chatbot using Large Language Models
      Goal: To develop an intelligent assistant chatbot that leverages Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to provide comprehensive support for both end-users and developers of our tool. The assistant will bridge the gap between users, documentation, and the codebase to ensure an interactive and efficient problem-solving experience.
      The chatbot will be designed to interactively assist new and existing users with various aspects of the tool, including:
      Feature Discovery:
      Answer questions about available features, their functionalities, and usage.
      Provide contextual information derived from the tool's wiki and feature documentation.
      Problem Resolution and Recommendations:
      Assist users during the project setup phase by identifying common setup errors.
      Provide troubleshooting steps for known issues by integrating knowledge from GitHub issues.
      Developer Support:
      Answer codebase-related queries by identifying relevant classes, methods, or functions.
      Enhance developers' understanding of the project by linking features to the corresponding implementation in the code.
      The chatbot will utilize LangChain, RAG, and a Vector Database for retrieval, enabling contextual conversations. A seamless pipeline will integrate multiple data sources, including documentation, GitHub issues, and the codebase.
      What We Want to Achieve:
      For End-Users:
      Improved Onboarding:
      Enable new users to quickly understand the tool's features and capabilities through interactive conversations.
      Efficient Problem Resolution:
      Provide real-time recommendations for known issues encountered during project setup.
      Reduce reliance on manual troubleshooting by surfacing relevant GitHub issues.
      Enhanced User Engagement:
      Increase user satisfaction by offering a conversational interface that adapts to their queries and knowledge level.
      For Developers:
      Codebase Exploration:
      Allow developers to query the codebase for insights into specific classes or functions, fostering faster understanding and debugging.
      Knowledge Consolidation:
      Create a unified interface where feature descriptions, documentation, and implementation details converge.
      Broader Objectives:
      Reduce the time spent on documentation searches.
      PS: There are some features which aligns with the goal but not be possible in short time interval. Topics like: Knowledge Consolidation & Codebase Exploration but the development should be done by taking all this in mind
      Category Rating
      Low Hanging Fruit *
      Risk/Exploratory *
      Fun/Peripheral ***
      Core Development *
      Project Infrastructure **
      Project size Large
      Preferred contributor Student/professional
      Skills needed Python, LLMs, Documentation Standards
      Contact @Kaushl2208 @GMishx @shaheemazmalmmd
      
      ~~~~~~~~~~
      Support text phrases and bulk based scanning for MONK a like agent
      Goal: Adding text phrases from UI to database and use existing bulk phrases and provide ability to scan them using MONK and identify files if the match is 100%
      FLOW :
      Create a UI Where user can add multiple text phrases associated with license(FROM FOSSology License Database).
      Use existing bulk phrases table from database.
      Create a new agent like existing MONK agent which not only identifies the matches but also decides the files.
      Test cases needs to be provided as well.
      Category Rating
      Low Hanging Fruit **
      Risk/Exploratory *
      Fun/Peripheral **
      Core Development *
      Project Infrastructure **
      Project size Medium
      Preferred contributor Student/professional
      Skills needed PHP, C++
      Contact @GMishx @shaheemazmalmmd
      
      ~~~~~~~~~~
      Enhance atarashi ability
      Goal: Improve license identification of atarashi
      Improve existing model which have 80 % accuracy.
      Use some model to identify the license-possibility using keywords.
      Once there is some license possibility pass this to existing trained model to identify the accurate license.
      If the trained model miss to find the license then add license-possibility to file so that users checks the file and clarify.
      Work on the existing branch(https://github.com/fossology/fossology/pull/1634) and make sure that this gets merged.
      Know more about atarashi.
      Category Rating
      Low Hanging Fruit *
      Risk/Exploratory **
      Fun/Peripheral ***
      Core Development *
      Project Infrastructure ***
      Project size Small
      Preferred contributor Student/Professional
      Skills needed Python, ML , CMake
      Contact @GMishx @shaheemazmalmmd @Kaushl2208 @hastagAB
      
      ~~~~~~~~~~
      Integrating Open Source Review Toolkit
      Goal: Using ORT to fetch dependencies and generate SBOM
      Build systems fetch the required dependencies (library/artifact) for a project while building the project. Its important to get an insight of these dependencies for license compliance check.
      The OSS Review Toolkit is an open source project helps to find dependencies in a project.
      The goal of this project is to render the project dependencies created by ort and display those in the fossology-UI. Dependencies can be scheduled directly from the UI and scan with fossology.
      Also vice versa integrate FOSSology to ORT to scan the opensource dependencies.
      Category Rating
      Low Hanging Fruit -
      Risk/Exploratory -
      Fun/Peripheral **
      Core Development ***
      Project Infrastructure *
      Project size Large
      Preferred contributor Student/Professional
      Skills Needed PHP, Cmake, Kotlin
      Contact @GMishx @shaheemazmalmmd @Kaushl2208
      
      ~~~~~~~~~~
      Complete microservices infrastructure for FOSSology
      Goal: Continue the work from previous GSoC and bring FOSSology to a working state on Kubernetes
      As part of GSoC 2021, a large portion of work was done to bring FOSSology to work on Kubernetes. Since then, there have been countless changes to the codebase and the build system. Here are a few objectives we expect to be achieved:
      Go through the changes in the codebase and devise strategies for integrating them
      Inspect the changes in #2086 and complete the work
      By the end, we should have a fully working FOSSology installation on Kubernetes
      Create documentation for setting up FOSSology on a cluster and all the options available
      Stretch goal: Create an all-in-one script for easy Kubernetes setup with FOSSology
      Stretch goal: Add mechanism for health checks of the installation
      Stretch goal: Expose usage and performance metrics
      References
      #2086
      #2075
      https://summerofcode.withgoogle.com/archive/2021/projects/4661860250419200
      Category Rating
      Low Hanging Fruit -
      Risk/Exploratory ***
      Fun/Peripheral *
      Core Development *
      Project Infrastructure ***
      Project Size Medium/Large
      Preferred Contributor Professional
      Skills Needed Kubernetes, Docker/Podman, CMake, Bash
      Contact @avinal @GMishx @shaheemazmalmmd @Kaushl2208
      
      ~~~~~~~~~~
      Rewrite FOSSology UI using React
      Goal: Rewrite FOSSologyUI using react.
      Existing code is old. and needs a fix.
      Implementation of new API'S to existing code.
      Implementation designed templates.
      Category Rating
      Low Hanging Fruit *
      Risk/Exploratory **
      Fun/Peripheral ***
      Core Development *
      Project Infrastructure ***
      Project size Small
      Preferred contributor Student/Professional
      Skills needed php, react, CMake
      Contact @GMishx @shaheemazmalmmd @Kaushl2208 @deo002
      
      ~~~~~~~~~~
      FOSSology UX and UI design
      Goal: Redesign the FOSSology UX and UI to modernize its interface and enhance user-friendliness.
      Understand the Primary Users
      Identify user personas: Determine who the key users of FOSSology are, such as developers, compliance officers, or open-source contributors.
      Analyze pain points: Conduct surveys, interviews, or user studies to understand the challenges users face while using the current system.
      Analyze the Current Interface
      Evaluate usability issues: Identify areas where the current interface is difficult to use or navigate.
      Highlight outdated design elements: Assess visual components and workflows that no longer align with modern design standards or user expectations.
      Identify Redesign Requirements
      Define goals: Establish clear objectives for the redesign, such as improving efficiency, accessibility, or ease of use.
      Prioritize features: Focus on addressing critical pain points and implementing high-impact improvements.
      Design Reusable Components
      Catalog interface elements: List existing components and determine which can be updated or replaced.
      Ensure consistency: Create reusable design components to maintain a cohesive user experience and simplify scalability.
      Draft Layouts and Workflows
      Streamline user journeys: Map out key workflows to reduce complexity and improve navigation.
      Prototype layouts: Create wireframes or mockups to visualize potential improvements and gather early feedback.
      Establish a Cohesive Design System
      Define visual guidelines: Standardize elements such as colors, typography, and spacing for a unified aesthetic.
      Componentize the UI: Build a library of modular components for easier development and maintenance.
      Gather Feedback and Refine
      Conduct usability testing: Engage users to validate the new designs and identify areas for improvement.
      Iterate based on feedback: Refine layouts, workflows, and components to ensure the redesign meets user needs effectively.
      Category Rating
      Low Hanging Fruit *
      Risk/Exploratory **
      Fun/Peripheral ***
      Core Development *
      Project Infrastructure ***
      Project size Medium/Large
      Preferred contributor Student/Professional
      Skills needed wireframe and other design techniques
      Contact @EttingerK @GMishx @shaheemazmalmmd @Kaushl2208
      
      ~~~~~~~~~~
      New single file view page to accommodate license + copyright clearing
      Goal: To Redesign & develop new single file view page accommodate all the clearings.
      Have a folder tree with blue & red buttons to indicate the clearing.
      Integrate drag and drop functionality to copy the clearing decisions from one file to another.
      Have a histogram feature to accommodate license groups in the current upload.
      Have a file view page with highlights of all the findings (licenses + copyrights + keywords + ECC).
      Refer the screenshot of the design.
      Category Rating
      Low Hanging Fruit *
      Risk/Exploratory **
      Fun/Peripheral ***
      Core Development *
      Project Infrastructure ***
      Project size Small
      Preferred contributor Student/Professional
      Skills needed wireframe and other design techniques
      Contact @EttingerK @GMishx @shaheemazmalmmd @Kaushl2208
      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fossology/
    idea_list_url: https://fossology.github.io/gsoc/docs/2025/GSoC-projects

  - organization_id: 42
    organization_name: Fedora Project
    no_of_ideas:
    ideas_content: |
      Fedora Mentored Projects
      Google Summer of Code
      2025
      Ideas
      en-US
      Contents
      Supporting Mentors
      Idea List
      AI-Powered Log Triage and Security Alert Aggregator for Fedora
      Create a service to get a new project to Fedora more easily
      Ideas for Student Projects for 2025
      Fedora is proud to have been accepted as a GSoC mentoring organization. Student applications open on March 24 2025 1800 UTC. Please make sure you carefully read through the general information and application process pages before applying.
      If you are a student looking forward to participating in Google Summer of Code with Fedora, please feel free to browse this idea list. There may be additional ideas added during the application period.
      Now please go read the What Can I do Today section of the main page. This has the answers to your questions and tells you how to apply
      Do not hesitate to contact the mentors or contributors listed on this page for any questions or clarification. You can find helpful people on the Matrix channel, or use the mailing list. can be used for getting help with programming problems.
      Supporting Mentors
      The following contributors are available to provide general help and support for the GSoC program If a specific project mentor is busy, you can contact one of the people below for short-term help on your project or task. add yourselves and your wiki page).
      Sumantro Mukherjee (General development, quality, general Linux, Fedora community, GSoC alumnus, questions about program, misc. advice)
      Fernando F. Mancera (GSoC, general linux, Fedora community, Mentoring, Networking)
      Idea List
      Ideas are subject to change as additional mentors are onboarded.
      AI-Powered Log Triage and Security Alert Aggregator for Fedora
      Create a service to get a new project to Fedora more easily
      AI-Powered Log Triage and Security Alert Aggregator for Fedora
      Difficulty : Easy
      Type : 1 person full time 350hrs (12 weeks)
      Technology : python, bash, scikit-learn, pytorch, tensorflow, security, AI, LLMs
      Mentor : Huzaifa Sidhpurwala
      Email : huzaifas@redhat.com
      Description
      This project aims to automatically parse, classify, and prioritize security-related logs on a Fedora system. The tool will aggregate logs from multiple sources (e.g., SELinux, systemd journal, audit logs) and apply basic machine learning (ML) or natural language processing (NLP) techniques to identify and prioritize potential security events. It will help administrators quickly spot critical alerts while reducing noise from routine messages.
      Deliverables
      As a GSoC intern, you will be responsible for the following :
      Source Code Repository: A publicly accessible GitHub/GitLab project containing all scripts, models, and integration logic.
      Packaged RPM: A Fedora-compliant RPM package that users can install to deploy the log triage tool.
      Documentation: Concise instructions covering installation, usage, configuration, and development/contribution guidelines.
      Demonstration/Prototype: A working setup (CLI or basic UI) showcasing how logs are collected, classified, and prioritized in real time.
      Testing & Evaluation Results: A set of tests (unit/integration) plus any benchmarking or evaluation reports on model performance and accuracy.
      Create a service to get a new project to Fedora more easily
      Difficulty : Easy
      Type : 1 person full time 350hrs (12 weeks)
      Technology : Python, Git, git-forges knowledge and Linux
      Mentor : František Lachman
      Email : flachman@redhat.com
      Description
      This project aims to help people with less experience add a project (=package) to Fedora Linux by using pull-request workflow to be able to get feedback both from tools and more experienced packagers.
      Deliverables
      Source Code Repository: Either a contribution to existing tools (e.g. FedoraReview, Fedora review service and/or Packit) and/or another publicly accessible GitHub/GitLab project containing all the code and scripts.
      Documentation: Concise instructions covering the service and its deployment, testing and development but also usage.
      Demonstration/Prototype: A working setup showcasing how this service works and integrating at least a single automatic feedback.
      Testing & Evaluation Results: A set of tests (unit/integration) of the new code.
      Deployment: The service is deployed and running or it is available to be run and deployed in the form of a container.
      Want to help? Learn how to contribute to Fedora Docs ›
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fedora-project/
    idea_list_url: https://docs.fedoraproject.org/en-US/mentored-projects/gsoc/2025/ideas/

  - organization_id: 43
    organization_name: Fortran-lang
    no_of_ideas: 26
    ideas_content: |
      
      Version Constraint Resolution (fpm)
      The current decentralized package system in fpm allows dependencies to be fetched via a git repository URL. As part of this, a git tag or commit can be given to require a specific version of a dependency. There is however no way of specifying version compatibility requirements (e.g. >= 1.0.0, <2.0.0) and no way to resolve such requirements across a dependency tree.
      This project will involve:
      Defining a manifest syntax for version compatibility matching
      Implementing support in fpm for solving a set of version compatibility constraints
      A possible approach would be to interface with an existing satisfiability solver such as:
      libsolv: interface via iso_c_binding as a separate fpm package
      See also: existing options for version matching syntax:
      conda
      npm
      cargo
      Expected outcomes: Implemented a working version constraint mechanism in fpm
      Skills preferred: Fortran programming, experience with one or more build systems
      Difficulty: Intermediate, 350 hours
      Mentors: Brad Richardson (@everythingfunctional), Sebastian Ehlert (@awvwgk), Umashankar Sivakumar (@usivakum)
      
      ~~~~~~~~~~
      Build Process Enhancements (fpm)
      Fortran Package Manager (fpm) is pivotal for long-term Fortran success. This GSoC project aims to improve fpm’s build process by improving dependency detection, optimizing linking, implementing shared libraries, ensuring safe concurrent builds, and introducing external Makefile generation.
      The project will address the following tasks:
      Custom flags and configurations
      Implement custom and compiler-dependent flags, and configurations
      External build system Generation:
      Enable generation of external Makefiles akin to cmake -G for advanced project configuration.
      Linking Optimization:
      Replace one-liner linking with static libraries to prevent line buffer overflow in Windows builds.
      Shared Library Implementation:
      Introduce support for shared library targets for project flexibility.
      Dependency Detection:
      Enhance fpm’s dependency detection to minimize rebuilds by parsing or hashing module/submodule files or parsing procedure interfaces in module files. fpm should not rebuild dependencies to a module whose public interface has not changed.
      Expected Outcomes:
      Enhanced dependency tracking and reduced rebuild times.
      Improved reliability in linking, particularly in Windows.
      Increased project versatility with shared library support.
      Safer concurrent builds through file locking.
      Greater project configuration flexibility with external Makefile generation.
      Difficulty: Intermediate, 175 hours.
      Skills preferred: Fortran programming, experience with one or more build systems
      Mentors: Federico Perini (@perazz), José Alves (@jalvesz), Henil Panchal (@henilp105)
      
      ~~~~~~~~~~
      Extended Testing Support (fpm)
      The aim of this project is to create a manifest specification to provide defaults to executable targets in fpm projects. Information can be passed as environment variables, command-line arguments or as a runner. Desired features include:
      Programs should have a way to find resources of which the relative position within the project source directory is known.
      The current binary directory to access other targets within a project.
      Default runners like mpirun/cafrun or scripts from test frameworks should be usable to launch programs.
      A general syntax to define environment variables and command-line arguments should be defined.
      Some features should be implemented directly in fpm, while more elaborated functionality could be implemented in a separate fpm package as an official Fortran-lang fpm package.
      Related issues:
      fpm#179: Testing with fpm test
      Related discussions:
      fpm#328: Example which requires external data
      Expected outcomes: fpm has broader and deeper testing functionality
      Skills preferred: Fortran programming and writing unit tests
      Difficulty: Easy, 175 hours
      Mentors: Sebastian Ehlert (@awvwgk), Brad Richardson (@everythingfunctional)
      
      ~~~~~~~~~~
      Export build order and compile_commands.json (fpm)
      fpm has the ability to automatically determine the build order of a project's source files. This information is valuable to third party tools such as language servers and code analysis tools. The goal of this project is to export the build order of a project's source files in the compile_commands.json.
      The second leg of this project is to implement the full syntax of compile_commands.json as described in the Clang documentation. This would bring fpm a step closer to being compatible with other build tools.
      Expected outcomes: fpm will export a complete compile_commands.json file.
      Skills preferred: Fortran programming, experience with one or more build systems
      Difficulty: Hard, 350 hours
      Mentors: Giannis Nikiteas (@gnikit)
      
      ~~~~~~~~~~
      Support of external third-party preprocessors
      Adding support for external third-party preprocessors is important for fpm due to the additional flexibility they provide when building complex packages. In particular, the Fortran-lang stdlib project exploits the powerful fypp preprocessor for code generation and the support of fypp by fpm is required for stdlib to eventually be compatible as an fpm package.
      This project will require to:
      Modify fpm to optionally invoke a third-party preprocessor before compiling sources;
      Extend the current manifest syntax of fpm for defining preprocessor variables in a preprocessor-independent manner, if necessary;
      Extend the current manifest syntax of fpm for specifying a third-party preprocessor and the corresponding file suffixes, if necessary;
      Passe defined preprocessor variables to built-in preprocessors if necessary;
      Third-party preprocessors should be specified on a per-project basis, i.e. multiple preprocessors might be required, and fpm should be able to report useful errors for missing third-party preprocessors.
      Related issues:
      fpm#78: support for third-party preprocessors (e.g. fypp)
      fpm#308: Fortran-based smart code generation in fpm
      fpm#469: Source pre-processing prior to determining dependencies
      Expected outcomes: fpm has a working preprocessing capability
      Skills preferred: Fortran, C, or Python programming, experience using one or more preprocessors
      Difficulty: easy, 175 hours
      Mentors: Laurence Kedward (@lkedward), Milan Curcic (@milancurcic), Federico Perini (@perazz), Jeremie Vandenplas (@jvdp1)

      ~~~~~~~~~~
      File system library (stdlib)
      Currently, file system operations such as listing contents of directories, traversing directories, and similar, are restricted to 3rd party libraries and compiler extensions that are platform-specific and not portable. This project will entail designing and implementing a cross-platform solution for file system operations.
      Related issues:
      stdlib#201: File system operations
      stdlib#220: API for file system operations, directory manipulation
      WIP implementation:
      stdlib_os
      Expected outcomes: Implemented an stdlib module that provides cross-platform file-system utilities
      Skills preferred: Fortran and C programming, experience using Linux, macOS, and Windows
      Difficulty: Intermediate, 350 hours
      Mentors: Arjen Markus (@arjenmarkus), Milan Curcic (@milancurcic)

      ~~~~~~~~~~
      Library to work with OS processes (stdlib)
      Cross-platform solution to abstract POSIX and Windows API for creating subprocesses.
      Related issues:
      stdlib#22: Interface to POSIX I/O API
      stdlib#308: Subprocesses and Multiprocessing
      Discourse thread:
      Ideas for command module
      Skills preferred: Fortran and C programming, experience using Linux, macOS, and Windows
      Difficulty: Intermediate, 350 hours
      Mentors: Sebastian Ehlert (@awvwgk)

      ~~~~~~~~~~
      Linear algebra and sparse matrices (stdlib)
      Improve dense and sparse linear algebra APIs in the Fortran Standard Library.
      The API development should closely follow the developements on dense linear algebra in order to keep a coherent interface for sparse and dense matrices.
      Related issue: #931 #930 #910 #898 #891 #763 #934
      WIP implementations: #915 #844 FSPARSE
      Expected outcomes: Improved linear algebra and sparse matrix functionality in the stdlib_linalg module
      Skills preferred: Fortran programming, understanding of linear algebra
      Difficulty: Hard, 350 hours
      Mentors: Ondřej Čertík (@certik), Ivan Pribec (@ivan-pi), Jeremie Vandenplas (@jvdp1), Jose Alves (@jalvesz), Federico Perini (@perazz)
      
      ~~~~~~~~~~
      String to number conversion (stdlib)
      This project will enhance stdlib's string handling capabilities for fast number parsing in Fortran.
      Recently, a new module was added to stdlib called stdlib_str2num which implements fast routines for converting strings to numerical types. The participant would get familiar with these implementations and subsequently:
      Create a full benchmark suite for the string to number conversion, across compiler vendors, operating systems, and CPU architectures.
      Explore ways to improve robustness and efficiency, e.g. error handling.
      Propose a shallow interface for the string_type facility in stdlib.
      Propose an enhancement to the loadtxt facility function to speed-up file reading.
      Depending on the advancement, the participant is also encouraged to include a roadmap for inclusion of the inverse conversion by following the intitiative in this thread ryu-based to_string function
      Relevant thread on Fortran Discrouse: Faster string to double
      Expected outcomes: Enhancement of stdlib fast string to number conversion
      Skills preferred: Fortran and C programming, understanding of floating-point arithmetic
      Difficulty: Hard, 350 hours
      Mentors: Jose Alves (@jalvesz), Carl Burkert (@carltoffel) Brad Richardson (@everythingfunctional), Ivan Pribec (@ivan-pi)
      
      ~~~~~~~~~~  
      Compile benchmarking code written in Fortran with LFortran and improving LFortran's performance on these benchmarks (LFortran)
      https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/fortran.html contains all the benchmark codes written for various problems such as n-body, sepctral norm, mandelbrot. The workflow would involve first doing bug fixes to compile the code (modifying the input code would be okay) with LFortran and producing correct outputs. Then, improving LFortran to perform better or equivalent to other Fortran compilers such as GFortran.
      n-body already compiles with workarounds with LFortran main. See, https://github.com/lfortran/lfortran/pull/1213. More work needs to be done for other benchmark codes.
      Expected outcomes: LFortran can compile as many benchmark codes as possible. Performing better than other compilers would be an additional plus.
      Skills preferred: Fortran and C++ programming
      Difficulty: intermediate/hard, 350 hours
      Mentors - Gagandeep Singh (Github - @czgdp1807)
      
      ~~~~~~~~~~
      Compile any Fortran code (LFortran)
      The primary goal is to compile as many codes as possible. We have identified and listed those at label:code-to-be-compiled.
      This project aims to pick up a code and get it compiled to ASR, then to LLVM, binary and assure that values align with GFortran (or other Fortran compilers). We can have several of these projects at the same time.
      Expected outcomes: LFortran can compile chosen code.
      Skills preferred: Fortran and C++ programming
      Difficulty: intermediate/hard, 350 hours
      Mentors - Pranav Goswami
      
      ~~~~~~~~~~
      Compiling SciPy with LFortran (LFortran)
      Currently LFortran compiles about 60% of all SciPy Fortran packages and can parse all the Fortran source code in SciPy. The goal of this project is to compile the rest of them. This project involves implementing the rest of the semantics that is needed to compile the Fortran files with LFortran.
      Being able to compile SciPy with LFortran would make a huge impact on both LFortran and SciPy.
      Expected outcomes: LFortran can compile all Fortran code in SciPy.
      Skills preferred: Fortran and C++ programming
      Difficulty: intermediate, 350 hours
      Mentors - Ondřej Čertík, Pranav Goswami
      
      ~~~~~~~~~~
      Compiling LAPACK with LFortran (LFortran)
      Progressing towards beta we need to compile as much of the LAPACK routines as possible. The goal of this project is to compile LAPACK Fortran codes. It involves implementing the rest of the semantics that is needed to compile the Fortran files with LFortran.
      Expected outcomes: LFortran can compile all code in LAPACK.
      Skills preferred: Fortran and C++ programming
      Difficulty: intermediate, 350 hours
      Mentors - Ondřej Čertík, Pranav Goswami
      
      ~~~~~~~~~~
      Allow running Fortran in the browser (LFortran)
      We have LFortran running in the browser using WASM here: https://dev.lfortran.org/, the goal of this project would be to improve the user interface. Here is a list of issues that the project can work on fixing: https://github.com/lfortran/lcompilers_frontend/issues
      This project would entail working with LFortran, LLVM, Emscripten, and Webassembly to allow running Fortran in the browser.
      Skills preferred: Fortran and C++ programming
      Difficulty: intermediate, 350 hours
      Mentors - Ondřej Čertík
      
      ~~~~~~~~~~
      Implementation of features on the ASR and LLVM level (LFortran)
      The roadmap https://gitlab.com/lfortran/lfortran/-/issues/272 issue contains a list of Fortran features that we want implemented. Each feature should be implemented at the ASR level and in the LLVM backend to be complete. If AST is missing for a given feature, then it has to be implemented also.
      Here you can pick a feature or a set of features from the list and propose it as a GSoC project. In other words, this project idea can accommodate multiple student projects.
      List of resources for more information and background:
      ASR.asdl, the comment at the top explains the design motivation
      asr_to_llvm.cpp is the LLVM backend
      ast_to_asr.cpp is the AST -> ASR conversion where all semantics checks are being done and compiler errors reported to the user
      Developer Tutorial
      If you have any questions, please do not hesitate to ask, we can discuss or provide more details.
      Mentors: Ondrej Certik (@certik)
      
      
      
      ~~~~~~~~~~
      MPI support (fortls)
      fortls has support for Fortran intrinsics, Standard modules and OpenMP. It does not however support MPI. The goal of this project is to add full support for completions, hover and signature help for MPI variables, subroutines and functions.
      Due to the size of the MPI standard, the process of extracting the necessary information from the standard such as names, interfaces and documentation will be automated. The student will be responsible for creating a scraper/parser to fetch the necessary information from the MPI standard and then create the serialised data (JSON) to be used by fortls.
      Discourse thread: MPI documentation and interfaces
      Expected outcomes: fortls will have completion and hover support for MPI.
      Skills preferred: Python programming and understanding of Fortran
      Difficulty: Intermediate, 175 hours
      Mentors: Giannis Nikiteas (@gnikit)
      
      
      ~~~~~~~~~~
      Semantic highlighting and collapsable scopes (fortls)
      As part of this project the student will add support to fortls for the Semantics Tokens request, which is used to provide improved syntax highlighting and the Folding Range request, which is used to provide collapsable scopes.
      Related Issues:
      fortls#56
      Expected outcomes: fortls will serve for semantic highlighting and collapsable scopes requests.
      Skills preferred: Python programming and understanding of Fortran
      Difficulty: Intermediate, 175 hours
      Mentors: Giannis Nikiteas (@gnikit)
      
      ~~~~~~~~~~
      Replace explicit LSP interface with pygls (fortls)
      fortls uses explicit interfaces to the Language Server Protocol (LSP). To decrease code duplication and increase maintainability, the work of maintaining the explicit interfaces should be replaced with the use of pygls' module.
      Related Issues:
      fortls#96
      Expected outcomes: fortls uses pygls' to define LSP interfaces, types and requests.
      Skills preferred: Python programming and understanding of the Language Server Protocol
      Difficulty: Hard, 350 hours
      Mentors: Giannis Nikiteas (@gnikit)
      
      
      ~~~~~~~~~~
      Python environment manager (vscode-fortran-support)
      In the Modern Fortran for VS Code extension, the use of Python as a means to install third party tools is essential. The goal of this project is to create a robust Python environment manager for installing and running third party tools such as fortls, fpm, findent, etc., taking into account the user's setup (venv, conda, system Python, etc.).
      Expected outcomes: Modern Fortran for VS Code will have a robust Python environment manager for installing and running third party tools.
      Skills preferred: Typescript, Python programming
      Difficulty: Hard, 175 hours
      Mentors: Giannis Nikiteas (@gnikit)
      
      
      ~~~~~~~~~~
      vscode integration with fpm (vscode-fortran-support)
      The goal of this project is to allow fpm integration with the Modern Fortran extension for Visual Studio Code, similar to how CMake and Meson are integrated in VS Code.
      Using an Activity bar icon, the user will be able to build and run projects, tests and examples. The student will be responsible for creating the GUI integration and the necessary backend to communicate with fpm.
      Expected outcomes: Modern Fortran for VS Code will have a GUI integration with fpm to build and run projects, tests and examples.
      Skills preferred: Typescript, Fortran programming
      Difficulty: Hard, 350 hours
      Mentors: Giannis Nikiteas (@gnikit)
      
      ~~~~~~~~~~
      Standard Conformance Suite
      Fortran compilers' support for ISO Fortran standards generally lag the publication of the standard by several years or longer. Fortran consultants Ian Chivers and Jane Sleightholme periodically publish a paper containing a table detailing the standard features supported by 10 compilers. Gathering the tabulated data requires a considerable amount of effort on the part of the authors and the compiler developers. The chosen venue for publishing the table also puts it behind a paywall: access requires a subscription to ACM SIGPLAN Fortran Forum. The project will automate the generation of the table, make it more detailed and empower the community to contribute to by submitting small tests to an open-source conformance test suite.
      Prior work:
      fortran-compiler-tests
      flibs chkfeatures
      Defunct
      Fortran Testsuite Proposal
      Expected outcomes: A comprehensive test suite that generates a report of standard conformance for any Fortran compiler. The suite is not expected to be 100% complete by the end of the project, but should be significant in terms of standard coverage.
      Skills preferred: Fortran programming, experience reading and interpreting the Fortran Standard, and writing tests
      Difficulty: Hard, 350 hours
      Mentors: Damian Rouson (@rouson), Arjen Markus (@arjenmarkus), Ondřej Čertík (@certik)
      
      ~~~~~~~~~~
      Coarray Fortran Framework of Efficient Interfaces to Network Environments (Caffeine)
      This project would add support for grouping images (parallel processes) into teams that allow submodes to execute independently. Caffeine 0.1.0 uses the GASNet-EX networking middleware software as a back end for supporting most of the non-coarray parallel features of Fortran 2018 except for the intrinsic derived team_type and related features. Work is underway to support the coarray features that most applications will need for expressing custom parallel algorithms. The teams feature set is the one significant non-coarray parallel group of features not yet implemented in Caffeine.
      Expected outcomes: Caffeine can be used to create images groups in execution parallel programs
      Skills preferred: Fortran and C programming
      Difficulty: Intermediate, 175 hours
      Mentors: Damian Rouson (@rouson)
      
      ~~~~~~~~~~
      Get fortran-lang/minpack to be used in SciPy
      fortran-lang/minpack #14
      The participant would work with Fortran-lang and SciPy teams toward implementing fortran-lang/minpack in SciPy.
      Expected outcomes: fortran-lang/minpack is incorporated into SciPy.
      Skills preferred: Fortran-C interop, Python programming
      Difficulty: Easy, 175 hours
      Mentors: Sebastian Ehlert (@awvwgk)
      
      ~~~~~~~~~~
      Improving fastGPT: Making it Faster, Easier to Use, and More General
      The fastGPT project is a Fortran implementation of GPT-2 that is comparable in speed to PyTorch. Although it is already very fast on CPUs, there is still room for improvement in terms of usability and performance on CPU and other architectures, such as GPUs.
      This project aims to explore various aspects of fastGPT to improve its usability and performance. Some potential areas of exploration include:
      Parallelism: Investigate the use of parallelism in fastGPT, including MPI and coarrays, to potentially make it even faster. Given that GPT inference is dominated by large matrix-matrix multiplications over a few layers, we will carefully investigate which parallel approach is the best (whether MPI, coarrays, OpenMP or just parallel BLAS that we already have).
      Reduced precision models: Experiment with using reduced precision models (e.g., 16-bit or 8-bit floats) instead of the default 32-bit to potentially speed up inference.
      GPU acceleration: Explore how to optimize fastGPT for GPU architectures to potentially make it even faster.
      UI improvements: Add a chat mode (similar to chatGPT). Explore how to make it easier to use as a grammar checker, or creating summaries, or other areas where GPT-2 is strong. Make it a nice Fortran library, installable using fpm, usable in other projects. Investigate how to use it with the neural-fortran project.
      Expected outcomes: Create an improved fastGPT implementation that is faster, easier to use, and more general.
      Skills preferred: Fortran, linear algebra
      Difficulty: Intermediate, 175 hours
      Mentors: Ondřej Čertík (@certik), Milan Curcic (@milancurcic)
      
      ~~~~~~~~~~
      Fortran Graphics Library
      Fortran does not have native graphics handling capabilities. While several bindings interfacing Fortran to graphics and plotting libraries are available (e.g., f03gl, sdl, pyplot, dislin, plplot ), no up-to-date open-source graphics package with a pure, modern Fortran API is available.
      The aim of this project is to lay out the basics of an object-oriented "canvas" representation in object-oriented Fortran. The contributor would implement, document, and test basic graphics classes (2d points, lines, brushes, etc.), an abstract graphics canvas API with backends to both file and graphics devices (i.e., bitmap, PNG, OpenGL, SVG, etc.) The outcome of this project would be a contribution to the development of a platform-agnostic graphics library for Fortran.
      Expected outcomes: Design and implement classes for 2d graphics primitives, a unified graphics canvas API, and several backend implementations.
      Skills preferred: Fortran, C, 2D graphics basics
      Difficulty: Intermediate, 350 hours
      Mentors: Federico Perini (@perazz)*
      
      
      ~~~~~~~~~~
      Improved generation of Fortran interfaces for PETSc
      PETSc, the Portable, Extensible Toolkit for Scientific Computation, pronounced PET-see, is for the scalable (parallel) solution of scientific applications modeled by partial differential equations (PDEs). It has bindings for C, Fortran, and Python (via petsc4py). PETSc also contains TAO, the Toolkit for Advanced Optimization, software library. It supports MPI, and GPUs through CUDA, HIP, Kokkos, or OpenCL, as well as hybrid MPI-GPU parallelism; it also supports the NEC-SX Tsubasa Vector Engine.
      Currently, only a part of the Fortran interfaces can be generated automatically using bfort. Since the manual generation of the remaining interfaces is tedious and error prone, this project is about an improved generation of Fortran interfaces from PETSc's C code.
      The main tasks of this project are
      Definition of a robust and future-proof structure for the Fortran interfaces
      Selection and/or development of a tool that creates the interfaces automatically
      More specifically, the first task is about finding a suitable structure of the C-to-Fortran interface that reduces the need of 'stubs' on the C and Fortran side making use of modern Fortran features where appropriate. This task will involve evaluating different approaches found in other projects taking into account the object-oriented approach of PETSc. Prototypes will be implemented manually and evaluated with the help of the PETSc community. The second task is then the automated generation of the Fortran interfaces using the approach selected in the first task. To this end, it will be evaluated whether an extension of bfort, the use of another existing tool, or the development of a completely new tool (probably in Python) is the most suitable approach.
      Links:
      PETSc
      bfort
      Fortran Wiki: Generating C Interfaces
      Fortran Discourse: ISO_C_binding
      Expected outcomes: Stable and robust autogeneration of Fortran interfaces for PETSc that works for almost all routines
      Skills preferred: Programming experience in multiple languages, ideally C and/or Fortran
      Difficulty: Intermediate, 350 hours
      Mentors: Martin Diehl (@MarDiehl)
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/fortran-lang/
    idea_list_url: https://github.com/fortran-lang/webpage/wiki/GSoC-2025-Project-ideas

  - organization_id: 44
    organization_name: Free and Open Source Silicon Foundation
    no_of_ideas: 19
    ideas_content: |
      
      ZynqParrot RISC-V Tracer
      ZynqParrot (https://github.com/black-parrot-hdk/zynq-parrot) is a framework for doing self-contained, FPGA-based "hostless" ASIC accelerator development. It is designed to be extremely general and has been used to prototype IP from individual ASIC/FPGA cores to full multicore processors. In addition, ZynqParrot has been used to bringup N=1 ASIC silicon in the lab.
      RISC-V provides a trace format specification (https://github.com/riscv-non-isa/riscv-trace-spec) which can be used for diagnostic performance and debugging. This project will design and integrate a RISC-V Trace implementation into the ZynqParrot environment, requiring SystemVerilog implementation + testing, Block Diagram (Vivado IPI) design and well as writing C++ driver to work in both Co-Simulation and Co-Emulation.
      Skill level: intermediate
      Project length: medium (175 hours)
      Mentors: Dan Petrisko
      Language/Tools: SystemVerilog, C++, some knowledge of computer architecture. RISC-V knowledge preferred but not required. FPGA tools such as Vivado strongly encouraged but not required.
      
      ~~~~~~~~~~
      Surfer memory and wide array support
      Surfer (https://surfer-project.org) is an open source waveform viewer designed to be snappy and extensible. Waveform viewers work well for visualizing individual signals, but for large arrays or memories users are often more interested in changes to individual elements rather than the whole array.
      For this, a separate UI element where memory content can be visualized as a table would be much more useful. Beyond just visualizing the content, also having the ability to highlights elements that have changed between timestamps or around the cursor would be extra useful.
      Skill level: intermediate
      Project Length: medium (175 hours)
      Mentors: Frans Skarman Oscar Gustafsson
      Languages/Tools: Rust. Familiarity with hardware design is helpful to have some context of what the tool is used for is helpful, but the project itself is pure software. Some familiarity with egui is also helpful though certainly not required.
      
      ~~~~~~~~~~
      cocotb v2 Code Migration Helper
      The upcoming cocotb v2.x release will have quite some breaking changes (see https://docs.cocotb.org/en/latest/release_notes.html), so users and extension developers will have to actively migrate existing code.
      A code migration helper tool would be helpful, even if it is not perfect.
      Some links:
      https://libcst.readthedocs.io/
      https://lukeplant.me.uk/blog/posts/tools-for-rewriting-python-code/
      Skill level: Intermediate/Advanced
      Duration: medium (175 hours)
      Language/Tools: Python, cocotb
      Mentor: Kaleb Barrett

      ~~~~~~~~~~
      Generate Counter Examples for Bounded Model Checks in CIRCT
      The CIRCT project has its own bounded model checking tool, circt-bmc. It takes a hardware design described in CIRCT's MLIR dialects and translates it into a program that uses the Z3 SMT solver to formally prove assertions. If it finds a way how assertions can be violated, it simply terminates with an error message. This is not very useful for a user that is trying to debug a hardware design that they have written. Instead, we would like circt-bmc to produce a counter-example, essentially a signal trace that shows and example of how the assertions can be violated. The Z3 SMT solver actually provides a counter-example as part of its checking, circt-bmc just does not use that yet.
      We would love you to extend circt-bmc with a counter-example feature that produces a signal trace for violated assertions, ideally a VCD waveform as a starting point. This will require you to modify the lowering pass that translates a hardware design to Z3 solver calls: in addition to the asserts that need to be checked, you will also want to translate any named ports, wires, registers into the corresponding Z3 solver expressions. The bounded model check can then take a snapshot of all these expressions in every time step. When Z3 finds a counter-example, you can go through every time step, evaluate all the solver expression for all user-visible names, and write them to a waveform.
      This may also be an excellent opportunity to introduce a waveform writing library for CIRCT. Eventually, we'd want different tools in CIRCT to be able to write various waveform formats such as VCD, FST, etc. It would be great if there is a common interface for waveform writers, and if CIRCT could then provide various implementations for different waveform formats. Tools like Arcilator, circt-bmc, and circt-lec would then use this library to produce signal traces.
      CIRCT is based on MLIR and LLVM, and are implemented in C++. So you'll definitely want to have some experience writing C++ code, since LLVM-based projects often follow a fairly peculiar and performance-conscious style of C++. You may also benefit from knowing a little bit about SAT and SMT solvers, and how bounded model checks can be implemented incrementally using these solvers.
      Skill Level: Medium
      Duration: 175 hours or 350 hours
      Language/Tools: C++, CIRCT, MLIR, LLVM
      Mentor: Fabian Schuiki,  Martin Erhart, and others in the CIRCT community

      ~~~~~~~~~~
      Spike + Sim-X
      The project is to interface Spike with Sim-X. Spike is a functional RISC-V ISA simulator and Sim-X is a high-level simulator for the Vortex GPGPU.
      Existing work allowed us to integrate the Vortex GPGPU RTL in the OpenPiton multi-core research platform (https://cea.hal.science/cea-04772235/document). To ease programming though, we would like to test software correctness using a high-level simulator. It would be faster than relying on RTL simulation. Interfacing Spike with Sim-X would allow us to simulate functionaly our heterogeneous CPUs+GPU shared-memory architecture, hence allowing us to ease future software development.
      Skill level: Intermediate
      Duration: medium (175 hours)
      Language/Tools: C++, RISC-V GNU Cross-compiler, Vortex LLVM compiler
      Mentor: Davy Million

      ~~~~~~~~~~
      OpenRISC Linux Feature Development
      The OpenRISC Linux kernel support is under constant development but there are certain Linux facilities that are not yet used or available on the OpenRISC platform.
      This project will have the student developing, testing and sending patches up to the Linux kernel. This includes:
      Use the cacheinfo API for reporting CPU details in OpenRISC Linux.
      Add tracing facilities to OpenRISC Linux including: jump_label, ftrace, kprobes, eBPF etc.
      Skill level: Advanced
      Project Length: large
      Language/Tools: Linux, C, Assembly, OpenRISC architecture
      Mentor: Stafford Horne
      
      ~~~~~~~~~~
      Generic MinimumLinuxBoot for RTL Simulations
      This project consists of booting Linux in Qemu, save the memory state, thencontinue the simulation in an RTL Simulation of OpenPiton. The first part of the project consists of understanding what states need to be saved, probably a combination of the TLB and MMU states as an starting point could be enough. Then, this state needs to be saved in a file format that the checkpoint mechanism of Verilator understand or create a synthetic benchmark that makes the proper MMU configuration. The second part of the project is adding the necessary support in OpenPiton Simulation infrastructure to continue the simulation and being able to launch some applications.
      OpenPiton uses different languages like Verilog, Python, Perl, and C. Verilator C++. Additionally, some background in hardware design is useful.
      Skill Level: Medium/Advanced
      Duration: 350 hours
      Language/Tools: Verilog, C++, SystemVerilog
      Mentors: Guillem López Paradís and Jonathan Balkind
      
      ~~~~~~~~~~
      Using AI to Improve Open-Source IP
      What if we could instantly improve all the existing open-source Verilog by reducing its size, improving its maintainability, making it more configurable, identifying bugs, and creating visualization for it? How could you possibly do all those things over one summer as a student? Well, you can't. But you could help to make significant strides in that direction.
      Transaction-Level Verilog (TL-Verilog) models are smaller, cleaner, and less bug-prone than their Verilog counterparts. But there's not much TL-Verilog in the wild yet. If you ask ChatGPT to convert your code today, you won't be happy with the results. But with careful coaching, AI models can be trained for the job.
      Since LLMs understand Verilog better than TL-Verilog, we do as much as possible with the Verilog to prepare it for conversion to TL-Verilog. An initial flow has been put in place for this. A Python program iterates through a recipe of prompts, each performing an incremental refactoring step. After each step, formal equivalence verification (FEV) is used to ensure functional correctness. Human intervention is possible and is currently needed at almost every step.
      Your project will be to use and enhance this flow to refactor an open-source Verilog project like SERV. In the process, you'll contribute to the automation, and your work will become training data to improve future LLMs for this task.
      Skill level: Intermediate/Advanced
      Duration: 350 hours
      Language/Tools: Verilog, Python, TL-Verilog
      Repo: https://github.com/stevehoover/conversion-to-TLV
      Mentor: Steve Hoover

      ~~~~~~~~~~
      Metro-MPI++
      Metro-MPI is a generic methodology to distribute RTL simulation and unlock SoCs’ inherent parallelism. We partition well-defined blocks within designs into isolated simulation processes that communicate via MPI message passing. Metro-MPI works particularly well with replicated blocks of comparable size, such as manycores with NoCs. Verilator is an open-source Verilog simulator and linting tool that translates Verilog HDL code into optimized C++ or SystemC code, allowing for fast, cycle-accurate simulation of digital circuits.
      Automatic partitions with Metro-MPI We want to add the automatic support of metro-MPI inside other tools, like Verilator or Essent. The idea would be to detect the top modules that are suitable to be interfaced with metro-MPI. An automatic partition algorithm would be ideal although we can start with a user-guided approach like pragmas. The project will be divided into two big milestones: the initial task is to use the methodology from Metro-MPI to speed up the simulation (e.g. using messages with MPI to communicate between partitions); the second task would be to influence the partitions of the design to ease the usage of MPI between them.
      Metro-MPI @FPGA We would like to explore the same methodology that Metro-MPI introduces but to connect multiple FPGAs with MPI.
      We are also open to other improvements on metro-MPI:
      Explore the support of OpenMP instead of openMPI
      Explore making the simulations faster with statistical analysis: predict values that will take the MPI messages on a certain simulation, making checkpoints and rolling back in case of predicting wrong.
      Improve current Verilator support from v4 to v5.
      Scale Simulations up to 10K cores (currently we support up to 1024 cores)
      Metro-MPI uses Verilog and C++. Additionally, some background in hardware design is useful.
      Skill Level: Medium/Advanced
      Duration: 350 hours
      Language/Tools: C++, MPI, SystemVerilog
      Mentors: Guillem López Paradís and Jonathan Balkind

      ~~~~~~~~~~  
      OpenLane Web-based Graphical User Interface
      Details: OpenLane is the premier open source RTL-to-GDSII flow. Versions 2.0 or higher's modular architecture allows for constructing complex flows using nodes called "steps," Users who are adept in Python can create many such complex flows, including flows that are parallel. A web-based GUI of some kind (based on a library such as ReactFlow https://reactflow.dev) would greatly enhance the ability of novice users to create custom OpenLane-based flows with ease.
      Skill level: Beginner or Intermediate
      Duration: 175 hrs.
      Language/Tools:: TypeScript (React), Python
      Mentor: Mohamed Gaber, Mohamed Shalan
      ~~~~~~~~~~
      LiteX SMP SoC for OpenRISC
      The LiteX project makes creating FPGA-based SoCs easy. LiteX supports creating SoCs containing OpenRISC CPU cores. Up until now however, there have been no LiteX SoCs that support running OpenRISC multicore/SMP Linux. The linux-on-litex-vexrisc project provides a good example of how to develop and document getting Linux up and running on a LiteX SoC; including multicore.
      Using linux-on-litex-vexrisc as an example, this project will have the student creating a project to help people get up and running with OpenRISC. The final goal shall be to have a documented multicore OpenRISC LiteX SoC running Linux SMP.
      Skill level: Advanced
      Project Length: large
      Language/Tools: Verilog, LiteX, Linux, Python, OpenRISC architecture
      Mentor: Stafford Horne

      ~~~~~~~~~~
      Improve CIRCT's Verilog Frontend
      The CIRCT project uses the Slang frontend to parse the SystemVerilog hardware description language. The sv-tests project runs many SystemVerilog frontends on a benchmark suite of input files to test their quality. We would love you to use the sv-tests results as a starting point to find key missing features that you can add to circt-verilog and fix failing tests. Tests often fail for similar reasons, and fixing small things can cause large numbers of tests to start passing.
      SystemVerilog is a complicated language and CIRCT builds a deep stack of intermediate representations using MLIR to process it. The Slang frontend produces an Abstract Syntax Tree which the ImportVerilog pass converts into the Moore dialect, the first IR level in circt-verilog. Various optimizations are already performed at this level. Then the MooreToCore conversion pass lowers the Moore dialect to the HW, Comb, Seq, and LLHD dialects for further processing. Finally, several optimization passed implemented on the LLHD dialect analyze the hardware design and detect common structures. If you want to sink your teeth into compiler and IR design, this is the perfect project for you!
      Slang and CIRCT are based on MLIR and LLVM, and are implemented in C++. So you'll definitely want to have some experience writing C++ code, since LLVM-based projects often follow a fairly peculiar and performance-conscious style of C++.
      Skill Level: Advanced
      Duration: 175 hours or 350 hours
      Language/Tools: C++, CIRCT, MLIR, LLVM
      Mentor: Fabian Schuiki, Martin Erhart, and others in the CIRCT community

      ~~~~~~~~~~
      Architectural Improvements to OpenPiton+Ariane for RISC-V Profile Compliance
      OpenPiton+Ariane is a permissively-licensed RISC-V manycore processor, built as a collaboration between the PULP Platform from ETH Zürich and the OpenPiton Platform from Princeton University. We would like to co-optimise OpenPiton and Ariane/CVA6 in their combined platform, to improve performance of the processor both in FPGA emulation systems and for eventual silicon chips. We are particularly interested in moving the platform toward RISC-V RVA23 profile compliance and so developing any new extension support needed for this purpose would be a great GSoC opportunity!
      Skill level: Intermediate
      Duration: 175 or 350 hours
      Language/Tools: Verilog, SystemVerilog, RISC-V
      Mentor: Jonathan Balkind, Nils Wistoff
      
      ~~~~~~~~~~
      Cohort++
      Cohort is a framework designed to integrate hardware accelerators into software systems while maximizing efficiency seamlessly. It introduces Software-Oriented Acceleration (SOA), a paradigm that simplifies and optimizes interactions between software and hardware accelerators. By leveraging existing software abstractions—such as shared-memory queues—Cohort enables a streamlined, high-performance communication channel between software components and accelerators.
      This project consists of improving the performance of OpenPiton memory hierarchy to better suit Cohort. For example, there is prior work on supporting wider NoCs, and cachelines in OpenPiton; we changing the Cohort engine's interaction with the coherence protocol; multiple MMU outstanding requests for higher performance.
      We have other ideas to work more on Cohort software support and we are also open to new proposals. Some examples:
      Support for other data structures instead of only queues
      Connect the openMP and/or openMPI runtime library to use Cohort queues
      Add the support for PRGA to be used with Cohort
      Skill Level: Medium/Advanced
      Duration: 350 hours
      Language/Tools: C++, SystemVerilog
      Mentors: Guillem López Paradís , Davy Million and Jonathan Balkind
      
      ~~~~~~~~~~
      Seamless multi-frontend support for OpenLane
      Details: OpenLane is the premier open source RTL-to-GDSII flow. Versions 2.0+ currently support handling multiple frontends for compilation:
      Yosys Default - Verilog
      Synlig - SystemVerilog
      GHDL - VHDL (x86-64-only)
      However, in the cases of VHDL and Verilog specifically– there is no way to mix and match Verilog and VHDL in one design, for example, which is common when reusing IPs.
      This task proposes a retool to OpenLane synthesis to, instead of having two different flows (Classic and VHDLClassic), have one flow accepting a heterogeneous list of files, which can then be inspected to determine the proper frontend to be used.
      The project may involve enhancements to one or more of the C++-based Yosys frontends, as well as the addition of more frontends for languages such as Chisel and Amaranth.
      Skill level: Intermediate
      Duration: 175 hrs.
      Language/Tools: Python, Verilog, C++, Nix
      Mentor: Kareem Farid, Mohamed Shalan

      ~~~~~~~~~~
      OpenRISC Benchmarking and Performance improvements
      The OpenRISC CPU architecture has multiple CPU implementations including the mor1kx and marocchino. Recent testing has shown that memory access on the marocchino is slightly slower compared to the mor1kx.
      This project will have the student:
      Continue from where the 2024 GSoC student left off.
      Use tools like the Embench modern benchmark suite to measure OpenRISC processor and compiler toolchain performance.
      Document the OpenRISC performance at Embench IoT results to be able to compare OpenRISC vs other popular CPUs.
      Track down and improve OpenRISC CPU performance by finding and fixing deficiencies in the verilog designed cores.
      Skill level: Advanced
      Project Length: large
      Language/Tools: Verilog, Shell scripting, C, Assembly, Python
      Mentor: Stafford Horne

      ~~~~~~~~~~
      OpenLane Flow Declaration GUI
      Details: OpenLane is the premier open source RTL-to-GDSII flow. Versions 2.0 or higher's modular architecture allows for constructing complex flows using nodes called "steps."
      Users who are adept in Python can create many such complex flows, including flows that may run multiple steps in parallel, but those who are not may face difficulty doing so.
      A web-based GUI of some kind (based on a library such as ReactFlow https://reactflow.dev) would greatly enhance the ability of novice users to create custom OpenLane-based flows with ease.
      Skill level: Beginner to Intermediate
      Duration: 175 hrs.
      Language/Tools:: TypeScript (React), Python
      Mentor: Mohamed Gaber, Mohamed Shalan
      
      ~~~~~~~~~~
      Adding TL-Verilog Support to Surfer
      Details: Surfer is a modern open-source waveform viewer that evolved alongside the Spade HDL. It has gained broader popularity beyond the Spade ecosystem, and adding support for other emerging HDL capabilities will benefit the community.
      TL-Verilog models have higher-level knowledge that can be reflected in a waveform viewer to enhance the debugging experience. Most notably, TL-Verilog signals can be "invalid". Invalidity is, in some respects, similar to dont-care state. One distinction is that validity is compatible with two-state simulators, like Verilator.
      This project will focus on two main features to enhance TL-Verilog waveforms in Surfer:
      Displaying TL-Verilog-style signal and hierarchy names in TL-Verilog standard colors.
      Reflecting validity on signal values.
      These two features can currently be seen in the Makerchip IDE's waveform viewer.
      Skill level: Medium/advanced
      Language/Tools: Rust
      Duration: 350 hrs
      Repo: https://gitlab.com/surfer-project/surfer
      Mentors: Frans Skarman (creator of Surfer and Spade), Oscar Gustafsson, Steve Hoover (creator of TL-Verilog & Makerchip)
      
      ~~~~~~~~~~
      Device-Under-Test Python Typing Stub Generator for cocotb tests
      cocotb tests manipulate the signals of the Device-Under-Test (DUT) to verify the design, but what was the name of that signal I needed to wiggle???
      If we had a Python typing stub for the DUT, we could get the Pylance VS Code extension to help us by listing the signals in the DUT in an autocomplete pop-up; we could use Python static type checkers like mypy to ensure we didn't fat-finger the name of some module; or use generated typing stubs to create abstract bus definitions that users could "mock out" when needed.
      cocotb has existing DUT introspection capabilities that could be leveraged to generate Python typing stubs. However, existing features may not be enough, and additional features may need to be added, in addition to the generator itself. Perhaps even, this code could be set up for future use by a language server, like slang, to generate even more informative typing stubs.
      See more information here.
      Skill level: Beginner to Intermediate
      Duration: medium (175 hours)
      Language/Tools: Python, cocotb
      Mentor: Kaleb Barrett
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/free-and-open-source-silicon-foundation/
    idea_list_url: https://fossi-foundation.org/gsoc/gsoc25-ideas

  - organization_id: 45
    organization_name: FreeCAD
    no_of_ideas: 38
    ideas_content: |

      Improve FreeCAD Hidden Line Removal
      Outline
      FreeCAD's Technical Drawing module (TechDraw) relies heavily on the OpenCascade Hidden Line Removal algorithms. These algorithms can be very slow, do not provide progress reporting and do not provide any linkage between the input shape and the output.

      Details
      The TechDraw module provides projections, section views and detail views of 3D model components and assemblies developed in FreeCAD modules such as Part, PartDesign and Draft.

      Expected Outcome
      a) develop new code for projecting shapes and creating the geometry for technical drawings.
      -or-
      b) modify the existing OpenCascade code as an enhancement.

      Project Properties
      Both OpenCascade and TechDraw are written in C++.

      Skills
      The student should have a good knowledge of C++ and be familar with graphics topics such as the painters algorithm, face detection and hidden line removal.
      Knowledge of technical drawing standards and previous exposure to Qt will be helpful. Familiarity with OpenCascade is a definite plus.

      Difficulty
      Hard

      Size
      long

      Additional Information
      Potential mentor(s): wandererfan
      Organization website: https://freecadweb.org
      Communication channels: https://forum.freecadweb.org

      ~~~~~~~~~~
      Create visual programming nodes for generating BIM data with IfcSverchok

      Outline
      Blender allows visual node programming, similar to Grasshopper in Rhino. Grasshopper has a really neat extension called GeometryGym which allows users to use visual node programming to generate building geometry and data using the "IFC" international standard. It's super awesome, and something like that doesn't exist yet in the open source world with only free software. So, let's create it!

      Some of this is already started, so the basics are already coded, but now it needs to be tested, debugged, and a whole bunch more nodes written and tested out in experiments in generating buildings with visual nodes.

      https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/ifcsverchok is the source code, take a look, install it, and play around!

      Expected Outcome
      A whole bunch more nodes, and little examples of generating buildings via nodes, and getting an IFC output.

      Future Possibilities
      If we code it right, in the future, these nodes can be made agnostic of Blender and also work in FreeCAD (e.g. through PyFlow). So this allows multiple authoring apps to benefit from visual programming.

      Project Properties
      Skills
      Python
      Knowledge of visual programming (knowledge with Grasshopper / Sverchok super useful!), if not, get ready to watch tutorials on them!
      Basic Blender knowledge
      Difficulty
      Medium

      Size
      Medium (175h)
      The IfcOpenShell API is exposed as visual nodes, and basic geometry creation nodes are related to IFC.

      Long (350h)
      Proof of concept of simple visual node programming tasks are recreated with Sverchok and IFC nodes.

      Additional Information
      Potential mentor(s): Dion Moult
      Organization website: https://ifcopenshell.org
      Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues
      
      ~~~~~~~~~~

      Scripts for generating simple animations (e.g. appear / disappear, bounce, appear left to right, fade in from above, etc)



      Outline
      Often, construction firms need to visualise animations of construction sequencing. A project timeline will be created, and related to individual model elements. For example, when a concrete slab is poured, it is linked to a 3D object called a slab. We need the ability to automatically generate animations from Blender where objects appear / disappear in various different ways when they start / end their task in the project timeline. The systems for describing project timelines is already in place, so now we need a little animation generator!

      Details
      Expected Outcome
      A series of small scripts that take objects and can automatically animate the visibility, locations, or staggered appearances of building elements, as well as sub elements, and basic scripts that correlate real world time to animation frames, and frames per second, and generate an animated timeline bar in various styles.

      Future Possibilities
      This animation system can be then used from BIM models either in Blender, FreeCAD, or via other software altogether, so it has quite a large impact on the ecosystem.

      Skills
      Basic knowledge of the principles of animation (keyframing)
      Basic Blender animation (you can do some tutorials and get up to speed pretty quick)
      Python
      Artistic sense! We should offer beautiful and elegant animations!
      Difficulty
      Easy

      Additional Information
      Potential mentor(s): Dion Moult
      Organization website: https://ifcopenshell.org
      Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues

      ~~~~~~~~~~

 
      Create a 2D nesting tool	C++/Python, CAM/BIM, OpenCasCade	350h	Medium/Hard
      Open
      Feature
      Open
      Create a 2D nesting tool
      #19576
      Feature
      @yorikvanhavre
      Description
      yorikvanhavre
      opened last month · edited by yorikvanhavre
      Problem description
      Nesting is the process of arranging different shapes inside one bigger container shape. This is often used in CNC or laser cutting operations, to try to figure out how to cut several shapes out of a piece of material, while wasting as little as possible of the material.

      FreeCAD currently has a nesting tool, but it is very inefficient and fails often.

      This project should:

      Explore and assess the current tool, where it works and where it fails
      Investigate existing, open-source nesting solutions
      Find out if any is usable in FreeCAD, both technically (code-wise) and legally (compatible license)
      Research possible algorithms, or ways to better the current algorithm
      Decide either to use an external solution, or build or refactor our own algorithm
      Adapt the current tool to use it
      Difficulty
      Medium/Hard

      Workbenches/Technologies
      CAM/BIM, OpenCasCade, C++/Python

      Project size
      350h

      ~~~~~~~~~~

      Create fillets between 2 arcs in Sketcher	C++, Sketcher	175h	Medium/Hard
      Open
      Feature
      @KrisCouvreur
      Description
      KrisCouvreur
      opened on Feb 8 · edited by yorikvanhavre
      Problem description
      Fillet between 2 arcs in Sketcher workbench : can someone write the Python code for this ?
      In attachment the math behind this.
      Pull request was not allowed.

      Fillet between 2 arcs.pdf

      Difficulty
      Medium/Hard

      Technology/workbenches
      Sketcher, OpenCasCade

      Project size
      175h

      ~~~~~~~~~~

      Allow to place windows and doors with different insertion points	Python, Qt, BIM, Coin3D, OpenCasCade	175h	Medium
      Open
      Feature
      Open
      [BIM] Add the "Window/Door Alignment" property
      #19552
      Feature
      @kaiwas
      Description
      kaiwas
      opened last month · edited by yorikvanhavre
      Gsoc idea summary
      See below

      Is there an existing issue for this?

      I have searched the existing issues
      Problem description
      Currently, all windows and doors are built relative to the lower left edge (origin).
      If you add the "Alignment" property, windows and doors can be placed in a more convenient way. For example, with the "Right" alignment, the origin is shifted by minus width and we can place the window relative to the end of the wall.
      If the alignment is "Center", the origin is shifted by minus half the width and we can snap to the center point of the wall.

      I have put together an approximate version using expressions, but this feature should be at the program level without using expressions.
      Video demonstration.

      vokoscreenNG-2025-02-12_09-14-56.mp4 
      Image

      The test file is attached. Remove the Zip extension.

      Win_align.FCStd.zip

      Full version info
      OS: Manjaro Linux (KDE/plasma/xcb)
      Architecture: x86_64
      Version: 1.1.0dev.40176 (Git) Conda AppImage
      Build type: Release
      Branch: main
      Hash: 2745f436026b6f1fb84dbaaf89ea22b5fb4c2295
      Python 3.11.9, Qt 5.15.13, Coin 4.0.3, Vtk 9.2.6, IfcOpenShell 0.7.0, OCC 7.7.2
      Locale: Russian/Russia (ru_RU)
      Stylesheet/Theme/QtStyle: OpenLight.qss/OpenLight/Fusion
      Logical/physical DPI: 96/91.7938
      Installed mods: 
        * FreeCAD_SketchArch
        * PitchedRoof
        * Road 2025.2.6
        * addFC 1.3.0
        * freecad.gears 1.3.0
        * sheetmetal 0.7.10
        * OpenTheme 2024.9.1
      Subproject(s) affected?
      BIM

      Anything else?
      No response

      Code of Conduct

      I agree to follow this project's Code of Conduct

      ~~~~~~~~~~

      Improve FreeCAD Hidden Line Removal	C++, OpenCasCade, Part, TechDraw	350h	Hard
      Open
      Feature
      Open
      [Part] Improve FreeCAD Hidden Line Removal
      #19442
      Feature
      @yorikvanhavre
      Description
      yorikvanhavre
      opened on Feb 6 · edited by yorikvanhavre
      Outline
      FreeCAD's Technical Drawing module (TechDraw) relies heavily on the OpenCascade Hidden Line Removal algorithms. These algorithms can be very slow, do not provide progress reporting and do not provide any linkage between the input shape and the output.

      Details
      The TechDraw module provides projections, section views and detail views of 3D model components and assemblies developed in FreeCAD modules such as Part, PartDesign and Draft.

      Expected Outcome
      a) develop new code for projecting shapes and creating the geometry for technical drawings.
      -or-
      b) modify the existing OpenCascade code as an enhancement.

      Project Properties
      Both OpenCascade and TechDraw are written in C++.

      Skills
      The student should have a good knowledge of C++ and be familar with graphics topics such as the painters algorithm, face detection and hidden line removal.
      Knowledge of technical drawing standards and previous exposure to Qt will be helpful. Familiarity with OpenCascade is a definite plus.

      Difficulty
      Hard

      Size
      long

      Additional Information
      Potential mentor(s): wandererfan
      Organization website: https://freecadweb.org
      Communication channels: https://forum.freecadweb.org
      Migrated from opencax/GSoC#69 (comment)

      ~~~~~~~~~~

      Improve API of the user preferences system	C++, Preferences, User Interface	175h	Medium
      Open
      Feature
      Open
      No way to reset individual preferences / display which setting was changed by the user or theme
      #19171
      Feature
      @maxwxyz
      Description
      maxwxyz
      opened on Jan 21 · edited by maxwxyz
      Is there an existing issue for this?

      I have searched the existing issues
      Problem description
      Currently, the user can only reset all preferences, the preferences from the tab or the group. There is no way to reset individual preferences.
      There is also no way to see, which preferences were changed in comparison to the default value.

      I propose a similar I as in existing apps, where any changed preference is displayed in bold with a reset button next to it. Hovering the reset button would show the default value, clicking it would only reset this preference field.

      UI Example from Prusa Slicer:

      Image

      Image

      Similar UI in Firefox:

      Image

      It would be great to have some sort of information on changes based on a different setting: For example, many TechDraw preferences changes when you switch from ASME to ISO. This relation, why a preference changed and because of which setting it has been changed (and the default when choosing the parent setting) would be great.

      Full version info
      OS: Windows 10 build 19045
      Architecture: x86_64
      Version: 1.1.0dev.39896 (Git) Conda
      Build type: Release
      Branch: main
      Hash: a977fade2de741c94c9efa59b1c2c26df18eb631
      Python 3.11.11, Qt 5.15.15, Coin 4.0.3, Vtk 9.3.0, OCC 7.8.1
      Locale: German/Germany (de_DE)
      Stylesheet/Theme/QtStyle: FreeCAD Light.qss/FreeCAD Light/Fusion
      Installed mods: 
        * Curves 0.6.51
        * fasteners 0.5.32
        * freecad.gears 1.2.0
        * Manipulator 1.5.7
      Subproject(s) affected?
      Core

      Anything else?
      No response

      Code of Conduct

      I agree to follow this project's Code of Conduct


      ~~~~~~~~~~

      Design a finer way to copy/paste object attributes	C++/Python, BIM, Part, Addons, User Interface	175h	Medium
      Open
      Feature
      Open
      Part: No way to copy / paste object properties or attributes
      #17749
      Feature
      @maxwxyz
      Description
      maxwxyz
      opened on Nov 8, 2024 · edited by maxwxyz
      Is there an existing issue for this?

      I have searched the existing issues
      Problem description
      There is currently no way to copy properties from one object to a different object, e.g. placement values.

      For example: Copying one object's placement properties and selecting a different object and pasting them so the placement property is transformed to the new object.
      The workflow could look like this:

      Right click an object --> copy placement
      Right click a different object --> paste placement
      In the case of placement there is a workaround: Open the placement editor while the old part is selected, select the new object and hit apply.

      This could be extended for other properties, e.g. material/appearance, display modes, ...
      Maybe the object can just be copied and then there is a Paste special command where the user can choose what to paste (everything = as today, or a selection of properties).

      Full version info
      OS: Windows 11 build 26100
      Architecture: x86_64
      Version: 1.1.0dev.39100 (Git)
      Build type: Release
      Branch: main
      Hash: 8865450a3e14220925e0e449c0f1f79056b4fb89
      Python 3.11.10, Qt 5.15.15, Coin 4.0.3, Vtk 9.3.0, OCC 7.8.1
      Locale: German/Germany (de_DE)
      Stylesheet/Theme/QtStyle: OpenDark.qss/OpenDark/Fusion
      Installed mods: 
        * CfdOF 1.27.13
        * Curves 0.6.51
        * dodo 1.0.1
        * fasteners 0.5.29
        * freecad.gears 1.3.0
        * Manipulator 1.5.7
        * OpenTheme 2024.9.1
        * OpticsWorkbench 1.0.25
        * Rocket 4.0.0
        * sheetmetal 0.5.3
        * Silk 0.1.5
      Subproject(s) affected?
      Part

      Anything else?
      No response

      Code of Conduct

      I agree to follow this project's Code of Conduct

      ~~~~~~~~~~

      Diagnose and explore solutions to performance issues with large files	C++, Core, Performance, User Interface	350h	Hard

      Performance issues with large files (hover/preselection/navigation) #17185
      Open
      Bug
      0 / 2
      0 of 2 issues completed
      Open
      Performance issues with large files (hover/preselection/navigation)
      #17185
      Bug
      0 / 2
      0 of 2 issues completed
      @maxwxyz
      Description
      maxwxyz
      opened on Oct 11, 2024 · edited by maxwxyz
      Is there an existing issue for this?

      I have searched the existing issues
      Problem description
      When working in large files selecting/preselection is super slow which leads to lagging when starting to orbit or move the camera.
      I guess that is also the case why moving around in the scene is so hard because it takes about 5 seconds where nothing happens, then I can rotate without performance issues

      It seems to be related that the app checks what is under the cursor. Just scrolling for zooming in and out takes 2 seconds until it starts. when it's started I can zoom in and out without issues when holding the mouse at the same position. Moving the mouse freezes again for 2 seconds then it works normally.

      The app behaves normally until the cursor is over geometry.
      When the mouse is moved without clicks over the geometry the preselection highlight of faces or edges lags behind significantly, it also takes a few seconds when clicking that the element gets selected.
      When the mouse cursor is not over geometry (just over the background) the navigation and orbiting starts instantly, so no issues.

      PC should be able to handle this without issues:
      11th Gen Intel i7-11850H @2,5GHz; 32 GB RAM; NVIDIA T1200.

      @Rexbas @kadet1090 FYI
      The file itself is around 190 mb and has a lot of nested bodies in parts in other part containers. I cannot provide the file.

      Full version info
      OS: Windows 10 build 19045
      Word size of FreeCAD: 64-bit
      Version: 1.1.0dev.38923 (Git)
      Build type: Release
      Branch: main
      Hash: d20cb9e6ee198beb2bfd7e72d3dec0a575e3f28c
      Python 3.11.9, Qt 5.15.13, Coin 4.0.3, Vtk 9.2.6, OCC 7.7.2
      Locale: German/Germany (de_DE)
      Stylesheet/Theme/QtStyle: unset/FreeCAD Classic/Qt default
      Installed mods: 
        * Curves 0.6.50
        * fasteners 0.5.29
        * freecad.gears 1.2.0
        * Manipulator 1.5.7
      Subproject(s) affected?
      Core

      Anything else?
      No response

      Code of Conduct

      I agree to follow this project's Code of Conduct


      ~~~~~~~~~~

      Implement Draft Analysis in Part Design	C++, Part Design	350h/175h	Medium
      Open
      Feature
      Open
      PartDesign: Draft Analysis not available
      #16099
      Feature
      @maxwxyz
      Description
      maxwxyz
      opened on Aug 28, 2024
      Is there an existing issue for this?

      I have searched the existing issues
      Problem description
      Currently there is no option to perform a draft analysis to inspect the model to be able to analyze whether the part model is manufacturable based on the draft condition applied.
      It is necessary that the part can be removed easily from the panel dies (in case of Sheet metal part) or Mold (in case of casting).

      Implementation in CATIA:
      https://skill-lync.com/blogs/all-about-draft-analysis-using-catia

      original

      grafik

      Full version info
      OS: Windows 10 build 19045
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.38553 (Git)
      Build type: Release
      Branch: main
      Hash: 59c1ccec3e6b70f56eeee8f94d361019b84bd850
      Python 3.11.9, Qt 5.15.13, Coin 4.0.2, Vtk 9.2.6, OCC 7.7.2
      Locale: German/Germany (de_DE)
      Installed mods: 
        * Curves 0.6.42
        * fasteners 0.5.25
        * freecad.gears 1.2.0
        * Manipulator 1.5.7
      Subproject(s) affected?
      PartDesign

      Anything else?
      Maybe related to the implementation of
      #15028

      @pierreporte FYI

      Code of Conduct

      I agree to follow this project's Code of Conduct


      ~~~~~~~~~~

      TechDraw: Export layered PDF File format	C++, PDF, TechDraw	175h	Medium

      Is there an existing issue for this?

      I have searched the existing issues
      Problem description
      All commercial CAD software usually export drawing in a layered PDF format. This allows the user to hide certain layers when viewing the PDF later or extract certain layers only.

      Usually they provide the following layers in this order:

      pixelated image rendering (colors/shades)
      hatching
      visible lines (of the model)
      symbols (balloons, tables, finishes,...)
      section view lines (cutting lines and name)
      Cosmetic geometry (incl. page drawing frame)
      title block template
      title block content (text/images)
      dimensions (values and lines/arrows)
      Demo:
      image

      With only one layer active:
      image

      Full version info
      OS: Windows 11 build 22631
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.37213 (Git)
      Build type: Release
      Branch: main
      Hash: 20e7deb86a8c6c2cd2378f09f8313760933f3a5c
      Python 3.11.9, Qt 5.15.13, Coin 4.0.2, Vtk 9.2.6, OCC 7.7.2
      Locale: German/Germany (de_DE)
      Installed mods: 
        * CfdOF 1.25.12
        * Curves 0.6.36
        * dodo 1.0.1
        * fasteners 0.5.20
        * freecad.gears 1.2.0
        * OpenTheme 2024.5.3
        * OpticsWorkbench 1.0.17
        * sheetmetal 0.4.13
      Subproject(s) affected?
      Techdraw

      Anything else?
      No response

      Code of Conduct

      I agree to follow this project's Code of Conduct

      ~~~~~~~~~~

      Support radius/diameter, also for cylindrical surfaces in measurement tools	C++, Measurement	175h	Medium

      Problem description
      Currently the unified measurement type cannot get results of the type radius or diameter when selecting a cylindrical surface. If a surface is selected, the area is chosen automatically. Switching manually to radius gives no result. There is no type to measure the diameter at all.
      The tool should support the measurement of the radius and diameter, when a cylindrical / arc surface and also curve is selected.

      Additionally, the annotation of radius/diameter should be attached and displayed at the center of the radius. Currently it is on a starting point of the selected curve.

      Full version info
      OS: Windows 10 build 19045
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.37063 (Git)
      Build type: Release
      Branch: main
      Hash: 680792030fd664c94829e2c1b78b21788e8d9879
      Python 3.11.9, Qt 5.15.13, Coin 4.0.2, Vtk 9.2.6, OCC 7.7.2
      Locale: German/Germany (de_DE)
      Installed mods: 
        * BIM 2021.12.0
        * CurvedShapes 1.0.9
        * Curves 0.6.35
        * fasteners 0.5.20
        * OpenTheme 2024.4.20
        * OpticsWorkbench 1.0.17
        * sheetmetal 0.4.13
      Subproject(s) affected?
      Core

      Anything else?
      No response

      Code of Conduct

      I agree to follow this project's Code of Conduct

      ~~~~~~~~~~  

      Measure inertia with material and coordinate system in measurement tools	C++, Measurement	175h	Medium

      Problem description
      With the material of the object the measurement tool should also support the measurement of inertia. The whole matrix should be displayed, plus the principal moments of inertia and its associated coordinate system, possibly different than the base one, with the origin at the center of gravity.

      Several implementations are possible and all are desirable:


      Most basic: use only the base coordinate system.

      Use a point as the origin. The coordinate system is then a translation of the base one.

      Use a local coordinate system.

      Use a single line to compute the inertia around it.
      Full version info
      0.22
      Subproject(s) affected?
      Core

      Anything else?
      Needs mass calculation, see #13715.

      Code of Conduct

      I agree to follow this project's Code of Conduct

      ~~~~~~~~~~

      Add attachment support to annotation labels	C++, Core	175h	Easy

      Problem description
      The AnnotationLabel when created has no attachment support for the origin of the annotation line(s).
      It is not possible to set the position and keep it updated when the annotation is placed at a selected geometry or object.

      Full version info
      OS: Windows 11 build 22631
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.36729 (Git)
      Build type: Release
      Branch: main
      Hash: 6ca35709ddc7f95fb84eef4c24973dc489e5acde
      Python 3.11.8, Qt 5.15.8, Coin 4.0.2, Vtk 9.2.6, OCC 7.7.2
      Locale: English/United States (en_US)
      Installed mods: 
        * BIM 2021.12.0
        * CfdOF 1.25.11
        * Curves 0.6.35
        * dodo 1.0.1
        * fasteners 0.5.20
        * fcVM-WB
        * freecad.gears 1.2.0
        * OpenTheme 2024.4.20
        * sheetmetal 0.4.13

      ~~~~~~~~~~
      
      Add snapping to measurements / picking points	C++, Measurement	175h/350h	Medium

      Currently when selecting two circles or cylinders, the measurement is displayed between the enveloped circle curve or between the mantle surfaces.
      It would be great if the unified measurement facility could support snapping like in the Draft WB to snap to circle midpoints, intersections, and so on.
      This would also allow to measure distances and angles of axes between shafts and holes.
      grafik

      Full version info
      OS: Windows 10 build 19045
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.37063 (Git)
      Build type: Release
      Branch: main
      Hash: 680792030fd664c94829e2c1b78b21788e8d9879
      Python 3.11.9, Qt 5.15.13, Coin 4.0.2, Vtk 9.2.6, OCC 7.7.2
      Locale: German/Germany (de_DE)
      Installed mods: 
        * BIM 2021.12.0
        * CurvedShapes 1.0.9
        * Curves 0.6.35
        * fasteners 0.5.20
        * OpenTheme 2024.4.20
        * OpticsWorkbench 1.0.17
        * sheetmetal 0.4.13

      ~~~~~~~~~~

      Unify DXF importers and exporters	C++/Python, DXF, Draft, Import	350h	Medium/Hard

      DXF import/export is a hot topic. There have been many issues with it and it seems that there are still some problems left. Apparently, the legacy importer/exporter should be used in most cases. But ideally, there should be only a single importer/exporter (with all the benefits and features of the current two implementations) or at least the better/recommended one should be the default to avoid the confusion. Are there any plans for that? I guess that it won't happen before the next release but it's something that should be resolved as soon as possible, in my opinion.

      Full version info
      OS: Windows 10 build 19045
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.36958 (Git)
      Build type: Release
      Branch: main
      Hash: a041129090b8dcbb8367636840d3c668ef30a933
      Python 3.11.9, Qt 5.15.13, Coin 4.0.2, Vtk 9.2.6, OCC 7.7.2
      Locale: Polish/Poland (pl_PL)

      ~~~~~~~~~~

      Allow adding text to the sketcher	C++, Sketcher, Qt	350h	Medium

      Currently the only way of adding text to FreeCAD is StringShape which doesn't play nicely with parametric design (e.g. font cannot be specified as function, neither can size as far as I can tell) and it's painful to use with Part Design.

      It would be good if user could just type string inside sketcher and extrude it if they use Part Design workflow.

      Full version info
      [code]
      OS: NixOS 24.05 (Uakari) (GNOME/gnome)
      Word size of FreeCAD: 64-bit
      Version: 0.21.2.Unknown
      Build type: Release
      Python 3.11.8, Qt 5.15.12, Coin 4.0.1, Vtk 9.2.6, OCC 7.6.2
      Locale: English/United States (en_US)
      Installed mods: 
        * kicadStepUpMod 10.22.9
        * Assembly4 0.50.6
        * fasteners 0.5.0
      [/code]

      ~~~~~~~~~~
      Export 3D PDF File format	C++, Core, PDF	350h	Hard

      There is no way to directly export into a 3D PDF and keep tree structure, materials and custom views.
      https://helpx.adobe.com/acrobat/using/adding-3d-models-pdfs-acrobat.html

      3D related file types to be used in 3D PDF are Universal 3D (.U3D) files (https://en.wikipedia.org/wiki/Universal_3D)

      Related issue for import of these files:

      import PRC file or 3D PDF into FreeCAD #6090

      OS: Windows 11 build 22631
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.36277 (Git)
      Build type: Release
      Branch: main
      Hash: 9e1903d46112b3660bf10c6a4537d728101d560b
      Python 3.10.13, Qt 5.15.8, Coin 4.0.2, Vtk 9.2.6, OCC 7.6.3
      Locale: German/Germany (de_DE)
      Installed mods: 
        * 3DfindIT 1.2.0
        * BIM 2021.12.0
        * CfdOF 1.25.2
        * CurvedShapes 1.0.5
        * Curves 0.6.23
        * Defeaturing 1.2.2
        * fasteners 0.5.12
        * FEMbyGEN 2.1.0
        * freecad.gears 1.2.0
        * freecad_metal_workbench 0.0.1
        * OpenDark 2023.12.17
        * sheetmetal 0.4.2

      ~~~~~~~~~~  

      Use only selected parts of a sketch / Features with different profiles of a master sketch	C++, Sketcher, Core	175h	Hard

      Currently it is not possible to create a master sketch with multiple, overlapping islands of wires and use the individual parts of the sketch in the 3D view for features. Always the entire sketch has to be used to create features. If there are open loops / wires or not one single closed profile, most PartDesign features fail.
      The idea is to create a sketch and use a a single loop / closed profile or combinations of the enclosed sketch for features in FreeCAD, for example PartDesign WB (Pad, Pocket, Revolve,...), Part WB (Extrude,...) or elsewhere.

      In other CAD software, when activating a sketch based feature (e.g. Pad) the user can select the parts of the sketch which should be included in the profile to be extruded. When hovering the sketch, the different - closed - profiles of overlapping sketch geometry are highlighted and can be selected individually to be added or removed from the final group of profiles which will be used for that feature.

      Example from Fusion360:
      extrude-shape

      Example from Onshape:

      onshape-profiles
      onshape-sketch-profiles

      This also lets the user create a single sketch (sometimes referred to as master sketch) and referencing different parts for different features, without the need to copy the sketch, redraw the sketch, recreate dimensions or create external geometry references and redrawing or constraining with regard to them.

      This was also mentioned as issue at the FreeCAD Day 2024 Complaint Session:
      Unable to easily apply operations to selected parts of a sketch

      Full version info
      OS: Windows 11 build 22631
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.36277 (Git)
      Build type: Release
      Branch: main
      Hash: 9e1903d46112b3660bf10c6a4537d728101d560b
      Python 3.10.13, Qt 5.15.8, Coin 4.0.2, Vtk 9.2.6, OCC 7.6.3
      Locale: German/Germany (de_DE)
      Installed mods: 
        * 3DfindIT 1.2.0
        * BIM 2021.12.0
        * CfdOF 1.25.2
        * CurvedShapes 1.0.5
        * Curves 0.6.23
        * Defeaturing 1.2.2
        * fasteners 0.5.12
        * FEMbyGEN 2.1.0
        * freecad.gears 1.2.0
        * freecad_metal_workbench 0.0.1
        * OpenDark 2023.12.17
        * sheetmetal 0.4.2

      ~~~~~~~~~~

      Improve the FreeCAD API documentation	C++/Python, Doxygen	175h	Easy

      Outline
      Work on the FreeCAD doxygen-generated documentation: Propose a better plan, document the modules better, make it clearer to read, etc.

      Details
      The API documentaiton of FreeCAD is generated with doxygen from the docstrings contained in the source code. It is hosted on https://github.com/FreeCAD/SourceDoc . It is currently not easily readable, the modules structure doesn't list classes and functions, python functionality is not well distinguishable from C++ functionality, and many other problems.

      Expected Outcome
      Identify problems and possible solutions, and propose changes to the docstrings and in-source doxygen instructions to build better docs, and possibly do some css work to produce a cleaner HTML result

      Project Properties
      Skills
      The student should have or build a good knowledge of doxygen, optionally be ready to do some css work too

      Difficulty
      Medium

      Additional Information
      Potential mentor(s): Yorik
      Organization website: https://freecad.org
      Communication channels: https://https://forum.freecad.org

      ~~~~~~~~~~
      
      Compare multiple parts / bodies (CAD versions and 3D Scan to Original CAD Model)	C++, OpenCasCade, Part	175h	Medium

      There is currently no feature to compare two parts or bodies. This would be helpful to analyze different parts or versions of the same body.
      Differences (more or less material/features) should be highlighted as an analysis report.
      The feature would also be helpful to inspect 3D scans (meshes) and compare them to the actual CAD model (quality inspection).

      In CATIA there are two option
      Example from CATIA:
      How-to-compare-2-Parts-in-CATIA-V5-2
      Demo Video: https://youtu.be/kb_iyLEO18Y?t=73&feature=shared

      In CATIA there are two options, to make a visual comparison (se image above) and a geometric comparison which shows results with added or removed material on both models in a linked view:
      6a0115711b8d26970b0240a4b4ac12200c-800wi

      Demo in Siemens NX: https://www.youtube.com/watch?app=desktop&v=DNx2AdKI7zg

      Demo with 3D scan comparison in SolidWorks: https://www.youtube.com/watch?v=06B6YcoNuoI

      I've tagged the PartDesign WB as this would be the most useful WB for such features but it would also be beneficial to compare meshes or assemblies and as mentioned above, scanned 3D meshes to actual CAD geometry.

      Full version info
      0.22



      ~~~~~~~~~~  

      Assembly: Possibility to Inspect the Assembly	C++, Assembly, Part	175h	Hard

      There should be some possibilities to inspect the assembly:

      display degrees of freedom (DoF) for the parts (maybe in the property view, maybe in an inspection dialog, maybe display them directly in the 3D view (e.g. arrows)
      alternatively an option to wiggle/animate the assembly in all open degrees of freedom, to see if the assembly is working correctly and all joints were made.
      Check for intersection of parts / collision (clash) detection. But also allow a maximum/minimum distance input and analyze if no parts have a closer or larger distance to each other
      Assembly Hierarchy and Structure: Displaying the organization of components within the assembly hierarchy is crucial for managing complex assemblies. The WB should provide tools for displaying the relation of all components, maybe like the dependency graph
      To add from the FreeCAD Day 2024 Complaint Session:
      No way to get the mass of a sub assembly. Material properties are not handled properly assemblies, you cant get aggregate properties

      Full version info
      OS: Windows 11 build 22631
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.35966 (Git)
      Build type: Release
      Branch: main
      Hash: 7f5d89fa1942fec79222e4d173655744037164dc
      Python 3.10.13, Qt 5.15.8, Coin 4.0.2, Vtk 9.2.6, OCC 7.6.3
      Locale: German/Germany (de_DE)
      Installed mods: 
        * 3DfindIT 1.2.0
        * BIM 2021.12.0
        * CfdOF 1.25.1
        * CurvedShapes 1.0.5
        * Curves 0.6.23
        * Defeaturing 1.2.2
        * fasteners 0.5.2
        * FEMbyGEN 2.1.0
        * freecad.gears 1.0.0
        * freecad_metal_workbench 0.0.1
        * OpenDark 2023.12.17
        * sheetmetal 0.4.0
        * woodworking 0.21.2.33771

      ~~~~~~~~~~

      Sketcher: Improve Dimension between Circle/Arc (Distance Constraints)	C++, Sketcher	175h	Hard

      Currently, it is only possible to constrain the minimum distance between two circles/arcs with the dimension tool:
      grafik

      The tool should be improved to behave like the distance tools on lines, depending on the cursor position of the mouse

      Minimum distance between circle/arcs, already implemented
      Maximum distance between circles/arcs
      Minimum horizontal distance between circles/arcs
      Maximum horizontal distance between circles/arcs
      Minimum vertical distance between circles/arcs
      Maximum vertical distance between circles/arcs
      What is meant:
      grafik
      grafik

      Building on top of @FlachyJoe #9166
      This FR is includes #11412 but includes additional cases to support all distances.
      Also includes #5864

      When implemented, the Dimension tool should be updated to include these different constraint cases and suggest the best case, depending on the cursor position.

      Full version info
      0.22

      ~~~~~~~~~~
      
      Edit multiple documents at a time (AKA make tabs independent editors)	C++, Core	350h	Hard

      freeCAD has been designed to edit one document at a time only. When two files are opened and an edition is occurring in one of them (for example a sketch is opened), the program stays in this edit mode when switching to the other document (the task panel is still the same). If this second document has a sketch that is also opened, it closes the one opened in the other document. In other words, tabs are not independent editor.

      This behavior is a limitation. It makes working on several documents in parallel difficult. A workaround is to open as much as FreeCAD sessions as needed, but it is impossible to detach a tab into its own FreeCAD session.

      This would be a big change because the architecture would be altered. Though it would make FreeCAD more predictable as basically all software with tabs make them independent editors.

      Full version info
      OS: Ubuntu 23.10 (KDE/plasma)
      Word size of FreeCAD: 64-bit
      Version: 0.22.0dev.35510 (Git) AppImage
      Build type: Release
      Branch: main
      Hash: da05f7c8e8b25ff783cc6c4eb3b73b640f134519
      Python 3.10.13, Qt 5.15.8, Coin 4.0.2, Vtk 9.2.6, OCC 7.6.3
      Locale: French/France (fr_FR)
      Installed mods: 
        * OpenDark 2023.12.1

      ~~~~~~~~~~  

      Improvement of Light Sources Preferences: Appearance, Multiple Lights in the Scene, Background Illumination	C++, Coin 3D, User Inteface	350h	Medium

      To extend the functionality of #11113 implemented by #11146 it would be great to add, adjust, and remove lights in the preference dialog of FreeCAD.
      The arrows could indicate a light each.
      grafik

      The direction of the lights should be editable as well as the intensity. If possible, even the color could be adjusted and all saved in the preferences. The current light which is edited could be highlighted in the view, maybe even represent color and intensity on the ball (like a real time preview). It should be possible to add new lights as well as to remove them again.
      Maybe themes could provide settings of the default lightning.

      There should be a setting to set the ambientColor and ambientIntensity with a default white ambient environment light in the scene, so no completely black areas on the model are visible.

      If possible default light settings e.g. three spot, default, ... could be provided and chosen via a dropdown menu and a custom entry could be saved.

      Full version info
      0.22





      ~~~~~~~~~~

      PartDesign: Design a good way to create walls / thin extrusions / ribs	C++, Part Design	350h	Hard

      Currently there is no good way to create "Thin" elements in the FreeCAD. Because of that creating wall-like elements in the Part Design is really hard and requires a lot of effort. Possible uses:

      Creating walls
      Creating ribs
      Creating reinforcements
      To illustrate this feature in OnShape:
      image

      It'd be nice to implement this in a way that would allow this feature to be easily used in most (all?) features without code duplication, so It probably should be implemented as separate "thicken" feature, that then can be: padded, pocketed, revolved, etc.

      This potential feature should also allow control on:

      Offset (One way, symmetric, Two Distances)
      Cap type (Round / Butt)
      Join type (Round / Tangent)
      If possible, there should be a way to create "Thicken" feature using for example "Pad" dialog, to simplify user workflow and make it more user friendly.

      Workarounds
      Using SheetMetal add base: https://wiki.freecad.org/SheetMetal_AddBase
      Creating SubShape binder to sketch with filled offset applied - it works, but it is not intended use of binder, and it has a lot of quirks
      Using Part workbench with 2d offset
      Sketch Offset tool will help a lot with many cases when (if) merged, but again it's not correct tool for the job
      Using Arch_Wall ?
      Full version info
      FreeCAD 0.21

      ~~~~~~~~~~
      
      Implement selection order priority	C++, Coin, User Interface	350h	Hard

      Recently we have a new tool which appeared called 'selection filter'. This also exist in Assembly4 as a toolbar. (introduced in core following #10243)

      This was in part introduced to fix an annoying issue that it is hard/impossible to select some points or lines.

      The technical reason for that issue (I think) is that the selection mechanism use pointOnRay function, but it uses the overload that returns only the 1st object found. So if you try to select a point but a face is closer to the camera, then the face is going to be selected.

      The selection mechanism should not behave like this. Instead it should use the pointOnRay that return ALL the objects found. Then these objects should be sorted by priority by the following order :

      Points
      Edges
      Faces
      Then the selection should take the item with the higher priority.
      @wwmayer Do you have any insight on the matter? Or can confirm if my analysis is correct?

      @Syres916 @FEA-eng you might be interested to follow this.

      Edited following @FEA-eng comments

      ~~~~~~~~~~  

      Sketcher: Implement automatic projection of external geometry for dimensions	C++, Sketcher	180h	Hard

      I would like to propose an easy improvement in the Sketcher module that can make it easier to use and faster.

      While using the following button commands: "Constrain horizontal distance" & "Constrain vertical distance"

      It would be great to run highlight edges from "Create external geometry" when the mouse hovers above external geometries and in case the user clicks, to create the external geometry automatically.

      I think all the functionality is existing already.

      This way the user can create constraints directly to the edges of the body without additional clicks

      ~~~~~~~~~~

      Design a compatibility system for different Assembly Workbenches	C++/Python, Assembly	350h	Hard

      ne of the key findings of the Ondsel Assembly workbench blog series was the need for a common data format for the interchange of assembly information.

      The creation of a data standard should be an early priority to allow the various addon assembly workbenches time to adapt and/or build migration scripts.

      This issue is for continued discussion about the requirements for a common data standard.

      ~~~~~~~~~~
      
      On-machine inspection for CAM	C++/Python, CAM	180h	Medium

      On-Machine Inspection
      Background
      In commercial manufacturing using CNC, the part is inspected to ensure that the resulting part matches the required tolerance. Tool wear, backlash, and other environmental factors can affect machining accuracy.

      Traditionally the part is removed from the machine after completion and placed and a Coordinate Measuring Machine (CMM) which probes the part and compares to the probe results to the required specifications. The problem with this approach is that the placement of the part is lost when it is removed from the machine. If re-work is necessary, the setup time to reestablish accurate placement can be very high.

      On-machine Inspection uses a touch probe in the CNC machine to replace the CMM. The part is left in place and a toolpath is executed.

      In manual mode, the machine probes the part and indicates to the operator if a probe point is out of tolerance.

      In automatic mode, the probe points are gathered and saved to an inspection report

      The CAM workbench in FreeCAD (Path) lacks any capability for generating an On Machine Inspection toolpath.

      Details
      Understand the basic workflow for creating a Path Job and generating tool paths for CNC machining operatoins. Understand the two main workflows for On-machine Inspection.
      Propose a Path operation and workflow for generating a toolpath, storing and displaying inspection results.
      Design the operation, task panels, and core logic.
      Implement unit tests for the core logic
      Implement the feature and demonstrate practical application
      Create a wiki page describing how on-machine inspection is used
      Expected Outcome
      Mergable code for an OMI feature
      Unit tests for the core logic
      A Wiki page describing the procedure for the user to create tests and the developer to create new comparators
      Future Possibilities
      There are numerous other places in the CAM workflow where Integration of touch probes and other sensing equipment would reduce errors and increase cycle time. These are valuable features to implement. They make FreeCAD more attractive to commercial use.

      Project Properties
      Skills
      Programming language mainly Python. Familiarity with or willingness to learn G-code.
      Familiarity or willingness to learn basic CNC / machining concepts
      Difficulty
      Medium

      Project size
      175h

      ~~~~~~~~~~  

      Advanced FreeCAD test system	C++/Python, Core	350h	Medium

      Advanced FreeCAD test system
      This page is dedicated to the Google Summer of Code project regarding the enhancement of FreeCAD's test system.

      Outline
      FreeCAD as a CAE application has a high level of complexity, both in its source code and also in its user interaction. To ensure a certain level of quality automatic testing is essential. However, as an open source application with spare time coders only this part of the project has not seen very much attention. One of the major reasons is the low-level handling required to write test cases. All actions to trigger, every result fetching and every single comparison needs to be hand coded. This makes it cumbersome to provide a test for every created functionality and possibly impossible to do so if deep document comparisons are needed. For example the Part and PartDesign workbench: An automated test for document objects require the resulting topology shape to be analysed. This is a tremendous part and cannot be handled on a per test basis.

      This project aims at reducing the work required to write meaningful tests. This should be accomplished by providing a infrastructure for result file storage and special "comparators" which compare the stored result files with the test result for equality.

      Details
      Create a result file infrastructure for the test system. It should allow to save an arbitrary number of files together with the test itself where the expected results are stored. It is intended to have one result file for each comparator used. The infrastructure should make the storage (file structure), loading and handling easy. It furthermore should define a specification for the generic file content, e.g. which comparator to use for it etc.
      Create a infrastructure for comparators and provide a few important ones. The comparators should be able to read in a result file and compare the available test output with it. As every workbench requires different types of comparisons the comparators need to be provided by the workbench itself, as well as possible top-level ones. The test infrastructure needs to be adopted to work with such workbench specific types. Furthermore there needs to be a way to generate result files for comparators. this can be done either by themselves or by a different class.
      a. Implement a global comparator for the document structure: It stores the Document object structure with all properties in a result file and compares the available document after a test run with it
      b. Implement a Part workbench comparator for shapes: It stores data about a certain TopoShape in a result file suitable for comparison, e.g. number of edges/vertices/faces, properties like area, mass, center of gravity.
      c. Advanced: Create a global comparator for the 3D output based on picture comparison. This is marked as advanced as this comparator needs to be tolerant to slight changes due to driver differences (see VTK for example) and also needs to somehow ensure the same display settings used for the comparators every time
      Create a wizard or GUI for test creation in the Test workbench. This would work like macro recording: the user starts the test recording, and everything plotted in the python console would be the test procedure. When hitting stop the user gets a dialog where he can choose which comparators to apply. The wizard than creates the appropriate test structure with the test itself, all needed result files etc.
      Create a Wiki page describing the working of the test system and how to create tests and new comparators
      Expected Outcome
      Mergable code for a result file based comparator system
      A GUI for simplified test generation based on macro recording
      A Wiki page describing the procedure for the user to create tests and the developer to create new comparators
      Tests which utilize the created comparators and show their use
      Future Possibilities
      Future contributions can include new comparators, e.g. for meshes. Also creating tests for existing functionality has a high priority and can be achieved with the new system. Futhermore a GUI based system can be created, where a test is defined by recorded UI events, see Record and replay events. This could also be a new GSoC project.

      Project Properties
      Skills
      Programming language mainly Python, some comparators may need C++ code.
      Understand and use APIs from FreeCAD and external libraries (OCC for Part comparator)
      Difficulty
      Medium

      Project size
      175h

      ~~~~~~~~~~

      FEM: Extend Z88 Solver	C++/Python, FEM, Z99	350h	Medium

      GSoC FEM Solver Z88
      This page is dedicated to the description of the Google Summer of Code project idea regarding extending FEM solver Z88OS.

      Outline
      FreeCAD is not only a traditional CAD platform but also aims at providing general engeneering functionality. One of the most valuable design tools for modern product development is the finite element method. It provides advanced means for design analysis, stress tests and optimisation. Over the last two years a FEM workbench in FreeCAD has been developed based on the CalculiX solver and already reached a usable state in the 0.16 release. However, since CalculiX has neither real Shell nor real Beam elements, CalculiX has limitations in the regard of these element types. This was the main reason to start an implementation of an additional solver for the FreeCAD FEM workbench. Z88OS was choosen, since it is OpenSource and thus runs on all major plattforms and has real Beam and Shell elements. The solver Z88OS has been integrated in FreeCAD FEM already. But only very limited parameter are supported at the moment. Only fixed constraints in main axis directions and simple node loads are supported on the pre processing side. On the contrary the post processing only the displacements are read into FreeCAD FEM. To really be able to use Z88 as an additional solver more of his capabilities need to be supported by FreeCAD.

      The GSoC project aims to exactly do this. On pre processing side of FreeCAD a lot of constraints and boundary conditions are supported by FEM and CalculiX solver. Most of them should be ported to Z88 too. On post processing side the FreeCAD FEM result object supports a wide range of result types as well as viewing them by the use of sophisticated VTK post processing tools. An implementation should be made to read more Z88 results into FreeCAD result object. Furthermore most Z88 solver adjustments are hard coded into FreeCAD FEM. They should be changed in a way they could be changed during run time of FreeCAD FEM.

      Details
      Get familiar with FreeCAD FEM workbench its capabilities and its architecture. Furthermore get familiar with Z88OS and its interface text files for pre- and post processing. It is important to have a good understanding of all involved components to be able to make the needed extensions.
      Post processing: Extend the importZ88result module to read stress and strains from Z88 result files and add them to the FreeCAD result object.
      Pre processing: Implement Z88 input file writing for all three element dimensions (beam, shell, volume) for the following constraints of FreeCAD FEM: fixed, force, pressure, self weight, displacement.
      Solver: extend the FreeCAD FEM solver object by FreeCAD properties to hold the the attributes which could be used to make the needed adjustments at Z88 solver binary.
      Advanced: add the free-ware solver Z88Aurora (not OpenSource, but free of license fee) as another possibility in FreeCAD FEM in addition to Z88OS. Z88Aurora supports much more constraints and analysis types than Z88OS
      Expected Outcome
      Fully functional advanced postprocessing in FreeCAD based on VTK
      Unit tests ensuring the functionality
      Documentation and tutorials for post processing
      Future Possibilities
      If this project is finished successfully futher work on the FEM workbench can be done. Advancing the preprocessing with better control over the meshing process come to mind, or integrating different solvers for other analysis types. Also calulix implementation can be advanced, for example allowing nonelinear calculations.

      Project Properties
      Skills
      Programming language Python
      Deep understanding and use of APIs from FreeCAD and Z88OS
      Knowledge of FEM pre- and postprocessing workflows and needs
      Difficulty
      Easy-Medium

      Project size
      175h



      ~~~~~~~~~~
      
      Upgrade the documentation system	C++/Python, Doxygen, Mediawiki, Markdown	350h	Easy  

      Upgrade the documentation system
      This page is dedicated to the description of the Google Summer of Code 2023 project idea of upgrading the documentation system of FreeCAD.

      Outline
      FreeCAD already possesses a vast documentation, written by its users and hosted on the FreeCAD wiki. On each FreeCAD release, the contents of the wiki get packed into a offline documentation package which is bundled with FreeCAD. When using the "what's this?" feature, FreeCAD users can quickly get documentation about a specific tool

      Additionally, this same documentation also has several translations, hosted on the same wiki and managed by a mediawiki plugin, and also counts on the source code structure and comments, automatically extracted by the doxygen tool, and hosted on https://www.freecadweb.org/api/

      However there are several problems:

      The mediawiki software is a dinosaur to maintain, and complicated to maintain up-to-date
      The mediawiki data is hard to back up. We are constantly at risk of loosing data
      The mediawiki search feature is notoriously weak
      The mediawiki is complicated to theme and make look good
      Our doc has become as important as FreeCAD itself. It would benefit from having the same level of decentralisation
      The current translation system is always at risk of becoming obsoleted, like many mediawiki plugins. It is not used anymore by the MediaWiki project
      A simpler, file-based system would allow much more automation, scripting and integrations. Python/C++ API documentation can be built on-the-fly and integrated
      A Git-based system would allow branching/tagging/versions of the documentation corresponding to FreeCAD versions and maintaining a more up-to-date documentation
      A markdown-based system would allow to produce many kinds of outputs such as online/HTML like the current wiki, but also better integrate into the FreeCAD Help system, or produce e-book formats or even printed books
      A markdown + Git based system would be easy to integrate into online translation platforms, and therefore free us from one more part of maintainance and unify with other translation systems of FreeCAD
      This project proposes to remedy to these problems by:

      switch the FreeCAD documentation, currently managed by a MediaWiki instance hosted on the FreeCAD website to a file-based, versioned system, preferably based on Git and markdown
      offer a nice web-based experience
      allowing the FreeCAD user to switch between online and offline documentation, and choose between different available languages
      extending and better integrating the autogenerated documentation, with special care for the python API
      enabling different e-book outputs (pdf, e-pub, etc)
      implement a good search system
      The above proposal also introduces a new problem: The ease-of-use of the Mediawiki platform would be lost. That ease-of-use is important because most of the edits done to the wiki are small edits, like corrrecting an info or fixing a typo. Many people who currently work on the wiki would probably do it less often if it required a complex Pull Request-based system. A wiki system, that allows easy and small edits, is therefore fundamental to keep.

      The solution to that problem could be found in using a wiki system that is based on files, markdown and git. As part of this project, available file-based wiki solutions should be researched.

      Details
      have a good knowledge of the markdown format, including how to work with yaml data
      be able to write scripts, preferably in Python, to automate some aspects of the migration or maintaining
      be able and ready to explore and work with the API of online platforms such as MediaWiki, GitHub or Crowdin
      be able to work with doxygen and autogenerated documentation systems
      be ready to document everything you do so others can take the work further
      Expected Outcome
      A better documentation system for FreeCAD, based on the proposed workflow below

      What's there to test already
      An automatic translation of the mediawiki content to markdown is already available on https://github.com/FreeCAD/FreeCAD-documentation . It is done automatically by the help of a python script at https://github.com/FreeCAD/FreeCAD-documentation/blob/main/migrate.py . The translated documentation can be read online, by browsing through the markdown pages, or from within FreeCAD's Help system. Both the Help addon and the markdown documentation form the core of what we want to develop.

      The goal of this project is to migrate for good to the markdown version, retire the migrate script, and shut down the wiki.

      Proposed workflow
      Research available file-based wiki solutions
      Write article explaining the migration to the community: why, what are the advantages, what are the issues
      Write a migration plan or adapt this one
      Define a directory / category structure that reflects the current documentation categories
      Define a tags / yaml system to allow further and finer classification of documentation pages
      Define a language switching system that allows user to use the documentation in their language
      Define a search system
      Define a sequence system so some pages can be assembled like a book
      Design an online HTML-based output
      Design a FreeCAD online output
      Design a FreeCAD offline output
      Design a multi-formats e-book output
      Design a printed book output
      Research and help choose a translation platform, investigate available plans (see below)
      Define what contributors should do while the wiki is in read-only mode
      Set the wiki in read-only mode
      Transfer all the contents from wiki to markdown files
      Verify that all the contents have been transferred
      Manually check each and fix each and every page for style errors
      Verify all the links
      Create a structure for translations and relocate all pages
      Setup the translation platform
      Tie the translation platform into the git repo
      Document the steps needed to setup the translation platform
      Document the process to adopt for translators
      Document the process to adopt for maintainers
      Define how dynamic contents such as FreeCAD API documentation can be extracted and added automatically to the documentation
      Write an article announcing the change
      Design a system to handle and redirect wiki.freecad.org links
      Remove the mediawiki installation
      Possible translation platforms, to be checked again
      Research which translation platform to use (transifex, crowdin?) based on:
      are they free for FOSS projects, or what are the costs
      how good they handle markdown files
      how already translated pages can be fed back into the translation platform
      automatic integration with github
      Crowdin - https://crowdin.com
      Free for FOSS projects
      Incomplete markdown support (looses images) (12/2020)
      Allows to upload a translated file as translation
      Integration with github
      Advantage: FreeCAD users know it
      Transifex - https://transifex.com
      Free for FOSS projects
      Incomplete markdown support (looses images) (12/2020)
      Allows to upload a translated page (git push)
      Integration with github
      Weblate - https://weblate.org
      Free for FOSS projects
      No markdown support (12/2020)
      Project Properties
      Skills
      Programming language: C++, but understanding some Python will be necessary
      Readiness to work with documentation, reasonably good english writing skills
      Difficulty
      Medium

      Project size
      175h or 350h

      ~~~~~~~~~~  

      FreeCAD ↔ BRLCAD integration	C++, BRL-CAD, OpenCasCade, Core	350h	Hard

      FreeCAD-BRLCAD integration
      This page is dedicated to the description of the Google Summer of Code 2023 project idea of integrating FreeCAD and BRL-CAD.

      Outline
      Is there an existing request for this?

      Details
      FreeCAD and BRL-CAD are very complementary applications: BRL-CAD is a powerful engine which could do with a better modelling UI, and FreeCAD has an increasingly vast modelling UI but could make great use of the support for large models that BRL-CAD can offer.

      FreeCAD being highly modular, and BRL-CAD having a C+ API, building a BRL-CAD module for FreeCAD is totally possible. This way, it would be possible to open BRL-CAD models (that are usually called databases, because they are often made of a collection of models) in FreeCAD, and it would also be possible to use FreeCAD as a modelling tool for BRL-CAD.

      This project idea will require a reasonable knowledge of C++, and, since it involves two different applications, a versatile mind able to learn quickly and navigate between many different concepts, as they are implemented differently in both applications.

      This project would be mentored commonly by both FreeCAD and BRL-CAD developers.
      https://wiki.freecad.org/FreeCAD-BRLCAD_integration

      Expected Outcome
      Documentation. Since this is a large task, that might not fit in a single GSOC project, other people (or yourself) will likely work on this project after the GSOC period ends. They must be able to take on the work where it has been stopped. Also, the first steps of this project will involve a lot of research, that should be made as available as possible to others.
      A basic prototype, that allows to open and visualize BRL-CAD databases in FreeCAD, and shows how modelling and saving BRL-CAD objects can work in FreeCAD
      Future Possibilities
      Such an integration could go a very long way, as both applications are very complex, and if the "wedding" works well, new possible fields of use could emerge. Also, we think this kind of inter-project integration could pave the way for more, so the possibilities are vast.
      Project Properties
      Skills
      Programming language: C++
      Good understanding and use of APIs from FreeCAD and BRL-CAD
      Knowledge of 3D modeling, topology and computational geometry is a plus
      Difficulty
      High, mostly because you have two different applications to learn and work with

      Project Size
      350h

      ~~~~~~~~~~

      Design a set of interactive controls in the 3D view for tools that have numeric values/inputs (drag to edit)	C++, Coin, Core, User Interface	350h	Hard

      Added from the FreeCAD Day 2024 Complaint Session:
      When you have done an operation, such as extrude, it’s impossible to visually edit that parameter via click and drag

      Currently all inputs for value/parameter based tools are handled. via the task dialog or the property view. It would be great to input data in the 3D view (widget/overlay) and also click and drag interactively in the 3d view to manipulate the feature. Also the direction of a feature could be indicated by arrows in the 3D view for an overall better UX and easier onboarding for new users.

      Applicable features would be:

      Pad feature: display direction with an arrow, click and drag arrow to define the length. (Double)click the arrow to reverse direction. Typing would insert a value in a widget right in the 3D view (like OVP in Sketcher WB).
      Revolve feature: display the direction and angle, dragging changes the angle
      Sheet metal flange: display direction, angle and length
      Pattern feature: define direction, offset, remove single copies
      and so on...
      Example of other software:

      fusion360-interactive-extrude

      solidworks-interactive-flange

      catia-interactive pattern

      fusion360-interactive-revolve

      Issue imported from https://tracker.freecad.org/view.php?id=4640

      Reporter: nukeRomancer
      Date submitted: 4/21/2021
      FreeCAD version: 0.2
      Category: Feature
      Status: new
      Tags:
      Original report text
      interactive controls in the viewport for all / most ( where applicable ) tools that have numeric values / inputs

      this would be a MASSIVE improvement to the general workflow

      https://forum.freecadweb.org/viewtopic.php?p=496292#p496292

      Other bug information
      Priority: immediate
      Severity: feature
      Category: Feature
      Updated: 4/22/2021
      Discussion from Mantis ticket
      Comment by Pauvres_honteux 2021-04-22 14:51
      Some good examples of how user interacting stuff can be made, begins at 28 minutes

      ~~~~~~~~~~
      
      Direct modeling tools	C++/Python, BIM, Core	350h	Medium

      Issue imported from https://tracker.freecad.org/view.php?id=3353

      Reporter: yorik
      Date submitted: 2/22/2018
      Original report text
      FreeCADs is a feature-based parametric modeling system, hence different modeling steps depend on one or multiple previous steps. Each step is at the same time a tool/operation, and a geometrical object, resulting from that operation. This mix of operation and object is called a feature. Direct modeling, often presented as the opposite of parametric modeling, allows to graphically move vertices, push or pull faces and edges to modify the geometry of an object.

      This issue is a GSOC idea https://www.freecadweb.org/wiki/Direct_modeling_tools

      and also as a test for bountysource...

      Other bug information
      Priority: normal
      Severity: minor
      Category: Feature
      OS: Debian Testing 64bit
      Platform: PC
      Updated: 2/6/2021
      Discussion from Mantis ticket
      Comment by Kunda1 2021-01-31 11:21
      Several attempts have already begun to tackle this issue:

      carlopav in https://forum.freecadweb.org/viewtopic.php?t=49837
      https://github.com/MariwanJ/Design456

      ~~~~~~~~~~
      Integrate a client for the buildingSMART Data Dictionary web service	Python, BIM	175h	Easy  

      The situation
      The BuildingSmarth Data Dictionary (BSDD) is an onlne service provided by BuildingSMART, the non-profit body that maintains the IFC file format, an universal and open file format for BIM.

      BSDD allows a user to query for terms and receive a series of records of where that term is used in different standards such as IFC itself, but also other classification systems used in the world. The final aim is to help BIM users, when they define something, for example a wall, to query the BSDD service to help them find what classes of what classification systems are applicable to that wall, and which one they should use. This can be used not only for objects, like a wall, but also materials (concrete, brick...) or even properties of an object (for ex the height of a wall).

      Classification systems are huge standards that define each and every item in a construction project. The aim is to have things defined as precisely as possible, so there is no ambiguity over what is what. Additionally, having a classification tag attached to an object also helps to overcome translation problems, as it's the same reference number across different languages.

      A BSDD client in FreeCAD
      Currently, assigning a class to an object works like this:

      A user designs a wall
      The user wishes to add a classification entry to that wall
      They can, in the BIM workbench, open the classification manager, choose a classification system, and search for the term "wall" in that system
      The user then picks the class they find most appropriate
      A BSDD workflow in FreeCAD could look like this:

      When opening the classification manager in the BIM workbench, a third panel could open
      That panel could already present a series of suggestions based on the current selected object
      The panel should also allow the user to see what have been search for automatically, and change that if that's not correct
      The user could click on one of the suggestions
      The appropriate classification would be used
      Expected Outcome
      Build a detailed plan of the workflow: What you imagine the user would need to do, and how that would happen
      Design a system to connect to the BSDD service. Beware, it requires authentication, one should carefully read the docs
      Extend the classification manager in the BIM workbench to add BSDD functionality
      Project Properties
      Skills
      The student should have reasonable coding skills in Python. The student should also get familiar with:

      All the details and requirements on https://wiki.freecad.org/Google_Summer_of_Code_2024
      Work with GIT, GitHub and pull requests
      The BuildingSMART Data Dictionary
      The BIM workbench
      The classification manager
      How BlenderBIM implements BSDD (see comments by Dion Moult below) and see how close we can match what they did
      Difficulty
      Medium

      Duration
      350h

      Additional Information
      Main FreeCAD GSoC page: https://wiki.freecad.org/Google_Summer_of_Code_2024
      Potential mentor(s): Yorik
      Organization website: https://freecad.org
      Communication channels: GSoC section on the FreeCAD forum and this issue
      
      
      
      

























      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/freecad/
    idea_list_url: https://wiki.freecad.org/Google_Summer_of_Code_2025


  - organization_id: 46
    organization_name: GNOME Foundation
    no_of_ideas: 5
    ideas_content: |
      
      Add eBPF profiling capabilities to Sysprof
      Papers: Proof of Concept for backend isolation
      Vala: Researching/Designing/Implementing xml/json/yaml/... integration
      Add printing support to GNOME Crosswords
      Add Wordlist Scoring to GNOME Crosswords Editor
      Project proposals will be listed here.
      Add eBPF profiling capabilities to Sysprof
      Currently, Sysprof works by recording stack traces from the Linux Perf subsystem using perf_event_open(). It also can record various system statistics using data from /proc such as CPU, memory, networking, and disk statistics.
      eBPF provides an existing new direction for tooling of this nature by uploading small programs into the kernel to extract the data you want without the parsing overhead. That data can be delivered to an application like Sysprof for recording into the capture files.
      This internship would involve creating the tooling within libsysprof to setup new eBPF programs by compiling, linking, and uploading them into the kernel along with necessary components to get data from the kernel back to Sysprof.
      This would then be used to port some collectors such as CPU or memory trackers to use eBPF instead of /proc files.
      Requirements
      Knowledge of C, preferably using GLib/GObject but that can be learned
      Minimal experience performance profiling software
      Minimal experience with the Linux kernel
      Learning how eBPF works and how to integrate that with the kernel can be learned on the job
      Communication
      Chat, email, video chat. Christian Hergert @chergert or hergertme on IRC/Matrix or @chergert at gnome.org
      Mentor(s): Christian Hergert
      Difficulty: Hard
      Mentor availability: ~350 hours
      More information
      https://gitlab.gnome.org/Teams/Engagement/internship-project-ideas/-/issues/51

      ~~~~~~~~~~
      Papers: Proof of Concept for backend isolation
      This project will mostly require work on Papers' libraries: ppsview and ppsdocument. The former contains an abstraction (PpsJob) to run (potentially slow) backend code in threads. The later is basically a common abstraction over different backends: PDF, DJVU, Tiff, etc. The idea is to create a component that will start a new process per-document (similar to web-browsers having one process per tab!). That side-car process will take care of all the calls to the backends (so embedded in ppsdocument), and be managed by PpsJobs. Further details are available in https://gitlab.gnome.org/GNOME/Incubator/papers/-/issues/104
      The idea for the GSoC will be for the Intern to prototype a solution to this problem, and investigate potential solutions and foot-guns. The intern will need quite a good knowledge on C, and have motivation to do some investigate work (e.g: look into solutions implemented by other projects like WebKit). I don't expect a full implementation or solution, even if that would be welcomed. A failed attempt at this might already gives us extremely valuable input.
      This will benefit Papers as the future Document Viewer in GNOME in two ways:
      By isolating documents from each other, we improve the overall security situation. Even CVEs that might allow somebody to gain access to execution code from rendering a PDF would not have access to the other documents.
      By isolating documents from the UI we improve the resilience of Papers. A document crashing during rendering (for which CVEs happen regularly (last one CVE-2024-6239) will not bring down with itself the complete application. This has been so far the main blocker to implement the Document Viewer tabbed view, which has been a feature request for Evince since 2005
      Requirements
      Good skill and experience in the C programming language. Both be able to write and read it
      The ability to investigate previous approaches to solve the same issue. We will guide the Intern on where to look
      Motivation to try different approaches. We know a big part of this project will be checking the feasibility of different solutions
      Communication
      Our preferred communication channel is Matrix (https://matrix.to/#/#papers:gnome.org), but we will also do video-calls to get onboarded and if deemed useful and necessary for the mentoring process
      Mentor(s): Pablo Correa Gomez, Qiu and Markus
      Difficulty: Hard
      Mentor availability: ~350 hours
      More information
      https://gitlab.gnome.org/Teams/Engagement/internship-project-ideas/-/issues/58

      ~~~~~~~~~~
      Vala: Researching/Designing/Implementing xml/json/yaml/... integration
      The goal of this project is to add xml, json and/or yaml format language integration to Vala. Other integrations already exist for DBus, GTK builder format, or GModule (https://gnome.pages.gitlab.gnome.org/vala/manual/attributes.html#dbus-attribute). Examples can be found here.
      Currently, parsing and emitting files in Vala with formats like xml, json, or yaml are relatively tedious, need a lot of boilerplate code, and are much more complicated than in other programming languages. But these file formats are very commonly used in modern software, and many internet protocols and existing standards depend on them. Improving their handling would allow more efficient development, as well as lower the barrier for newcomers to Vala.
      First, research is necessary, to find out how other programming languages have done the integrations. Also wanted features will need to be evaluated. For that also different example projects need to be looked into or created, to find the different use cases and related challenges and how to address them. For proposing designs of the new Syntax, "code mock-ups" are going to be written. These can later also serve as test cases for the vala test suite. Then the community will discuss and give feedback to iterate on the syntax design. As the last step (and probably the one taking the longest time), the proposed syntax('s) will be prototyped inside the Vala compiler. Also test cases for the new features, documentation and example code snippets should be developed at the end of the project. If time is left at the end, also other projects that benefit from the new features can be updated to use the new syntax, for example the Vala Language Server.
      Requirements
      Experience with programming in general, as well as a good understanding of object orientation
      Ready to learn the Vala language, and to understand and investigate the syntax of other programming languages for research purposes
      Being open to reach out to the Vala community, and to contributors of other projects written in Vala, to collect feedback about the current situation and the new syntax
      Ability to grow familiar with a relatively large codebase, to find solutions to problems you encounter (There will be a lot of guidance of course)
      Communication
      Main communication channel: Matrix: https://matrix.to/#/#vala:gnome.org Calls are possible for onboarding and mentoring whenever there is a need. (And when enough time)
      Mentor(s): Lorenz Wildberg
      Difficulty: Medium
      Mentor availability
      TBD
      More information
      https://gitlab.gnome.org/Teams/internship/project-ideas/-/issues/61

      ~~~~~~~~~~
      Add printing support to GNOME Crosswords
      GNOME Crosswords has some cursory svg code, but doesn't support printing. It is relatively straightforward to extend the svg code to produce printable puzzles. This project consists of multiple sequential stages, with some simple initial milestones and some stretch goals as well. In particular:
      Initial design and planning
      Refactor the svg code to make it handle the existing board and thumbnail usecases, as well as printable puzzles
      Add clues to the generated svg, and make it look good
      Bring the printing dialog to both the game and the editor
      Add crossword-specific options to the printing flow
      As stretch goals, it would be good to support additional puzzle types, as well as add an ipuz2pdf utility to the app.
      Requirements
      Knowledge of C, especially glib-style C programming. Ability to understand a complex codebase and issues.
      Familiarity with svg a plus
      A physical printer is not necessary
      Communication
      Primarily matrix. The main crosswords channel can be found at https://matrix.to/#/#crosswords:gnome.org
      We also will use gitlab issues and email as appropriate. Video conferencing occasionally when necessary.
      Mentor(s): Jonathan Blandford, Federico Mena Quintero, tanmayp@gnome.org
      Difficulty: Medium
      Mentor availability: ~350 hours (Could be ~175 hours if necessary, with fewer milestones reached
      More information
      https://gitlab.gnome.org/Teams/internship/project-ideas/-/issues/62

      ~~~~~~~~~~
      Add Wordlist Scoring to GNOME Crosswords Editor
      GNOME Crosswords Editor uses some lists of words in order to produce crosswords. These lists lack the metadata to make more interesting puzzles. We'd like to find, gather, and encode that metadata with the lists in order to choose appropriate words when working with the editor.
      For more information on the problem, please read this design doc.
      This project will involve finding ways to calculate, measure, and encode values for each of the five traits listed. We will then use it when creating a puzzle to try and create more interesting grids.
      Requirements
      The biggest requirement for this project is a love of words, and a certain amount of comfort with uncertainty. I will expect the intern to do some independent research and exploration
      This project will mostly be in python, and will involve a fair amount of data analysis as well as codings
      Some low-level C programming knowledge is a strong plus, as well as the ability to use a hex-editor. The current wordlist is stored in a custom data structure
      This project will involve working with some large data sets — possibly as big as 20 GB. A relatively fast machine with sufficient disk space will be required, as well as the ability to download large files.
      Communication
      Primarily matrix. The main crosswords channel can be found at https://matrix.to/#/#crosswords:gnome.org
      We also will use gitlab issues and email as appropriate. Video conferencing occasionally when necessary.
      Mentor(s): Jonathan Blandford, Federico Mena Quintero, tanmayp@gnome.org
      Difficulty: Medium
      Mentor availability: ~350 hours (Could be ~175 hours if necessary, with fewer milestones reached
      More information
      https://gitlab.gnome.org/Teams/internship/project-ideas/-/issues/63
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnome-foundation/
    idea_list_url: https://gsoc.gnome.org/2025/


  - organization_id: 47
    organization_name: GNSS-SDR
    no_of_ideas: 4
    ideas_content: |
     
          Project Title: WAAS
      Description:
      Large-sized project (350 h)
      This Google Summer of Code (GSoC) project focuses on advancing the functionalities of GNSS-SDR receivers related to WAAS.
      The primary goal for the summer is to provide a working implementation of a GNSS-SDR receiver working with WAAS signals: Signal acquisition and tracking algorithms for their specific signals. The outcome should be a robust GNSS receiver capable of delivering RINEX-B files and real-time navigation solutions including SBAS information.
      Implement acquisition and tracking algorithms for WAAS signals, following the examples already implemented for other GNSS signals. This would facilitate research on precise positioning, safety positioning and drone-related activities working with real signals. Demodulation of the navigation message, opening the door to open innovation in multi-constellation receivers.
      Required skills:
      Basic knowledge of digital signal processing and proficiency in C++ programming are essential. Familiarity with the GNU Radio framework or GNSS-SDR is considered a valuable plus.
      Potential mentor(s):
      Miguel Ángel Gómez, Luis Esteve, Javier Arribas.


      ~~~~~~~~~~
          Project Title: Sensor Fusion
      Description:
      Large-sized project (350 h)
      This Google Summer of Code initiative aims to enhance sensor fusion capabilities between GNSS (Global Navigation Satellite System) and other sensors. The goal is to develop a functional GNSS receiver capable of integrating additional sensor data into its architecture, generating RINEX files, and enabling real-time navigation solutions—providing on-the-fly computation of position, velocity, and time. The fusion of GNSS signals with data from new sensors will leverage state-of-the-art AI techniques, such as Bayesian filters (e.g., Kalman filters and particle filters), graph neural networks (GNNs), and transformers for spatiotemporal data modeling. These methods will enhance research on sensor fusion, precise positioning, and urban canyon navigation using real-world signals.
      Integrating additional sensors into GNSS receivers is a key step in advancing next-generation multi-constellation systems. This innovation fosters open research and development while addressing critical challenges such as integrity, reliability, robustness, extended coverage, and high-accuracy positioning.
      Required skills:
      Applicants should possess a fundamental understanding of digital signal processing and demonstrate proficiency in C++ programming. Knowledge of GNSS principles and prior experience with sensor fusion, particularly between GNSS and INS, will be advantageous.
      Potential mentor(s):
      Miguel Ángel Gómez.

      ~~~~~~~~~~
          Project Title: Vector Tracking
      Description:
      Large-sized project (350 h)
      This Google Summer of Code initiative aims to enhance vector tracking capabilities between GNSS (Global Navigation Satellite System). The goal is to develop a functional GNSS-SDR receiver capable of performing Vector Tracking. It is well-known that the use of Vector Tracking Loops (VTL) in GNSS receivers can result in improved tracking performance and sensitivity, faster acquisition, and improved interference robustness. This project leads to a real-time SDR GNSS VTL receiver capable of working with different COTS front-ends. These methods will enhance research on sensor fusion, precise positioning, and urban canyon navigation using real-world signals.
      Required skills:
      Applicants should possess a fundamental understanding of digital signal processing and demonstrate proficiency in C++ programming. Knowledge of GNSS principles and prior experience with sensor fusion, particularly between GNSS and INS, will be advantageous.
      Potential mentor(s):
      Miguel Ángel Gómez.

      ~~~~~~~~~~
          Project Title: Improving the volk-gnsssdr library
      Description:
      Medium project (175 h)
      This project aims to improve volk-gnsssdr, the Vector-Optimized Library of Kernels for GNSS-SDR. This library provides SIMD-optimized implementations of essential signal processing functions (named kernels in this context) for efficient execution on modern processors.
      Objectives: During the summer, the focus will be on:
      Identifying performance-critical kernels that significantly impact GNSS-SDR execution speed.
      Implementing missing SIMD optimizations by adding NEON (for ARM architectures) and RISC-V vector extensions to existing kernels.
      Benchmarking and validating improvements to ensure enhanced performance and correctness across different hardware platforms. No physical access to those hardware platforms is required.
      Expected outcomes: By the end of the project, volk-gnsssdr will have broader SIMD coverage, improving the efficiency of GNSS-SDR on ARM and RISC-V architectures, making it more portable and performant for diverse GNSS applications.
      Required skills:
      Applicants should have a solid understanding of numerical computations and be proficient in C programming.
      Potential mentor(s):
      Carles Fernández-Prades.
         
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnss-sdr/
    idea_list_url: https://gnss-sdr.org/google-summer-code-2025-ideas-list/

  - organization_id: 48
    organization_name: GNU Compiler Collection (GCC)
    no_of_ideas: 9
    ideas_content: |
      
      Rust Front-End. a new compiler front-end for Rust is in development please see: https://github.com/Rust-GCC/gccrs. A number of projects are available, you can choose one of the following. Required skills include C/C++ and finding a way through a large code-base, some knowledge of Rust would of course also be highly beneficial.
      Rewrite Rust lints to operate on our frontend's HIR instead of using GCC's existing infrastructure
      Our frontend has mutliple lint passes (mark-live, scanning for dead code, unused variables...) which are currently implemented by making use of existing GCC middle-end passes. However, this approach poses certain issues for our frontend, and we would like to instead implement these passes using our frontend's internal representation, and more specifically our HIR.
      To facilitate writing these lints, you will first need to introduce a new base HIR visitor class which will allow you to avoid repeating yourself with boilerplate visitor pattern code. Once this is done, you will need to reimplement existing lints to work using this new base visitor class. If time permits, it would be helpful to rewrite other HIR passes to make use of the new visitor framework you will introduce.
      This can be both a medium-sized or large project.
      Complete name resolution pass rewrite
      In order to handle complex imports and exports in the Rust programming language, we have started a rewrite of our name-resolution pass with a new data-structure and a new algorithm for resolving uses. We are planning to get the new algorithm to a working state by Spring 2025, but this rewrite will still be missing some of the features of the old name-resolver. Your goal will be to take care of these features and help us get the name-resolution rewrite to feature-parity with our old algorithm.
      This project should be medium-sized.
      Improving match expressions and pattern matching
      Our frontend is currently lacking support for important Rust patterns such as struct rebinding patterns (destructuring a structure instance's fields and binding them to new names) which prevents us from completing certain compiler milestones, as well as handle existing Rust code within the Rust standard library and Rust-for-Linux project. You will be tasked with improving multiple areas of the compiler in order to improve our handling of match expressions and pattern matching.
      This project can be medium-sized or large.
      
      ~~~~~~~~~~
      
      Fortran – DO CONCURRENT – see GFortranStandards for language links (Fortran standard and what's new documents for 2018 and 202x). Project would be mentored by Tobias Burnus. Required skills include C/C++; some knowledge of Fortran helps, but is not needed. Difficulty medium, size: 175 hours (medium)
      "DO CONCURRENT" is a special way to write loops such that each iteration is independent of another (except for reductions), permitting to run it concurrently.
      Goal is to execute the loops actually in parallel, namely:
      Handling do-concurrent loops in the generic OpenMP code, possibly starting without MASK support and only for those annotated by '!$omp loop'
      Extending it for MASK support / or optimizing the loop count for it.
      Handling parallelization without '!$omp loop' using the command line flag -fdo-concurrent= (like: no parallelization, OpenMP loop, etc.), "parallel" (pthread parallelization similar to (based on?) -ftree-parallelize-loops=n).
      For some experiments and results, see also https://arxiv.org/pdf/2110.10151.pdf or experiments by other compiler vendors (search the internet)
      As of Feb 2025, local/local_init for Fortran loops isn't fully implemented, PR101602, this does not affect the OpenMP implementation but just scalar code, but should eventually be fixed. (Hopefully, it will be fixed by the time this project starts. If not, it could be a first task.)
      
      ~~~~~~~~~~
      Fortran – 2018/202x – Several Fortran 2018 and all Fortran 202x features are unimplemented. See GFortranStandards for language links (Fortran standard and what's new documents for 2018 and 202x).
      Project would be mentored by Tobias Burnus. Required skills include C/C++; some knowledge of Fortran helps, but is not needed.
      The size and difficulty of the project depends on its agreed scope, i.e. it can be both a 175-hour (medium-sized) or a 350 hour (large) project, can be both medium difficulty or hard.
      Effort depends on which new feature(s) are implemented; requires some research about what's missing and about the effort. If interested, please ask via the fortran@ mailing list, https://gcc.gnu.org/lists.html
      For instance, the "Extracting tokens from string", "Interoperability with C", and "Trig functions changes" documented in "what's new in 202x" document would be a medium sized project.
      
      ~~~~~~~~~~

      Fortran – run-time argument checking. – In particular older Fortran code, which does not use modules, but also code which uses implicit-size or explicit-size arrays is prone to argument mismatches. The goal of this item is to add an optional run-time test which works by storing the argument-type/size data before the call in a global variable – and check against it in the callee. (A pointer to the called function is stored alongside to permit calls from uninstrumented code to instrumented code.) This project would/could be mentored by Tobias Burnus. Required skills include C/C++; some knowledge of Fortran helps, but is not needed. Difficulty medium, size: 175 hours (medium).
      
      ~~~~~~~~~~

      Fortran – improved argument compile-time checking – The compiler does check for the arguments in the same file – but it could do better in some cases, i.e. checking better the interface data or updating the expected input better from the use. This project would/could be mentored by Tobias Burnus. Required skills include C/C++; some knowledge of Fortran helps, but is not needed. Difficulty medium, size: 175 hours (medium).
      
      ~~~~~~~~~~

      Enhance OpenACC support. OpenACC is parallel programming model for heterogeneous HPC hardware. GCC currently supports most but not all of OpenACC 2.6. The project idea here is to fill some of the gaps, for example, implement:
      OpenACC acc_memcpy_device runtime API routine
      OpenACC init, shutdown, set directives
      These complement the corresponding acc_init etc. runtime API routines, which are already implemented.
      Make the OpenACC cache directive actually do something
      It's currently only parsed, but we're not actually using it for optimization purposes: prefetch data, move data to low-latency memory.
      OpenACC bind clause
      OpenACC device_type clause
      To work on these items, it's definitely very helpful to have available a GNU/Linux system with an AMD or Nvidia GPU supported by GCC Offloading, but it's not strictly necessary. Mentors: Thomas Schwinge, Tobias Burnus. The size and difficulty of the project depends on the agreed number of items to be implemented, i.e. it can be both a 175-hour (medium-sized) or a 350 hour (large) project, can be both medium difficulty or hard.
      
      Notes on OpenACC init, shutdown, set directives:
      Certain functionality in OpenACC exists both in a directive variant and a runtime API routine variant. For example, OpenACC 2.6 has 2.16.3. "Wait Directive" (directive variant) and 3.2.11. "acc_wait" etc. (runtime API routine variants). In GCC, the front ends map the directive variant to gcc/omp-builtins.def:BUILT_IN_GOACC_WAIT (see git grep --cached BUILT_IN_GOACC_WAIT\\\|c_finish_oacc_wait -- gcc/). This eventually gets translated to a regular function call to libgomp/oacc-async.c:GOACC_wait, which uses the same building blocks as do acc_wait etc., which are also implemented in libgomp/oacc-async.c. (libgomp is the GCC runtime library for OpenMP originally, but then also OpenACC, implementing both the user-level OpenACC "Runtime Library Routines" and the compiler-used GOACC_[...] etc. routines.) Similar for #pragma acc enter data create(var) vs. acc_create, and others. Some users like to use one of directive vs. runtime API routine variants over the other; generally some prefer using the directive variants instead of C/C++ #include <openacc.h> or Fortran use openacc module. Corresponding to the acc_init, acc_shutdown, acc_set_device_num/acc_set_device_type runtime API routine variants implemented in GCC, in OpenACC 2.5, "New init, shutdown, set directives were added", which are not yet implemented in GCC. Implementation of those is assumed to be very much similar as the OpenACC wait directive is via BUILT_IN_GOACC_WAIT, for example, so would enhance the GCC code along these lines, plus proper testsuite coverage.
      
      ~~~~~~~~~~
      Simple file system for use during Nvidia and AMD GPU code generation testing. GCC supports code offloading from a host system to a device: Nvidia and AMD GPUs, via the OpenACC and OpenMP target programming models for heterogeneous HPC hardware. In order to test Nvidia PTX and AMD GCN (etc.) code generation, bare of the OpenACC/OpenMP code offloading infrastructure, we run GCC's make check in configurations where the GPU is treated similar to an embedded device: generate (single-threaded) GPU code, use a run tool to load it to the device and execute it (Nvidia, AMD), and return success status. Other than simple malloc and printf implementations, these device kernels don't have any "access to the outside world". In particular, they have no way to read/write files -- which a number of GCC test cases like to do, which thus currently FAIL to execute. The idea of this GSoC project is either (a) to implement a simple "in-memory" file system (volatile), which is either (a.1) initially empty or (a.2) initially contains files as determined by the test harness (dg-additional-files directives used in test cases), and may grow additional files that the respective test case writes, and then is able to read back, or (b) to implement an RPC mechanism from the device to the host, so that device kernels may access host files. (The latter is implemented by LLVM, for example.) Either variant is to be implemented in the run tools mentioned above, and newlib, which provides a libc for the devices. In order to execute this project, you have to have a system with a GPU that is supported with GCC code offloading (a laptop with Nvidia GPU should work, for example), and some prior experience with low-level GPU or embedded programming is highly desirable, as well as understanding of low-level file access: how are open, write, etc. typically implemented. For variant (a), how to implement a simple "in-memory" file system; for variant (b), how to implement an RPC mechanism between device and host? Performance is not a big concern in this context. Mentor: Thomas Schwinge. The size and difficulty of the project depends on the agreed number of items to be implemented, i.e. it can be both a 175-hour (medium-sized) or a 350 hour (large) project, can be both medium difficulty or hard.
      
      ~~~~~~~~~~
      
      Extend the static analysis pass GCC has gained an experimental static analysis pass which performs some rudimentary checking of malloc/free and the stdio FILE stream API. There is plenty of scope for extending this pass in ways that may interest a contributor, such as
      Add format-string support to -fanalyzer. We currently have two different implementations of warnings for format strings (e.g. printf) in GCC; gcc/c-family/c-format.cc implements -Wformat in the C/C++ frontends, doing type-checking on format strings against their arguments, and gcc/gimple-ssa-sprintf.cc implements parts of -Wformat_overflow=, -Wformat_truncation=, and -Wrestrict in the middle-end). Now that the analyzer has -Wanalyzer-out-of-bounds, it might be good to refactor and generalize this format-string parsing to share more code, and so that the analyzer can reuse it, and do similar range analysis (but with the analyzer's more precise path-sensitive interprocedural approach; see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107017)
      Add a checker for some API or project of interest to the contributor (e.g. the Linux kernel, a POSIX API that we're not yet checking, or something else), either as a plugin, or as part of the analyzer core for e.g. POSIX.
      Extending the analyzer's support for C++. See https://gcc.gnu.org/bugzilla/showdependencytree.cgi?id=97110.
      Extend the plugin to add checking for usage of the CPython API (e.g. reference-counting); see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107646
      This project would be mentored by David Malcolm. Required skills include C/C++ and finding a way through a large code-base. The size of the project depends on its agreed scope, i.e. it can be both a 175-hour (medium-sized) or a 350 hour (large) project but it is probably easier to define a large one. Difficulty also depends on scope but is likely to be hard.
      One more project idea, co-mentored by Thomas Schwinge:
      e. add checks for programs using OpenACC (parallel programming model for heterogeneous HPC hardware), for example: extend the analyzer's existing malloc/free-like checks to understand OpenACC host vs. device memory, and OpenACC runtime API memory allocator and mapper routines (acc_malloc vs. acc_create vs. acc_map_data etc.; see also here). Depending on the number of checks to be implemented, this can also be both a 175-hour (medium-sized) or a 350 hour (large) project. As the suggested checks only concern host-side code, having an AMD or Nvidia GPU supported by GCC Offloading is not necessary for this project.
      
      ~~~~~~~~~~
      
      Tooling for running BPF GCC tests on a live kernel. eBPF is a technology for injecting code into, for example, a running Linux kernel, in a safe way. GCC contains a back end for eBPF code generation. There is a target specific testsuite with many compile-time tests, and a lot of the other compiler testsuites can also be tested. However, the BPF backend doesn't have any execution tests. Other backends rely on a simulator in order to run target GCC tests. However, BPF programs are not like userland programs. They have no main function. Instead every compiled object may feature several entry points, which are designed to be invoked by a kernel. These entry points are of different kinds, and the context provided via a pointer argument to each entry point is different depending on the kind of entry point. For example, a BPF entry point for a kprobe gets passed an argument that points to a description of the probed task. Due to this, it would be very difficult to have a simulator actually emulating a running kernel with all its complexities. We do have a simulator, but it is designed just to emulate simple instructions in combination with GDB. The goal of this project is to:
      Write a test tool (probably within the contrib/ directory in the GCC sources) to launch a suitable kernel in some sort of virtual environment (like qemu) and load and execute a given BPF program object. This is similar to what the kernel BPF sefltests infrastructure does. This also requires to write some glue infrastructure for the BPF tests, in order to communicate the result of execution of tests etc.
      Once we have the test tool, the next step will be to integrate it in the Dejagnu based testing infrastructure in GCC.
      Finally, using the integrated infrastructure, a new testsuite for the BPF target shall be added to the GCC testsuite, containing an initial set of well choosen tests.
      Note that the BPF, by its own nature, will really benefit from having execution tests in the compiler testsuite, because in practice every compiler change (not just in the BPF backend) has the potential of causing the compiler to generate programs that are not verifiable by the kernel BPF verifier. This is thus an important project. Prior knowledge of eBPF specifically is not necessary but useful; some prior knowledge of (virtual) instruction set architectures is desirable. Knowledge on building the kernel and virtualization environments would also be very useful. Mentors: David Faust, Jose E. Marchesi, Thomas Schwinge. The difficulty of this project is medium, and the size is large: 350 hours. However, if only the point 1 and 2 are implemented then it could be medium-sized: 175 hours.
      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-compiler-collection-(gcc)/
    idea_list_url: https://gcc.gnu.org/wiki/SummerOfCode

  - organization_id: 49
    organization_name: GNU Image Manipulation Program
    no_of_ideas: 14
    ideas_content: |
      
      Implement In-Painting Tool
      Category
      User Interface, Core, Tools, GEGL
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, CmykStudent
      Difficulty
      Intermediate
      Outcome
      implementation of the tool in GIMP codebase
      GIMP has had support for in-painting (filling in an area based on the surrounding image) for many years with the third-party Resythesizer plug-in. There have been many requests to implement the feature as a tool directly in core GIMP. In addition to this algorithm, there is also the GEGL operation alpha-inpaint which works similarly.
      Relevant discussions that would assist with implementing this feature can be found here and here.
      Study the Resynthesizer plug-in, gegl:alpha-inpaint operation, and other implementations
      Design a potential implementation and UI
      Improve the implementation of the algorithms as needed
      Implement the tool

      ~~~~~~~~~~
      Image Segmentation Improvements
      Category
      Core, Tools
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, CmykStudent
      Difficulty
      Intermediate
      Outcome
      New and/or improved algorithms for selecting parts of images
      GIMP has many different tools and algorithms for selecting specific sections of an image - from the standard Rectangle and Ellipse Select tools to the more advanced Paint Select and Foreground Select tools. Yet there are always new algorithms and methods for segmenting images.
      Find an existing algorithm or propose a new one
      Implement, optimize and test for real image processing work on a variety of images
      Design how it should be used in GIMP
      An additional mode in an existing tool
      A new tool entirely

      ~~~~~~~~~~
      Improving unit testing
      Category
      Unit testing
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, Jacob
      Difficulty
      Intermediate
      Outcome
      Improved unit testing infrastructure and new unit tests
      Currently GIMP unit testing framework is really outdated, adding new tests is complex and therefore never happens. We should specify and code a proper framework for testing GIMP features.
      This implies automated tests we can run in our Continuous Integration in Gitlab and not interactive tools (though such tools can be interesting too, as additional process, if someone has something nice to propose).
      Port existing tests to the new framework;
      Testing all libgimp functions;
      Testing GEGL operations implemented within GIMP codebase;
      Testing plug-ins (in priority the file import/export ones, but not only);
      Testing core code;
      Testing GUI code if possible;
      Writing down the procedure to add unit tests to make it a mandatory process in future development.

      ~~~~~~~~~~
      Fuzz testing integration
      Category
      Unit testing, Security
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, Jacob
      Difficulty
      Intermediate
      In addition to unit testing, we would also like to build a robust automated fuzz testing suite. Integrating a fuzzer would help us better detect when new code could lead to a security vulnerability or incorrect behavior in GIMP. This project would cover many aspects of GIMP, from core code to plug-ins to public API.
      Study GIMP and determine what areas to cover in initial implementation
      Review fuzzing techniques and tools
      Design a test suite and process
      Implement fuzz testing suite

      ~~~~~~~~~~
      Implement sandboxing for plug-ins
      Category
      Security, Plug-ins
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, Jacob
      Difficulty
      Complicated
      Many features of GIMP such as image import and export are implemented with separate plug-ins. For this project, we would like to run them in a sandbox environment for safety and security.
      This is a complex project, and requires knowledge of both GIMP’s architecture as well as extensive research into security. Mentors would be learning alongside the student, so any interested individual would need to be able to work well independently. Please contact us to discuss your proposal for this project.

      ~~~~~~~~~~
      Improving the text tool
      Category
      GEGL, color science
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Liam Quin
      Difficulty
      Intermediate
      Outcome
      Improvement of the text tool
      This is a project following up a few previous GSoC projects, which deserves further work as this is a complicated topic.
      Our text tool is a bit of a UI and UX mess and deserves a proper rewrite/enhancement project:
      Re-specify text editing and formating as well as the tool option, for existing features, but also adding new features for modern text editing (see also this draft);
      Add OpenType support.
      Continue previous years experiments on a new text layout library.

      ~~~~~~~~~~
      Implement GEGL operations for GIMP
      Category
      GEGL, image processing
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, Øyvind
      Difficulty
      Intermediate
      Outcome
      implementation or improvements of GEGL operations in GIMP or GEGL codebase
      The migration of GIMP to use GEGL has accelerated - for some GIMP functionality the main hurdle to migrate is having GEGL ops that can act as drop in replacement for the core processing functionality (some ops would be desired directly in GIMP others could likely go directly into GEGL).
      For most code involved, porting to GEGL involves understanding what the current code does; and port or reimplement it as a floating point processing operation (floating point math often ends up shorter and more readable than the 8bit equivalents.
      There are also some filters which were ported to GEGL, but some people prefer the old one (e.g. the Sharpen filter). It would be worth investigating the difference and either implement the old one or improve the new one.
      Talk to us for specifics on which operations would be a good project`.

      ~~~~~~~~~~
      Implement OpenColorIO Color Management
      Category
      User Interface, Core, Tools, GEGL
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      drc, CmykStudent
      Difficulty
      Intermediate
      Outcome
      implementation of the OCIO color management system in GIMP codebase
      GIMP uses industry standard ICC Color profiles to allow users to match and maintain colors for image editing and printing. The film industry utilizes a separate standard, OpenColorIO, which focuses more on manipulating colors in a space rather than trying to keep them consistent across multiple devices.
      This project would involve adding support for OCIO color management in addition to the existing system. This addition would improve user workflows for motion picture and animation work, as well as improve compatibility with other OCIO-supporting software like Blender and Krita. The project would involve the following:
      Research OCIO and color management systems in general
      The Krita manual has an excellent overview of the subject
      Design and test a user interface
      Implement the code
      
      ~~~~~~~~~~
      Implement GEGL Filter Browser
      Category
      GEGL, User Interface
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan
      Difficulty
      Intermediate
      Outcome
      implementation of the feature in GIMP codebase
      During the 3.0 development process, a new filter API was created that allows script and plug-in developers to apply any valid GEGL filter they have installed. However, GIMP doesn’t not currently have a built-in browser to easily find the name, descriptions, and properties of these filters.
      For this project, you would design and develop a GEGL Browser, similar to the existing Plug-in and Procedure Browsers. This will involve both code development and UX/UI Design.
      
      ~~~~~~~~~~
      
      Improve Non-Destructive Editing
      Category
      GEGL, User Interface
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, CmykStudent
      Difficulty
      Intermediate
      Outcome
      implementation of the feature in GIMP codebase
      As of version 3.0, GIMP now has initial support for non-destructive editing with layer effects. Yet there is much more work to be done. Our roadmap provides some ideas for the next areas to improve, or you can propose your own:
      Studying the current implementation
      Design improvements to UI or functionality
      Implement the improvements
      
      ~~~~~~~~~~
      Improving off-canvas editing
      Category
      User Interface, Core
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan
      Difficulty
      Intermediate
      Outcome
      implementation of the feature in GIMP codebase
      GIMP recently got the ability to view the image out of the canvas. This is still incomplete. Among the many possible improvements:
      Being able to select off-canvas.
      Being able to see off-canvas but with an effect (e.g. dimming).
      Having various tools and features working differently when “Show All” is enabled.
      
      ~~~~~~~~~~
      Improve Metadata Editor and Viewer
      Category
      Metadata, Plug-in, User Interface
      Project size (GSoC)
      Large (350 hours)
      Skills
      C
      Possible mentors
      Jehan, Jacob
      Difficulty
      Intermediate
      Outcome
      Improved metadata UI and codebase
      Our image metadata viewer and editor could use some code review and improvements. It currently supports only a subset of all valid metadata, and the UI could be improved to allow easier editing and viewing of metadata.
      Additionally, some image formats such as HEIC, FITs, and DICOM have custom metadata. Another aspect of this project might be considering how to handle these in a way that is easily extensible and maintainable.
      For further inspiration, you can review open issues tagged with the Metadata label in our tracker.
      Review metadata related issues and develop a plan
      Design user interface and code structure
      Implement planned changes in the metadata plug-in
      
      ~~~~~~~~~~
      Extension website
      Category
      Web
      Project size (GSoC)
      Large (350 hours)
      Skills
      Python, HTML, Javascript and other web technologies
      Possible mentors
      Jehan
      Difficulty
      Intermediate
      Outcome
      New website and build scripts for continuous integration
      We would want a website for our future extension platform, with very specific criteria. Apart from some necessary dynamic parts, we want a website as static as possible, with generated pages when possible. GIMP is a software project, which relies on community. We don’t want to spend all our time having to maintain and manage a website with a lot of moving parts. So we need simplicity first, security first, with just the right amount of dynamicity. The static website framework which we seem to want to go with the most in our project right now is Hugo.
      Even though it has a “web” component, this project is also about building a proper backend, which includes processing XML metadata (AppStream) in a secure way (considering third-party received data as unsafe and possibly malicious), generic static web and repository data from these repositories, and more.
      See this document for an early overview of what we are looking for.
      
      ~~~~~~~~~~
      Convert gimp-help build system from autotools to meson
      Category
      gimp-help, build system
      Project size (GSoC)
      Large (350 hours)
      Skills
      Python, some understanding of autotools and Makefiles
      Possible mentors
      Jacob
      Difficulty
      Intermediate
      Outcome
      The gimp-help repository can be built using meson
      More and more projects move from autotools to meson for building. GIMP itself already uses meson as main build system. We would want all targets in our autotools build converted to meson. The main ones being able to build html for all languages, validate the xml files, create the distribution packages, making pdf quickreference guides, etc.
      Some guidelines for porting from autotools to meson can be found here, and more Gnome specific here.
      The build system needs to function on Linux, Windows and Mac. This would need to be integrated in our CI builds. There is an almost 2 year old repository where some work was done, but we ran into some problems due to the more strict directory and other requirements of meson. However, there have been improvements to meson which may make it easier now.
      It is possible that you may have to write a meson module like the gnome one, which has functions for handling yelp, gtkdoc, etc. Don’t forget to check out the i18n module, you may be able to use that too.
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-image-manipulation-program/
    idea_list_url: https://developer.gimp.org/core/internship/ideas/


  - organization_id: 50
    organization_name: GNU Octave
    no_of_ideas: 3
    ideas_content: |
      
      
      Adding clustering *Searcher classes in statistics package
      Although the statistics package already has knnsearch and rangesearch functions, it misses classdefs for extending their functionality. Furthermore, the KDTree method in the aforementioned functions is currently disabled, because it is very slow and poorly implemented (see GitHub issue #151). The goal of this project if to implement KDTreeSearcher, ExhaustiveSearcher, and hnswSearcher classes (including their knnsearch and rangesearch methods) along with the createns helper function. Beyond MATLAB compatibility, the KDTree implementation should ideally utilize a compiled oct library for faster construction and queries of points.
      Project size [?] and Difficulty
      ~350 hours (hard)
      Required skills
      Octave, classdef, C++, good knowledge of clustering methods
      Potential mentors
      Andreas Bertsatos

      ~~~~~~~~~~



      Custom re-implementation of the texi2html (v.1.82) command line tool
      Implement a compiled .oct function to relax the dependency of the pkg-octave-doc package on texi2html (v.1.82) command line tool, which is no longer maintained or further developed but also not readily available to all linux distributions. The idea is to have a `texi2html` function within the pkg-octave-doc package that will replace the functionality of the texi2html (v.1.82) command line tool. This will also help improve the speed of pkg-octave-doc processing large packages, which contain specific tags (such as @math) which are currently handled within Octave code.
      Project size [?] and Difficulty
      ~350 hours (hard)
      Required skills
      Perl, C++, Octave, Texinfo, HTML
      Potential mentors
      Andreas Bertsatos

      ~~~~~~~~~~
      
      Port Chebfun to Octave and improve classdef support
      Chebfun uses interpolation to approximate functions to very high accuracy, giving numerical computing that feels like symbolic computing. The software is implemented as collection of "classdef" classes and is Free and Open Source Software. However, Chebfun does not yet work with Octave, largely due to differences and issues with Octave's classdef implementation. This project has two aims: (1) make changes to the Chebfun code to make it work on Octave and (2) improve Octave's classdef functionality. Some initial steps toward to first goal can be found on this octave_dev branch. The second goal will likely involve a collaborative effort because classdef is a priority on | Octave's Development Roadmap and because other proposed projects also involve classdef.
      Project size [?] and Difficulty
      ~350 hours (hard)
      Required skills
      Octave, object-oriented programming, polynomial interpolation and approximation theory, C++.
      Potential mentors
      Colin B. Macdonald

    
      
      
      
      
      
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-octave/
    idea_list_url: https://wiki.octave.org/Summer_of_Code_-_Getting_Started#Suggested_projects
  

  - organization_id: 51
    organization_name: GNU Radio
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gnu-radio/
    idea_list_url: https://wiki.gnuradio.org/index.php?title=GSoCIdeas

  - organization_id: 52
    organization_name: GRAME
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/grame/
    idea_list_url: https://github.com/grame-cncm/faustideas

  - organization_id: 53
    organization_name: GeomScale
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/geomscale/
    idea_list_url: https://github.com/GeomScale/gsoc25/wiki/table-of-proposed-coding-projects

  - organization_id: 54
    organization_name: Git
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/git/
    idea_list_url: https://git.github.io/SoC-2025-Ideas/

  - organization_id: 55
    organization_name: GitLab
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gitlab/
    idea_list_url: https://gitlab.com/gitlab-org/developer-relations/contributor-success/google-summer-of-code-2025/-/issues


  - organization_id: 56
    organization_name: Google DeepMind
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/google-deepmind/
    idea_list_url: https://goo.gle/deepmind-gsoc-projects-2025


  - organization_id: 57
    organization_name: Graphite
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/graphite/
    idea_list_url: https://graphite.rs/volunteer/guide/student-projects/

  - organization_id: 58
    organization_name: Haskell.org
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/haskell.org/
    idea_list_url: https://summer.haskell.org/ideas.html

  - organization_id: 59
    organization_name: HumanAI
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/humanai/
    idea_list_url: https://humanai.foundation/


  - organization_id: 60
    organization_name: INCF
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/incf/
    idea_list_url: https://docs.google.com/document/d/1T7U4nYFbVbZuIUw_2bB89YASj-ra26hRNtG2a20Gjlw/edit?tab=t.0
  

  - organization_id: 61
    organization_name: IOOS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ioos/
    idea_list_url: https://github.com/ioos/gsoc/blob/main/2025/ideas-list.md

  - organization_id: 62
    organization_name: Inkscape
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/inkscape/
    idea_list_url: https://gitlab.com/inkscape/inkscape/-/blob/master/doc/gsoc/summerofcode.md

  - organization_id: 63
    organization_name: International Catrobat Association
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/international-catrobat-association/
    idea_list_url: https://developer.catrobat.org/pages/development/google-summer-of-code/2025/

  - organization_id: 64
    organization_name: Internet Archive
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/internet-archive/
    idea_list_url: https://docs.google.com/document/d/1oHNwPNYmHV5q3puBfv6IQFs-4gTe9XLN2iz2Lgse-1k/edit?tab=t.0

  - organization_id: 65
    organization_name: Internet Health Report
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/internet-health-report/
    idea_list_url: https://github.com/InternetHealthReport/gsoc/blob/main/ideas.md


  - organization_id: 66
    organization_name: Invesalius
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/invesalius/
    idea_list_url: https://github.com/invesalius/gsoc/blob/main/gsoc_2025_ideas.md


  - organization_id: 67
    organization_name: JAX and Keras
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jax-and-keras/
    idea_list_url: https://docs.google.com/document/d/16uAZEldrXUBHjrszSO22Hr81OmVSjzVXza0szZl6Fxw/edit?usp=sharing

  - organization_id: 68
    organization_name: JSON Schema
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/json-schema/
    idea_list_url: https://github.com/json-schema-org/community/blob/main/programs/mentoring/gsoc/gsoc-2025.md


  - organization_id: 69
    organization_name: JabRef e.V.
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jabref-e.v./
    idea_list_url: https://github.com/JabRef/jabref/wiki/GSoC-2025-ideas-list


  - organization_id: 70
    organization_name: JdeRobot
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jderobot/
    idea_list_url: https://jderobot.github.io/activities/gsoc/2025#ideas-list
  

  - organization_id: 71
    organization_name: Jenkins
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jenkins/
    idea_list_url: https://www.jenkins.io/projects/gsoc/2025/project-ideas/

  - organization_id: 72
    organization_name: Jitsi
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/jitsi/
    idea_list_url: https://github.com/jitsi/gsoc-ideas/blob/master/2025/README.md

  - organization_id: 73
    organization_name: Joomla!
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/joomla!/
    idea_list_url: https://docs.joomla.org/GSoC_2025_Project_Ideas

  - organization_id: 74
    organization_name: KDE Community
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kde-community/
    idea_list_url: https://community.kde.org/GSoC/2025/Ideas

  - organization_id: 75
    organization_name: Keploy
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/keploy/
    idea_list_url: https://github.com/keploy/gsoc/tree/main/2025


  - organization_id: 76
    organization_name: Kiwix
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kiwix/
    idea_list_url: https://kiwix.org/en/google-summer-of-code/


  - organization_id: 77
    organization_name: Kornia
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kornia/
    idea_list_url: https://github.com/kornia/kornia-rs/wiki/Google-Summer-of-Code-Ideas-2025

  - organization_id: 78
    organization_name: Kotlin Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kotlin-foundation/
    idea_list_url: https://kotlinlang.org/docs/gsoc-2025.html


  - organization_id: 79
    organization_name: KubeVirt
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kubevirt/
    idea_list_url: https://github.com/kubevirt/community/wiki/Google-Summer-of-Code-2025


  - organization_id: 80
    organization_name: Kubeflow
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kubeflow/
    idea_list_url: https://www.kubeflow.org/events/gsoc-2025/
  

  - organization_id: 81
    organization_name: LLVM Compiler Infrastructure
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/llvm-compiler-infrastructure/
    idea_list_url: https://llvm.org/OpenProjects.html

  - organization_id: 82
    organization_name: LabLua
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/lablua/
    idea_list_url: https://github.com/labluapucrio/gsoc

  - organization_id: 83
    organization_name: Learning Equality
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/learning-equality/
    idea_list_url: https://docs.google.com/document/d/e/2PACX-1vT49WKgbsAIUoJ0jJS8LTWWm7UTByM0Pw3qt0KgyU_BShB1oGtZBkrEWuannFsRMVvGd0QBytZt8blh/pub

  - organization_id: 84
    organization_name: LibreOffice
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/libreoffice/
    idea_list_url: https://wiki.documentfoundation.org/Development/GSoC/Ideas

  - organization_id: 85
    organization_name: Liquid Galaxy project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/liquid-galaxy-project/
    idea_list_url: https://www.liquidgalaxy.eu/2024/11/unique-post-for-gsoc-2025-at-liquid.html


  - organization_id: 86
    organization_name: MBDyn
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mbdyn/
    idea_list_url: https://public.gitlab.polimi.it/DAER/mbdyn/-/wikis/GSoc-Project-Ideas


  - organization_id: 87
    organization_name: MDAnalysis
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mdanalysis/
    idea_list_url: https://github.com/MDAnalysis/mdanalysis/wiki/GSoC-2025-Project-Ideas

  - organization_id: 88
    organization_name: MIT App Inventor
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mit-app-inventor/
    idea_list_url: https://github.com/mit-cml/appinventor-sources/wiki/Google-Summer-of-Code-2025

  - organization_id: 89
    organization_name: Machine Learning for Science (ML4SCI)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/machine-learning-for-science-(ml4sci)/
    idea_list_url: https://ml4sci.org/


  - organization_id: 90
    organization_name: MariaDB
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mariadb/
    idea_list_url: https://mariadb.com/kb/en/google-summer-of-code-2025/
  

  - organization_id: 91
    organization_name: Meshery
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/meshery/
    idea_list_url: https://meshery.io/programs/gsoc/2025

  - organization_id: 92
    organization_name: MetaBrainz Foundation Inc
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/metabrainz-foundation-inc/
    idea_list_url: https://wiki.musicbrainz.org/Development/Summer_of_Code/2025

  - organization_id: 93
    organization_name: Micro Electronics Research Lab - UITU
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/micro-electronics-research-lab-uitu/
    idea_list_url: https://github.com/merledu/Google-Summer-of-Code/wiki/GSoC'25-Project-Ideas-List

  - organization_id: 94
    organization_name: Mixxx
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/mixxx/
    idea_list_url: https://github.com/mixxxdj/mixxx/wiki/GSOC-2025-Ideas

  - organization_id: 95
    organization_name: National Resource for Network Biology (NRNB)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/national-resource-for-network-biology-(nrnb)/
    idea_list_url: https://github.com/nrnb/GoogleSummerOfCode/issues


  - organization_id: 96
    organization_name: Neovim
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/neovim/
    idea_list_url: https://github.com/neovim/neovim/wiki/Google-Summer-of-Code#gsoc-ideas-2025


  - organization_id: 97
    organization_name: Neuroinformatics Unit
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/neuroinformatics-unit/
    idea_list_url: https://neuroinformatics.dev/get-involved/gsoc

  - organization_id: 98
    organization_name: NumFOCUS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/numfocus/
    idea_list_url: https://github.com/numfocus/gsoc/blob/master/2025/ideas-list.md


  - organization_id: 99
    organization_name: OSGeo (Open Source Geospatial Foundation)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/osgeo-(open-source-geospatial-foundation)/
    idea_list_url: https://wiki.osgeo.org/wiki/Google_Summer_of_Code_2025_Ideas


  - organization_id: 100
    organization_name: OWASP Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/owasp-foundation/
    idea_list_url: https://owasp.org/www-community/initiatives/gsoc/gsoc2025ideas
  

  - organization_id: 101
    organization_name: Open Climate Fix
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-climate-fix/
    idea_list_url: https://docs.google.com/document/d/1nh6pdIjCTSLWwgFhgIV1gpG8CbIqGyoCFDHOfWmKdwA/edit?usp=sharing

  - organization_id: 102
    organization_name: Open Food Facts
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-food-facts/
    idea_list_url: https://wiki.openfoodfacts.org/GSOC/2025_ideas_list

  - organization_id: 103
    organization_name: Open HealthCare Network
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-healthcare-network/
    idea_list_url: https://contributors.ohc.network/projects

  - organization_id: 104
    organization_name: Open Robotics
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-robotics/
    idea_list_url: https://github.com/osrf/osrf_wiki/wiki/GSoC25

  - organization_id: 105
    organization_name: Open Science Initiative for Perfusion Imaging
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-science-initiative-for-perfusion-imaging/
    idea_list_url: https://docs.google.com/document/d/e/2PACX-1vSuYh57hsLUXbmrA5tozX4Ucne0sRXnmFt5xBA88gzDZJKZYD4-Bq04J9acer2d_i6NP6xhimmz4m5i/pub


  - organization_id: 106
    organization_name: Open Science Labs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-science-labs/
    idea_list_url: https://opensciencelabs.org/opportunities/gsoc/project-ideas/


  - organization_id: 107
    organization_name: Open Technologies Alliance - GFOSS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-technologies-alliance-gfoss/
    idea_list_url: https://ellak.gr/wiki/index.php?title=Google_Summer_of_Code_2025_proposed_ideas

  - organization_id: 108
    organization_name: Open Transit Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/open-transit-software-foundation/
    idea_list_url: https://opentransitsoftwarefoundation.org/2025/01/google-summer-of-code-2025-project-ideas/


  - organization_id: 109
    organization_name: OpenAFS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openafs/
    idea_list_url: https://www.openafs.org/gsoc/project-ideas.html


  - organization_id: 110
    organization_name: OpenAstronomy
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openastronomy/
    idea_list_url: https://openastronomy.org/gsoc/gsoc2025/#/projects
  

  - organization_id: 111
    organization_name: OpenCV
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/opencv/
    idea_list_url: https://github.com/opencv/opencv/wiki/GSoC_2025

  - organization_id: 112
    organization_name: OpenELIS Global
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openelis-global/
    idea_list_url: https://uwdigi.atlassian.net/wiki/spaces/OG/pages/321224705/GSoC+2025

  - organization_id: 113
    organization_name: OpenMRS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openmrs/
    idea_list_url: https://openmrs.atlassian.net/wiki/spaces/RES/pages/322404353/Summer+of+Code+2025

  - organization_id: 114
    organization_name: OpenMS
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openms/
    idea_list_url: https://www.openms.org/news/gsoc2025/

  - organization_id: 115
    organization_name: OpenStreetMap
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openstreetmap/
    idea_list_url: https://wiki.openstreetmap.org/wiki/Google_Summer_of_Code/2025/Project_ideas


  - organization_id: 116
    organization_name: OpenVINO Toolkit
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openvino-toolkit/
    idea_list_url: https://github.com/openvinotoolkit/openvino/wiki/Google-Summer-Of-Code#project-ideas-for-2025


  - organization_id: 117
    organization_name: OpenWISP
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/openwisp/
    idea_list_url: https://openwisp.io/docs/dev/developer/gsoc-ideas-2025.html

  - organization_id: 118
    organization_name: Oppia Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/oppia-foundation/
    idea_list_url: https://github.com/oppia/oppia/wiki/Google-Summer-of-Code-2025#oppias-project-ideas-list


  - organization_id: 119
    organization_name: Organic Maps
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/organic-maps/
    idea_list_url: https://github.com/organicmaps/organicmaps/wiki/GSoC-2025-ideas


  - organization_id: 120
    organization_name: PAL Robotics
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/pal-robotics/
    idea_list_url: https://pal-robotics.com/2025-google-summer-code-proposals/#tips-successful-gsoc
  

  - organization_id: 121
    organization_name: PEcAn Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/pecan-project/
    idea_list_url: https://pecanproject.github.io/gsoc_ideas

  - organization_id: 122
    organization_name: Pharo Consortium
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/pharo-consortium/
    idea_list_url: https://gsoc.pharo.org/ideas

  - organization_id: 123
    organization_name: Plone Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/plone-foundation/
    idea_list_url: https://plone.org/community/gsoc/2025

  - organization_id: 124
    organization_name: PostgreSQL
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/postgresql/
    idea_list_url: https://wiki.postgresql.org/wiki/GSoC_2025

  - organization_id: 125
    organization_name: Processing Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/processing-foundation/
    idea_list_url: https://github.com/processing/Processing-Foundation-GSoC/wiki/Project-Ideas-List-(GSoC-2025)


  - organization_id: 126
    organization_name: Project Mesa
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/project-mesa/
    idea_list_url: https://github.com/projectmesa/mesa/wiki/Google-Summer-of-Code-2025


  - organization_id: 127
    organization_name: Prometheus-Operator
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/prometheus-operator/
    idea_list_url: https://github.com/prometheus-operator/community/blob/main/mentoring/gsoc/2025/project_ideas.md

  - organization_id: 128
    organization_name: Python Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/python-software-foundation/
    idea_list_url: https://python-gsoc.org/ideas.html

  

  - organization_id: 129
    organization_name: QC-Devs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/qc-devs/
    idea_list_url: https://qcdevs.org/join/qcdevs_gsoc/


  - organization_id: 130
    organization_name: QEMU
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/qemu/
    idea_list_url: https://wiki.qemu.org/Google_Summer_of_Code_2025
  

  - organization_id: 131
    organization_name: R project for statistical computing
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/r-project-for-statistical-computing/
    idea_list_url: https://github.com/rstats-gsoc/gsoc2025/wiki/table-of-proposed-coding-projects

  - organization_id: 132
    organization_name: RTEMS Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rtems-project/
    idea_list_url: https://projects.rtems.org/gsoc/

  - organization_id: 133
    organization_name: Rizin
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rizin/
    idea_list_url: https://rizin.re/gsoc/2025/

  - organization_id: 134
    organization_name: Rspamd
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rspamd/
    idea_list_url: https://rspamd.com/gsoc2025_ideas.html

  - organization_id: 135
    organization_name: SQLancer
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sqlancer/
    idea_list_url: https://github.com/sqlancer/sqlancer/wiki/GSoC-2025-Ideas


  - organization_id: 136
    organization_name: SW360
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sw360/
    idea_list_url: https://eclipse.dev/sw360/gsoc/gsoc-projects-2025/


  - organization_id: 137
    organization_name: SageMath
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sagemath/
    idea_list_url: https://wiki.sagemath.org/GSoC/2025

  - organization_id: 138
    organization_name: Scala Center
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/scala-center/
    idea_list_url: https://github.com/scalacenter/GoogleSummerOfCode

  - organization_id: 139
    organization_name: ScummVM
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/scummvm/
    idea_list_url: https://www.scummvm.org/gsoc-2025-ideas


  - organization_id: 140
    organization_name: Society for Arts and Technology (SAT)
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/society-for-arts-and-technology-(sat)/
    idea_list_url: https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/categories/ideas/
  

  - organization_id: 141
    organization_name: Software and Computational Systems Lab at LMU Munich
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/software-and-computational-systems-lab-at-lmu-munich/
    idea_list_url: https://www.sosy-lab.org/gsoc/gsoc2025.php

  - organization_id: 142
    organization_name: St. Jude Children's Research Hospital
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/st.-jude-children's-research-hospital/
    idea_list_url: https://docs.google.com/document/d/1ze8OpsltCbAkksjmhd3hCeS6VDqAaolAhWAQQRJZBpc/edit?tab=t.0

  - organization_id: 143
    organization_name: Ste||ar group
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/steororar-group/
    idea_list_url: https://github.com/STEllAR-GROUP/hpx/wiki/Google-Summer-of-Code-%28GSoC%29-2025#2025-hpx-project-ideas

  - organization_id: 144
    organization_name: Stichting SU2
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/stichting-su2/
    idea_list_url: https://www.cfd-online.com/Forums/su2-news-announcements/259420-gsoc-2025-projects.html

  - organization_id: 145
    organization_name: Sugar Labs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sugar-labs/
    idea_list_url: https://github.com/sugarlabs/GSoC/blob/master/Ideas-2025.md


  - organization_id: 146
    organization_name: Swift
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/swift/
    idea_list_url: https://www.swift.org/gsoc2025/


  - organization_id: 147
    organization_name: SymPy
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/sympy/
    idea_list_url: https://github.com/sympy/sympy/wiki/GSoC-Ideas

  - organization_id: 148
    organization_name: Synfig
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/synfig/
    idea_list_url: https://github.com/synfig/synfig-docs-dev/blob/master/docs/gsoc/2025/ideas.rst

  - organization_id: 149
    organization_name: TARDIS RT Collaboration
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/tardis-rt-collaboration/
    idea_list_url: https://tardis-sn.github.io/summer_of_code/ideas/


  - organization_id: 150
    organization_name: The Apache Software Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-apache-software-foundation/
    idea_list_url: https://s.apache.org/gsoc2025ideas
  

  - organization_id: 151
    organization_name: The FreeBSD Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-freebsd-project/
    idea_list_url: https://wiki.freebsd.org/SummerOfCodeIdeas

  - organization_id: 152
    organization_name: The Honeynet Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-honeynet-project/
    idea_list_url: https://www.honeynet.org/gsoc/gsoc-2025/google-summer-of-code-2025-project-ideas/

  - organization_id: 153
    organization_name: The JPF team
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-jpf-team/
    idea_list_url: https://github.com/javapathfinder/jpf-core/wiki/GSoC-2025-Project-Ideas

  - organization_id: 154
    organization_name: The Julia Language
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-julia-language/
    idea_list_url: https://julialang.org/jsoc/projects/

  - organization_id: 155
    organization_name: The Linux Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-linux-foundation/
    idea_list_url: https://wiki.linuxfoundation.org/gsoc/google-summer-code-2025


  - organization_id: 156
    organization_name: The Mifos Initiative
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-mifos-initiative/
    idea_list_url: https://mifosforge.jira.com/wiki/spaces/RES/pages/4271669249/Google+Summer+of+Code+2025+Ideas


  - organization_id: 157
    organization_name: The NetBSD Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-netbsd-foundation/
    idea_list_url: https://wiki.netbsd.org/projects/gsoc/

  - organization_id: 158
    organization_name: The P4 Language Consortium
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-p4-language-consortium/
    idea_list_url: https://github.com/p4lang/gsoc/blob/main/2025/ideas_list.md

  - organization_id: 159
    organization_name: The Palisadoes Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-palisadoes-foundation/
    idea_list_url: https://developer.palisadoes.org/docs/internships/gsoc/gsoc-ideas


  - organization_id: 160
    organization_name: The Rust Foundation
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-rust-foundation/
    idea_list_url: https://github.com/rust-lang/google-summer-of-code
  

  - organization_id: 161
    organization_name: The Tor Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-tor-project/
    idea_list_url: https://gitlab.torproject.org/tpo/team/-/wikis/GSoC

  - organization_id: 162
    organization_name: The ns-3 Network Simulator Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/the-ns-3-network-simulator-project/
    idea_list_url: https://www.nsnam.org/wiki/GSOC2025Projects

  - organization_id: 163
    organization_name: Typelevel
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/typelevel/
    idea_list_url: https://typelevel.org/gsoc/ideas

  - organization_id: 164
    organization_name: UC OSPO
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/uc-ospo/
    idea_list_url: https://ucsc-ospo.github.io/osre25/#projects

  - organization_id: 165
    organization_name: Unicode, Inc.
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/unicode-inc./
    idea_list_url: https://docs.google.com/document/u/2/d/e/2PACX-1vQbj0-VFkRjYdnivuPPXuHM3IW4LuHxK6E0LVO3O8ZU_-k8CYH_eFMZ_IwFg_r-oBw3FCEOmHCb5jrn/pub


  - organization_id: 166
    organization_name: Unikraft
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/unikraft/
    idea_list_url: https://github.com/unikraft/gsoc/blob/staging/gsoc-2025/ideas.md


  - organization_id: 167
    organization_name: Uramaki LAB
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/uramaki-lab/
    idea_list_url: https://github.com/ruxailab/gsoc/blob/main/ideas2025.md

  - organization_id: 168
    organization_name: VideoLAN
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/videolan/
    idea_list_url: https://wiki.videolan.org/SoC_2025/


  - organization_id: 169
    organization_name: Wagtail
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/wagtail/
    idea_list_url: https://github.com/wagtail/gsoc/blob/main/project-ideas.md


  - organization_id: 170
    organization_name: Waycrate
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/waycrate/
    idea_list_url: https://waycrate.github.io/outreach/gsoc/2025/idea-list/
  

  - organization_id: 171
    organization_name: Wellcome Sanger Tree of Life
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/wellcome-sanger-tree-of-life/
    idea_list_url: https://docs.google.com/document/d/1dRHwnHAXlwQbRDstnd0OZTDJYA9cTsl1jG_hOempqII/edit?usp=sharing

  - organization_id: 172
    organization_name: Zendalona
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/zendalona/
    idea_list_url: https://docs.google.com/document/d/1HshVGE-oQq_xx09zG2LfqfVYWRhyrASteXzabCV9eIQ/edit?usp=sharing

  - organization_id: 173
    organization_name: Zulip
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/zulip/
    idea_list_url: https://zulip.readthedocs.io/en/latest/outreach/gsoc.html

  - organization_id: 174
    organization_name: cBioPortal for Cancer Genomics
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cbioportal-for-cancer-genomics/
    idea_list_url: https://github.com/cBioPortal/gsoc/issues?q=is%3Aissue%20state%3Aopen%20label%3AGSoC-2025

  - organization_id: 175
    organization_name: checkstyle
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/checkstyle/
    idea_list_url: https://github.com/checkstyle/checkstyle/wiki/Checkstyle-GSoC-2025-Project-Ideas


  - organization_id: 176
    organization_name: dora-rs
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/dora-rs/
    idea_list_url: https://github.com/dora-rs/dora/wiki/GSoC_2025


  - organization_id: 177
    organization_name: freifunk
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/freifunk/
    idea_list_url: https://projects.freifunk.net/

  - organization_id: 178
    organization_name: gprMax
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/gprmax/
    idea_list_url: https://github.com/gprMax/GSoC/blob/main/project-ideas-2025.md


  - organization_id: 179
    organization_name: kro | Kube Resource Orchestrator
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/kro-or-kube-resource-orchestrator/
    idea_list_url: https://github.com/kro-run/kro/issues/288


  - organization_id: 180
    organization_name: libssh
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/libssh/
    idea_list_url: https://www.libssh.org/development/google-summer-of-code/
  

  - organization_id: 181
    organization_name: omegaUp
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/omegaup/
    idea_list_url: https://github.com/omegaup/omegaup/wiki/Google-Summer-of-Code-2025

  - organization_id: 182
    organization_name: openSUSE Project
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/opensuse-project/
    idea_list_url: https://github.com/openSUSE/mentoring/issues

  - organization_id: 183
    organization_name: rocket.chat
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/rocket.chat/
    idea_list_url: https://github.com/RocketChat/google-summer-of-code/blob/main/google-summer-of-code-2025.md#-project-ideas

  - organization_id: 184
    organization_name: stdlib
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/stdlib/
    idea_list_url: https://github.com/stdlib-js/google-summer-of-code/blob/main/ideas.md

  - organization_id: 185
    organization_name: webpack
    no_of_ideas:
    ideas_content: >-

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/webpack/
    idea_list_url: https://docs.google.com/document/d/1JOtAdpoqHGieg_nJpjkWDv1BoErPQ9L1GnZmX55E-W0/edit?usp=sharing


  




  


  

 