organizations:
  - organization_id: 1
    organization_name: 52°North Spatial Information Research GmbH
    no_of_ideas: 3
    ideas_content: >-
                1. KomMonitor
                Angular migration of the KomMonitor Web Client
                Explanation
                KomMonitor is a web based tool that combines methods of GIS (Geographic information System) and statistical data and helps in providing a simpler and easier way to monitor geo-spatial data. Many municipalities have established KomMonitor to address a wide range of challenges in fields such as urban planning, environmental management, and disaster response. The current version of the KomMonitor Web Client has been developed using AngularJS, which has served as a reliable foundation for its functionalities. However, AngularJS has been deprecated for some years now. Therefore, relying on the current code base has several potential drawbacks associated with using AngularJS, such as compatibility issues, limited community support, reduced performance, and version support. To overcome these challenges and take KomMonitor to the next level, it is necessary to adopt the KomMonitor Web Client to the more modern and widely-supported framework Angular. As part of GSoC 2023, essential work has been done by developing a general approach for the Angular migration. The Web Client has been restructured so that it can be deployed as a hybrid web application, which runs both legacy AngularJS components and migrated or new Angular components. This year, the project aims to continue the migration tasks. Hence, the goal of this project is to reimplement several selected components of the KomMonitor Web Client by using the Angular framework.

                Expected Results
                As a result of the project, it is expected that several selected components of the KomMonitor Web Client will have been reimplemented with the Angular framework. The resulting UI of the reimplemented components should be as close as possible to the previous design to preserve the current look&feel. As an additional requirement, the reimplementation should take into account best practices and common design patterns in Angular. This results in also restructuring some of the existing components rather than simply transferring a component from AngularJS to Angular. Finally, the hybrid Web Client, including legacy AngularJS components and new Angular components side-by-side, should run properly without any bugs.

                Code Challenge
                Migrate the kommonitorToastHelperService of the KomMonitor Web Client to Angular and make use of it in a new Angular component as part of the Web Client. Follow the steps below:

                Create a fork of https://github.com/KomMonitor/web-client and checkout the GSoC2025 Branch
                Create a new Angular service as part of the KomMonitor Web Client that provides the same functionality as the existing AngularJS version of the kommonitorToastHelperService
                Create a new Angular component that makes use of the previously implemented kommonitorToastHelperService. Take into account these requirements:
                The component should be opened and closed by clicking on a button on the left sidebar.
                The component should include a text area and a button.
                The required functionality should be to display a message as toast on the screen by filling the text area and clicking on the button. For this purpose the kommonitorToastHelperService should be used.
                Push the code to your fork at GitHub
                Link to the fork within your official GSoC application. Your GSoC application should also include a description of which components you plan to migrate during GSoC as well as an estimation of time required for implementing it.


                Community and Code License
                Apache Software License, Version 2

                Mentors
                Sebastian Drost (s.drost @52north.org), Christoph Wagner (c.wagner @52north.org)

                Project Duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                Chat
                TBD

                ~~~~~~~~~~

                2. LLM and GeoData
                Explanation
                During 52°North’s Student Innovation Challenge in 2024, a first open-source implementation connecting spatial data and Large Language Models (LLM) was developed.

                The ambition was to address the pain points of searchability in Research and Spatial Data Infrastructures (RDI/SDI). Search functionality in such systems is typically limited to a metadata-based approach. However, geospatial data – whether vector or raster based – provides a wealth of interesting data that can currently only be identified by looking at the individual dataset. The challenge of the 2024 Student Innovation Prize was to develop a concept and a possible implementation that allows searching within datasets of/and RDI/SDI, e.g. on the attribute level. There are many interesting aspects related to this challenge: technical solutions, taxonomies and semantics, language/i18n, searching in raster data, and many more such as LLMs.

                The available Proof of Concept (PoC) features a prompt that makes it easier to search and access to spatial data. More user stories are documented in the Innovation Prize project backlog on GitHub: https://github.com/52North/innovation-prize.

                Expected Results
                The PoC should be hardened and developed beyond its current state. For example, less verbose prompts are needed as more sophisticated LLMs emerge. Also, improved software frameworks may provide a better development experience. Various extensions are possible and a selection should be outlined in the proposal. Additional user stories from the backlog in the github project (see above) could be addressed. Another interesting extension could also entail a federated architecture. Furthermore, the use of different LLMs is also a possible option for further development.

                Code Challenge
                Set up the entire working environment based on the existing open source code

                https://github.com/52North/innovation-prize/tree/2024

                and add two more data sets. Share the code and the deployed system.

                Community and Code License
                TBChecked: Apache Software License, Version 2

                Mentors
                Henning Bredel (h.bredel @52north.org), Simeon Wetzel

                Project duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                ~~~~~~~~~~

                3. Weather Routing Tool
                Explanation
                The open-source 52°North Weather Routing Tool (WRT) was initially developed during the MariData project. It provides means to find an optimal ship route that minimizes fuel consumption under varying weather conditions. In the optimization process, several constraints can be integrated, e.g. water depth and traffic separation zones. Currently, there are two algorithms available: an isofuel algorithm and a genetic algorithm. Details of the MariData project and example applications of the Weather Routing Tool can be found in the following publication: https://proceedings.open.tudelft.nl/imdc24/article/view/875.

                Expected Results
                The Weather Routing Tool should be extended by new features and its robustness should be improved. There are three major directions of possible developments:

                Ship speed optimization
                Currently, only the geometry of the route is optimized while the ship speed is assumed to be constant. To cover a broader range of real-world use cases, the Weather Routing Tool should provide the option to optimize ship speed. This could be along a fixed route or simultaneous with the route geometry.
                Genetic algorithm
                The implementation of the genetic algorithm is still very basic. Possible improvements include the generation of the initial population and the strategies for crossover and mutation. Moreover, a multi-objective optimization could be implemented.
                General consumption model
                An important aspect of the Weather Routing Tool is the underlying (fuel) consumption model. The best results can generally be obtained by using a consumption model which is developed specifically for a ship, e.g. based on hydrodynamic modeling or machine learning models. However, developing such specific models is cumbersome and restricts the applicability of the tool. Thus, having a general consumption model which only requires a few characteristics of a ship (e.g. type of vessel, length, breadth, displacement) would be a great improvement. The model should have reasonable accuracy. As this feature includes research aspects and can only be successfully developed with the necessary background knowledge, interested candidates have to provide a clear plan of their approach.
                The features can be implemented in different ways. How they are implemented is up to the candidate and might include deterministic, machine learning or AI methods.

                Code Challenge
                New ship class:

                Implement a new ship class
                It should inherit from the Boat base class
                The get_ship_parameters method has to be implemented; it should return a “synthetic” fuel rate which depends on at least one environmental parameter (e.g. wave height)
                Make sure the fuel rates (kg per second) are within a reasonable value range. Besides the weather conditions, typical fuel rates also depend on the ship size, type (e.g. container ship, tanker, fishing vessel) and speed.
                The choice of the considered environmental parameters and the type of the function is free
                You can take the ConstantFuelBoat class as an example
                Prepare weather conditions
                Options:
                Create your own synthetic weather conditions
                Download actual historical or forecast data from public portals (Copernicus, NOAA, …). You can use the Python package maridatadownloader directly or indirectly by setting “DATA_MODE” to “automatic“.
                Run the Weather Routing Tool with your new ship class and a route of your free choice
                Hint: because the Python package mariPower is not publicly available, you need to comment or delete the corresponding lines in ship.py.
                Configuration:
                Set “ALGORITHM_TYPE” to “isofuel”
                Provide the expected results for review
                Mandatory:
                Final route as GeoJSON file
                Python code of new ship class
                Optional:
                Log file (info.log)
                Snapshots of routing steps (WRT_FIGURE_PATH)
                Used weather data
                Community and Code License
                MIT License

                Mentors
                Martin Pontius (m.pontius @52north.org), Katharina Demmich (k.demmich @52north.org)

                Project Duration
                The duration of the project is estimated at 175 hours. An extension is possible.

                TBD

                Cloud Native OGC SensorThings API 2
                enviroCar
    totalCharacters_of_ideas_content_parent: 10019
    totalwords_of_ideas_content_parent: 1397
    totalTokenCount_of_ideas_content_parent: 2059
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/52north-spatial-information-research-gmbh/
    idea_list_url: https://52north.org/outreach-dissemination/google-summer-of-code/project-ideas/


  - organization_id: 2
    organization_name: AFLplusplus
    no_of_ideas: 4
    ideas_content: >-
      Proposal 1: Tool for Automated generic/bounds simplification
        Create a (general, not LibAFL-specific) rust tool to simplify/minimze bounds

        Description
        As commented by many users and maintainers of LibAFL, our codebase is absolutely full of complicated generics. We use these to allow for structured and statically-checked compatibility between various components provided in our codebase, and is a critical part of how LibAFL is structured.

        Unfortunately, these can be very difficult to maintain. Our goal is to develop a tool capable of assisting developers in this maintenance process.

        Please check out issue #2868 for more details.

        Expected Outcomes
        A tool that works on any rust code, tries to minimize the used bounds, and fixes the code

        Skills Expected
        Rust
        A good understanding of Generics and the Rust Type system
        Possible Mentors
        @addisoncrump
        @tokatoka
        Expected size of the project
        The project is expected to take either 175 or 350 hours.

        Difficulty Rating
        The overall difficulty of this project is expected to be medium.
        ~~~~~~~~~~
        Proposal 2: Adapt qemuafl Frontend to LibAFL QEMU
        The project consists of adapting the frontend of qemuafl, the AFL++'s QEMU fork, with LibAFL QEMU.

        Description
        The end goal of this project would be to run fuzzers built for qemuafl while using LibAFL QEMU as the backend, in a retrocompatible way.
        A draft PR is already available and can be used as a starting point by the student.
        Ideally, the student would measure the performance (in terms of exec/s and coverage) of the new qemuafl adaptation with some fuzzers to evaluate how the final implementation compares with the reference.

        Expected Outcomes
        In short, we expect the student to make the new frontend work for most fuzzers developed for qemuafl while maintaining (at least) similar performance.

        See #1983 for an initial implementation that still lacks features.

        The main tasks the student would have to perform are the following:

        Speak the AFL++ forkserver protocol (check the draft PR).
        Add TCG caching to the LibAFL QEMU forkserver
        Use LibAFL QEMU snapshots where possible
        Add as many env variable features as possible
        Skills Expected
        We expect the student to:

        have a strong background in the Rust and C languages.
        be familiar with fuzzing.
        ideally, have some experience using AFL++ and / or LibAFL.
        ideally, have prior experience with the QEMU project.
        Possible Mentors
        The possible mentors for this project are:

        @domenukk
        @rmalmain
        Expected size of the project
        The project is expected to take either 175 or 350 hours.

        Difficulty Rating
        The overall difficulty of this project is expected to be medium.

        Original post
        This proposition is mostly an adaptation of issue #2964.
        ~~~~~~~~~~
        Proposal 3: Network Emulation for LibAFL_QEMU
        Implement syscall emulation for filesystem and network in libafl_qemu.

        Description
        The student must implement something similar to preeny and FitM to hook the network API and an emulator filesystem that can be snapshot-restored always hooking the syscall in libafl_qemu user mode

        Expected Outcomes
        A working network emulation layer for LibAFL_QEMU

        Required Skills
        Good understanding of Rust, C, system programming
        Ideally: prior knowledge in emulators and fuzzing
        Difficulty Rating
        The overall difficulty of this project is expected to be medium.

        Possible mentors
        @domenukk
        @rmalmain
        Expected size of the project
        The project is expected to take either 175 or 350 hours, depending on details
        ~~~~~~~~~~
        Proposal 4: Remote Worker Stage
        Mutations and execution of a Stage is always on the machine LibAFL runs at. For very slow targets it may be beneficial to offload the actual executions to stateless worker.

        Description
        We could add a RemoteWorkerLauncherStage that builds n work packages, each including a next scheduled corpus entry, all metadata for this Testcase, the current feedback state, as well as additional random corpus entries for splicing.
        The work package should then be posted to Redis or some other queue db (very much like celery, whatever a rust alternative is).
        After the execution, the results should be collected in an extra stage

        Expected Outcome:
        The implementation and a set of working examples, including:
        LibAFL Workers / RemoteWorkerLauncherStage + RemoteWorkerCollectorStage

        Required Skills
        Rust
        Prior knowledge in distributed computing and/or fuzzing are a plus
        Difficulty Rating
        easy to medium

        Possible mentors
        @domenukk
        @tokatoka
        @addisoncrump
        Length
        175 hours
    totalCharacters_of_ideas_content_parent: 4418
    totalwords_of_ideas_content_parent: 600
    totalTokenCount_of_ideas_content_parent: 1003
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aflplusplus/
    idea_list_url: https://github.com/AFLplusplus/LibAFL/issues/2992

  - organization_id: 3
    organization_name: AOSSIE
    no_of_ideas: 13
    ideas_content: >-
      Agora Blockchain
        Project Type: Medium
        Description:
        Agora Blockchain is a decentralized voting platform designed to enhance electoral integrity and accessibility. It enables transparent, tamper-proof voting through smart contracts, leveraging Chainlink CCIP for cross-chain functionality. Agora ensures fair participation and trust in election results by eliminating centralized control and providing a verifiable, immutable ledger of votes.

        Key features include:

        Multi-algorithm voting: Supports different voting mechanisms like ranked choice, quadratic voting, and stake-based voting.
        Cross-chain voting: Uses Chainlink CCIP to enable voting across multiple blockchains.
        Gas-efficient smart contracts: Optimized Solidity contracts reduce transaction costs.
        Decentralized governance: Community-driven elections and decision-making.
        User-friendly interface: Built with Next.js, Wagmi, and MetaMask for seamless interaction.
        Expected Outcomes:
        Smart Contract Enhancements:

        Implement private elections for confidential voting.
        Further optimize election factory contracts for gas efficiency.
        Cross-Chain Expansion:

        Extend Chainlink CCIP integration to support multiple blockchains.
        Frontend & dApp Integration:

        Build an intuitive UI using Next.js and Wagmi.
        Ensure smooth wallet connectivity and real-time vote updates.
        Analytics & Insights:

        Develop a real-time dashboard for election statistics.
        Track voter participation and engagement metrics.
        Required Skills:
        Solidity
        Hardhat
        Chainlink CCIP
        Next.js
        MetaMask
        Wagmi
        TailwindCSS
        Zustand
        Mentors:
        Ronnie

        ~~~~~~~~~~

        BabyNest
        Project Type: Large
        Description:
        Pregnancy is a life-changing journey filled with crucial medical appointments, tests, and healthcare decisions. However, expecting parents often struggle to keep track of these milestones, which can lead to missed appointments and added stress. Studies show that adherence to prenatal checkups directly impacts pregnancy outcomes, yet there is no universally accessible tool to assist parents in navigating healthcare requirements based on their country and trimester.

        BabyNest is designed to solve this problem through a minimalist React Native app integrated with an AI-powered assistant. This intelligent assistant acts as a personal pregnancy planner and guide, ensuring that expecting parents stay informed, organized, and stress-free.

        Users can benefit from features such as:

        Automated tracking of trimester-specific medical appointments
        Country-specific healthcare requirement notifications
        Offline access to pregnancy care guidelines
        AI-powered personalized recommendations and reminders
        Expected Outcomes:

        Mobile application built with React Native for cross-platform support
        AI agent integration for intelligent pregnancy milestone scheduling, reminders and tracking
        Offline-first architecture with local storage of healthcare guidelines
        Required Skills:

        Frontend Development (React Native)
        Backend Development (Node.js, FastAPI)
        AI and NLP (Python, LangChain)
        Database Management (SQLite, Pinecone)
        Mentors: Bhavik Mangla

        ~~~~~~~~~~
        DebateAI
        Project Type: Large
        Description:
        DebateAI is an interactive, AI-enhanced debate game platform designed to improve users communication skills through structured competitive debates. Users can engage in real-time debates against both human opponents and AI-driven challengers on a wide range of real-world topics. The platform mimics formal debate competition structures, making it an effective practice and competitive tool.

        Expected Outcomes:
        User vs. User Debates:

        Real-time interaction using WebSockets and WebRTC for audio, video, and text communication.
        Structured debate formats with timed rounds, including opening statements, rebuttals, cross-examinations, and closing arguments.
        User vs. AI Debates:

        AI-driven opponents using LLMs to generate realistic counterarguments and adapt to user inputs.
        User Management and Profiles:

        Secure authentication and access control.
        Personal dashboards to track debate history and manage settings.
        Elo rating system for matchmaking and ranking users.
        Custom Debate Spaces:

        Users can create private rooms to debate on topics of their choice.
        Platform Enhancement & Codebase Refactoring:

        Refactor the existing codebase for better maintainability, scalability, and performance.
        Improve real-time communication efficiency and backend services.
        Required Skills:
        ReactJS
        TypeScript
        GoLang
        Python
        Databases
        LLMs
        Mentors:
        Bruno Keshav


        ~~~~~~~~~~
        Devr.AI
        Project Type: Large
        Description:
        Devr.AI is an AI-powered Developer Relations (DevRel) assistant designed to seamlessly integrate with open-source communities across platforms like Discord, Slack, GitHub, and Discourse. It acts as a virtual DevRel advocate, helping maintainers engage with contributors, onboard new developers, and provide real-time project updates.

        By leveraging LLMs, knowledge retrieval, and workflow automation, the assistant enhances community engagement, simplifies contributor onboarding, and ensures open-source projects remain active and well-supported.

        Expected Outcomes:
        AI-Driven Contributor Engagement

        Automates interactions, welcomes new contributors, and guides them through onboarding.
        Automated Issue Triage & PR Assistance

        Helps maintainers prioritize issues and assists contributors in resolving them efficiently.
        Knowledge Base & FAQ Automation

        Provides instant answers to common queries, reducing repetitive maintainer workload.
        AI-Powered Community Analytics

        Tracks engagement metrics, identifies active contributors, and generates insights.
        Required Skills:
        GenAI
        Supabase
        FastAPI
        Integrations:
        Discord
        Slack
        GitHub
        Mentors:
        Chandan


        ~~~~~~~~~~
        DocPilot
        Project Type: Large
        Description:
        Build a new age EMR application using conversational AI at its best. Existing EMR solutioning is Age-old! Doctors resist the overwhelming software which is high on costs and difficult to operate. Last innovation was made in 1990's. DocPilot listens to the whole consultation conversation between a doctor and patient, and generates a prescription for the doctor to just sign, print and save digitally.

        The app should be able to separate out things like symptoms, diagnosis, medications and tests from the conversation it listens to. These are just the basic requirements. Research more on OPD appointments and include them in our solutioning.

        Expected Outcomes:
        Conversational AI-powered EMR that listens and auto-generates prescriptions.
        Eliminates outdated, complex, and costly software for doctors.
        Affordable and easy to use, reducing resistance from medical professionals.
        Extracts symptoms, diagnosis, medications, and tests from conversations.
        Allows doctors to review, sign, print, and save prescriptions digitally.
        Integrates OPD appointment management for a seamless experience.
        A modern solution replacing decades-old EMR systems.
        Required Skills:
        Flutter
        AI
        Appwrite
        Mentors:
        Jaideep

        ~~~~~~~~~~

        EduAid
        Project Type: Medium
        Description:
        EduAid is an AI-driven tool designed to enhance online learning by generating quizzes from educational content, helping students improve retention and engagement. Currently available as a browser extension, we aim to expand it into a full-fledged platform with a website, optimized model pipelines, and better system performance.

        Our current model supports difficulty-controlled quizzes for short-answer and multiple-choice questions (MCQs). We plan to extend this functionality to other formats, including fill-in-the-blanks, boolean, and match-the-following, by improving our models for diverse question generation. Additionally, we seek to integrate EduAid with other educational platforms to make it a seamless part of the learning ecosystem.

        Expected Outcomes:
        Fully deploy the EduAid browser extension and website.
        Optimize model pipelines for better accuracy and response time.
        Improve system performance for a smoother user experience.
        Expand difficulty-controlled question generation to new formats.
        Enhance UI/UX for better usability.
        Integrate with other educational platforms for wider adoption.
        Required Skills:
        Frontend Development
        Backend Development
        PyTorch & NLP
        System Design & Architecture
        Mentors:
        Aditya Dubey


        ~~~~~~~~~~
        Ell-ena
        Project Type: Large
        Description:
        Ell-ena is an AI-powered product manager that automates task management by creating to-do items, tickets, and transcribing meetings while maintaining full work context. It is input-agnostic and features a chat interface where users can interact naturally.

        Users can ask Ell-ena to perform tasks such as:

        Create a ticket to work on the dark mode feature.
        Add a to-do list item for my math assignment.
        The AI understands the context and adds relevant details automatically. Advanced algorithms like Graph RAG can be leveraged for efficient context retrieval and decision-making.

        Expected Outcomes:
        AI-powered system that generates tasks, tickets, and meeting transcriptions.
        Seamless chat-based interface for intuitive user interactions.
        Context-aware automation to enrich task details automatically.
        Implementation of Graph RAG or similar techniques for intelligent processing.
        Scalable backend to support real-time task creation and management.
        Required Skills:
        ReactJS / NextJS
        NodeJS / Any backend tech stack
        AI / NLP
        Graph RAG
        Mentors:
        Jaideep


        ~~~~~~~~~~

        Inpact
        Project Type: Large
        Description:
        Inpact is an AI-powered creator collaboration and sponsorship matchmaking platform designed to connect content creators, brands, and agencies through data-driven insights. This open-source platform enables influencers to discover relevant sponsorship deals, collaborate with like-minded creators, and optimize brand partnerships.

        By leveraging GenAI, audience analytics, and engagement metrics, Inpact ensures highly relevant sponsorship opportunities for creators while maximizing ROI for brands investing in influencer marketing.

        Expected Outcomes:
        AI-Driven Sponsorship Matchmaking

        Automatically connects creators with brands based on audience demographics, engagement rates, and content style.
        AI-Powered Creator Collaboration Hub

        Facilitates partnerships between creators with complementary audiences and content niches.
        AI-Based Pricing & Deal Optimization

        Provides fair sponsorship pricing recommendations based on engagement, market trends, and historical data.
        AI-Powered Negotiation & Contract Assistant

        Assists in structuring deals, generating contracts, and optimizing terms using AI insights.
        Performance Analytics & ROI Tracking

        Enables brands and creators to track sponsorship performance, audience engagement, and campaign success.
        Required Skills:
        ReactJS
        GenAI
        Supabase
        FastAPI
        Mentors:
        Chandan

        ~~~~~~~~~~
        Monumento
        Project Type: Large
        Description:
        Monumento is an AR-integrated social app that transforms how you connect with the world’s most iconic landmarks. Through Monumento, you can check in to popular monuments, explore famous sites, and discover new people, all within a social platform dedicated to cultural and historical experiences. Whether you're a traveler or a history enthusiast, Monumento offers an immersive way to engage with the world’s most treasured locations.

        Expected Outcomes:
        Improved UI responsiveness

        Improve the app's responsiveness across different devices and screen sizes.
        Ensure a seamless user experience on various platforms.
        Better social system

        Improve the social aspect of the app by improving the feed and user profiles and the ability to interact with other users.
        Introduce new features like events, communities to keep users engaged
        Make Popular Monumnets Dynamic

        Introduce a dynamic system where popular monuments can be updated with new information and images by the users.
        Allow users to add new monuments to the app and make them available for users to check in to.
        Itineray

        Introduce a itinerary feature to help users plan their trips and discover new places.
        Allow users to save their favorite monuments and create personalized itineraries.
        Required Skills:
        Flutter
        Appwrite/Pocketbase/Supabase
        Generative AI
        ARCore/ARKit
        UI/UX Design
        Mentor:
        Mohammed Mohsin

        ~~~~~~~~~~

        Neurotrack
        Project Type: Medium
        Description:
        Neurotrack is an AI-powered platform designed for schools and therapy centers to detect, assess, and manage neurodevelopmental conditions like Autism, ADHD, and learning difficulties. By automating assessments, personalized education plans, and therapy tracking, it empowers educators, therapists, and parents to provide more effective, data-driven support.

        Expected Outcomes:
        AI-Powered Student Grouping

        Identifies patterns and groups students with similar needs for tailored interventions.
        Automated Individualized Education Plans (IEPs)

        Creates personalized learning strategies with AI-driven recommendations.
        Digital Assessments

        Conducts efficient, research-backed evaluations to track progress.
        Real-Time Reports & Insights

        Provides actionable data for educators, therapists, and parents.
        Comprehensive Therapy Tracking

        Logs sessions, progress, and improvements over time.
        Parent Support Assistant

        AI-driven chat support for guidance and resource recommendations.
        Seamless Scheduling

        Simplifies session planning for educators and therapists.
        Required Skills:
        GenAI
        Supabase/Appwrite
        Flutter
        Mentors:
        Mohsin
        ~~~~~~~~~~
        Perspective
        Project Type: Large
        Description:
        In today's digital landscape, personalized content algorithms and social media feeds often create echo chambers of various news and different perspectives and narratives. Users are repeatedly exposed to viewpoints confirming their beliefs. This reinforcement of confirmation bias leads to increased polarization and limits critical thinking.

        The Perspective app tackles the issue of echo chambers and confirmation bias by actively presenting users with well-researched, alternative viewpoints alongside their regularly consumed content. It analyzes the current narrative of a news article, social media post, or online discussion, then curates counterarguments from credible sources. This exposure encourages critical evaluation and helps users see beyond the single perspective they might be constantly fed, ultimately fostering a more balanced and nuanced understanding of complex facts. You don't need to rely on truncated news, get complete facts.

        Users can benefit from features such as:

        Counter-perspective: Instantly see counterarguments and narration of why other perspective.
        Reasoned Thinking: The tool will provide a counter-narrative of the same fact with strongly connected facts.
        Updated Facts: With the help of context-aware LLMs, we will provide the latest facts and counter-facts.
        Seamless Integration: Works with news, blogs, and social media applications.
        Real-Time Analysis: You don't need to wait for any author, make Perspective your companion for immediate insights as you browse.
        Expected Outcomes:

        Less Bias in narratives: Break out of echo chambers.
        Wider Perspectives: Broaden your understanding of the news you are watching.
        Better Discourse: More balanced discussions.
        Sharper Analysis: Improved critical thinking and decreased your mind's polarisation.
        Required Skills:

        Frontend Development (ReactJS)
        Backend Development (Python, FastAPI)
        AI and NLP (Python, LangChain, Langgraph, Prompt Engineering)
        Database Management (Any VectorDB)
        Mentors: Manav Sarkar

        ~~~~~~~~~~

        Pictopy
        Project Type: Medium
        Description:
        Pictopy is currently built using Tauri, relying on Rust, but it comes with platform-specific dependencies that make it difficult to containerize and ship. Electron has been considered as an alternative, but issues with rendering local machine photos and bypassing security have caused challenges in the past. This has led to difficulty in onboarding new contributors as many give up during the setup process, resulting in fewer active contributors.

        The backend has been stable but stagnant and could use refactoring and design enhancements to improve its growth and functionality. While the backend is working without issues, there is potential for improvement and future scaling.

        Expected Outcomes:
        Rework the frontend to explore other options that can simplify setup and containerization.
        Address issues related to Electron, including photo rendering and security bypassing.
        Increase contributions from new developers by simplifying the setup process.
        Refactor and enhance the backend for better growth and scalability.
        Provide design improvements to the backend for smoother development and future expansions.
        Required Skills:
        Rust
        Electron
        Backend Development
        Frontend Development
        Mentors:
        Pranav Aggarwal
        ~~~~~~~~~~
        Resonate
        Project Type: Medium
        Description:
        Resonate is an open-source social voice platform designed to enable real-time audio interactions, storytelling, and voice-based social networking. The project is built with a strong focus on open collaboration, accessibility, and innovation in voice communication. Whether it's live discussions, pair chats, or immersive story experiences, Resonate is designed to put voice at the center of social engagement.

        Expected Outcomes:
        Expanded Audio Story Marketplace

        Develop a fully-fledged marketplace for audio stories, allowing users to create, browse, and follow creators.
        Implement profile pages with a follower system, showcasing user content and social interactions.
        User & Creator Search Functionality

        Enhance the explore page by adding user search functionality.
        Enable users to follow creators, view their profiles, and stay updated on their latest audio stories.
        Friend System for Personal Communication

        Implement a friend request and acceptance system.
        Enable direct personal chats and voice calls between friends.
        Improved Pair Chat Experience

        Introduce a lobby system where users can see the number of people waiting before joining a pair chat.
        Improve UI/UX to enhance user engagement and interaction.
        Required Skills:
        Flutter
        Appwrite
        LiveKit
        WebRTC
        UI/UX Design
        Mentor:
        Aarush Acharya
    totalCharacters_of_ideas_content_parent: 17886
    totalwords_of_ideas_content_parent: 2039
    totalTokenCount_of_ideas_content_parent: 3492
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aossie/
    idea_list_url: https://aossie.org/ideas

  
  - organization_id: 4
    organization_name: API Dash
    no_of_ideas: 10
    ideas_content: >-

          1. DashBot
          Related Issue - #621

          Develop DashBot - the AI assistant for API Dash which supercharges developer productivity by helping developers automate tedious tasks, follow best practices, interact & obtain contextual suggestions, all via natural-language input. DashBot must be designed in a modular and extensible manner and provide the following list of features (suggestive, not exhaustive):

          Explain responses & identify any discrepancy
          Debug requests based on Status codes & Error messages
          Generate API documentation
          Understand API and generate tests
          Generate plots & visualizations for API responses along with ability to customize
          Generate API integration frontend code for frontend frameworks like React, Flutter, etc.
          For each of the tasks you are also required to prepare benchmark evaluations so that it is easier for end users to choose the right backend LLM.

          Skills: AI, Agent, LLM Evaluation, Testing, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          2. AI Agent for API Testing & Tool Generation
          Related Issue - #620

          Develop an AI Agent which leverages the power of Large Language Models (LLMs) to automate and enhance the process of testing APIs. Also, simplify the process of converting APIs into structured tool definitions to enable seamless integration with popular AI agent frameworks like crewAI, smolagents, pydantic-ai, langgraph, etc.

          Traditional API testing involves manually crafting requests, validating responses, and writing test cases. However, AI Agents can significantly streamline this process by generating test cases, validating API responses against expected outputs, and even suggesting improvements based on API documentation. Developers can describe test scenarios in natural language, and the agent can automatically generates API requests, parameter variations, and edge cases. It can also interpret API responses, checking for correctness, consistency, and performance benchmarks. This reduces manual effort while increasing coverage and efficiency, making API testing smarter and more efficient.

          You are also required to prepare benchmark dataset & evaluations so that the right backend LLM can be selected for the end user.

          Skills: AI, Agent, LLM Evaluation, Testing, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          3. API Explorer
          Related Issue - #619

          This project is designed to enhance the API Dash user experience by integrating a curated library of popular and publicly available APIs. This feature allows users to discover, browse, search, and directly import API endpoints into their workspace for seamless testing and exploration. Developers can access pre-configured API request templates, complete with authentication details, sample payloads, and expected responses. This eliminates the need to manually set up API requests, reducing onboarding time and improving efficiency. APIs spanning various domains—such as AI, finance, weather, and social media—are organized into categories, making it easy for users to find relevant services. You are required to develop the entire process backend in the form of an automation pipeline which parses OpenAPI/HTML files, auto-tag it to relevant category, enrich the data, create templates. You can also add features such as user ratings, reviews, and community contributions (via GitHub) to ensure accurate and up-to-date resources.

          Skills: UX Design, OpenAPI, Automation, Dart, Flutter
          Difficulty: Low-Medium
          Length: 175 hours

          ~~~~~~~~~~

          4. AI API Eval Framework
          Related Issue - #618

          Develop an end-to-end AI API eval framework and integrate it in API Dash. This framework should (list is suggestive, not exhaustive):

          Provide an intuitive interface for configuring API requests, where users can input test datasets, configure request parameters, and send queries to various AI API services
          Support evaluation AI APIs (text, multimedia, etc) across various industry task benchmarks
          Allow users to add custom dataset/benchmark & criteria for evaluation. This custom scoring mechanisms allow tailored evaluations based on specific project needs
          Visualize the results of API eval via tables, charts, and graphs, making it easy to identify trends, outliers, and performance variations
          Allow execution of batch evaluations
          Work with both offline & online models and datasets
          Skills: AI, Evaluations, Dart, Python, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~

          5. API Testing Support for - WebSocket, SSE, MQTT & gRPC
          Related Issue - #15 #115 #116 #14

          Testing WebSocket, MQTT (Message Queuing Telemetry Transport), and SSE (Server-Sent Events) protocols is crucial for ensuring the reliability, scalability, and security of real-time communication systems. Whereas, gRPC (Remote Procedure Call) facilitates efficient communication between distributed systems using Protocol Buffers (protobuf) as its interface definition language (IDL) and offers features such as bi-directional streaming, authentication, and built-in support for load balancing and health checking. Each of these API protocols/styles serves different purposes and is utilized in various applications ranging from finance to web applications to IoT (Internet of Things) devices. The objective of this project is to design the architecture of the core library, understand the specs & implement the support for testing, visualization & integration code generation of these APIs in API Dash.

          Skills: Understanding Specs/Protocols, UX Design, Dart, Flutter
          Difficulty: Medium-High
          Length: 350 hours

          ~~~~~~~~~~

          6. AI UI Designer for APIs
          Related Issue - #617

          Develop an AI Agent which transforms API responses into dynamic, user-friendly UI components, enabling developers to visualize and interact with data effortlessly. By analyzing API response structures—such as JSON or XML—the agent automatically generates UI elements like tables, charts, forms, and cards, eliminating the need for manual UI development. One can connect an API endpoint, receive real-time responses, and instantly generate UI components that adapt to the data format. It must also support customization options, allowing developers to configure layouts, styles, and interactive elements such as filters, pagination, and sorting. Finally, users must be able to easily export the generated UI and integrate it in their Flutter or Web apps.

          Skills: AI, UX, Parsing, XML, JSON, Python, Dart, Flutter
          Difficulty: Easy-Medium
          Length: 90 hours

          ~~~~~~~~~~

          7. API Testing Suite, Workflow Builder, Collection Runner & Monitor
          Related Issues - #96 #100 #120

          The objective of this project to design and implement an API testing & workflow builder suite which allows various types of API testing:

          Validation Testing: Verify that the API meets functional and business requirements. Automate the testing & validation of responses received from an API against predefined expectations (assertions), Schema validations, etc.
          Integration Testing: Checks proper interaction between different APIs
          Security Testing: Identifies vulnerabilities and safeguards data
          Performance Testing: Measures speed, responsiveness, and stability under varying loads
          Scalability Testing: Evaluates the system's ability to grow with demand
          Users should be able to easily create collections of APIs for testing. It will also be useful to provide a API workflow builder (a drag and drop environment) to create API workflows and chain requests. The UI must allow users to execute this collection of API requests and test it in a systematic and automated manner (Collection Runner) and finally monitor the results.

          Skills: UI/UX Design, Automation, Testing, Dart, Flutter
          Difficulty: Medium-High
          Length: 350 hours

          ~~~~~~~~~~

          8. Adding Support for API Authentication Methods
          Issue - #609

          Add support for various API authentication methods:

          Basic authentication: Sending a verified username and password with API request Add API Auth: Basic authentication #610
          API key: Sending a key-value pair to the API either in the request headers or query parameters Add API Auth: API key #611
          Bearer token: Authenticate using an access key, such as a JSON Web Token (JWT) Add API Auth: Bearer token #612
          JWT Bearer: Generate JWT bearer tokens to authorize requests Add API Auth: JWT Bearer #613
          Digest Auth: Client must send two requests. First request sent to the server receives a nonce value, which is then used to produce a one-time-use hash key to authenticate the request Add API Auth: Digest Auth #614
          OAuth 1.0 Add API Auth: OAuth 1.0 #615
          OAuth 2.0 Implement OAuth 2.0 authentication #481
          Skills: Authentication, Dart, Flutter
          Difficulty: Low-Medium
          Length: 90 hours

          ~~~~~~~~~~

          9. mem0 for Dart
          mem0 is the goto memory layer for developing personalized AI Agents in Python. It offers comprehensive memory management, self-improving memory capabilities, cross-platform consistency, and centralized memory control. It leverages advanced LLMs and algorithms to detect, store, and retrieve memories from conversations and interactions. It identifies key information such as facts, user preferences, and other contextual information, smartly updates memories over time by resolving contradictions, and supports the development of an AI Agent that evolves with the user interactions. When needed, mem0 employs a smart search system to find memories, ranking them based on relevance, importance, and recency to ensure only the most useful information is presented.

          Currently, we lack this memory layer in Flutter AI applications and your task is to port mem0 to Dart.

          Skills: AI, Database, Data Structures, Python, Dart, Flutter
          Difficulty: Medium-High
          Length: 175 hours

          ~~~~~~~~~~    

          10. API Dash Feature Improvements
          We always believe in improving our core features to help the end user. A suggestive list of features that can be improved are:

          Adding pre-request script/post request script Pre-request and post-request for api collections #557
          Importing from/Exporting to OpenAPI/Swagger specification Importing Requests from OpenAPI Specification file #121
          Adding support for more content types in request Support for application/x-www-form-urlencoded Content-Type as body type formdata currently only supports multipart/form-data #337 Support File as Request Body #352
          JSON body syntax highlighting, beautification, validation - Enhance Request Body Editor: JSON formatting, syntax highlighting, validation and other features #22 Add option to automatically/manually beautify JSON request body #581 Add syntax highlighting for JSON request body #582 Add validation for JSON request body #583 Add environment variable support in request body #590 Env. Variable Support for Text request body #591 Env. Variable Support for JSON request body #592 Env. Variable Support for Form request body #593
          Support for comments in JSON body Support comments in JSON request body #599
          Reading environment variables from OS environment Reading environment variables directly from OS environment #600
          Adding color support for environments (like RED for prod, GREEN for dev) Adding color support for environments #601
          Tab & whitespace settings
          Notification when new app updates are available [feat] in-app update check #373
          Better GraphQL editor
          Beautify and expand/collapse feature for GraphQL query
          Allow inspecting GraphQL schema
          Support for GraphQL variables, fragments, mutation, subscription, etc.
          More widget & integration tests
          More code coverage
          Skills: UX Design, Dart, Flutter
          Difficulty: Easy-Medium
          Length: 175 hours
    totalCharacters_of_ideas_content_parent: 12602
    totalwords_of_ideas_content_parent: 2654
    totalTokenCount_of_ideas_content_parent: 2473
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/api-dash/
    idea_list_url: https://github.com/foss42/apidash/discussions/565


  
  - organization_id: 5
    organization_name: AboutCode
    no_of_ideas: 10
    ideas_content: >-
        PURLdb - DeadCode: track End-Of-Life code
        Code Repositories: https://github.com/aboutcode-org/purldb

        Description:

        Eventually old code goes unmaintained and dies. The goal of this project are:

        To add data structures, models, and APIs in purldb to track end-of-life code and in general package and projects activities
        To improve purl coverage at endoflife.date, see https://github.com/endoflife-date/endoflife.date/issues/763
        To import and sync data from projects such as https://github.com/endoflife-date/endoflife.date
        To design a module that can detect when
        a project is turning end-of-life (using the above)
        a project is unmaintained (use metrics from scorecard/other tools)
        Note that on the endoflife.date side, we need to help improve PURL coverage of the database there, as this would be key to integrate with purldb. There

        Priority: High

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        EOL
        End of life
        Mentors:

        @pombredanne
        @JonoYang
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/purldb/issues/42
        https://github.com/aboutcode-org/vulnerablecode/issues/722
        https://github.com/endoflife-date/endoflife.date/issues/763

        ~~~~~~~~~~
        PURLdb - PopularCode - Find and track actually used and most popular open source code
        Code Repositories: https://github.com/aboutcode-org/purldb

        Description:

        There are between 100 and 200 million open source projects and repos out there. Not all of them are equal. Some are much more useful than others, and some could be safely ignored. For instance, the linux kernel is more important, used and popular than a 1st year computer student school assignment project. The goal of this project is to determine when a project is popular and what are the most popular projects. If we do not know what code is used, we can spend a lot of resources to index less used code.

        There are some simple approaches to this, using available statistics for downloads or Github stars, but that is not satisfying alone.

        An idea would be to consider multiple factors to rank popularity and usage.

        For instance: create a (current and updated) graph of dependencies and compute something like a pagerank but for packages
        Then create with a metric on the freshness of the code like when last release and how much downloaded or based on git activity (excluding bots). This would grow for used code and decay for declining packages
        Then combine this with the dependencies "connectedness"
        Or, just a use the graph connections and no download stats, just a giant graph on top of purldb

        Or something like this:

        Finding strongly connected components
        Relate packages ignoring versions
        Find most connected
        Discount distant connections, boost closest
        Apply decay based on version freshness or git activity
        The approach would be to start small with a single ecosystem as PoC and then extend this to all packages types.

        Ideally, this should be exposed in PurlDB API and integrated in data collection operations.

        Priority: High

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        Popularity
        Mentors:

        @pombredanne
        @JonoYang
        @AyanSinhaMahapatra

        ~~~~~~~~~~
        VulnerableCode project ideas
        There are two main categories of projects for VulnerableCode:

        A. COLLECTION: this category is to mine and collect or infer more new and improved data. This includes collecting new data sources, inferring and improving existing data or collecting new primary data (such as finding a fix commit of a vulnerability)

        B. USAGE: this category is about using and consuming the vulnerability database and includes the API proper, the GUI, the integrations, and data sharing, feedback and curation.

        VulnerableCode: Process unstructured data sources for vulnerabilities (Category A)
        Code Repositories:

        https://github.com/aboutcode-org/vulnerablecode
        Description:

        The project would be to provide a way to effectively mine unstructured data sources for possible unreported vulnerabilities.

        For a start this should be focused on a few prominent repos. This project could also find Fix Commits.

        Some sources are:

        mailing lists
        changelogs
        reflogs of commit
        bug and issue trackers
        This requires systems to "understand" vulnerability descriptions: as often security advisories do not provide structured information on which package and package versions are vulnerable. The end goal is creating a system which would infer vulnerable package name and version(s) by parsing the vulnerability description using specialized techniques and heuristics.

        There is no need to train a model from scratch, we can use AI models pre-trained on code repositories (maybe https://github.com/bigcode-project/starcoder?) and then fine-tune on some prepared datasets of CVEs in code.

        We can either use NLP/machine Learning and automate it all, potentially training data masking algorithms to find these specific data (this also involved creating a dataset) but that's going to be super difficult.

        We could also start to craft a curation queue and parse as much as we can to make it easy to curate by humans and progressively also improve some mini NLP models and classification to help further automate the work.

        References: https://github.com/aboutcode-org/vulnerablecode/issues/251

        Priority: Medium

        Size: Large

        Difficulty Level: Advanced

        Tags:

        Python
        Django
        PostgreSQL
        Security
        Vulnerability
        NLP
        AI/ML
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        @Hritik14
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues/251

        ~~~~~~~~~~
        VulnerableCode: Add more data sources and mine the graph to find correlations between vulnerabilities (Category A)
        Code Repositories:

        https://github.com/aboutcode-org/vulnerablecode
        Description:

        See https://github.com/aboutcode-org/vulnerablecode#how for background info. We want to search for more vulnerability data sources and consume them.

        There is a large number of pending tickets for data sources. See https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        Also see tutorials for adding new importers and improvers:

        https://vulnerablecode.readthedocs.io/en/latest/tutorial_add_new_importer.html
        https://vulnerablecode.readthedocs.io/en/latest/tutorial_add_new_improver.html
        More reference documentation in improvers and importers:

        https://vulnerablecode.readthedocs.io/en/latest/reference_importer_overview.html
        https://vulnerablecode.readthedocs.io/en/latest/reference_improver_overview.html
        Note that this is similar to this GSoC 2022 project (a continuation):

        https://summerofcode.withgoogle.com/organizations/aboutcode/projects/details/7d7Sxtqo
        References: https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        Priority: High

        Size: Medium/Large

        Difficulty Level: Intermediate

        Tags:

        Django
        PostgreSQL
        Security
        Vulnerability
        API
        Scraping
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        @Hritik14
        @jmhoran
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues?q=is%3Aissue+is%3Aopen+label%3A"Data+collection"

        ~~~~~~~~~~
        VulnerableCode: On demand live evaluation of packages (Category A)
        Code Repositories: https://github.com/aboutcode-org/vulnerablecode

        Description:

        Currently VulnerableCode runs importers in bulk where all the data from advisories are imported (and reimported) at once and stored to be displayed and queried.

        The objective of this project is to have another endpoint and API where we can dynamically import available advisories for a single PURL at a time.

        At a high level this would mean:

        Support querying a specific package by PURL. This is not for an approximate search but only an exact PURL lookup.

        Visit advisories/package ecosystem-specific vulnerability data sources and query for this specific package. For instance, for PyPi, the vulnerabilities may be available when querying the main API. An example is https://pypi.org/pypi/lxml/4.1.0/json that lists vulnerabilities. In some other cases, we may need to fetch larger datasets, like when doing this in batch.

        This is irrespective of whether data related to this package being present in the db (i.e. both for new packages and refreshing old packages).

        A good test case would be to start with a completely empty database. Then we call the new API endpoint for one PURL, and the vulnerability data is fetched, imported/stored on the fly and the API results are returned live to the caller. After that API call, the database should now have vulnerability data for that one PURL.

        This would likely imply to modify or update importers to support querying by purl to get advisory data for a specific package. The actual low level fetching should likely be done in FetchCode.

        This is not straightforward as many advisories data source do not store data keyed by package, as they are not package-first, but they are stored by security issue. See specific issues/discussions on these importers for more info. See also how things are done in vulntotal.

        Priority: Medium

        Size: Medium/Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        PostgreSQL
        Security
        web
        Vulnerability
        API
        Mentors:

        @pombredanne
        @tg1999
        @keshav-space
        Related Issues:

        https://github.com/aboutcode-org/vulnerablecode/issues/1046
        https://github.com/aboutcode-org/vulnerablecode/issues/1008

        ~~~~~~~~~~
        ScanCode.io project ideas
        ScanCode.io: Create file-system tree view for project scans
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        Description:

        When large packages/containers are scanned in scancode.io it is useful to have a tree-view to explore thorugh the file-tree for that package/container to look into scan data for a particular subset of the file-tree/directory or to research more into detections and detection issues.

        This would be something similar to what we have at scancode-workbench for example: https://scancode-workbench.readthedocs.io/en/develop/ui-reference/directory-tree.html

        I.e. we need the following features:

        To be able to toggle showing the directory contents from the directory icon
        Show nested directory contents in a tree like structure
        Have this view ideally in a pane left to the table-view of resources
        Show only info from the selected directory in the table-view of resources
        Note that we do have a ProjectCodebaseView in the projects page currently in scancode.io but this is fairly limited as it only lets you browse through the codebase one directory at a time (only shows the files/directories in one directory), and lets you navigate to directories in the current directory or the parent directory from there.

        Priority: High

        Size: Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        UI/UX
        File-system
        Navigation
        Mentors:

        @tdruez
        @pombredanne
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/697

        ~~~~~~~~~~
        ScanCode.io: Add ability to store/query downloaded packages
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        Description:

        Packages which are downloaded and scanned in SCIO can be optionally stored and accessed to have a copy of the packages which are being used for a specific product for reference and future use, and could be used to meet source redistribution obligations.

        The specific tasks would be:

        Store all packages/archives which are downloaded and scanned in SCIO
        Create an API and index by URL/checksum to get these packages on-demand
        Create models to store metadata/history and logs for these downloaded/stored packages
        Additionally support and design external storage/fetch options
        There should be configuration variable to turn this on to enable these features, and connect external databases/storage.

        Priority: Low

        Size: Medium

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        CI
        Security
        Vulnerability
        SBOM
        Mentors:

        @tdruez
        @keshav-space
        @jyang
        @pombredanne
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/1063


        ~~~~~~~~~~

        ScanCode.io: Update SCIO/SCTK for use in CI/CD:
        Code Repositories:

        https://github.com/aboutcode-org/scancode.io
        https://github.com/aboutcode-org/scancode-action
        Description:

        Enhance SCIO/SCTK to be integrated into CI/CD pipelines such as Github Actions, Azure Piplines, Gitlab, Jenkins. We can start with any one CI/CD provider like GitHub Actions and later support others.

        These should be enabled and configured as required by scancode configuration files to enable specific functions to be carried out in the pipeline.

        There are several types of CI/CD pipelines to choose from potentially:

        Generate SBOM/VDRs/VEX with scan results:

        Scan the repo to get all purls: packages, dependencies/requirements
        Scan repository for package, license and copyrights
        Query public.vulnerablecode.io for Vulnerabilities by PackageURL
        Generate SPDX/CycloneDX SBOMs from them with scan and vulnerability data
        License/other Compliance CI/CD pipelines

        Scan repo for licenses and check for detection accuracy
        Scan repo for licenses and check for license clarity score
        Scan repo for licenses and check compliance with specified license policy
        Check for OpenSSF scorecard data and specified policy on community health metrics
        The jobs should pass/fail based on the scan results of these specific cases, so we can have:
        a special mode to fail with error codes
        description of issues and failure reasons, and docs on how to fix these
        ways to configure and set up for these cases with configuration files
        Dependency checkers/linters:

        download and scan all package dependencies, get scan results/SBOM/SBOMs
        check for vulnerable packages and do non-vulnerable dependency resolutuion
        check for test failures after dependency upgrades and add PR only if passes
        Jobs which checks and fixes for misc other errors:

        Replaces standard license notices with SPDX license declarations
        checks and adds ABOUT files for vendored code
        We have an initial CI runner at https://github.com/nexB/scancode-action but we need to improve this with more functions, specially checking against predefined policies and failing/successful CI based on that.

        References:

        https://github.com/aboutcode-org/scancode.io/issues/599
        https://github.com/aboutcode-org/scancode.io/issues/1582
        Priority: High

        Size: Large

        Difficulty Level: Intermediate

        Tags:

        Python
        Django
        CI
        Security
        License
        SBOM
        Compliance
        Mentors:

        @pombredanne
        @tdruez
        @keshav-space
        @tg1999
        @AyanSinhaMahapatra
        Related Issues:

        https://github.com/aboutcode-org/scancode.io/issues/599

        ~~~~~~~~~~
        ScanCode Toolkit project ideas
        Have variable license sections in license rules:
        Code Repositories:

        https://github.com/aboutcode-org/scancode-toolkit
        Description:

        There are lots of variability in license notices and declarations in practice, and one example of modeling this is the SPDX matching guidelines. Note that this was also one of the major ways scancode used to detect licenses earlier.

        Support grammar for variability in license rules (brackets, no of words)
        Do a massive analysis on license rules and check for similarity and variable sections This can be used to add variable sections (for copyright/names/companies) and reduce rules.
        Support variability in license detection post-processing for extra-words case
        Add scripts to add variable sections to rules from detection issues (like bsd detections)
        Priority: Medium

        Size: Medium

        Difficulty Level: Intermediate

        Tags:

        Python
        Licenses
        LicenseDetection
        SPDX
        Matching
        Mentors:

        @AyanSinhaMahapatra
        @pombredanne
        @jyang
        @DennisClark
        Related Issues:

        https://github.com/aboutcode-org/scancode-toolkit/issues/3601

        ~~~~~~~~~~

        Mark required phrases for rules automatically using NLP/AI:
        Code Repositories:

        https://github.com/aboutcode-org/scancode-toolkit
        Description:

        Required phrases are present in rules to make sure the rule is not matched to text in a case where the required phrase is not present in the text, which would be a false-positive detection.

        We are marking required phrases automatically based on what is present in other rules and license attributes, but this still leaves a lot of rules without them. See https://github.com/aboutcode-org/scancode-toolkit/pull/3924 where we are also adding a script to add required phrases as individual rules if applicable and also adding required phrases added to other rules.

        research and choose a model pre-trained on code (StarCoder?)
        use the dataset of current SCTK rules to train a model
        Mark required phrases in licenses automatically with the model
        Test required phrase additions, improve and iterate
        Bonus: Create a minimal UI to review rule updates massively
        Priority: Medium

        Size: Medium

        Difficulty Level: Advanced

        Tags:

        Python
        ML/AI
        Licenses
        Mentors:

        @AyanSinhaMahapatra
        @tg1999
        @pombredanne
        Related Issues:

        https://github.com/aboutcode-org/scancode-toolkit/issues/2878

          
    totalCharacters_of_ideas_content_parent: 19350
    totalwords_of_ideas_content_parent: 4455
    totalTokenCount_of_ideas_content_parent: 4378
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/aboutcode/
    idea_list_url: https://github.com/aboutcode-org/aboutcode/wiki/GSOC-2025-Project-Ideas




  - organization_id: 6
    organization_name: Accord Project
    no_of_ideas: 7
    ideas_content: >-
        1. Linter for Concerto
        Write a linter in TypeScript for Concerto Source files. It should make use of existing functionality to validate the Concerto DSL syntax and JSON AST of Concerto model against a set of rules. Rules should be defined in Typescript and which rules are run should be configurable. You may be able to make use of a tool like Spectral as the framework for defining our own rules over the Concerto AST (JSON).

        Expected Outcomes:
        A tool that allow users to:

        Specify the naming of declarations. E.g. all names of scalars should be in camel case.
        Specify the naming of properties, enum cases e.t.c
        Specify which language features can be used. E.g. disallow maps, disallow forward references in regex validators.
        Enforce the use of certain features. E.g. all string properties should have a length validator.
        Enforce the use of @Term decorators on all declarations and properties e.t.c
        All concepts in a namespace should extend a given concept
        All concepts in a namespace must have unique names across multiple namespaces
        Skills required/preferred:
        Algorithms, Functional programming, Back end development, NodeJS, TypeScript

        Possible Mentors:
        Jamie Shorten, Sanket Shevkar

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        2. Decorator Command Set JSON<->YAML Convertor
        Design and implement a convertor that would convert Decorator Command Sets JSON Objects to a much more human readable YAML format and vice-versa. Currently DCS JSON objects are very verbose to read, write and edit. With the new custom YAML format we aim to make DCS objects much more easier to read, write and edit.

        Expected Outcomes:
        A utility/method in DecoratorManager to convert DCS JSON to YAML and from YAML to JSON.
        1:1 conversion is not expected. YAML should have a custom format that is less verbose and more readable.
        Skills required/preferred:
        NodeJS, Typescript, Javascript, Basic understanding of Data Formats like JSON and YAML

        Possible Mentors:
        Sanket Shevkar

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        3. Accord Project Agreement Protocol
        The Accord Project Agreement Protocol (APAP) defines the protocol used between a document generation engine or contract management platform and an agreement server that provides agreement features like template management, document generation, format conversion etc.

        Expected Outcomes:
        Updated Open API specification
        Updated reference implementation for the specification
        Address (some of) open issues
        Skills required/preferred:
        NodeJS, Typescript, Javascript, REST API design

        Possible Mentors:
        Dan Selman, Niall Roche

        Expected size of project:
        350 hours (large)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        4. Specification Conformance Tests
        Our specification conformance testing is in need of an overhaul! We'd like to migrate to a robust, proven testing framework like Vitest which would support ESM, and be performant and have a new dedicated concerto package used for Concerto conformance testing. An AI tool may be useful in helping with the migration, so feel free to mention how AI could help you with this project! The goal is to have a set of tests that can be run against any Concerto implementation to assess whether it is conformant with the specification.

        Expected Outcomes:
        Migration to Vitest (or other appropriate framework)
        Consolidation of testing methodology and tooling
        New concerto package for tests, focused on conformance
        Build a set of tests for the Concerto validation rules
        Skills required/preferred:
        Node / Javascript
        Unit testing (Mocha / Jest for example)
        Behaviour driven testing (optional, Cucumber, for example)
        Possible Mentors:
        Dan Selman, Ertugrul Karademir

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        5. Incorporating AI into Template Playground
        Our Template Playground web application is used to help onboard users to our technologies. We'd love to make this even easier by adding AI features to make it easier to create, edit, and preview contract templates. This project will build upon the work that was carried out last year in the context of VS Code.

        Expected Outcomes:
        Allow users to upload a file and we'd use AI to convert it to an Accord Project template
        Possibly incorporate auto-complete suggestions when editing using the code editors built into the web app
        Skills required/preferred:
        ReactJS, AI tooling

        Possible Mentors:
        Diana Lease

        Expected size of project:
        350 hours (large)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        6. Testing for Code Generation Targets
        We have tools that allow users to generate code from their Concerto models, supporting several languages. We would like to introduce a way of testing this code generation that compiles code for each language we are generating.

        Expected Outcomes:
        Set of Docker images for each code generation target
        Run code gen tests within the correct image using GitHub actions, for example, generate Java code and then compile and run it using javac to ensure the generated code is correct
        Skills required/preferred:
        Systems engineering, CI/CD
        Docker, Docker compose
        GitHub actions
        Possible Mentors:
        Dan Selman, Ertugrul Karademir

        Expected size of project:
        175 hours (medium)

        Expected difficulty:
        Medium

        ~~~~~~~~~~

        7. Migration of Template Playground to use Tailwind CSS
        As mentioned previously, our Template Playground web application is used to help onboard users to our technologies. By using a popular, well-maintained CSS framework like Tailwind, we could improve performance and code maintainability.

        Expected Outcomes:
        Template Playground updated to use Tailwind CSS
        Existing UI tests updated
        Possibly other UI changes to make user experience better, more performant, and/or optimized for multiple screen sizes
        Skills required/preferred:
        ReactJS, Tailwind CSS

        Possible Mentors:
        Diana Lease

        Expected size of project:
        175 hours (medium) - 350 hours (large)

        Expected difficulty:
        Medium


          
    totalCharacters_of_ideas_content_parent: 6850
    totalwords_of_ideas_content_parent: 1663
    totalTokenCount_of_ideas_content_parent: 1391
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/accord-project/
    idea_list_url: https://github.com/accordproject/techdocs/wiki/Google-Summer-of-Code-2025-Ideas-List



  - organization_id: 7
    organization_name: Alaska 
    no_of_ideas: 15
    ideas_content: >-
        [1] Automated coastline extraction for erosion modeling in Alaska.

        Mentors: Frank Witmer (fwitmer -at- alaska.edu) and Rawan Elframawy (rawann.elframawy -at- gmail.com)

        Overview: The rapidly warming Arctic is leading to increased rates of coastal erosion, placing hundreds of Alaska communities at the frontline of climate change. Understanding current rates of coastline change and accurately forecasting future changes is critical for communities to mitigate and adapt to these changes. Current modeling approaches typically use a simple linear model based solely on historical coastline positions to measure rates of change and extrapolate them into the future. In doing so, these models fail to capture the dynamic effects associated with decreasing sea ice, increasing annual wave energy, and increasing temperatures. To improve the quality of these coastal models, we need to increase the quantity of digitized coastlines, but manual photointerpretation is slow and laborious.

        Current Status: An initial model and pipeline have been developed to automatically extract coastlines from PlanetLabs imagery. An auto-download script is available to retrieve PlanetLabs imagery (3-5m spatial resolution) by specifying any timeframe, cloud coverage percentage, and geometry. Additionally, NDWI with a majority sliding window has been introduced, allowing a specific threshold for each window to improve water detection accuracy. The DeepWaterMap algorithm was originally trained with the Global Surface Water (GSW) dataset at 30 m resolution from Landsat imagery, but the model did not not work well applied to PlanetLabs imagery. We are working to re-train the model using PlanetLabs imagery automatically labeled using the NDWI thresholding method. This project extends and expands on the progress made in 2024.

        Potential areas of improvement:

        Data Expansion (Deering 2017–2019 and Beyond): Currently using data from 2017 to 2019 for Deering; we plan to include more recent data to extend the time series.
        Improved Cliff Area Segmentation: Enhance segmentation performance specifically in steep or cliff-like coastal areas.
        Handling Challenging Conditions: Improve segmentation in regions with water shadows, buildings, satellite artifacts, and other data quality issues.
        SWIR and Elevation Data Integration: Investigate combining short-wave infrared (SWIR) data and elevation data (e.g., DEMs) to further refine segmentation accuracy.
        Expected Outcomes: A finished model with high accuracy that automatically extracts a vectorized coastline representation from PlanetLabs satellite imagery. Then, the model can be applied to large amounts of imagery to model coastline changes over time.

        Required Skills: Python

        Code Challenge: Experience with multi-band satellite imagery, geospatial data processing, and machine learning.

        Source Code: https://github.com/fwitmer/CoastlineExtraction

        Discussion Forum: https://github.com/fwitmer/CoastlineExtraction/discussions

        Effort: 350 Hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [2] Support for Logarithmic Number Systems in a Deep-Learning Framework.

        Mentors: Mark Arnold (markgarnold -at- yahoo.com), Ed Chester (ed.chester -at- gmail.com), and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: The Logarithmic Number System (LNS) is an alternative to built-in Floating Point (FP), which makes multiplication and division easy at the expense of more difficult addition. Using overloaded operators, xlns provides an open-source Python library for LNS arithmetic. Interest in fabricating LNS hardware has grown since it may reduce power consumption for applications that tolerate approximate results, such as deep learning (see [1]-[5]). The problem is deep learning often relies on open-source Python frameworks (like Tensorflow or Pytorch) that are hardcoded to use FP hardware. A key feature of these frameworks is the ability to automatically compute gradients (based on the chain rule) by recording extra information about the computation stored in FP format. Such gradients are used during backpropagation training to update network weights.

        Current Status: xlns, Tensorflow and Pytorch are all interoperable with the widely-used open-source Numpy library, but xlns is not interoperable with the Tensorflow and Pytorch frameworks because both frameworks are hard coded to use built-in int or FP data internally instead of LNS.

        Expected Outcomes: The goal of this project is to provide support for a deep learning framework that uses xlns instead of FP internally (including network specification, automatic gradient, forward inference, and back-propagation training) while keeping high-level compatibility with the framework. This might be as part of xlns, or as a forked version of the chosen framework, or both. The contributor may choose either Pytorch or Tensorflow. The contributor should justify these decisions as part of the proposed design.

        Required Skills: Calculus, Python, Numpy, and either Pytorch or Tensorflow

        Code Challenge: The following three challenges illustrate the breath of issues involved. Each involves only a few lines of Python. Each involves working with both xlns and the framework. Doing all three in both Tensorflow and Pytorch might give evidence for which framework is more likely to lead to the expected outcome.

        Currently, when the data starts in xlns format, Pytorch/Tensorflow converts to FP. As part of the code challenge, we expect the contributor to provide short Python code snippets that demonstrate that if the data starts in xlns format, the computation cannot be carried out in the xlns format.

        xlns/examples/arn_generic.py is a hard-coded illustration of training a fully connected MLP with 28*28 input nodes, 100 hidden nodes and 10 output nodes using MNIST digit set. The hidden layer uses RELU and the output layer uses softmax. The FP weights for this are initialized as:

        W1 = np.array((list(np.random.normal(0, 0.1, (785, 100)))))                    
        W2 = np.array((list(np.random.normal(0, 0.1, (101, 10)))))
        Because there is an extra weight for a constant 1.0 input in each layer, the number of rows is one larger than the inputs to the layer. The example can be run with various data types, for example with xlnsnp (LNS internally implemented with int64 Numpy ufuncs):

        python3 arn_generic.py --type xlnsnp --num_epoch 7
        or more conventionally

        python3 arn_generic.py --type float --num_epoch 7
        The code challenge is to implement a similar size fully connected network (in FP) using the provided features of Pytorch or Tensorflow and compare its convergence with arn_generic.py (Note: arn_generic.py uses manual differentiation, ie, the derivative of RELU is a constant, which depends on the sign of the argument, and elementary backpropagation implements the chain rule).

        Consider LNS addition (1+2=3 and 3-1=2). The following illustrates the overloaded operator and xlnsnp internal representation (sign is LSB of the int64 value; the log portion is the rest):
        >>> import xlns as xl
        >>> x=xl.xlnsnp([2.0, 3.0])
        >>> x
        xlnsnp([xlns(1.9999999986889088) xlns(2.9999999688096786)])
        >>> x.nd
        array([16777216, 26591258])
        By default, the log portion here is given with 23 bits of precision (see help for xl.xlnssetF for details on how to lower the precision as would be useful in machine learning), which is why the log(2.0) is given as 16777216.

        >>> 2*np.int64(np.log2([2.0, 3.0])*2**23)
        array([16777216, 26591258])
        The expression with log2 double checks the answer for x in 23-bit format (with the additional *2 to make room for the sign bit). Had the +2.0 been -2.0, the representation would have been 16777217.

        >>> y=xl.xlnsnp([1.,-1.])
        >>> y
        xlnsnp([xlns(1.0) xlns(-1.0)])
        >>> y.nd
        array([0, 1])
        The above illustrates that the log(1.0)=0, and that the sign bit is one for negative values.

        >>> x+y
        xlnsnp([xlns(2.9999999688096786) xlns(1.9999999986889088)])
        >>> (x+y).nd
        array([26591258, 16777216])
        Although the Pytorch/Tensorflow frameworks don’t support LNS, LNS can be constructed from int64 and float operations (which is how xlnsnp works). In xlns/src/xlns.py, there is a function sbdb_ufunc_ideal(x,y). If you call this with the following code:

        >>> import numpy as np
        >>> def myadd(x,y):  
                  return np.maximum(x,y)+xl.sbdb_ufunc_ideal(-np.abs(x//2-y//2), (x^y)&1) ))
        it performs the same operation internally on int64 values as the overloaded operator:

        >>> myadd(x.nd,y.nd)
        array([26591258, 16777216])
        Such operations are supported by the frameworks (rather than here from np). This code challenge is to do a similar toy example within the tensor types provided by the framework, which gives a small taste of the difficulty involved in this project. (The code above for myadd is a slight oversimplification of xl.xlnsnp.__add__; see this for details on the treatment of 0.0.)

        References:

        [1] G. Alsuhli, et al., “Number Systems for Deep Neural Network Architectures: A Survey,” https://arxiv.org/abs/2307.05035, 2023.

        [2] M. Arnold, E. Chester, et al., “Training neural nets using only an approximate tableless LNS ALU”. 31st International Conference on Application-specific Systems, Architectures and Processors. IEEE. 2020, pp. 69–72. https://doi.org/10.1109/ASAP49362.2020.00020

        [3] O. Kosheleva, et al., “Logarithmic Number System Is Optimal for AI Computations: Theoretical Explanation of Empirical Success”, https://www.cs.utep.edu/vladik/2024/tr24-55.pdf

        [4] D. Miyashita, et al., “Convolutional Neural Networks using Logarithmic Data Representation,” https://arxiv.org/abs/1603.01025, Mar 2016.

        [5] J. Zhao et al., “LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update,” IEEE Trans. Computers, vol. 71, no. 12, pp.3179–3190, Dec. 2022, https://doi.org/10.1109/TC.2022.3202747

        Source Code: https://github.com/xlnsresearch/xlns

        Discussion Forum: https://github.com/xlnsresearch/xlns/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [3] Developing Distributed Algorithm for Metagenomic Error Correction and Assembly.

        Mentors: Arghya Kusum Das (akdas -at- alaska.edu) and Yali Wang (ywang35 -at- alaska.edu)

        Overview: A metagenomics study of Alaska would explore the diverse microbial communities in its unique environments, including the Arctic, marine, and terrestrial ecosystems. Such research could uncover insights into microbial adaptation to extreme conditions and contribute to understanding environmental and climate-related changes in the region. Metagenomic study has an immense impact on multiple science and engineering projects in Alaska such as, arctic healthcare, arctic water pollution, bio leaching on rare earth elements, arctic environmental sustainability and resilience, understanding boreal forest dynamics, wildfire mitigation, and so on. The list is never ending. Shotgun metagenomics, which involves sequencing DNA from a mixed sample of genomes within a community, offers a high-throughput approach to examine the genomic diversity of microbial populations. A key step in metagenomic analysis is assembling the shotgun reads into longer contiguous sequences, or contigs. However, genome assemblies from short reads are often highly fragmented, potentially generating millions of contigs per sample, especially in diverse communities. This challenge arises due to issues like sequence repeats within and between genomes, low coverage of certain species, and strain variability.

        Current Status: Because of the variability in abundance in multiple species in the mixed sample of genomes, it is hard to design a theoretically solid algorithm to rectify the error in the sample and assemble it accurately. The low abundance species in the mixed sample are often wrongly classified as error if we use a traditional/existing algorithms that can rectify the error in a single species’ whole genome sequence. For the similar reason, the existing metagenomic assemblers are perform sub-optimally. Further, the existing software are limited in terms of their data handling capability. Most of them are capable to operate in a single node only. So, their data nailing is severely limited by the RAM available in one node. Also the time consumed for large datasets are often unreasonable.

        Expected Outcomes: In this project, we will address the first two steps in metagenomic analysis i.e., error correction and assembly which are paramount for any downstream project. Metagenomic data is often large in size spanning to hundreds of gigabytes to terabyte scale. Our motivation is to develop distributed, HPC compatible solution for metagenomic error correction and assembly

        (1) We are looking for working solutions (a solid algorithm and its implementation) for metagenomic error correction and assembly. The solutions should be theoretically justifiable and/or biologically meaningful. (2) The algorithm and the software implementation for both error correction and assembly should be distributed in nature. (3) We are open for AI/ML-enabled solutions but that is not a requirement. (4) GPU-enabled solutions are also encouraged but, it’s also not a requirement.

        Required Skills: Python and experience with Deep Neural Networks

        Code Challenge: Prior experience creating deep learning models is expected.

        Source Code: https://github.com/akdasUAF/Metagenome

        Discussion Forum: https://github.com/akdasUAF/Metagenome/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium/Hard

        ~~~~~~~~~~

        [4] Telehealth over L4S.

        Mentors: Kolawole Daramola (koladaramola -at- icloud.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service 1, 2, 3 has shown promising performance, by rethinking congestion control. Can we have a telehealth deployment with pairs of L4S nodes? Perhaps starting with something simple, such as two DICOM endpoints to send radiographic images in between? Linux kernel with L4S patches can be a good point to start for the endpoints. How L4S, with telehealth and other applications, as well as classic non-L4S traffic, share the network will be an interesting test.

        Current Status: A prototype has been built as part of the GSoC 2024. As rural Alaska is largely unconnected by the road network, people often need to fly into larger towns such as Fairbanks and Anchorage for their healthcare needs. This state of affairs has steered the telehealth initiatives in Alaska much more than elsewhere in the US. Our research partners from healthcare organizations such as Alaska Native Tribal Health Consortium (ANTHC) utilize telehealth in their daily operations. Improved telehealth access and performance can significantly benefit the patients and providers in terms of patient satisfaction and comfort.

        Expected Outcomes: This project will review the latest advances from the research, deployment, and testing perspectives with using L4S in telehealth. The contributor will look into how this can be deployed in practice for various telehealth applications – sending DICOM images for diagnostics (high volume of data but tolerance for high latency), telemonitoring via wearable devices (low volume of data but demand for low latency), televisits (a video call through apps such as Zoom – high volume of data and demand for high latency). As a result of this project, we will understand whether we need any optimizations for L4S to use for telehealth applications and potential alternative approaches.

        Required Skills: Python

        Code Challenge: Experience with network protocols and installing Linux servers is a plus. Coding experience demonstrating such experiences is considered positive.

        Source Code: https://github.com/KathiraveluLab/L4SBOA

        Discussion Forum: https://github.com/KathiraveluLab/L4SBOA/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [5] Creating shareable "albums" from locally stored DICOM images

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM data sets downloaded from PACS environments typically remain in the local environments, such as a research server or a cluster where the DICOM retriever (C-MOVE) is run. To use this data, researchers must identify certain subsets of data. This can be achieved by querying the retrieved data. DICOM images consist of textual metadata. By querying the metadata, subsets of images can be identified. However, currently, creating "albums" from locally stored DICOM images is not seamless.

        Current Status: This feature does not exist in our open-source frameworks. We share images through other orthogonal approaches (via rclone, for example). This project will implement a stand-alone utility to effectively create albums from locally stored DICOM images.

        Expected Outcomes: Several approaches to implementing such album features exist. One approach is to use Kheops to provide an interface to create and view the albums. MEDIator can be extended to create subsets and share the images via a unique URL as well. The proposed feature will make the images accessible to more researchers for their experiments by replacing the current manual data sharing efforts. Moreover, Kheops natively integrates with OHIF Viewer. As such, images retrieved locally can be viewed through OHIF Viewer by creating albums with Kheops. Contributors are encouraged to use Kheops or alternatives rather than reinventing the wheel (unless there is a convincing reason).

        Required Skills: Python and Java.

        Code Challenge: Experience working with DICOM images from previous projects or through a sample dummy project will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Easy

        ~~~~~~~~~~

        [6] Beehive: Integrated Community Health Metrics Framework for Behavioral Health to Supplement Healthcare Practice in Alaska.

        Mentors: David Moxley (dpmoxley -at- alaska.edu) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: This project, a collaboration between the University of Alaska Anchorage Departments of Computer Science and Human Services, seeks to create a digital approach to translating the digitalization of art and photographic images into a digital database that stores in retrievable formats those images for use in advancing the delivery of human services and health care to people who experience considerable vulnerability and marginalization within the community. One of the project goals is to create a digital repository of these images, many of which reflect Outsider Art since the people who produce them are not formally trained as artists and experience considerable discrimination. The repository can be used to support research on Outsider art and Outsider Artists, education of health and human services practitioners about the impact of negative stereotypes on the health and well-being of people who are highly vulnerable, and arts programs devoted to advancing the health of vulnerable people.

        This project aims to develop Beehive, a prototype implementation as an open-source data federation framework that can be used in research environments in Alaska and elsewhere.

        Current Status: A prototype has been built as part of Alaska Season of Code. We are researching the approach for its use with our community partners in Anchorage, aiming to support marginalized folks such as the unhoused.

        Expected Outcomes: In this project, the contributor will develop the Beehive platform for (1) translating digital images into the database, (2) developing the database to support user interactions with content, and (3) facilitating retrieval of images. The contributor will obtain an orientation to the project, instruction in how the arts and photography can represent health and well-being, and insight into using digital representations as an advocacy tool for improving the well-being of highly vulnerable people.

        Required Skills: Database (MySQL or Mongo) and Python or Java. A build management tool such as Apache Maven is recommended if using Java.

        Code Challenge: Prior experience with database management through established coding examples.

        Source Code: https://github.com/kathiraveluLab/beehive.

        Discussion Forum: https://github.com/KathiraveluLab/Beehive/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [7] DICOM Image Retrieval and Processing in Matlab.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM (Digital Imaging and Communications in Medicine) is a radiographic imaging standard for how various modalities of scanners, PACS (Picture archiving and communication system) and other imaging systems communicate. As a storage protocol, it defines how images are stored in a standard way. It also functions as a messaging protocol, an extension to TCP.

        Many DICOM processing tools exist. They support receiving images from the scanners and PACS to a research cluster in real-time as an imaging stream or on-demand selectively. They also provide means to anonymize the DICOM images to preserve patient privacy, export the DICOM images into a format such as PNG or JPEG, and extract the textual metadata from DICOM files to store it in a CSV file format or a database. Machine learning pipelines cannot be executed in clinical systems such as scanners and PACS. Therefore, the DICOM images and their metadata in the research clusters can be used to run machine learning pipelines.

        Matlab has some out-of-the-box support for certain DICOM functions, and it could make our job easy in certain projects. This facilitates processing the files from the file system 2. Region-of-Interest is natively supported for DICOM-RT files in Matlab 3. It also supports deep learning on DICOM and NifTi files 4. Matlab currently does not support receiving images from DICOM systems such as PACS and Scanners over the network. Matlab used to have functions that utilize the Dicom toolkit to pull images from another server. It was available through Matlab's file exchange at one point called "dicom server connection." This is not publicly available anymore. However, we have the implementation available locally. The code was not recently tested, and therefore, its usability with the latest Matlab versions needs to be confirmed.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This project aims to create an easy-to-use open-source Matlab DICOM processing framework. We start with processing DICOM images since the current status of the DICOM networking in Matlab is unknown. But we will explore it, if possible and time permitting. Since this is a research project, we should study the existing projects first to avoid re-inventing the wheel. From Google Scholar, we see many processing and pipelines (ROI, deep learning, ...) on DICOM/DICOM-RT have been implemented using Matlab. Regardless of the scientific novelty, we can get an open-source solution to help with further ML stuff using Matlab on the DICOM files. However, we should also observe how this could be a scientific contribution and its merits beyond what is already available. We can use readily available public DICOM data sources to test our implementations, such as the Cancer Imaging Archive (TCIA), as that avoids having to deal with sensitive patient data with PHI. We will narrow down on a specific research use case to highlight the framework's usage in research.

        Required Skills: Matlab

        Code Challenge: Experience working with DICOM images from previous projects and prior experience with Matlab, as demonstrated through code examples, will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Hard


        ~~~~~~~~~~

        [8] Making ZeroMQ a first-class feature of concore.

        Mentors: Shivang vijay (shivangvijay -at- gmail.com), Rahul Jagwani (rahuljagwani1012 -at- gmail.com), and Mayuresh Kothare (mvk2 -at- lehigh.edu)

        Overview: concore is a lightweight framework for closed-loop peripheral neuromodulation control systems. concore consists of a file-sharing based concore protocol to communicate between the programs in a study. concore also allows a shared-memory based communication between programs. This project will implement a ZeroMQ-based communication between programs, as an alternative to the file-sharing based and shared-memory based communications. ZeroMQ is a message-oriented middleware implemented in multiple languages, which natively supports communications across computing nodes. Such an implementation will improve the usability of concore in distributed environments.

        The study with 0MQ

        Current Status: We experimented with an osparc-control based communication as an alternative to this default file-sharing based concore protocol. osparc-control is an extension of ZeroMQ. Our experimental osparc-control based implementation replaces the file-sharing mechanism restricted to one local machine with message queues that can be transmitted between locally networked machines. The contributor will use this osparc-control based communication as an inspiration for the proposed ZeroMQ-based implementation, which will function as a first-class approach to implement the edges of concore without using osparc-control. In our current experimental osparc-control based implementation, these ZeroMQ edges are not visible in the concore editor, the browser-based visual editor for concore. Consequently, studies with osparc-control are represented as forests instead of directed hypergraphs due to the "invisible" ZeroMQ communication. This also means to run a concore study with ZeroMQ communication, we have to run each hypergraph in the forest separately.

        Expected Outcomes: We need to promote a unified experience in concore, whether the edges are implemented via the default file-sharing approach, shared-memory approach, or through this ZeroMQ message-based approach. In the concore file-sharing approach, we label the edges with alphabetical characters. In the concore shared-memory approach, we label the edges starting with positive decimal integers (specifying the memory channels used for the sharing). Therefore, to denote the concore ZeroMQ-based edges, the contributor should assume that all the ZeroMQ-edges must start with "0" in their labels, followed by a hexadecimal port, followed by an underscore (_). For example, edge 0x1234_Y assigns the logical Y to port 1234 and edge 0xabcd_U assigns the logical U to port abcd. Once such a graph with ZeroMQ-edges is made (a single directed hypergraph, rather than a forest with disjoint two or more directed hypergraphs), we should be able to seamlessly build and run the study regardless of the underlying communication mechanism. Thus, we aim to demonstrate the possibility of a seamless local vs. distributed execution in a cluster through ZeroMQ.

        As the expected outcome of this project, we propose a ZeroMQ-based communication for concore with Python. In addition, the contributor may also implement the ZeroMQ-based communication with other programming languages supported by concore such as Matlab and C++. The contributor may also get inspiration from how the shared-memory based communication is implemented in concore.

        Required Skills: Python

        Code Challenge: Prior experience in Python must be demonstrated. Prior experience with message-oriented middleware frameworks such as ZeroMQ can be a plus, although not mandatory.

        Source Code: https://github.com/ControlCore-Project/concore

        Discussion Forum: https://github.com/ControlCore-Project/concore/discussions

        Effort: 350 Hours

        Difficulty Level: Medium


        ~~~~~~~~~~

        [9] Dynamic DICOM Endpoints.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: DICOM (Digital Imaging and Communications in Medicine) is a radiographic imaging standard for how various modalities of scanners, PACS (Picture archiving and communication system), and other imaging systems communicate. As a storage protocol, it defines how images are stored in a standard way. It also functions as a messaging protocol, an extension to TCP. DICOM implementations often have a queue to hold the images sent from the source. Since this is a networking communication, a queue may degrade the performance or introduce data loss. DICOM communications are defined by static source, query, and destination endpoints. Each endpoint is defined by hostname/IP address, port, and AE (Application Entity) Title. A DICOM endpoint such as a PACS or a scanner usually has these endpoints statically configured to ensure security and patient privacy.

        This project attempts to send data from a source to dynamic destinations based on the queue and the performance. This can be a use case for teleradiology with multiple remote healthcare/radiologist sites present or a potential framework to enable federated learning on radiographic images. Orthanc can be set up as a DICOM endpoint that mimics a PACS 1. With multiple Orthanc servers configured, such a federated deployment can be prototyped. Ultimately, this project aims to study the possibilities and opportunities of supporting dynamic DICOM endpoints in practice.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: A prototype implementation that supports dynamic DICOM endpoints.

        Required Skills: Python

        Code Challenge: Experience working with DICOM images from previous projects or through a sample dummy project will be a plus.

        Source Code: https://github.com/KathiraveluLab/Diomede (New Project).

        Discussion Forum: https://github.com/KathiraveluLab/Diomede/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [10] Bio-Block: A Blockchain-based Data Repository and Payment Portal.

        Mentors: Chalinda Weerasinghe (chalindaweerasinghe -at- gmail.com), Erik Zvaigzne (erik.zvaigzne-at-gmail.com), and Forrester Kane Manis (Forrester-at-headword.co)

        Overview: Most biological, genomic, genetic, medical, and behavioral data are currently collected, stored, and sold by vendors who initially offer products and services to clients in order to accumulate this data. The data, once given to companies, remains the property of the company, with very little compensation and autonomy offered to customers who provided the data in the first place. Can we create a secure, decentralized, and scalable data repository of such information for humans and animals, a true bio-block available to all and open-sourced, whereby the data owners get directly compensated? This project offers a response in the affirmative and leverages blockchains for data distribution, archiving, recording, and payments using a dual-chain structure on the Ethereum blockchain.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This overall project will be one of the first offerings of an open-source platform for all biological/medical/genomic/behavioral data that leverages the advantages of blockchains. While proprietary dual-chain blockchain architectures are used by companies in this space, our endeavor, through its sub-projects, aims to proof up the architecture that can be scaled and extended to all forms of client-submitted data and multiple retrieval and payment options. A proof of concept of the architecture will be tested using multivariate, heterogenous synthetic data.

        Required Skills: Python is proposed as the programming language. However, students can also propose their preferred alternative programming language and frameworks. Prior experience developing on Ethereum is a plus.

        Code Challenge: Prior experience in Python (or the proposed alternative language) and, preferably, Ethereum blockchain through established coding examples. Students are expected to establish their experience with Blockchain technologies and architecting and programming them through previous projects - ideally through their respective GitHub repository (or similar code repositories).

        Source Code: https://github.com/bio-block/healthy (New Project).

        Discussion Forum: https://github.com/bio-block/healthy/discussions

        Effort: 350 hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [11] Adopting Nunaliit for Alaska Native Healthcare Practices.

        Mentors: Jessica Ross (jmross2 -at- alaska.edu) and Maria Williams (mdwilliams6 -at- alaska.edu)

        Overview: Nughejagh is an Alaska Native holistic healthcare application. It uses Nunaliit as its map-based interface to store its data. The data is curated from various sources in the form of images, stories, and videos - which are stored using the Nunaliit map-based interface, supported by its CouchDB database. However, currently, Nunaliit lacks several desired features for Nughejagh. This project aims to fill the gap by implementing those features and developing scripts to automate the installation, configuration, and data loading process.

        Current Status: This project is currently in the research stage.

        Expected Outcomes: The expected goal is to have Nunaliit fine-tuned and configured to run Nughejagh with all its requirements. The project outcome might be a new stand-alone repository that uses Nunaliit, a forked version of Nunaliit, or more likely both. The contributor should justify their design decisions as part of the proposed design.

        Required Skills: Prior experience in Javascript, Java, and Python.

        Code Challenge: Deploy and configure Nunaliit locally and share a screenshot of a locally-running Nunaliit instance. Nunaliit runs well on Ubuntu 24.04.

        Source Code: https://github.com/Nughejagh/nughejagh (New Project).

        Discussion Forum: https://github.com/Nughejagh/nughejagh/discussions

        Effort: 350 hours

        Difficulty Level: Medium

        ~~~~~~~~~~

        [12] AWANTA: A Virtual Router based on RIPE Atlas Internet Measurements.

        Mentors: Ananth Reddy (bananthreddy30 -at- gmail.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: RIPE Atlas is an Internet Measurement network composed of small network devices, known as RIPE Atlas Probes and Anchors, connected to the participating volunteers' routers. Using RIPE Atlas, we can measure the Internet latency and routing path through ping and traceroute measurements. This project aims to develop a software router that dynamically uses RIPE Atlas measurements to change the scheduling path. Before the implementation of the project, we should study the existing works on using RIPE Atlas probe for such network optimization tasks at the Internet scale to quickly understand the state-of-the-art and ensure scientific novelty in our approach.

        Current Status: A prototype has been built as part of the GSoC 2024. We observe the use of such a framework in the Circumpolar North. Such an approach can provide significant benefits, especially in Alaska and the Canadian North, where Internet connectivity can be spotty.

        Expected Outcomes: This project extends the RIPE Atlas client to use the measurements in network scheduling decisions. First, the measurements should be streamlined to perform periodically across several probes set as sources and destinations. The measurements across several probes in a single city can provide a more generalized measurement for a city rather than restricting to individual changes of any given probe when multiple such probes are available to a given city. Second, we will build a virtual router to use these measurements to dynamically influence the network scheduling decisions across several nodes. As the network performance changes with time, we can observe how the network path changes with time. We have more than 60 million RIPE Atlas credits that we accumulated by hosting a RIPE Atlas probe for the past five years. So, we have sufficient resources for these Internet measurement experiments.

        Required Skills: Python.

        Code Challenge: Prior experience in Python through established coding examples.

        Source Code: https://github.com/KathiraveluLab/AWANTA

        Discussion Forum: https://github.com/KathiraveluLab/AWANTA/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium


        ~~~~~~~~~~

        [13] Alaska Wildfire Prediction Using Satellite Imagery.

        Mentors: Yali Wang (ywang35 -at- alaska.edu) and Arghya Kusum Das (akdas -at- alaska.edu)

        Overview: Given Alaska’s unique wildfire patterns, where large-scale fires occur annually in boreal forests, tundra, and remote wilderness, predicting fire-prone areas can help mitigate disasters and optimize resource allocation. The presence of vegetation (fuel) is necessary for a fire, but the determining factors are weather conditions (humidity, wind speed, temperature) and an ignition source (lightning, human activity, etc.). This project aims to develop a hybrid deep learning model to predict wildfire risk in Alaska by integrating optical, thermal, and synthetic aperture radar (SAR) satellite imagery with ground-based weather data. Traditional wildfire prediction relies on weather data, historical fire records, and human observations, which can be delayed or inaccurate in remote areas like Alaska. In contrast, satellite imagery provides real-time, high-resolution insights into vegetation health, thermal anomalies, burn severity mapping, soil moisture, fuel dryness, and even cloud-penetrating fire detection.

        Satellite choices:

        Satellite	Resolution	Revisit Frequency	Why Use It?
        Landsat 8 & 9 (NASA/USGS)	30m (multispectral), 100m (thermal)	16 days	Tracks pre/post-fire vegetation and burn severity with great detail.
        Sentinel-2 (ESA)	10m (RGB, NIR), 20m (SWIR)	5 days	High-resolution images for fire risk classification and early warnings.
        MODIS (Terra/Aqua, NASA)	250m (fire detection), 1km (thermal)	Daily	Provides historical fire perimeters and active fire locations.
        VIIRS (Suomi NPP & NOAA-20)	375m (fire detection), 750m (thermal)	Daily	Real-time fire monitoring, capturing active hotspots.
        Sentinel-1 (ESA)	5m - 20m	6-12 days	SAR imaging for vegetation moisture & burned area mapping.
        ALOS-2 (JAXA)	10m - 100m	14 days	L-band SAR for detecting dry fuel and terrain changes.
        Additional ground data sources:

        1). ERA5 Climate Reanalysis (ECMWF): Provides historical & real-time temperature, wind, and humidity data.

        2). NOAA NWS Weather Data: Near real-time humidity, wind, and temperature.

        3). Alaska Fire Service (AFS) Wildfire Data: Historical ignition source data (lightning, human activity).

        Current Status: This project is currently in the research stage.

        Expected Outcomes: This project aims to develop a deep-learning model that predicts wildfire risk in Alaska using a combination of satellite and ground-based weather data. The expected outcome of this project would involve both the dataset preprocessing pipeline and the performance of the developed model. Especially, the dataset preprocessing would include how to process the pre-fire and post-fire images efficiently and integrate the ground-based data with satellite imagery. Expected outcomes include:

        Minimum viable product (MVP):

        Fire risk classification: Given pre-fire satellite images, the model predicts the probability of a fire occurring within a defined time frame like 1 month, 3 months, or 6 months. The classifications should be "High Fire Risk," "Moderate Risk," or "No Risk."

        1). Data pipeline development:

        Preprocessing satellite images: Band selection, geospatial cropping, cloud removal (For this step, we are mostly interested in analyzing Sentinel-2 data);

        Synthetic Aperture Radar (SAR) analysis: Extracting fuel moisture & terrain features (For this step, we are mostly interested in extracting information like vegetation density and soil moisture from Sentinel-1 SAR data);

        Time-series weather data integration: Incorporating temperature, wind, and humidity. We have access to past decades of weather data for almost the past 30 years for multiple different places in Alaska.

        2). Model training and prediction:

        A hybrid model such as CNN-LSTM that analyzes satellite data and time-series weather trends (CNN-LSTM is just an example. We are open to multiple different types of analysis methodology);

        A web-based GIS dashboard to visualize fire-prone regions in Alaska;

        A report on model performance and fire risk metrics.

        Required Skills: Python. Experience with deep learning and machine learning.

        Code Challenge: Experience with multi-band satellite imagery, geospatial data processing (like ArcGIS Pro), and remote sensing.

        Source Code: https://github.com/YaliWang2019/AK-Satellite-Imagery-Wildfire-Prediction (New Project)

        Discussion Forum: https://github.com/YaliWang2019/AK-Satellite-Imagery-Wildfire-Prediction/discussions

        Effort: 350 Hours

        Difficulty Level: Medium/Hard

        You are welcome to propose new open-source project ideas, especially those that serve the state of Alaska and its people. Please use the below template to create new project ideas. However, if you are proposing a new project idea as a contributor, make sure they are relevant to Alaska specifically and the circumpolar north in general. Also, contact potential mentors from the above-listed mentors and confirm their interest in your project idea before drafting an entire proposal based on your own idea.

        ~~~~~~~~~~
        
        [14] Support for Logarithmic Number Systems in Large Language Models.

        Mentors: Mark Arnold (markgarnold -at- yahoo.com) and Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu)

        Overview: The Logarithmic Number System (LNS) is an alternative to built-in Floating Point (FP), which makes multiplication and division easy at the expense of more difficult addition. Using overloaded operators, xlnscpp provides an open-source C++ library for both 16- and 32-bit LNS arithmetic. Interest in fabricating LNS hardware has grown since it may reduce power consumption for applications that tolerate approximate results, such as deep learning (see [1]-[5]). Although LNS has been studied extensively for feed-forward networks, only recently [6] has LNS been considered for Large Language Models (LLMs).

        LLMs consist of two main computations: a) feed-forward neural networks for which LNS has been shown to be useful, and b) an operation known as attention. The training of an LLM produces weights for both of these computations, which are often quantized to reduce data storage requirements. These quantized weights are reconstructed (usually in either 16- or 32-bit FP) and operated on by vectors of tokens (usually in similar FP format).

        Existing LLM engines, such as the open-source llama.cpp, perform vector/matrix/tensor operations (mostly matrix multiply) between the FP tokens and the weights (in a variety of formats, not including LNS).

        llama.cpp uses a library called ggml to do the actual math. The design of ggml supports a variety of FP hardware, such as CPUs and GPUs.

        Current Status: xlnscpp is not supported by llama.cpp or ggml. Weights can be stored in a variety of built-in int or FP formats instead of LNS. Matrix operations are carried out in FP.

        Expected Outcomes: The goal of this project is to provide support for xlnscpp instead of FP in ggml (and indirectly) llama.com. At a minimum, this involves modifying ggml to support a "virtual" LNS "machine" using xlnscpp to perform the actual LNS computation, but which appears to the calling llama.cpp like another hardware platform, like a GPU. The storage format of the quantized weights would still be the same, but they would be converted to LNS for computations like attention on LNS-format tokens. It is not expected that the speed would be as fast as if hardware FP were used, although a design that minimizes the slowdown is desirable (for instance, converting to LNS once, and reusing LNS many times, much as data is transferred to GPU memory and reused many times there). The purpose is a proof of concept that LNS yields valid output from an LLM. The design needs to implement enough ggml features to support an actual LLM, like Deepseek.

        Required Skills: C++ and some familarity with LLMs

        Code Challenge:

        Run the xlns16test.cpp and xlns32test.cpp examples.

        Go through the ggml example for 32-bit FP matrix multiplication on CPU ( https://huggingface.co/blog/introduction-to-ggml) which illustrates concepts like: ggml_backend (the code that does the computation on a GPU or CPU), ggml_context (a "container" that holds data), ggml_cgraph: (what computation the backend performs), ggml_backend_buffer: (hold the data of multiple tensors), and ggml_backend_buffer_type: (a "memory allocator" connected to each ggml_backend). This is quite involved because of the ggml_backend concept. Such experience will help you design a new ggml_backend for LNS (which your design proposal will describe as running on CPU using xlnscpp).

        Write a standalone C++ program that has a function to do 32-bit FP matrix multiply with a main program that prints the FP result. Test it with the same matrix data as the previous ggml example. (Hint: use nested for loops to compute the sum of products that form the matrix product).

        Modify this program to include xlns32.cpp (define xlns_ideal first) and perform the internal computation in LNS format. The main program and the signature of the function it calls remain the same (32-bit FP), which requires that the function convert to/from LNS before and after the matrix multiply. (Hint: if you do it properly, the overloaded xlnscpp assignment operator takes care of this automatically.) The sum of products should be computed entirely in LNS (not FP). Notice the numeric results are close to what FP produces.

        Modify the program to include xlns16.cpp instead. Notice the numeric results are slightly less accurate (the 16-bit LNS product is stored in the 32-bit FP result). This illustrates the tradeoff of using reduced precision LNS, which is what we want to experiment with in this project.

        These code challenges provide possible insight as to how the LNS-CPU backend your design proposal will describe can "look like" an FP backend to llama.cpp. When data would be transferred to the backend, it is converted to LNS. When data is transfered back to llama.cpp, it is converted back to 32-bit FP. This is one idea for this project. You may incorporate improvements to this concept in your design proposal that considers the features of ggml.

        References:

        [1] G. Alsuhli, et al., “Number Systems for Deep Neural Network Architectures: A Survey,” https://arxiv.org/abs/2307.05035, 2023.

        [2] M. Arnold, E. Chester, et al., “Training neural nets using only an approximate tableless LNS ALU”. 31st International Conference on Application-specific Systems, Architectures and Processors. IEEE. 2020, pp. 69–72. https://doi.org/10.1109/ASAP49362.2020.00020

        [3] O. Kosheleva, et al., “Logarithmic Number System Is Optimal for AI Computations: Theoretical Explanation of Empirical Success”, https://www.cs.utep.edu/vladik/2024/tr24-55.pdf

        [4] D. Miyashita, et al., “Convolutional Neural Networks using Logarithmic Data Representation,” https://arxiv.org/abs/1603.01025, Mar 2016.

        [5] J. Zhao et al., “LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update,” IEEE Trans. Computers, vol. 71, no. 12, pp.3179–3190, Dec. 2022, https://doi.org/10.1109/TC.2022.3202747

        [6] P. Haghi, C. Wu, Z. Azad, Y. Li, A. Gui, Y. Hao, A. Li, and T. T. Geng, “Bridging the Gap Between LLMs and LNS with Dynamic Data Format and Architecture Codesign ,” in 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO). Los Alamitos, CA, USA: IEEE Computer Society, Nov. 2024, pp. 1617–1631. https://doi.ieeecomputersociety.org/10.1109/MICRO61859.2024.00118

        Source Code: https://github.com/xlnsresearch/xlnscpp

        Discussion Forum: https://github.com/xlnsresearch/xlnscpp/discussions

        Effort: 350 Hours

        Difficulty Level: Hard

        ~~~~~~~~~~

        [15] Time and Ordering in Beehive.

        Mentors: Pradeeban Kathiravelu (pkathiravelu -at- alaska.edu) and David Moxley (dpmoxley -at- alaska.edu)

        Overview: Beehive was initiated as a collaboration between the University of Alaska Anchorage Departments of Computer Science and Human Services, but grew largely into a software platform through open-source contributions. Beehive seeks to create a digital approach to translating the digitalization of art and photographic images into a digital database that stores in retrievable formats those images for use in advancing the delivery of human services and health care to people who experience considerable vulnerability and marginalization within the community. One of the project goals is to extend the current Beehive software as a repository of photomemories (a 2D projection of 3D spaces) X time. This project aims to extend Beehive with these additional capacities and develop data mining algorithms to support this use case of photos as frozen snapshots in an individual's life.

        Current Status: The current Beehive prototype does not consider the complexities of time and ordering in the use of behavioral patterns and narratives in the journey to recovery.

        Expected Outcomes: In this project, the contributor will (1) extend the Beehive platform to support time and ordering as attributes across images, (2) develop algorithms to understand the impact of past events through the series of images and their narratives, and (3) implement data mining algorithms that could fetch and understannd evolving narratives around photomemories. We see spaces as 3D or 2D if we are referring to geolocations. Photos are 2D projections of a 3D space. There is one dimension that we omit in most of these projections. That is time. Time as a 4th dimension is not entirely new in research and applications. A search on spatiotemporal data and space-time continuum will give you plenty of examples, from climate change to science novels. Time, or more specifically, ordering, is an essential variable in behavior. Don't you wonder how you see places differently just because you have seen the same or something similar before? Where it gets more interesting or challenging (depending on how you see it) is how the time affects the exact location and even those "near" it. When we say "near," it is in terms of data, not necessarily in terms of geographical proximity. Sometimes, it is just a minor change, and the location is the same! In data mining, we call this "near duplicates." A change in the name of a place (can be a city or a restaurant!). Other times, these are two entirely different places. Perhaps, Kivalina has moved over time due to the Arctic Erosion (sadly). But that is still geographical proximity. For instance, your visit to Portugal will influence your visits to other Portuguese-speaking nations (such as Angola and Brazil) because they share a language and culture, although they are oceans apart. On a smaller scale, your experience in a library will impact how you perceive another library in a different location. How do we consider time (or in a more accurate sense, "relative time" or "ordering") in our analysis/perception? This is intertwined as the 4th dimension (or 3rd dimension, if you are already projecting the 3D world into a 2D map/photo). This project aims to understand these complexities in a prototype version over simulated/synthetic data.

        Required Skills: Database (MySQL or Mongo) and Python or Java. Experience and interest in data mining is a plus.

        Code Challenge: Prior experience with database management through established coding examples.

        Source Code: https://github.com/kathiraveluLab/beehive.

        Discussion Forum: https://github.com/KathiraveluLab/Beehive/discussions/

        Effort: 350 Hours

        Difficulty Level: Medium

        [N] PROJECT TITLE.

        Mentors: FIRSTNAME1 LASTNAME1 (email-address) and FIRSTNAME2 LASTNAME2 (email-address)

        Overview:

        Current Status:

        Expected Outcomes:

        Required Skills:

        Code Challenge:

        Source Code:

        Discussion Forum:

        Effort: 90/175/350 Hours

        Difficulty Level: Easy/Medium/Hard


          
    totalCharacters_of_ideas_content_parent: 54320
    totalwords_of_ideas_content_parent: 9380
    totalTokenCount_of_ideas_content_parent: 12130
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/alaska/
    idea_list_url: https://github.com/uaanchorage/GSoC




  - organization_id: 8
    organization_name: AnkiDroid
    no_of_ideas: 5
    ideas_content: >-
    
          Multiple Profiles (175 hours)
          Problem
          Currently, AnkiDroid only allows a single profile as opposed to Anki (Desktop) and AnkiMobile (iphone) which allow multiple profiles. In particular, a family using a shared device will be able to have one profile per user, instead of sharing data. If you have multiple ankiweb accounts, you can sync all of them on your android. You can see discussion about this topic on https://github.com/ankidroid/Anki-Android/issues/2545.


          Expected Outcomes
          A user should be able to add a profile, sync this profile with an ankiweb account, switch between those accounts, and update the preference on a profile-by-profile level.
          Each profile should have its own collection, as in Anki. 

          There are at least three big questions that you need to consider. The UI, the backend, and the data structure. 
          The UI
          Firstly, you should design the UI. Your proposal should show us what would be the path the user will take to add a second profile, and switch to another profile. 
          The data structure
          AnkiDroid follows anki data structure. We have a folder, called Ankidroid, which contains all the files used to store user data. We should figure out a way to change the data structure in order to allow us to store data of multiple profiles.
          When the device does not use scoped storage, the AnkiDroid folder is in the top level general purpose folder of the AndroidDevice. We expect and hope most users understand this folder is used by ankidroid and leave it alone. We can’t just add other top level folders as this may clutter the user storage system and increase the risk a user delete folder, hence losing their data.
          The backend
          We need to figure out everything that will need to be changed. At the very least:
          We need a way to determine which profile is currently being used, and remember this information when ankidroid is restarted
          Each access to the data structure needs to take the profile into account, in order to access the right collection and media folder. This information should be available to the backend; so we should determine whether any change to the backend is required (probably not)
          We need to determine which preferences are profile based and which are used by the whole app. We need to update all preferences access to ensure we use the profile ones. We probably will need a lint rule that ensure that most preferences are profile-dependent unless there is a good reason not to. When creating an account, we need to determine whether to use default preferences or copy existing ones.


          Stretch goals
          If you have remaining time after the main project is done, there are two extensions that could be worth considering.

          Advertise this feature

          On the first update where multiple accounts are available, show a message to the user inviting them to add other accounts. This message should be similar to the message to new users.
          Saving space by avoiding to duplicate the media

          If multiple accounts have the same media (let’s say, the device is used by multiple users, who all should learn Ultimate Geography or Anking deck), ensure that the media are not duplicated in order to save storage on the device. This may require collaboration with the backend, because this optimization is not done on desktop. 
          It is probably worth doing it because storage is very precious on mobile.
          Language:
          Kotlin & XML
          Potentially some rust if we need to touch the backend for the stretch goal
          Difficulty: Medium
          Mentor(s):
          Arthur Milchior
          Shridhar Goel



          ~~~~~~~~~~



          Review Reminder (175 hours)
          Problem
          The user can request AnkiDroid to send them a daily notification in Android to remind them to review their cards if there are cards to review today.
          At least in theory. In practice those notifications have been broken for a long time. We tried years ago to solve the issue. Our solution was to remove most features, but what remains is still far from ideal.  It’s now clear that we should just scratch the current notification system and recreate one from scratch (and automatically migrate users from the previous feature to the new one). 
          Expected Outcomes
          In your proposal, you should tell us what the notification system will look like.
          We need to know what user interface you plan to implement. Every journey the user can take.

          The most basic idea is to have a notification shown if any card is due. This is what we currently have. Also let the user decide at which hour the notification should be shown. Maybe the notification should have a snooze button, to remind the user later (when?)
          We could also consider adding notification for specific decks.
          If so, there should probably be a way to see all notifications currently planned, in order to easily remove them, or edit them together.
          We should also find a way to test those notifications, manually and with automated tests. Ensuring they only trigger once a day in order not to overwhelm the user.
          You may consider reaching users over reddit and the forum in order to gather feedback 

          Language:
          EITHER:
          Kotlin & XML
          Difficulty: Hard
          Mentor(s):
          Arthur Milchior
          criticalAY


          ~~~~~~~~~~

          Note Editor: Note Type Preview (175 hours)
          Problem
          AnkiDroid is a flashcard app with a complex HTML/field-based templating engine. We currently have difficulties explaining a number of concepts to new users, both while onboarding, and for intermediate users:
          The unexpected fact that the user adds ‘notes’ to the app, not ‘cards’
          One note can generate multiple cards
          Various ‘Note Types’ have unique properties
          A user can create or download additional note types
          Currently, the user is provided with a text-based selection screen:

          Expected Outcomes
          In order to resolve the above issues, we want to modify this screen to provide a preview of each note type available in the system, showing
          The number of cards which will be produced when the note type is used
          A visual preview of how each of the cards will look
          Each card has a separate HTML template, so the designs may vary
          Taking into account some special features: 
          A note type may request that the user types in the answer
          Cloze deletions: 1 input produces 1…n cards
          Image occlusion: 1 input
          The ability to open our Card Template Editor
          The screen should allow a user to open up our Note Type Management screen and our manual. We should aim for the screen to prefer graphical elements over text
          Language:
          EITHER:
          Kotlin & XML
          If the screen is Android-specific
          Svelte (Typescript + HTML)
          If the screen is to be integrated into all Anki clients
          Difficulty: Medium
          Mentor(s):
          David Allison
          criticalAY

          ~~~~~~~~~~



          Tablet & Chromebook UI (175/350 hours)
          Problem
          AnkiDroid was initially designed for Android mobile phones. Over the years, Android has come to tablets and Chromebooks, but our UI has continued to be designed around the mobile phone.

          We currently have ~10% of our users on Tablets or Chromebooks, and we want to improve their user experience using the app, both with the aim to improve the user experience for our existing users, and increasing the number of users who can effectively use our app on larger devices

          Sanjay Sargam greatly improved the user experience on table and chromebook through GSoC 24’, and I invite you to read his report. Still, much remains to do, and what was done can certainly be polished.
          Expected Outcomes
          This primarily depends on your proposal. 
          Any screen in the app is open for your suggestions. 

          Suggestions
          Show NoteEditor and Previewer Side by Side
          Currently, the NoteEditor and Previewer are separate screens in AnkiDroid. On mobile devices, this makes sense due to limited scree n space. However,on tablets and Chromebooks, users have larger displays, and constantly switching between editing and previewing can feel cumbersome.

          Resizable Layout in DeckPicker and CardTemplateEditor
          The goal is to introduce a draggable slider that lets users dynamically adjust the size of two sections.

          Language: Kotlin, XML
          Difficulty: Medium
          Mentor(s):
          David Allison
          Arthur Milchior
          Sanjay Sargam


          ~~~~~~~~~~

          Additional Widgets (175/350 hours)
          Problem
          Widgets were introduced to AnkiDroid in 2010. These provide significant benefit to our power users and we started using them through GSoC 24. I invite you to read last year’s contributor’s report to see what was done.

          
          Expected Outcomes
          Android 12 Widget-based functionality is evaluated and integrated with the widgets when appropriate


          The GSoC proposal is expected to propose additional widgets that would be useful to our users
          Language: Kotlin, XML, UI & UX
          Difficulty: Medium
          Mentor(s):
          David Allison
          criticalAY





          
    totalCharacters_of_ideas_content_parent: 9691
    totalwords_of_ideas_content_parent: 2448
    totalTokenCount_of_ideas_content_parent: 1986
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ankidroid/
    idea_list_url: https://docs.google.com/document/d/1Va6IWEYcWTkK4KDtyoFxtOKzpdcYe54GdrVpuMcFvlI/edit?pli=1&tab=t.0



  - organization_id: 9
    organization_name: Apache DataFusion
    no_of_ideas: 11
    ideas_content: >-
        
        Implement Continuous Monitoring of DataFusion Performance
        Description and Outcomes: DataFusion lacks continuous monitoring of how performance evolves over time – we do this somewhat manually today. Even though performance has been one of our top priorities for a while now, we didn’t build a continuous monitoring system yet. This linked issue contains a summary of all the previous efforts that made us inch closer to having such a system, but a functioning system needs to built on top of that progress. A student successfully completing this project would gain experience in building an end-to-end monitoring system that integrates with GitHub, scheduling/running benchmarks on some sort of a cloud infrastructure, and building a versatile web UI to expose the results. The outcome of this project will benefit Apache DataFusion on an ongoing basis in its quest for ever-more performance.

        Category: Tooling

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and mertak-synnada

        Skills: DevOps, Cloud Computing, Web Development, Integrations

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Supporting Correlated Subqueries
        Description and Outcomes: Correlated subqueries are an important SQL feature that enables some users to express their business logic more intuitively without thinking about “joins”. Even though DataFusion has decent join support, it doesn’t fully support correlated subqueries. The linked epic contains bite-size pieces of the steps necessary to achieve full support. For students interested in internals of data systems and databases, this project is a good opportunity to apply and/or improve their computer science knowledge. The experience of adding such a feature to a widely-used foundational query engine can also serve as a good opportunity to kickstart a career in the area of databases and data systems.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): jayzhan-synnada and xudong963

        Skills: Databases, Algorithms, Data Structures, Testing Techniques

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Improving DataFusion DX (e.g. 1 and 2)
        Description and Outcomes: While performance, extensibility and customizability is DataFusion’s strong aspects, we have much work to do in terms of user-friendliness and ease of debug-ability. This project aims to make strides in these areas by improving terminal visualizations of query plans and increasing the “deployment” of the newly-added diagnostics framework. This project is a potential high-impact project with high output visibility, and reduce the barrier to entry to new users.

        Category: DX

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): eliaperantoni and mkarbo

        Skills: Software Engineering, Terminal Visualizations

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Robust WASM Support
        Description and Outcomes: DataFusion can be compiled today to WASM with some care. However, it is somewhat tricky and brittle. Having robust WASM support improves the embeddability aspect of DataFusion, and can enable many practical use cases. A good conclusion of this project would be the addition of a live demo sub-page to the DataFusion homepage.

        Category: Build

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and waynexia

        Skills: WASM, Advanced Rust, Web Development, Software Engineering

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        High Performance Aggregations
        Description and Outcomes: An aggregation is one of the most fundamental operations within a query engine. Practical performance in many use cases, and results in many well-known benchmarks (e.g. ClickBench), depend heavily on aggregation performance. DataFusion community has been working on improving aggregation performance for a while now, but there is still work to do. A student working on this project will get the chance to hone their skills on high-performance, low(ish) level coding, intricacies of measuring performance, data structures and others.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): jayzhan-synnada and Rachelint

        Skills: Algorithms, Data Structures, Advanced Rust, Databases, Benchmarking Techniques

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Improving Python Bindings
        Description and Outcomes: DataFusion offers Python bindings that enable users to build data systems using Python. However, the Python bindings are still relatively low-level, and do not expose all APIs libraries like Pandas and Polars with a end-user focus offer. This project aims to improve DataFusion’s Python bindings to make progress towards moving it closer to such libraries in terms of built-in APIs and functionality.

        Category: Python Bindings

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): timsaucer

        Skills: APIs, FFIs, DataFrame Libraries

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Optimizing DataFusion Binary Size
        Description and Outcomes: DataFusion is a foundational library with a large feature set. Even though we try to avoid adding too many dependencies and implement many low-level functionalities inside the codebase, the fast moving nature of the project results in an accumulation of dependencies over time. This inflates DataFusion’s binary size over time, which reduces portability and embeddability. This project involves a study of the codebase, using compiler tooling, to understand where code bloat comes from, simplifying/reducing the number of dependencies by efficient in-house implementations, and avoiding code duplications.

        Category: Core/Build

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): comphead and alamb

        Skills: Software Engineering, Refactoring, Dependency Management, Compilers

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        Ergonomic SQL Features
        Description and Outcomes: DuckDB has many innovative features that significantly improve the SQL UX. Even though some of those features are already implemented in DataFusion, there are many others we can implement (and get inspiration from). This page contains a good summary of such features. Each such feature will serve as a bite-size, achievable milestone for a cool GSoC project that will have user-facing impact improving the UX on a broad basis. The project will start with a survey of what is already implemented, what is missing, and kick off with a prioritization proposal/implementation plan.

        Category: SQL FE

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): berkaysynnada

        Skills: SQL, Planning, Parsing, Software Engineering

        Expected Project Size: 350 hours


        ~~~~~~~~~~

        Advanced Interval Analysis
        Description and Outcomes: DataFusion implements interval arithmetic and utilizes it for range estimations, which enables use cases in data pruning, optimizations and statistics. However, the current implementation only works efficiently for forward evaluation; i.e. calculating the output range of an expression given input ranges (ranges of columns). When propagating constraints using the same graph, the current approach requires multiple bottom-up and top-down traversals to narrow column bounds fully. This project aims to fix this deficiency by utilizing a better algorithmic approach. Note that this is a very advanced project for students with a deep interest in computational methods, expression graphs, and constraint solvers.

        Category: Core

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): ozankabak and berkaysynnada

        Skills: Algorithms, Data Structures, Applied Mathematics, Software Engineering

        Expected Project Size: 350 hours

        ~~~~~~~~~~

        Spark-Compatible Functions Crate
        Description and Outcomes: In general, DataFusion aims to be compatible with PostgreSQL in terms of functions and behaviors. However, there are many users (and downstream projects, such as DataFusion Comet) that desire compatibility with Apache Spark. This project aims to collect Spark-compatible functions into a separate crate to help such users and/or projects. The project will be an exercise in creating the right APIs, explaining how to use them, and then telling the world about them (e.g. via creating a compatibility-tracking page cataloging such functions, writing blog posts etc.).

        Category: Extensions

        Difficulty: Medium

        Possible Mentor(s) and/or Helper(s): alamb and andygrove

        Skills: SQL, Spark, Software Engineering

        Expected Project Size: 175 to 350 hours*

        ~~~~~~~~~~

        SQL Fuzzing Framework in Rust
        Description and Outcomes: Fuzz testing is a very important technique we utilize often in DataFusion. Having SQL-level fuzz testing enables us to battle-test DataFusion in an end-to-end fashion. Initial version of our fuzzing framework is Java-based, but the time has come to migrate to Rust-native solution. This will simplify the overall implementation (by avoiding things like JDBC), enable us to implement more advanced algorithms for query generation, and attract more contributors over time. This project is a good blend of software engineering, algorithms and testing techniques (i.e. fuzzing techniques).

        Category: Extensions

        Difficulty: Advanced

        Possible Mentor(s) and/or Helper(s): 2010YOUY01

        Skills: SQL, Testing Techniques, Advanced Rust, Software Engineering

        Expected Project Size: 175 to 350 hours*

        *There is enough material to make this a 350-hour project, but it is granular enough to make it a 175-hour project as well. The student can choose the size of the project based on their availability and interest.

          
    totalCharacters_of_ideas_content_parent: 10170
    totalwords_of_ideas_content_parent: 1940
    totalTokenCount_of_ideas_content_parent: 2086
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/apache-datafusion/
    idea_list_url: https://datafusion.apache.org/contributor-guide/gsoc_project_ideas



  - organization_id: 10
    organization_name: ArduPilot
    no_of_ideas: 6 
    ideas_content: >-
          Non-GPS Position Estimation Using 3D Camera and Pre-Generated Map¶
          Skills required: Python, C++

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Hard

          Expected Outcome: Copter with low-cost 3D camera estimates its local position by comparing the camera point cloud to a pre-generated 3D map

          The goal of this project is to allow a Copter to estimate its local position using a low-cost 3D camera (e.g. Intel D465) by comparing the camera’s point cloud to a pre-generated 3D map. The steps involved include:

          Create a tool to capture a 3D map of the flight area. The resulting map should be loaded onto the vehicle’s companion computer (e.g. RPI5)

          Mount a low-cost 3D camera (e.g. Intel D465) onto an ArduPilot copter (e.g. EDU650 or similar) equipped with a companion computer

          Write localisation software (e.g. python code) to compare the output of the 3D camera to the pre-generated 3D map and send the estimated position to the vehicle’s EKF (see Non-GPS Position Estimation)

          Implement a simulator of the system (e.g. gazebo)

          Document the setup and operation for future developers and users

          Funding will be provided for hardware including a copter (e.g. Hexsoon EDU650), companion computer and 3D camera (e.g. Intel D465) if necessary

          ~~~~~~~~~~

          AI Chat WebTool for use with MP and/or QGC
          Skills required: JavaScript, OpenAI, Google Gemini

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Web tool capable following a pilot’s verbal commands and converting them to MAVLink in order to control an ArduPilot multicopter

          This project involves re-implementing the MAVProxy’s AI chat module (see blog here) to run as a WebTool

          Once complete the WebTool should be capable of:

          Connecting to the vehicle via Mission Planner or QGC

          Responding to verbal or written questions and commands from the pilot

          Arming the vehicle

          Issuing takeoff commands and flying the vehicle a specified distance in any direction

          Changing the vehicle’s flight mode

          Most of the development can be completed using the SITL simulator and any OpenAI or Google Gemini usage costs will be covered

          ~~~~~~~~~~
           
          AI Chat Integration with all WebTools¶
          Skills required: JavaScript, OpenAI, Google Gemini

          Mentor: Randy Mackay

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: All WebTools include AI chat to help users understand and use the tool

          This project involves adding an OpenAI or Google Gemini chat window into some or all of the ArduPilot Webtools

          Once complete some or all of the WebTools should:

          Include a new chat widget allowing users to ask an AI assistant questions about the tool using text or voice

          Allow the AI assistant to operate the tool based on user input (e.g. push buttons, change zoom of graphs, etc)

          The top priority WebTool is the “UAV Log viewer” although simpler tools like the “Hardware Report” could be a good starting point

          Most of the development can be completed using the SITL simulator and any OpenAI or Google Gemini usage costs will be covered
          ~~~~~~~~~~


          Gazebo Plug-in Model of a Motor¶
          Skills required: Gazebo, C++

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: ArduPilot Gazebo plugin simulates a Motor

          As part of the ArduPilot_Gazebo plugin, we ask a student to model the electromechanical properties of a motor (no thrust/aero, just the motor angular acceleration/power itself)
          ~~~~~~~~~~

          SITL AI Reinforcement Learning Concept Script¶
          Skills required: Gaazebo, Lua, AI

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Lua script that uses re-inforcement learning to automate changing some parameters

          An AP-SITL reinforcement learning script concept, focuses on using Lua applets or some python to automate parameter changes according to some basic implementation of online reinforcement learning (actor-critic/SARSA/Q-learning)
          ~~~~~~~~~~

          SITL Test Script for Controls Testing¶
          Skills required: Gaazebo, Python

          Mentor: Nate Mailhot

          Expected Size: 175h

          Level of Difficulty: Medium

          Expected Outcome: Python code that allows easily setting up an AP vehicle in SITL for controls testing

          A safe “for education/rookies” SITL test script that strips away the majority of complexity in set-up and gives a Copter (and Plane if time permits) that requires some basic tuning and gives hints/pointers in a UI (this could lower the threshold for earlier year mech/electrical engineers to get their hands dirty on some software and try out basic controls testing)

          
    totalCharacters_of_ideas_content_parent: 5201
    totalwords_of_ideas_content_parent: 1290
    totalTokenCount_of_ideas_content_parent: 1143
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ardupilot/
    idea_list_url: https://ardupilot.org/dev/docs/gsoc-ideas-list.html


  - organization_id: 11
    organization_name: AsyncAPI
    no_of_ideas: 9
    ideas_content: >-
          1) Enhancing Performance and Reliability of AsyncAPI CLI
          Improve the AsyncAPI CLI by optimizing performance, enhancing test reliability, and introducing long-requested features such as publishing and syncing AsyncAPI files with remote repositories.

          🎯 Outcome: Achieve a faster CLI execution, stable tests, file sync/publish support, and enhanced validation.
          🛠️ Skills Required: JavaScript/TypeScript, Node.js, Testing Frameworks, API, and testing automation.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AayushSaini101 | @Souvikns
          ⏳ Length: 175 Hours


          ~~~~~~~~~~
          2) AI-Powered Assistant for AsyncAPI
          Build an AI-powered assistant fine-tuned on AsyncAPI to provide accurate answers, generate code snippets, debug specifications, and recommend best practices.

          🎯 Outcome: A fine-tuned LLM-powered chatbot integrated with AsyncAPI’s ecosystem for enhanced developer support.
          🛠️ Skills Required: Javascript/Typescript, Machine Learning (LLMs), NLP, OpenAI/Llama, Chatbot Integration.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AceTheCreator
          ⏳ Length: 175 Hours

          ~~~~~~~~~~
          3) AsyncAPI Generator Maintainership
          This initiative aims to guide you from contributing to maintaining the project. You'll gain insight into the responsibilities of a maintainer, which involve tasks beyond mere coding.

          🎯 Outcome: Responsible for the project's future and continuous improvement.
          🛠️ Skills: JavaScript/TypeScript, testing libraries, Docker, virtualization, and test automation.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @derberg
          ⏳ Length: 350 Hours

          ~~~~~~~~~~
          4) AsyncAPI Conference Website UI Kit Development
          Develop a comprehensive UI Kit to enhance design consistency, modularity, and maintainability of the AsyncAPI Conference website.

          🎯 Outcome: A structured UI Kit with reusable components, Storybook integration, and improved design consistency.
          🛠️ Skills Required: React, TypeScript, Storybook, UI/UX Design, Component Development.
          🧩 Difficulty: Medium
          👩🏿‍🏫 Mentor(s): @AceTheCreator
          ⏳ Length: 175 Hours

          ~~~~~~~~~~
          5) VS Code Extension Maintainership
          This initiative will guide you from contributing to becoming a maintainer of the VS Code AsyncAPI Preview extension. You'll learn the responsibilities of a maintainer, including code contributions, issue triaging, release management, and community engagement.

          🎯 Outcome: Taking ownership of the VS Code extension to ensure its long-term stability and improvement.
          🛠️ Skills Required: TypeScript/JavaScript, VS Code Extensions, Spectral Linting, Testing, and Open Source Contribution.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @ivangsa
          ⏳ Length: 350 Hours

          ~~~~~~~~~~
          6) Java + Quarkus Template for AsyncAPI Generator
          Develop a new AsyncAPI Generator template for Java with Quarkus, leveraging its growing adoption in cloud-native development.

          🎯 Outcome: A fully functional Java + Quarkus template for generating AsyncAPI-based applications.
          🛠️ Skills Required: Java, Quarkus, Templating Engines (Nunjucks/Handlebars), AsyncAPI Generator.
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @AayushSaini101, @Souvikns
          ⏳ Length: 350 Hours
          ~~~~~~~~~~
          7) Refactor the Scripts inside the website and add Integration tests
          Add the script execution to a new folder inside the website, and add integration tests for those scripts.

          🎯 Outcome: A full Unit + Integration tests setup will be added for the scripts to fully test the functionalities
          🛠️ Skills Required: Typescript, Node js, Jest, Github actions
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @akshatnema
          ⏳ Length: 350 Hours
          ~~~~~~~~~~
          8) Add E2E tests for the Website critical flows
          Add E2E tests for the website where some of the critical flows (that are centered around user experience are tested thoroughly).

          🎯 Outcome: This project will ensure that we are not breaking any critical flows where user experience is our topmost priority
          🛠️ Skills Required: Typescript, Node js, E2E Testing, Github actions
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @sambhavgupta0705
          ⏳ Length: 175 hours
          ~~~~~~~~~~
          9) Redesign of website and addition of Dark theme
          Create new designs for the website pages based on the theme chosen by @Mayaleeeee and replicate those designs inside the website, along with the Dark mode theme.

          🎯 Outcome: This project will ensure that we are not breaking any critical flows where user experience is our topmost priority
          🛠️ Skills Required: Typescript, Node js, Figma, TailwindCSS
          🧩 Difficulty: Medium/Hard
          👩🏿‍🏫 Mentor(s): @Mayaleeeee, @devilkiller-ag
          ⏳ Length: 350 hours

          
    totalCharacters_of_ideas_content_parent: 5231
    totalwords_of_ideas_content_parent: 1242
    totalTokenCount_of_ideas_content_parent: 1147
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/asyncapi/
    idea_list_url: https://github.com/asyncapi/community/blob/master/mentorship/summerofcode/2025/asyncapi-gsoc-ideas-page.md



  - organization_id: 12 
    organization_name: BRL-CAD 
    no_of_ideas: 25
    ideas_content: >-
          Improving the k-File to BRL-CAD Converter
          Outline
          In the past years, we put some effort in the development of a LS-DYNA keyword file to BRL-CAD converter. Although we made great progress there, we still can't convert every k-file to g, i.e. the native BRL-CAD format. The goal of this project is to increase the number of covertable k-files.

          Details
          The sources of the current k-file to BRL-CAD converter can be found in the brlcad repository at src/conv/g. You have to compile BRL-CAD from its sources to work on this project and see the effects of your changes.

          Examples of k-files, which cannot be converted with k-g, can be found here: THUMS You can however use your own examples.

          Expected Outcome
          We expect an improved k-g LS-DYNA keyword file to BRL-CAD converting program as the outcome from this project.

          Project Properties
          Skills
          C/C++
          LS-DYNA or a similar FE solver software, which can read k-files (needed as reference for how the geometry should look like)
          Difficulty
          medium

          Size
          This project could have any size, short (90h), medium (175h) or long (350h), depending on the amount of functionality you want to add.

          Additional Information
          Potential mentor(s):
          Ali Haydar
          Daniel Rossberg
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Development and Build Support for Native Windows
          Outline
          OpenSCAD is multi-platform desktop application, with official support for Windows, macOS and Linux, and unoffical support for various other Unix-like OSes like FreeBSD.

          While OpenSCAD does support Windows, Windows development and build setup is a bit suboptimal:

          Windows binaries are built on Linux using the MXE cross compilation environment (https://mxe.cc). Build setup
          Windows native builds, including tests are done on msys2 (https://www.msys2.org) Build setup
          Goal of this project is to improve Windows support for native Windows development.

          Details
          Adjust OpenSCAD source code as needed to build OpenSCAD using MSVC
          Adjust the CMake build system as needed to support MSVC
          Find a way of managing building (and packaging if needed) 3rd party dependencies. OpenSCAD depends on a number of 3rd party packages (e.g. Qt, QScintilla2, CGAL, Manifold, GMP, MPFR, boost, OpenCSG, GLEW, Eigen, glib2, fontconfig, freetype2, harfbuzz, libzip, Bison, Flex, double-conversion). Not all of these have great Windows build support.
          Integrate library building/packaging into our Continuous Integration framework (e.g. GitHub Actions or CircleCI).
          Build OpenSCAD on MSVC + run tests natively for every PR, as we do for other build environments. This to make sure contributions don't fall out of maintenance.
          Establish support for debugging OpenSCAD in the MSVC debugger, and documentation on how to set that up, if necessary
          Write/update documentation on how to establish a native WIndows development environment
          Consider switching the official OpenSCAD Windows build to use MSVC.
          Prior work: openscad/openscad#4976

          Expected Outcome
          A native MSVC build environment for OpenSCAD is reasonable easy to set up, and such an environment is continuously integrated.

          Project Properties
          Skills
          Good understanding of the Windows OS and native components (DLLs, executables) on a low enough level to be able to debug odd behaviors.
          Experience using MSVC and native Windows development for C++ projects
          Good understanding of how 3rd party libraries are built and distributed.
          Experience with or interest in acquiring skills using CMake and writing custom CMake configs and macros
          Experience with GitHub CI or CircleCI is a bonus
          Difficulty
          medium

          Size
          long (350h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Note: None of the mentors have relevant Windows skills, but have excellent understanding on how all the relevant technologies work and how they are integrated with OpenSCAD. It's important that the candidate is able to acquire any necessary Windows-specific skills independently.
          Organization website: https://www.openscad.org/

          ~~~~~~~~~~

          Integrated language help feature in OpenSCAD #100
          Outline
          Add more interactive help features for built-in functions and modules. Right now there's already a nice summary of parameters linked as cheat sheet. Scope of this projects would be to use this information in extended form and make it available in a more direct way in the editor.

          Details
          Convert the cheat sheet information into machine readable format
          Find a way to generate the existing HTML format based on the core data
          Add context help to editor giving help for built-in functions and modules, e.g. by adding formatted help output to the console window, including the links to further documentation like the language manual on Wikibooks
          Expected Outcome
          Cheat sheet is integrated into the application and additional context help for built-in functions and modules is available.

          Project Properties
          Skills
          Programming language is C++
          GUI programming with the Qt framework
          Difficulty
          Easy

          Size
          Medium (175h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Organization website: https://www.openscad.org/


          ~~~~~~~~~~

          Create a compelling interface and functionality for the IfcOpenShell WASM / pyodide module #99

          Outline
          http://wasm.ifcopenshell.org/

          Details
          Expected Outcome
          Future Possibilities
          Project Properties
          Skills
          Difficulty
          Size
          Additional Information
          Potential mentor(s): NAME
          Organization website: https://
          Communication channels: https://******.zulipchat.com


          ~~~~~~~~~~

          Authoring interface for IFC4.3 alignment geometry in Bonsai #98
          Outline
          Industry Foundation Classes (IFC) offer the ability for rich information exchanges between modeling, analysis, planning, and other software tools in the Architecture, Engineering, and Construction (AEC) industry. Specifically, the latest release of IFC (version 4.3, also referred to as IFC4X3) adds linear referencing via alignment modeling, which is core to describing the construction and maintenance of infrastructure assets such as roads, bridges, and railways.

          Details
          Alignment import (read) capabilities have been added to IfcOpenShell and the Bonsai add-in for Blender. They have reached a state of maturity such that the next logic step is to enable alignment authoring (write) capabilities.

          Expected Outcome
          Alignment authoring will take place in Blender via the Bonsai add-in. A user-focused workflow has been developed and documented, along with preliminary user interface mockups. This project would add alignment authoring capabilities via new panels and other items within Blender. The ifcopenshell.api namespace will also need to be enhanced incrementally to support the new user interface tools.

          Project Properties
          Skills
          Understanding and general working knowledge of python.

          Difficulty
          Medium

          Size
          Medium (175 h)
          The participant focuses on authoring horizontal alignments via the PI method. This could be via interactive icons or primarily through a table-based interface. The user would need to be able to add, edit, and remove PI (point of intersection) points. Additionally the user would need to be able to adjust the radius that corresponds to each PI point. Though not strictly required for this project, the authoring tool would also enable definition and editing of entry and exit transition curve type (clothoid, sine spiral, polynomial spiral, etc.) and length.

          Long (350 h)
          PI-based alignment would be added for vertical and cant as well. A basic corridor modeling UI tools would be implemented to allow for sweeping geometry (open or closed profile) along an alignment curve to generate 3D linear geometry via IfcSectionedSolidHorizontal and related IFC entities.

          Additional information
          Mentors: Rick Brice @RickBrice & Scott Lecher @civilx64

          Organization website: https://ifcopenshell.org

          Communication channels: https://github.com/IfcOpenShell/IfcOpenShell/discussions

          Technical resources:

          https://docs.bonsaibim.org/guides/development/index.html

          Blender 4.3: Precise Modeling for Architecture, Engineering, and 3D Printing

          Python Scripting in Blender


          ~~~~~~~~~~

          Manifoldness repair 
          Outline
          Repair triangle soup that are not manifold.

          Details
          Basically, a valid solid mesh should be both manifold and has no self-intersection. However, models from the internet may contain defects. This project is about coming up with an algorithm that converts and repair a triangle soup into a manifold mesh.

          This will contain a lot of heuristics, basically what we need is:

          Stitching faces together, and maybe join faces that are close enough.
          Fill holes.
          Duplicate vertices and edges such that the result is a manifold in terms of connectivity.
          Expected Outcome
          Implementation of said algorithm.

          Future Possibilities
          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): @elalish @pca006132
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions

          ~~~~~~~~~~

          Overlap removal
          Outline
          Remove overlaps in meshes that contain self-intersection, assuming the mesh is a manifold.

          Details
          Basically, a valid solid mesh should be both manifold and has no self-intersection. However, models from the internet may contain defects. This project is about coming up with an algorithm that removes self-intersections.

          See elalish/manifold#289 for details about ideas for the algorithm.

          Expected Outcome
          Implementation of said algorithm.

          Future Possibilities
          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): @elalish
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions

          ~~~~~~~~~~

          Creation of an IFC geometry library in IfcOpenShell that uses Manifold

          Outline
          For the past 10 years, IfcOpenShell has had a tight coupling with OpenCASCADE as its only geometry library and OCCT providing the datatypes in the IfcOpenShell C++ APIs.

          In IfcOpenShell v0.8 an additional abstraction is introduced over the geometric concepts in IFC (taxonomy.h) and the evaluation of such concepts using pre-existing geometry libraries (AbstractKernel).

          Also in v0.8, CGAL is introduced as an additional runtime selectable choice besides OpenCASCADE, because of (a) it's extensive set of modules for analysis (e.g convex decomposition, skeleton, ...) and (b) it's arbitrarily robust (and precise) implementation of boolean operations using Nef polyhedra on a number type represents a binary tree of operands taking part in the construction of that number.

          Both OpenCASCADE and CGAL are high quality efforts, but quite complex and resulting in fairly large compiled object sizes. This project proposal aims at introducing Manifold as a 3rd geometry library implementation. Manifold is modern, efficient and robust.

          https://github.com/elalish/manifold

          cc @elalish just fyi.

          Expected Outcome
          Another AbstractKernel implementation that uses Manifold to evaluate a small set of geometrical concepts (boolean, extrusion, brep for example) in IFC. Expecting reasonable outcomes on a small building model (such as the Duplex A model) without necessarily resolving all complexities and corner cases encountered in that model.

          Future Possibilities
          Comparison between implementations and development of a hybrid composition of these libraries that based on prior inspection picks the most suitable implementation for a specific IfcProduct or representation item. For example, OpenCASCADE will likely still excel at curved surfaces (e.g nurbs), but suffers a monumental performance overhead when ingesting detailed triangular meshes (that are also prevalent in IFC) due the overheads of it BRep data model.

          Additional Information
          Potential mentor(s): Thomas Krijnen (aothms)



          ~~~~~~~~~~

          Turn BlenderBIM into a client for remote BIM-collaboration on existing OpenCDE-API-server with a graph backend

          Outline
          The project aim is to turn BlenderBIM into a client for remote BIM-collaboration and a client for remote BIM-model-sharing through a Common Data Environment (a CDE working as BIM/IFC-server) using the already developed OpenCDE API server and the OpenCDE API specifications provided by buildingSMART: BCF API and Documents API.

          OpenCDE API:s are open standards. This project will hence enable usage of BlenderBIM as a client on other BIM-servers that implements the OpenCDE API:s.

          Details
          An OpenCDE API server that implements all buildingSMART OpenCDE API:s (BCF API, Documents API and Foundation API) has been developed in python and the FastAPI framework. Solibri Office was used as a client for testing this server software during development.

          The code of the OpenCDE server is located in the IfcOpenShell repository here: https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/opencdeserver

          The OpenCDE API:s is a set of open API-specifications provided by buildingSMART. https://github.com/buildingSMART/OpenCDE-API

          BIM Collaboration Format (BCF) API is used for collaboration on shared BIM models through a remote BCF-server. BCF API has the same purpose as BCF XML (which is a file format) but the difference is that the data is communicated as JSON through a BCF-server, instead of sending XML-files. https://github.com/buildingSMART/BCF-API
          Documents API is used communication between a client and a CDE (acc. ISO 19650-1). The purpose is a common data environement for sharing models, documents et.c. https://github.com/buildingSMART/documents-API
          Foundation API is used for authentication et.c. and must be implemented by any client or server that implements anyone of the other two OpenCDE API:s. https://github.com/buildingSMART/foundation-API
          To summarize: The model (the IFC data) will normally be shared to the server using Documents API, and downloaded form the server using Documents API. BCF API can be used for remote collaboration on the models located on the server et.c.

          The purpose of the open API specification is to enable independent development of clients and servers that can communication with eachother. A server has already been developed and shared as open source on IfcOpenShell. However, at the moment there is no open source client with a graphical user interface for the OpenCDE API:s. A python library for BCF API communication is available: https://pypi.org/project/bcf-client/.

          The aim of this project is to turn BlenderBIM into a OpenCDE-client (a client that already have BIM-capabilities) that can communicate remotely with (and make use of) the existing open source OpenCDE-server on: https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/opencdeserver

          An add-on for GIT-collaboration have already been developed:

          https://blenderbim.org/docs/users/git_support.html
          https://www.youtube.com/watch?v=cJZhSCSSWdA
          Collaboration using Documents API and BCF API is just another way of collaboration. This way of collaboration might be more suitable for AEC-professionals who does not have experience of GIT. Som features of GIT are not possible. But other features that are possible when using the OpenCDE API:s are not possible using GIT.

          Expected Outcome
          The BlenderBIM can connect as a client to the OpenCDE API server using the Foundation API
          User management functionality is added to OpenCDE API server: Register, invite, delete users
          BlenderBIM is a BCF API client - can collaborate on BIM-models remotely using the already developed OpenCDE-server
          BlenderBIM is a Documents API client - can share/download BIM-models remotely with the already developed OpenCDE-server
          User interface in BlenderBIM for setting up OpenCDE-server on localhost and user management (inviting collegues, adding/deleting users et.c.)
          User interface in BlenderBIM for remote BIM collaboration using BCF API
          User interface in BlenderBIM for remote model download and sharing using Documents API
          Simplify the process of turning your computer into an OpenCDE-server and inviting colleages to collaborate on your BIM model
          Simplify the process of deploying the OpenCDE-server as a BIM-server to the cloud for remote collaboration with BlenderBIM as a client
          Extra: Implement som of the routes/endpoints of the OpenCDE API specifications that Solibri Office does not support. I.e. implement the full official buildingSMART open specifications using the open source server (OpenCDE API server) and open source client (BlenderBIM).

          Future Possibilities
          IFC-server capabilities: Round-tripping of IFC data between IFC STEP (or python or C++ objects in IfcOpenShell) and the OpenCDE API server graph DB. More info on storing IFC data as label property graph (LPG) here: https://www.sciencedirect.com/science/article/pii/S0926580523000389
          Potential synergies with the other GSoC project "Web-based UI integration with Blender" Web-based UI integration with Blender #87 because the Web-based UI could be hosted by the same OpenCDE API server as in this project.
          Visualization of IFC data as graph in that Web-based UI. For example using pyviz or similar tools.
          Project Properties
          Skills
          Python, including how to setup a minimal basic server on localhost using FastAPI. https://fastapi.tiangolo.com/
          Blender Python API to develop user interfaces in BlenderBIM.
          Cypher query language and any graph DB that implements Cypher (such as Neo4j or MemGraph). https://neo4j.com/docs/cypher-manual/current/introduction/
          API development.
          Difficulty
          Hard

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): [Martin Wiss]
          Organization website: https://blenderbim.org/ http://ifcopenshell.org/
          Communication channels: OSArch

          ~~~~~~~~~~

          Geometry Verification and Validation GUI in Qt (AI Project)

          Outline
          Help develop a new GUI application that checks geometry for common issues and/or helps fix them.

          Details
          A new GUI is in prototype development (built on Arbalest) that checks geometry files for common verification and validation (V&V) issues such as topology errors, solidity errors, and more. It's very much an experimental work in progress and we'd like your help to make it complete. The overarching goal of this effort is to extend our prototype in a significant way, either improving usability, checking for more issues, improving the Qt GUI infrastructure integration, integrating workflow(s) for review and repair, or leveraging AI to identify and/or fix issues.

          Expected Outcome
          You will propose a complete project description that identifies the specific objectives you'll aim to achieve. It's expected that you'll leverage the previous work (talk with us to get access to those materials). The proposal should identify 3-10 primary objectives that are researched and specific, starting with our previous effort.

          We essentially want a tool that "compiles" geometry reporting warnings and errors for issues encountered, akin to compiling source code in Visual Studio or Eclipse. There are questions of application architecture to resolve (e.g., whether to extend 'arbalest', integrate 'qged', integrate 'gist', etc). We want the tool to be graphical and interactive. We want it to have the ability to generate reports for auditing. Some of those capabilities exist in isolation, but none exist as a tool tailor-made for 3D geometry V&V.

          Future Possibilities
          This is a long term priority project with future possibilities in:

          GUI infrastructure
          AI integration
          geometry healing and repair workflows
          geometry auditing
          geometry standards development
          Skills
          Qt, C/C++

          Difficulty
          Easy or Medium depending on the objectives

          Size
          long (350h) preferred, but medium (175h) also possible

          Additional Information
          Potential mentor(s): Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com
          https://brlcad.org/design/v&v/


          ~~~~~~~~~~

          Blender UI / integration with voxelisation toolkit software

          Outline
          There is software known as the Voxelisation Toolkit (pip install voxec). It converts the 3D model into voxels (e.g. 3D cubes that represent geometry), analyses and transforms those voxels, and outputs statistics (e.g. distance between voxels, etc).

          Image

          Image

          Voxels are super cool and can be used to calculate head heights, resolve complex non-manifold geometry, egress distances, or concrete formwork areas and strutting distances, and air volume for mechanical calculations. All of this stuff is useful for engineers and construction professionals.

          This project is to add a UI in Blender to start making this general purpose analysis tool available to non-programmers.

          Details
          You will be expected to design an interface for the voxelisation toolkit, prepack some simple recipes, and write scripts that take the output (currently visualised as images or plots) and instead visualise the results in 3D by generating 3D coloured meshes that represent the output.

          Expected Outcome
          Bundle the voxelisation toolkit software with Blender.
          A UI to execute the voxelisation toolkit.
          Simple presets to run the toolkit.
          Visualise the output of the voxelisation analysis as a 3D coloured mesh.
          Future Possibilities
          Bundle scripts for common usecases, like formwork calculation, air volume calculation, or external / internal metadata addition.

          Project Properties
          Skills
          Python
          Difficulty
          Medium

          Size
          Medium to Long

          Additional Information
          Potential mentor(s): Dion Moult, Thomas Krijnen
          Organization website: https://blenderbim.org http://ifcopenshell.org
          Communication channels: https://osarch.org/chat


          ~~~~~~~~~~

          Features for CG artists to visualise beautiful IFC models in Blender 
          Outline
          The architecture, engineering, and construction industry creates 3D models of buildings. These models are generally quite poor and do not contain any textures, lighting, or high quality objects that are suitable for 3D rendering. They often hire artists to help create beautiful renders of their designs.

          This project will build utility functions and workflows to easily get beautiful pictures of 3D models.

          Details
          3D artists typically do the following steps to make a 3D model look beautiful. They:

          Set camera angles with specific camera settings, with "clay" (e.g. all white colours) materials.
          Add lights and sun / sky settings.
          Add simple colours and textures.
          Remodel low quality geometry
          Add new objects (e.g. entourage) to decorate the scene, like trees, grass, people, extra furniture, walruses, shrimp, etc.
          Set common compositing and post processing rules
          You will use the Blender Python API to set simple presets for most of these steps to allow less skilled artists to quickly setup renders. You will also setup a workflow to guide artists on how to organise their files relative to the IFC model and keep the IFC model separate so that when the IFC model is changed, the artists doesn't need to start from scratch or play spot the difference.

          You do not need to be an expert in 3D modeling or CG visualisation or rendering. You will be taught what type of settings and options are appropriate for presets and the details of the workflow. However, you will be expected to automate that detail (every aspect of the Blender settings can be set using Python trivially).

          You will also be expected to create a Blender interface to interact with the settings, e.g. a button to add camera, a button to set a preset sky, etc.

          Expected Outcome
          Note: scope is flexible and you may achieve less or more or different to the below:

          A graphical interface in Blender that relate to the 6 steps above
          Buttons to add cameras, set common camera aspect ratios and settings. Buttons to add common types of lights, set sun angles and sky settings with bundled HDRI textures.
          Buttons to add simple material presets.
          Buttons to mark an object to be replaced by another
          A few preset assets using Blender's built in asset tools to drag and drop in entourage.
          Future Possibilities
          Project Properties
          Skills
          Python (definitely required!)
          Artistic sense (do you like 3D graphics? rendering?) If you have ever rendered a 3D scene before, this is the project for you!
          Difficulty
          Easy to Medium

          Size
          Medium to Long

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://blenderbim.org http://ifcopenshell.org
          Communication channels: https://osarch.org/chat

          ~~~~~~~~~~

          Implement 3D mesh offset
          Outline
          Implement efficient 3D mesh offset, instead of using minkowski sum with high resolution spheres. (elalish/manifold#192)

          Details
          3D mesh offset is a useful feature that many users asked for, but is difficult to implement efficiently. Many users use minkowski sum with sphere to perform positive offset, but this can be very slow due to the need for exact convex decomposition.

          Our approach will only work for positive offset, negative offset can be implemented by performing additional mesh boolean operations, so this is not an issue. The approach has four phases:

          Figure out all pairs of faces that do not share any vertex and may overlap after offsetting. (let's call them conflict pairs)
          Cut the mesh in a way such that for each part, no two faces are in the same conflict pair. (decomposition step, requires monte carlo tree search)
          Perform the positive offset on each part, using a modified algorithm from Offset Triangular Mesh Using the Multiple Normal Vectors of a Vertex. Note that we need to figure out how to blend the surfaces for smooth results.
          Union the parts.
          Expected Outcome
          A fast 3D mesh decomposition algorithm!

          Project Properties
          Skills
          C++
          Graph data structure.
          Algorithms.
          Difficulty
          Hard.
          Size
          Long.
          Additional Information
          Potential mentor(s): @elalish @pca006132 @zalo
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions


          ~~~~~~~~~~

          Add fuzzing tests

          Outline
          Add more fuzzing tests for both 2D and 3D operations.

          Details
          Fuzzing is an effective technique to expose bugs in software. Fuzzing tests randomly generate structured inputs (according to specification), and test if the program crashes/failed assertions.

          This project aims to test 2D and 3D CSG operations on geometrically valid polygons/meshes. To do this, we will define a very simple AST for our CSG operations, and use the recursive domain feature of fuzztest for the tests.

          We will also randomly apply slight perturbation to make the valid geometry only epsilon-valid, to test for robustness of the algorithm.

          Expected Outcome
          Fuzz tests that test for union, intersection, difference, 2D extrude/revolve, etc.

          Project Properties
          Skills
          C++
          Basic understanding of graph data structure.
          Difficulty
          Medium
          Size
          Medium
          Additional Information
          Potential mentor(s): @pca006132
          Organization website: https://manifoldcad.org/
          Communication channels: https://github.com/elalish/manifold/discussions


          ~~~~~~~~~~

          Physically-Based Rendering (PBR) advanced shaders
          Outline
          Get BRL-CAD physically-based rendering working with advanced shaders.

          Details
          BRL-CAD recently integrated with Appleseed which provides physically-based rendering. It's presently a command-line renderer called 'art'. For art rendering to work, a shader and colors are specified on geometry. BRL-CAD has preliminary support for material objects including OSL shaders and MaterialX shaders in art, however their support has only been tested with basic shaders such as the Disney Principled Shader. It's hard-wired to single-file shaders.

          Expected Outcome
          The goal of this task will be to make art read and work with any OSL or MaterialX shader networks, including ones using texturing, emission, subsurface scattering, etc. applied to BRL-CAD geometry.

          Project Properties
          Skills
          Decent C/C++ skills
          Some basic familiarity with PBR.
          Basic familiarity with shaders.
          Difficulty
          medium

          Size
          long

          Additional Information
          Potential mentor(s): Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          Improve FreeCAD Hidden Line Removal
          Outline
          FreeCAD's Technical Drawing module (TechDraw) relies heavily on the OpenCascade Hidden Line Removal algorithms. These algorithms can be very slow, do not provide progress reporting and do not provide any linkage between the input shape and the output.

          Details
          The TechDraw module provides projections, section views and detail views of 3D model components and assemblies developed in FreeCAD modules such as Part, PartDesign and Draft.

          Expected Outcome
          a) develop new code for projecting shapes and creating the geometry for technical drawings.
          -or-
          b) modify the existing OpenCascade code as an enhancement.

          Project Properties
          Both OpenCascade and TechDraw are written in C++.

          Skills
          The student should have a good knowledge of C++ and be familar with graphics topics such as the painters algorithm, face detection and hidden line removal.
          Knowledge of technical drawing standards and previous exposure to Qt will be helpful. Familiarity with OpenCascade is a definite plus.

          Difficulty
          Hard

          Size
          long

          Additional Information
          Potential mentor(s): wandererfan
          Organization website: https://freecadweb.org
          Communication channels: https://forum.freecadweb.org


          ~~~~~~~~~~

          Continuation of a prior BRL-CAD GSoC effort
          Outline
          BRL-CAD has been participating in GSoC for over 10 years with nearly 100 students! Any past accepted projects can be submitted as a continuation project.

          Details
          You can find all past participants documented on BRL-CAD's wiki by selecting a given year (e.g., 2018). Even the most successful and completely integrated projects have room for improvement! If any of those past efforts for any prior year sound very interesting to you, you can propose a continuation effort for it.

          Of course, you will need to research the prior effort to determine the status of the work, whether code was integrated or is sitting pending integration in a patch, whether it's functional or was in an intermediate state, etc. You'll also want to come chat with us on Zulip to make sure there is mentoring support for it, but there usually is if you're passionate and independently productive.

          For your proposal, note that it's a continuation effort. Explain what you are doing and how it relates to the prior effort. It's strongly recommended that your development plan focus on production-quality integration aspects such as making sure there are no usability or user experience (UX) issues, no build integration issues, that testing is covered adequately, and with focus on UX.

          Expected Outcome
          The expected outcome of a continuation effort is new capability and features that are "complete", integrated, bug-free, and issue-free, in the hands of users. This means your project covers all vertical integration aspects of development integration including build system and usability / UX concerns. Not prototyped. Not simply rewritten or re-attempted.

          If the prior effort was integrated, your outcome will be specific polish, adaptiveness, and robustness improvements.

          If the prior effort was not integrated, your outcome will be issue-free integration that addresses prior issues preventing integration (which will require research and understanding on your part).

          Project Properties
          Skills
          This varies greatly by continuation. There are continuation projects for C/C++, Python, Javascript/Node.js, Tcl/Tk, OpenCL, OpenGL, Qt, GPGPU, and more.

          Difficulty
          Varies.

          Size
          You are welcome to scope your project medium (175h) or long (350h) depending on the objectives and development scope.

          Additional Information
          Potential mentor(s): Morrison (contact devs@brlcad.org)
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Implement AP242 STEP geometry importer for BRL-CAD
          Outline
          Implement a geometry importer for the ISO 10303 STEP AP242 standard.

          Details
          BRL-CAD has geometry import support for STEP AP203 (v1), but AP242 has emerged as its industry replacement. This project entails implementing as comprehensive import support as possible in BRL-CAD.

          In order to track implementation progress and manage development risk, you will need to track implementation coverage by setting up a dashboard similar to what is used by the CAx-IF -- it can be a simple text file or web page.

          Existing conversion support can be examined for AP203 and other formats in BRL-CAD's repository under src/conv/step

          Expected Outcome
          New AP242 importer that converts STEP entities into BRL-CAD's .g geometry file format.

          Future Possibilities
          AP242 export support...

          Project Properties
          Skills
          C/C++
          STEPcode

          Difficulty
          Hard.

          Size
          This project can be scoped medium (175h) or long (350h) depending on your familiarity and expertise, or you can propose a subset of entities in a shorter timeframe (note though that advanced boundary representation entities should be prioritized).

          Additional Information
          Potential mentor(s): Morrison (contact devs@brlcad.org)
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          BRL-CAD Python bindings
          Outline
          Implement bindings for the BRL-CAD functionality to Python programming language

          Details
          There are long time on-going efforts to wrap BRL-CAD functionality with python code, e.g.

          https://github.com/kanzure/python-brlcad
          https://github.com/nmz787/python-brlcad-tcl
          These projects are however still in early development stages.

          Expected Outcome
          A Python module which can read and write BRL-CAD databases, and provide access to their contents to read, create, and modify the objects.

          Project Properties
          Skills
          C/C++
          Python
          Difficulty
          This project may be of easy or medium difficulty, depending on your familiarity and expertise.

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Daniel Rossberg
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          Webapp to create and check BIM project exchange requirements for IfcOpenShell
          Outline
          When projects exchange data, we often need to set contractual requirements about what data we expect to see in their CAD or Building information data. The is an international standard for describing project requirements in XML called the Information Delivery Specifications (IDS).

          There is a half-built webapp which allows viewing and minor editing of IDS files here: https://blenderbim.org/ifctester/

          Your job would be to finish this web app, building features for more editing, drag and drop from a library of specifications, adding and removing requirements, etc.

          Expected Outcome
          A working example of the web application.

          Project Properties
          Skills
          HTML, CSS, and vanilla Javascript (i.e. no frameworks).
          Difficulty
          Easy

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://ifcopenshell.org
          Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues


          ~~~~~~~~~~

          Scripts for generating simple animations (e.g. appear / disappear, bounce, appear left to right, fade in from above, etc)

          Outline
          Often, construction firms need to visualise animations of construction sequencing. A project timeline will be created, and related to individual model elements. For example, when a concrete slab is poured, it is linked to a 3D object called a slab. We need the ability to automatically generate animations from Blender where objects appear / disappear in various different ways when they start / end their task in the project timeline. The systems for describing project timelines is already in place, so now we need a little animation generator!

          Details
          Expected Outcome
          A series of small scripts that take objects and can automatically animate the visibility, locations, or staggered appearances of building elements, as well as sub elements, and basic scripts that correlate real world time to animation frames, and frames per second, and generate an animated timeline bar in various styles.

          Future Possibilities
          This animation system can be then used from BIM models either in Blender, FreeCAD, or via other software altogether, so it has quite a large impact on the ecosystem.

          Skills
          Basic knowledge of the principles of animation (keyframing)
          Basic Blender animation (you can do some tutorials and get up to speed pretty quick)
          Python
          Artistic sense! We should offer beautiful and elegant animations!
          Difficulty
          Easy

          Additional Information
          Potential mentor(s): Dion Moult
          Organization website: https://ifcopenshell.org
          Communication channels: https://community.osarch.org , ##architect on Freenode , and https://github.com/IfcOpenShell/IfcOpenShell/issues


          ~~~~~~~~~~

          NURBS Editing Support in BRL-CAD

          Outline
          Implement the prerequisites for NURBS editing in BRL-CAD's GUIs

          Details
          BRL-CAD has support for raytracing of NURBS surfaces implemented, but they are handed over as BLOBs to the openNURBS library. Beyond basic operations such as rotation and translation, the BRL-CAD core has no ability to edit them. This project would implement support for editing NURBS curves and surfaces in the BRL-CAD core, thus creating the prerequisites to handle them with higher level (i.e. GUI) tools.

          See this task's description in former GSoCs for some more information: https://brlcad.org/wiki/NURBS_Editing_Support

          The key-feature would be to have ged command(s) that lets you build NURBS objects from scratch. This could be done by having a declarative ASCII description of these entities and/or wrapping the openNURBS library by a scripting language.

          Describe in your proposal which approach you want to use and why. You may let inspire you by solutions in other programs:

          NURBS-Python: https://github.com/orbingol/NURBS-Python
          Blender: https://blender.stackexchange.com/questions/7020/create-nurbs-surface-with-python
          Web3D: https://www.web3d.org/x3d/content/examples/Basic/NURBS/
          3DSMax/Maya: https://help.autodesk.com/view/3DSMAX/2016/ENU/?guid=__files_GUID_75CD4DE9_8024_4E25_B147_0A0EC8B10031_htm
          Ayam: http://ayam.sourceforge.net/docsdraft/ayam-6.html
          Expected Outcome
          Implementing the necessary logic for NURBS handling in librt, libbrep, and libged

          Future Possibilities
          Implementing a visual NURBS editor in a BRL-CAD GUI (mged, Archer, Arbalest)

          Project Properties
          Skills
          C/C++
          Difficulty
          medium

          Size
          long (350h)

          Additional Information
          Potential mentor(s): Daniel Rossberg, Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com


          ~~~~~~~~~~

          New BRL-CAD GUI

          Outline
          Develop further the new GUI for BRL-CAD!

          Details
          BRL-CAD has two main graphical applications called 'mged' and 'archer' which look like they were developed in the 80's and 90's respectively (because they were). We need a modern GUI, ideally using Qt.

          This new GUI will need to leverage our existing libraries in a big way. This includes the C++ coreInterface ( see https://brlcad.org/wiki/Object-oriented_interfaces) or its successor MOOSE (see https://github.com/BRL-CAD/MOOSE) and LIBGED (see src/libged). The latter is basically all commands available to both mged and archer.

          During past GSoCs an amazing start was made with arbalest. Based on this, the development of a GUI called 'qged' (see src/qged) was started, which you should include in your considerations too. This program implements the traditional BRL-CAD workflow under a modern Qt-based user interface.

          You may propose a complete different approach, but we recommend to use arbalest as starting point for your work. Which additions would you like to program in this years GSoC? You can use the results of the former prototype CAD GUI Google Code-in tasks (http://brlcad.org/gci/data/uncategorized/, search for CAD_GUI there) for inspiration.

          Keep your proposal lean and simple. The main emphasis should be on adding features and/or improvements to our next generation GUI.

          Expected Outcome
          An improved BRL-CAD GUI.

          Project Properties
          Skills
          C/C++
          Qt
          Difficulty
          medium

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Daniel Rossberg
          Himanshu Sekhar Nayak
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Online Geometry Viewer (OGV)

          Outline
          Write a proposal that leverages rewrite of the existing application in the latest tech stack for frontend and backend.

          Details
          We have been working on OGV for over many years. It started with PHP and then was revamped to meteor.js. We want to focus on the backend of OGV, making sure it works properly, converts the models properly, and basically finish a 1.0 version of OGV so we can launch it for the masses! For that, we are planning to change the legacy backend to be rewritten along with the frontend.

          You can use any tech stack (react, vue) for frontend and node, C/C++ for the backend. We faced some problems like removing certain deprecated dependencies and adding new features with Meteor. We are planning to port the application and add all specified features.

          Possible New Features
          Integrating BRL-CAD GCV(Geometry Conversion Vocabulary) to add support for more file formats like .stl, .obj, and .3dm.
          Automated Conversion for Web Display. Convert uploaded files automatically into polygonal formats for web visualization. Ensure smooth rendering and compatibility with web-based 3D viewers.
          Implementing a Model Repository based project architecture for storing and downloading 3d models.
          Conduct a full STIG compliance audit (Security Technical Implementation Guide), which involves ~200+ security checks.
          Run security scans using OWASP and Dependency-Check, addressing any reported vulnerabilities.
          However, you don't have to limit yourself to those ideas.

          Checklist to write proposal for OGV
          Download and clone OGV from https://github.com/BRL-CAD/OGV-meteor
          Setup and Run OGV on your local machine.
          Fork OGV repo
          Understand the flow of existing application
          Talk to mentors
          Choose list of issues that you would like to solve this summer
          Make a detailed weekly implementation plan
          Share your proposal with your mentors
          Submit it to the GSoC website
          Expected Outcome
          You're expected to propose an outcome useful to end-users. That is a broad range of possibilities that will depend on your interests and experience level. For example, you might propose focusing on the backend conversion to triangles for display (C/C++/Node.js). Or you might propose changing the backend to NURBS surfaces (C/C++) and using verbnurb or three.js (Javascript) to display them instead of triangles. Or you might propose keeping the backend the way it is and focus on front-end robustness (Vue, React), website features, or deployment infrastructure. You hopefully get the idea.

          Project Properties
          Skills
          JavaScript (Vue, React)
          Node.js (required)
          C/C++ (optional)
          Verbnurb (optional)
          Three.js (optional)
          Difficulty
          Hard

          Size
          This project can be scoped medium (175h) or long (350h), depending on the amount of functionality you want to include.

          Additional Information
          Potential mentor(s):
          Amanjot Singh
          Daniel Rossberg
          Divyanshu Garg
          Sean Morrison
          Organization website: https://brlcad.org
          Communication channels: https://brlcad.zulipchat.com

          ~~~~~~~~~~

          Add OpenSCAD support for exporting models in STEP format
          Outline
          The STEP format is widely used in the industry to transfer CAD data between different systems. Currently OpenSCAD does not support STEP import or export. Adding STEP export would open up a number of new usecases or simplify the workflow as no external conversion tools are needed to convert to STEP. This includes the design of 3D models for other CAD tools, e.g. for KiCAD where STEP models are used to render 3D representations of PCBs. Other use cases are for manufacturing where sometimes only STEP files are accepted as input, e.g. for CNC milling services.

          Details
          The main focus of this project is to get the ground work done for exporting more detailed models, as opposed to just exporting the fully rendered single mesh which is the normal case right now.

          Topics that need to be solved

          Research options of usable libraries
          Investigate what type of STEP files are accepted as input by various tools
          Select library and integrate into OpenSCAD
          Implement base functionality to export single meshes
          Add test cases to verify the new export functionality
          Update build system to include the new library into installers
          Prototype how more advanced models can be exported
          Expected Outcome
          OpenSCAD supports exporting single meshes as STEP
          (optional) Understanding/Plan of how to support additional features supported by STEP
          Project Properties
          Skills
          Programming language is C++
          Understand and use APIs from external libraries
          Integrate new libraries into the build system for the 3 supported platforms
          Add test cases with files using the new features to allow regression testing
          Difficulty
          Hard

          Size
          Long (350h)

          Additional Information
          Potential mentor(s): Marius Kintel (IRC: kintel), Torsten Paul (IRC: teepee)
          Organization website: https://www.openscad.org/
          Known libraries:

          StepCode - http://stepcode.org/ (https://github.com/stepcode/stepcode)
          OpenCASCADE - https://www.opencascade.com/


          
    totalCharacters_of_ideas_content_parent: 51942
    totalwords_of_ideas_content_parent: 12320
    totalTokenCount_of_ideas_content_parent: 10905
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/brl-cad/
    idea_list_url: https://github.com/opencax/GSoC/issues?q=is%3Aissue+is%3Aopen+label%3A%22GSoC+2025%22



  - organization_id: 13
    organization_name: BeagleBoard.org
    no_of_ideas: 5
    ideas_content: >-
        Deep Learning 
        Medium complexity 175 hours

        A Conversational AI Assistant for BeagleBoard using RAG and Fine-tuning
        BeagleBoard currently lacks an AI-powered assistant to help users troubleshoot errors. This project aims to address that need while also streamlining the onboarding process for new contributors, enabling them to get started more quickly.

        Goal: Develop a domain-specific chatbot for BeagleBoard using a combination of RAG and fine-tuning of an open-source LLM (like Llama 3, Mixtral, or Gemma). This chatbot will assist users with troubleshooting, provide information about BeagleBoard products, and streamline the onboarding process for new contributors.
        Hardware Skills: Ability to test applications on BeagleBone AI-64/BeagleY-AI and optimize for performance using quantization techniques.
        Software Skills: Python, RAG, Scraping techniques, Fine tuning LLMs, Gradio, Hugging Face Inference Endpoints, NLTK/spaCy, Git
        Possible Mentors: Aryan Nanda

        ~~~~~~~~~~



        Linux kernel improvements
         Medium complexity 350 hours

        Update beagle-tester for mainline testing
        Utilize the beagle-tester application and Buildroot along with device-tree and udev symlink concepts within the OpenBeagle continuous integration server context to create a regression test suite for the Linux kernel and device-tree overlays on various Beagle computers.

        Goal: Execution on Beagle test farm with over 30 mikroBUS boards testing all mikroBUS enabled cape interfaces (PWM, ADC, UART, I2C, SPI, GPIO and interrupt) performing weekly mainline Linux regression verification
        Hardware Skills: basic wiring, embedded serial interfaces
        Software Skills: device-tree, Linux, C, OpenBeagle CI, Buildroot
        Possible Mentors: Deepak Khatri, Anuj Deshpande, Dhruva Gole

        ~~~~~~~~~~

        Linux kernel improvements
         Medium complexity 175 hours

        Upstream wpanusb and bcfserial
        These are the drivers that are used to enable Linux to use a BeagleConnect Freedom as a SubGHz IEEE802.15.4 radio (gateway). They need to be part of upstream Linux to simplify on-going support. There are several gaps that are known before they are acceptable upstream.

        Goal: Add functional gaps, submit upstream patches for these drivers and respond to feedback
        Hardware Skills: wireless communications
        Software Skills: C, Linux
        Possible Mentors: Ayush Singh, Jason Kridner

        ~~~~~~~~~~

        Automation and industrial I/O Medium complexity 175 hours

        librobotcontrol support for newer boards
        Preliminary librobotcontrol support for BeagleBone AI, BeagleBone AI-64 and BeagleV-Fire has been drafted, but it needs to be cleaned up. We can also work on support for Raspberry Pi if UCSD releases their Hat for it.

        Goal: Update librobotcontrol for Robotics Cape on BeagleBone AI, BeagleBone AI-64 and BeagleV-Fire
        Hardware Skills: basic wiring, motors
        Software Skills: C, Linux
        Possible Mentors: Deepak Khatri, Jason Kridner

        ~~~~~~~~~~

        RTOS/microkernel imporvements
        Medium complexity 350 hours

        Upstream Zephyr Support on BBAI-64 R5
        Incorporating Zephyr RTOS support onto the Cortex-R5 cores of the TDA4VM SoC along with Linux operation on the A72 core. The objective is to harness the combined capabilities of both systems to support BeagleBone AI-64.

        Goal: submit upstream patches to support BeagleBone AI-64 and respond to feedback
        Hardware Skills: Familiarity with ARM Cortex R5
        Software Skills: C, RTOS
        Possible Mentors: Dhruva Gole, Nishanth Menon
        Upstream Repository: The primary repository for Zephyr Project

          
    totalCharacters_of_ideas_content_parent: 3803
    totalwords_of_ideas_content_parent: 786
    totalTokenCount_of_ideas_content_parent: 816
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/beagleboard.org/
    idea_list_url:  https://gsoc.beagleboard.io/ideas/


  - organization_id: 14
    organization_name: Blender
    no_of_ideas: 15
    ideas_content: >-
        Flamenco¶
        Flamenco is Blender's render farm management system. For all the ideas below, a requirement is that you've set up Flamenco for yourself and used it to render things. Of course also some experience with Blender itself is needed.

        Statistics¶
        Description: Design & build a system for Flamenco to collect and display statistics.
        The goal is to show per-job, per-task, and per-worker statistics.
        This should make it possible for users to predict how long a render job will be, as they can look up things like render times of similar render jobs.
        The underlying design should be generic enough to store all kinds of statistics and events, not specific to Blender render times only.
        This project would include the technical design of this feature, the frontend / UI / UX design, and the implementation of both front- and back-end.
        Optional: record more statistics, such as per-frame render time, memory usage, etc.
        Optional: show progress of jobs in the jobs list.
        Expected outcomes: A way for users to get a better understanding of how a new render job will behave, as they can look up information about past & currently running jobs.
        Skills required: Familiarity with Go and unit testing. Familiarity with web languages (HTML/CSS/JavaScript, VueJS).
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        RNA Overrides¶
        Description: Render jobs should be able to specify "RNA overrides" (#101569). In other words, the job definition should be able to include some Python code that sets certain properties in Blender to certain values.
        Design a way to include this in a job definition, and how it affects tasks & commands.
        This could include the ability to add new RNA overrides to existing jobs.
        This should include the ability to update those override values via the web interface.
        Design how such additions / changes affect already-created tasks/commands in the database. Or a way to make this work without changing things in the database?
        This can be used both when a user wants to change something (like re-rendering with increased sample count), or for Flamenco itself to adjust things in the blend file without having to save those values in the blend file itself (#104264)
        Expected outcomes: Give users a simpler way to configure Flamenco for their needs.
        Skills required: Familiarity with Blender (for making the RNA overrides themselves work). Familiarity with Go and unit testing for adjusting Flamenco. Familiarity with web languages (HTML/CSS/JavaScript, VueJS) for the web frontend.
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        Configuration Web Interface¶
        Description: Flamenco's configuration file can be created via its Setup Assistant, but that's only for the initial configuration. For managing more complex things, like the two-way variables for cross-platform support, users still have to manually edit the YAML file. This project is about introducing a configuration editor in the web frontend, and potentially new backend API functions to support that.
        New tab in the frontend for managing the configuration.
        A way to retrieve and visualise the configuration.
        New front-end widgets to represent these, including more complex cases like one-way and two-way variables.
        A way to save the edited configuration.
        Optional: A validator for the configuration options, so that changes can be checked before they take hold.
        Optional: A way for Flamenco Manager to load and apply the configuration without restarting the process.
        Optional: A custom job type for validating configuration paths, so that, for example, a macOS path can be actually checked on a Worker running macOS.
        Expected outcomes: Give users a simpler way to configure Flamenco for their needs.
        Skills required: Familiarity with web languages (HTML/CSS/JavaScript, VueJS, OpenAPI). Potentially also familiarity with Go in case of backend work.
        Possible mentors: Sybren Stüvel.
        Expected project size: 350 hours
        Difficulty: hard

        ~~~~~~~~~~
        Polish & Shine¶
        Description: Fix various issues & implement missing pieces to solve common bumps in the road.
        Reconsider some design aspects of the web frontend, so that it works better on narrower / smaller screens.
        Allow finishing setup assistant without Blender on Manager (#100195).
        Introduce per-Worker logs on the Manager, for introspection and debugging.
        Fix issues with two-way variables (#104336, #104293)
        Add a web interface for the mass deletion of jobs. There is already an API call for this, which deletes all jobs older than a certain timestamp.
        Other issues from the tracker.
        Expected outcomes: Improve the overall experience people have when working with Flamenco.
        Skills required: Familiarity with web languages (HTML/CSS/JavaScript, VueJS) for front-end work. Familiarity with Go and OpenAPI for backend work.
        Possible mentors: Sybren Stüvel.
        Expected project size: 175 hours
        Difficulty: medium

        ~~~~~~~~~~
        Geometry Nodes¶
        Regression Testing¶
        Description: As people build more assets on top of Geometry Nodes, it becomes more and more important to ensure good backwards compatibility. This project focuses on improving our regression tests to cover more issues as early as possible. This involves:
        Adding new tests in our existing test framework.
        Extending the test framework to cover node tools, baking and maybe other areas we still have to find.
        Preparing more complex production files for use in regression tests.
        Expected outcomes: Improved stability of Geometry Nodes.
        Skills required: Good in C/C++ and Python.
        Possible mentors: Jacques Lucke, Hans Goudey
        Expected project size: 90 or 175 hours depending on how many of the mentioned areas are covered
        Difficulty: Easy (using existing framework) and Medium (extending framework)

        ~~~~~~~~~~
        Modeling¶
        Improve Edit-Mesh Mirror¶
        Description: Blender's mesh mirroring in mesh edit-mode works for basic transformations, but does not work for most other operations such as sliding, smoothing, marking seams, etc. In practice, this makes the edit-mode mirror only useful in very specific circumstances and not for general modeling.

        While supporting every operation isn't practical, enabling it for a subset of operators such as those that only adjust existing geometry (rather than adding or removing geometry) would be immediately useful for artists.

        Expected outcomes: Improved edit-mesh mirror support for existing tools.
        Skills required: Proficient in C/C++.
        Possible mentors: Campbell Barton
        Expected project size: 175 or 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        Sculpt & Paint¶
        Mesh Sculpting Performance Improvements¶
        Description: Last year's sculpting rewrite project gave a large improvement in performance, but the team didn't have the time to pursue everything. This task lists possible future improvements. This GSoC project would explore one or more of those ideas with in depth performance testing and experimentation.
        Expected outcomes: More interactive sculping with large meshes
        Skills required: Proficient in C++, familiarity with data-oriented-design.
        Possible mentors: Hans Goudey
        Expected project size: 175 or 350 hours
        Difficulty: medium or hard
        
        ~~~~~~~~~~
        VFX & Video¶
        Hardware accelerated video encoding/decoding¶
        Description: Currently Blender encodes and decodes video though ffmpeg C libraries, on the CPU. ffmpeg also has support for hardware video processing (various kinds depending on platform), this project would enable usage of that.
        Build ffmpeg with hardware video processing support included.
        Note: Blender can't include "non-free" ffmpeg libraries (which means cuda_nvcc, cuda_sdk, libnpp can't be used).
        On Blender's video decoding and encoding side, implement code that would use any relevant ffmpeg C libraries parts for hardware video processing, when supported.
        Decide which additional UI settings need to be exposed to the user, to control hardware video processing.
        Implement code needed to transfer video frames between hardware memory and CPU memory as needed (the rest of VSE processing pipeline is purely on CPU currently).
        Expected outcomes: Video encoding or decoding is more efficient by using dedicated hardware.
        Skills required: Proficient in C/C++, familiarity with ffmpeg.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        High Dynamic Range (HDR) support for video¶
        Description: This project has several partially dependent parts that are all about HDR support within VSE:
        Make Sequencer preview window be able to display HDR content on a capable display (like 3D viewport or Image window can).
        Make blender movie reading code be able to decode HDR videos into proper scene-linear or sequencer color space as needed. HDR video data might be PQ or HLG encoded, and this might need special decoding into destination color space.
        Make blender movie writing code be able to encode HDR videos. Blender already can encode 10/12 bit videos, but only for regular LDR. Additional PQ or HLG data encoding and necessary video metadata is not currently done.
        Expected outcomes: HDR video handling is improved within Blender.
        Skills required: Proficient in C/C++, familiarity with ffmpeg, knowledge of color spaces and color science.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: OpenTimelineIO support¶
        Description: built-in support for OpenTimelineIO import/export within Blender VSE. Blender Studio has experimented with it in 2021, by using and extending a 3rd party addon vse_io. It might be useful to have built-in support for this.
        Expected outcomes: Blender VSE can import and export .otio files.
        Skills required: Proficient in C/C++, familiarity with video editing workflows.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Pitch correction for sound playback¶
        Description: Currently when audio is retimed, the pitch changes, would be nice to have an option to preserve pitch. Different approaches could be researched and implemented (e.g. pitch correction for mostly human speech might be different from pitch correction of music). Might need integration of some 3rd party library if it is suitable for the task, or implementing the correction algorithms manually.
        Expected outcomes: Retimed sound playback has options to preserve original pitch.
        Skills required: Proficient in C/C++, sound processing algorithms.
        Possible mentors: Aras Pranckevicius
        Expected project size: 350 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Animation retiming¶
        Description: Modify animation of strips, when changing their playback speed. Retiming allows changing playback speed of strips, but when strips are animated, the animation keys are fixed in position. These could be moved, such that animation is seemingly mapped to frames of the content.
        Expected outcomes: Animation proportionally is scaled with strip when retiming.
        Skills required: Proficient in C++.
        Possible mentors: Richard Antalik
        Expected project size: 175 hours
        Difficulty: medium

        ~~~~~~~~~~
        VSE: Keyframing in preview¶
        Description: Open workflow quick animation in VSE preview region. In 3D viewport it is possible to transform object and press I key to add key for its position. This also could be done in sequencer preview. This feature should follow same rules and preferences.
        Expected outcomes: Possibility of quick and easy animation in VSE preview
        Skills required: Proficient in C++.
        Possible mentors: Richard Antalik
        Expected project size: 90 hours
        Difficulty: medium

        ~~~~~~~~~~
        Compositor: Implement new nodes¶
        Description: The compositor has been rewritten to be more efficient and future proof. Moving forward, it would be nice to implement new nodes to make the compositor as powerful as it can be. Interested students are encouraged to propose their own ideas. Some example nodes include:
        Define low / high points
        Expected outcomes: Implemented one or more nodes for both CPU and GPU backends.
        Skills required: Good in C/C++, image processing algorithms, familiarity with shaders.
        Possible mentors: Habib Gahbiche / Omar Emara
        Expected project size: 90 or 175 hours depending on the node
        Difficulty: Easy or Medium depending on the node

        ~~~~~~~~~~
        Compositor: UI improvements¶
        Description: The compositor has been rewritten to be more efficient and future proof. It would be nice to improve the UI of some nodes as well as the workflow overall. Interested students are encouraged to suggest ideas. Some examples include:
        Implement 2D gizmos for exisiting nodes.
        Re-design the UI of exisiting nodes
        Expected outcomes: Improved UI within node editors.
        Skills required: Proficient in C/C++, familiarity with design patterns.
        Possible mentors: Habib Gahbiche / Omar Emara
        Expected project size: 90 or 175 hours depending on the scope of the project
        Difficulty: Easy or Medium depending on the scope

          
    totalCharacters_of_ideas_content_parent: 14181
    totalwords_of_ideas_content_parent: 3076
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/blender-foundation/
    idea_list_url: https://developer.blender.org/docs/programs/gsoc/ideas/

  - organization_id: 15
    organization_name: CCExtractor Development
    no_of_ideas: 13
    ideas_content: >-
      
        CCExtractor Release 1.00	This is our ambitious project for the summer - work directly with the core team to prepare 1.00 - our first major version bump ever, by getting our PR's from last year vetted, tested and integrated	Some of these: Rust, C, Flutter, Docker, GitHub actions	The rest from the previous list.	Hard	350 hours
        ~~~~~~~~~~
        Ultimate Alarm Clock III	The ultimate alarm clock, with features no other one has. And free!	Flutter	Good application design	Medium	350 hours
        ~~~~~~~~~~
        Beacon Watch Companion	Beacon was started in 2021 and it got a great push also during 2022 and 2024. It aims to ease the group travelling (or hiking). This project is intended to be a companion for the beacon project for smart watches.	Flutter	Scalability	Medium	175 hours
        ~~~~~~~~~~
        Ultimate Alarm Clock Watch Companion	Ultimate Alarm Clock launched in 2023 and gained significant momentum in 2024. It aims to offer unique features that set it apart from other alarm clock apps—all for free!. This project is intended to be a companion for the ultimate alarm clock project for smart watches.	Flutter	Scalability	Medium	175 hours
        ~~~~~~~~~~
        Smart Health Reminder	A fun and interactive health tracking app with smart reminders, challenges, and gamification. Stay healthy effortlessly!	Flutter	Gamification & UX design	Medium	350 hours
        ~~~~~~~~~~
        support more torrent clients	We'd like to add support for other clients to our ruTorrent mobile interface (which of course will get a new name): Flood and Deluge.	Flutter	API, Teamwork	Medium	Discuss
        ~~~~~~~~~~
        URL shortener, with a twist	A URL shortener converts a long URL into a shorter one. There are many use cases. Some times it's just the shortening itself we want, for example to share it on twitter. Other times it's about obfuscation. We want to create our own, but with some unique features.	Any language you want	Internet infrastructure	Medium	175 hours
        ~~~~~~~~~~
        COSMIC Session For Regolith	COSMIC is a wayland based desktop environment written from scratch in rust, with modularity in mind. We're interested in swapping the GNOME components of Regolith DE with COSMIC.	Rust	Wayland, Iced, DBus, etc	Medium	350 hours
        ~~~~~~~~~~
        Add complex layouts to sway	Sway is a drop-in replacement for i3, a popular windows manager for Linux that finally gets rid of the ancient X11 protocol. It's fantastic, but it's still missing support for complex scenarios. We'd like you to work on that support.	C	Sway	Hard	350 hours
        ~~~~~~~~~~
        Expose ectool functionality as a library	ectool is a CLI that lets you interact with an embedded controller for laptops. Expose its functionality as a library so it's possible to use it without spawning the CLI.	C, Python	Interlanguage connectivity	Medium	350 hours
        ~~~~~~~~~~
        CCSync	This project aims to develop a comprehensive platform that can be used sync tasks with taskserver.A hosted solution for syncing your TaskWarrior client.Setting up your own TaskServer takes some effort.And platforms like inthe.am,freecinc have shut down their services.So we want to create a platform similar to inthe.am , freecinc and wingtask.	Any language you want	Internet infrastructure	Medium	175 hours
        ~~~~~~~~~~
        Mouseless for Linux v2 - i3 edition	Mouseless is a nice tool to practice keyboard shortcuts for a few popular apps. Unfortunately it's only available for Mac. Last year we created an open source one that runs on Linux. Using that work or not (this is your choice) we want to create one that helps use i3vm (the fantastic windows manager) using keys only.	Your choice	??	Unknown	175 hours
        ~~~~~~~~~~
        Desktop Actions in Ilia	Desktop Actions defined in .desktop files are used by app launcher to provide access to additional functionalities, typically via context menus. Ilia is an app launcher that currently doesn't support for Desktop Actions due to its keyboard based approach.	Vala, GTK	GTK	Medium	175 hours




          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/ccextractor-development/
    idea_list_url: https://ccextractor.org/docs/ideas_page_for_summer_of_code_2025/

  - organization_id: 16
    organization_name: CERN-HSF
    no_of_ideas: 37
    ideas_content: >-
          Precision Recovery in Lossy-Compressed Floating Point Data for High Energy Physics
          Description
          ATLAS is one of the particle physics experiments at the Large Hadron Collider (LHC) at CERN. With the planned upgrade of the LHC (the so-called High Luminosity phase), allowing for even more detailed exploration of fundamental particles and forces of nature, it is expected that the recorded data rate will be up to ten times greater than today. One of the methods of addressing this storage challenge is data compression. The traditional approach involves lossless compression algorithms such as zstd and zlib. To further reduce storage footprint, methods involving lossy compression are being investigated. One of the solutions in High Energy Physics is the reduction of floating point precision, as stored precision may be higher than detector resolution. However, when reading data back, physicists may be interested in restoring the precision of the floating point numbers. This is obviously impossible in the strict sense, as the process of removing bits is irreversible. Nevertheless, given that the data volume is high, some variables are correlated, and follow specific distributions, one may consider a machine learning approach to recover the lossy-compressed floating-point data.

          Task ideas
          Perform lossy compression of data sample from the ATLAS experiment
          Investigate ML techniques for data recovery, prediction and upscaling
          Integrate the chosen technique into HEP workflow
          Expected results
          Implementation of ML-based procedure to restore precision of lossy-compressed floating-point numbers in ATLAS data
          Evaluation of the method’s performance (decompression accuracy) and its applicability in HEP workflow
          Requirements
          C++, Python, Machine Learning
          Links
          IEEE_754
          Implementation of FloatCompressor in Athena
          Mentors
          Maciej Szymański - ANL
          Peter Van Gemmeren - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: July-September
          Corresponding Project
          ATLAS
          Participating Organizations
          ANL
          CERN

          ~~~~~~~~~~

          The rise of the machine (learning) in data compression for high energy physics and beyond
    

          Short description of the project
          The Large Hadron Collider (LHC) hosts multiple large-scale experiments, LHC experiments such as ATLAS, ALICE, LHCb, and CMS. These together produce roughly 1 Petabyte of data per second, but bandwidth and storage limitations force them to only pick the most interesting data, and discard the rest. The final data stored on disk is roughly 1 Petabyte per day [1]. Despite such steep methods of data reduction, the upgraded High Luminosity LHC in 2029 will produce 10 times more particle collisions. This is a great improvement for the potential to discover new physics, but poses a challenge both for data processing and data storage, as the resources needed in both departments are expected to be 3 and 5 times larger than the projected resources available [2][3].

          Data compression would be the go-to solution to this issue, but general data formats used for big data and the ROOT data format used at the LHC are already highly compressed, meaning that the data does not compress much under normal loss-less compression methods like zip [4]. However, since the observables in these experiments benefit from more events and higher statistics, lossy compression is a good alternative. By using lossy compression some data accuracy is lost, but the compression will allow for the storage of more data which will increase the statistical precision of the final analysis.

          BALER is a compression tool undergoing development at the particle physics division of the University of Manchester. BALER uses autoencoder and other neural networks as a type of lossy machine learning-based compression to compress multi-dimensional data and evaluate the accuracy of the dataset after compression.

          Since data storage is a problem in many fields of science and industry, BALER aims to be an open source tool that can support the compression of data formats from vastly different fields of science. For example, catalog data in astronomy and time series data in computational fluid dynamics.

          This project aims to work on the machine learning models in BALER to optimize performance for LHC data and evaluate its performance in real LHC analyses.

          Task ideas
          This internship can focus on a range of work packages, and the project can be tailored to the intern. Possible projects include:

          New auto-encoder models could be developed, better identifying correlations between data objects in a given particle physics dataset entry (event, typically containing thousands of objects and around 1MB each). New models could also improve performance on live / unseen data. These could include transformer, GNN, probabilistic and other tiypes of networks.
          Existing models could be applied on an FPGA, potentially significantly reducing latency and power consumption, opening the possibility of live compression before transmission of data on a network.
          BALER could also be integrated into standard research data storage formats and programs used by hundreds of thousands of physics researchers (ROOT).
          Finally the compression could be applied to particle physics datasets and the effect on the physics discovery sensitivity of an analysis could be assessed and compared to the possible increased sensitivity from additional data bandwidth.
          Ideas from the intern are also welcomed.

          Expected results
          An improved compression performance with documentation and figures of merit that may include:

          Plots made in matplotlib that demonstrate the performance of the new models compared to the old
          Documentation of the design choices made for the improved models
          Documented evaluation of a physics analysis on data before and after compression
          Requirements
          The candidate should have experience with the python language and a Linux environment, familiarity with AI fundamentals, and familiarity with PyTorch.

          Desirable skills include familiarity with AI fundamentals including transformers and/or graph neural networks, particle physics theory and experiments, PyTorch, FPGA programming and/or simulation.

          Links
          BALER GitHub
          BALER Paper

          Previous work:
          Thesis by Eric Wulff, Lund University
          Thesis by Erik Wallin, Lund University
          GSOC 2020 project: Medium post by Honey Gupta
          GSOC 2021 project: Zenodo entry by George Dialektakis
          ROOT
          Jupyter
          PyTorch
          Mentors
          James Smith - UManchester
          Caterina Doglioni - CERN
          Leonid Didukh
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          baler
          Participating Organizations
          baler
          UManchester
          CERN

          ~~~~~~~~~~

          Probabilistic circuit for lossless HEP data compression
          

          Short description of the project
          Neural data compression is an efficient solution for reducing the cost and computational resources of data storage in many LHC experiments. However, it suffers from the ability to precisely reconstruct compressed data, as most of the neural compression algorithms perform the decompression with the information loosage. On another hand, the lossless neural data compression schemas (VAE, IDF) have a lower compression ratio and are not fast enough for file IO. This project’s task is to overcome the disadvantages of the neural compression algorithm by using the probabilistic circuit for HEP data compression.

          Task ideas
          Implement the probabilistic circuit using the PyTorch
          Train and compress the HEP data (Higgs data, TopQuark Dataset)
          Measure the cost and quantify the optimal compression ratio of the probabilistic circuit
          Perform the benchmark, and compare the results with AE, Transformer
          Expected results
          An improved compression performance with documentation and figures of merit that may include:

          Implemented model of the probabilistic circuit
          Documentation of the benchmark and experiment of compression of the HEP data
          Requirements
          Required: Good knowledge of UNIX, Python, matplotlib, Pytorch, Julia, Pandas, ROOT.

          Links
          Previous work:

          GSOC 2021 project: Zenodo entry by George Dialektakis
          Baler – Machine Learning Based Compression of Scientific Data
          ROOT
          Jupyter
          Lossless compression with probabilistic circuits
          iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder
          Integer Discrete Flows and Lossless Compression
          Mentors
          Leonid Didukh
          Caterina Doglioni - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October (with 3 weeks mentor vacation where student will work independently with minimal guidance)
          Corresponding Project
          baler
          Participating Organizations
          CERN

          ~~~~~~~~~~
          Agent-Based Simulation of CAR-T Cell Therapy Using BioDynaMo
          Description
          Chimeric Antigen Receptor T-cell (CAR-T) therapy has revolutionized cancer treatment by harnessing the immune system to target and destroy tumor cells. While CAR-T has demonstrated success in blood cancers, its effectiveness in solid tumors remains limited due to challenges such as poor tumor infiltration, immune suppression, and T-cell exhaustion. To improve therapy outcomes, computational modeling is essential for optimizing treatment parameters, predicting failures, and testing novel interventions. However, existing models of CAR-T behavior are often overly simplistic or computationally expensive, making them impractical for large-scale simulations.

          This project aims to develop a scalable agent-based simulation of CAR-T therapy using BioDynaMo, an open-source high-performance biological simulation platform. By modeling T-cell migration, tumor engagement, and microenvironmental factors, we will investigate key treatment variables such as dosage, administration timing, and combination therapies. The simulation will allow researchers to explore how tumor microenvironment suppression (e.g., regulatory T-cells, hypoxia, immunosuppressive cytokines) affects CAR-T efficacy and what strategies such as checkpoint inhibitors or cytokine support can improve outcomes.

          The final deliverable will be a fully documented, reproducible BioDynaMo simulation, along with analysis tools for visualizing treatment dynamics. The model will provide insights into the optimal CAR-T cell dosing, tumor penetration efficiency, and factors influencing therapy resistance. This project will serve as a foundation for in silico testing of immunotherapies, reducing the need for costly and time-consuming laboratory experiments while accelerating the development of more effective cancer treatments.

          Expected plan of work:
          Phase 1: Initial Setup & Simple T-cell Dynamics
          Phase 2: Advanced CAR-T Cell Behavior & Tumor Interaction
          Phase 3: Integration of Immunosuppressive Factors & Data Visualization
          Expected deliverables
          A fully documented BioDynaMo simulation of CAR-T therapy.
          Analysis scripts for visualizing tumor reduction and CAR-T efficacy.
          Performance benchmarks comparing different treatment strategies.
          A research-style report summarizing findings.
          Requirements
          C++ (for BioDynaMo simulations)
          Agent-based modeling (understanding immune dynamics)
          Basic immunology & cancer biology (optional but helpful)
          Data visualization (Python, Matplotlib, Seaborn)
          Links
          Mapping CAR T-Cell Design Space Using Agent-Based Models
          BioDynaMo: A Modular Platform for High-Performance Agent-Based Simulation
          Computational Modeling of Chimeric Antigen Receptor (CAR) T-Cell Therapy of a Binary Model of Antigen Receptors in Breast Cancer
          Investigating Two Modes of Cancer-Associated Antigen Presentation in CAR T-Cell Therapy Using Agent-Based Modeling
          BioDynaMo: Cutting-Edge Software Helps Battle Cancer
          Mentors
          Vassil Vassilev
          Lukas Breitwieser - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          BioDynamo
          Participating Organizations
          CERN
          CompRes

          ~~~~~~~~~~

          Development of an auto-tuning tool for the CLUEstering library
          Description
          CLUE is a fast and fully parallelizable density-based clustering algorithm, optimized for high- occupancy scenarios, where the number of clusters is much larger than the average number of hits in a cluster (Rovere et al. 2020). The algorithm uses a grid spatial index for fast querying of neighbors and its timing scales linearly with the number of points within the range considered. It is currently used in the CMS and CLIC event reconstruction software for clustering calorimetric hits in two dimensions based on their energy. The CLUE algorithm has been generalized to an arbitrary number of dimensions and to a wider range of applications in CLUEstering, a general purpose clustering library, with the backend implemented in C++ and providing a Python interface for easier use. The backend can be executed on multiple backends (serial, TBB, GPUs, ecc) thanks to the Alpaka performance portability library. One feature currently lacking from CLUEstering and that would be extremely useful for every user, is an autotuning of the parameters, that given the expected number of clusters computes the combination of input parameters that results in the best clustering.
          For this task, one of the options to be explored is “The Optimizer”, a Python library developed by the Patatrack group of the CMS experiment which provides a collection of optimization algorithm, in particular MOPSO (Multi-Objective Particle Swarm Optimization).

          Expected results
          Consider the best techniques and tools for the task
          Develop an auto-tuning tool for the parameters of CLUEstering
          Test it on a wide range of commonly used datasets
          Benchmark and profile to identify the bottlenecks of the tool and optimize it
          Evaluation Task
          Interested students please contact simone.balducci@cern.ch

          Technologies
          C++, Python
          Desirable skills
          Experience with development in C++17/20
          Experience with GPU computing
          Experience with machine learning and optimization techniques
          Experience with development of Python libraries
          Links
          CLUE
          CLUEstering
          Alpaka
          Mentors
          Simone Balducci - CERN UNIBO
          Felice Pantaleo - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Patatrack
          Participating Organizations
          CERN


          ~~~~~~~~~~

          Evaluate Distribution of ML model files on CVMFS
          Description
          Particle physicists studying nature at highest energy scales at the Large Hadron Collider rely on simulations and data processing for their experiments. These workloads run on the “computing grid”, a massive globally distributed computing infrastructure. Deploying software efficiently and reliable to this grid is an important and challenging task. CVMFS is an optimised shared file system developed specifically for this purpose: it is implemented as a POSIX read-only file system in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs. In many cases, it replaces package managers and shared software areas on cluster file systems as means to distribute the software used to process experiment data.

          Task idea
          CVMFS is optimized for the distribution of software (header files, scripts and libraries), taking advantage of the repeated access pattern for its caching, and the possibility to deduplicate files present in several versions. CVMFS is capable to provide a general read-only POSIX file system view on data in external storage. A very common use case is to make conditions databases available to workloads running in distributed computing infrastructure, but various datasets have been published in CVMFS. How efficient CVMFS can be always depends on the details in these use cases - often the benefit for the users is simply in leveraging the existing server and proxy infrastructure.

          In this project proposal, we’d like to evaluate CVMFS as a means to distribute machine learning model files used in inference, for example .onnx files. The main focus will be on creating a test deployment and benchmarking the access, as well as possible coding utilities and scripts to aid in the deployment of models on CVMFS. We’d also like to contrast CVMFS to existing inference servers like KServe, and see if it could integrate as a backend storage.

          Expected results and milestones
          Familiarization with the CVMFS server infrastructure
          Familiarization with the ML model usage at CERN, Survey of different common inference model file formats.
          Test deployment of models relevant to ML4EP
          Benchmark and evaluation of inference using models served from CVMFS
          Addition of the benchmark to the CVMFS continuous benchmarking infrastructure
          Writing a best practices document for the CVMFS documentation
          Requirements
          UNIX/Linux
          Interest in scientific computing devops
          Familiarity with common ML libraries, in particular ONNX
          Links
          CVMFS
          KServe
          Mentors
          Valentin Volkl - CERN
          Lorenzo Moneta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-October
          Corresponding Project
          CernVM-FS
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Implement and improve an efficient, layered tape with prefetching capabilities
          Description
          In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Automatic differentiation is an alternative technique to Symbolic differentiation and Numerical differentiation (the method of finite differences). Clad is based on Clang which provides the necessary facilities for code transformation. The AD library can differentiate non-trivial functions, to find a partial derivative for trivial cases and has good unit test coverage.

          The most heavily used entity in AD is a stack-like data structure called a tape. For example, the first-in last-out access pattern, which naturally occurs in the storage of intermediate values for reverse mode AD, lends itself towards asynchronous storage. Asynchronous prefetching of values during the reverse pass allows checkpoints deeper in the stack to be stored furthest away in the memory hierarchy. Checkpointing provides a mechanism to parallelize segments of a function that can be executed on independent cores. Inserting checkpoints in these segments using separate tapes enables keeping the memory local and not sharing memory between cores. We will research techniques for local parallelization of the gradient reverse pass, and extend it to achieve better scalability and/or lower constant overheads on CPUs and potentially accelerators. We will evaluate techniques for efficient memory use, such as multi-level checkpointing support. Combining already developed techniques will allow executing gradient segments across different cores or in heterogeneous computing systems. These techniques must be robust and user-friendly, and minimize required application code and build system changes.

          This project aims to improve the efficiency of the clad tape and generalize it into a tool-agnostic facility that could be used outside of clad as well.

          Expected Results
          Optimize the current tape by avoiding re-allocating on resize in favor of using connected slabs of array
          Enhance existing benchmarks demonstrating the efficiency of the new tape
          Add the tape thread safety
          Implement multilayer tape being stored in memory and on disk
          [Stretch goal] Support cpu-gpu transfer of the tape
          [Stretch goal] Add infrastructure to enable checkpointing offload to the new tape
          [Stretch goal] Performance benchmarks
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes


          ~~~~~~~~~~

          Enhancing LLM Training with Clad for efficient differentiation
          Description
          This project aims to leverage Clad, an automatic differentiation (AD) plugin for Clang, to optimize large language model (LLM) training primarily in C++. Automatic differentiation is a crucial component of deep learning training, enabling efficient computation of gradients for optimization algorithms such as stochastic gradient descent (SGD). While most modern LLM frameworks rely on Python-based ecosystems, their heavy reliance on interpreted code and dynamic computation graphs can introduce performance bottlenecks. By integrating Clad into C++-based deep learning pipelines, we can enable high-performance differentiation at the compiler level, reducing computational overhead and improving memory efficiency. This will allow developers to build more optimized training workflows without sacrificing flexibility or precision.

          Beyond performance improvements, integrating Clad with LLM training in C++ opens new possibilities for deploying AI models in resource-constrained environments, such as embedded systems and HPC clusters, where minimizing memory footprint and maximizing computational efficiency are critical. Additionally, this work will bridge the gap between modern deep learning research and traditional scientific computing by providing a more robust and scalable AD solution for physics-informed machine learning models. By optimizing the differentiation process at the compiler level, this project has the potential to enhance both research and production-level AI applications, aligning with compiler-research.org’s broader goal of advancing computational techniques for scientific discovery.

          Expected Results
          Develop a simplified LLM setup in C++
          Apply Clad to compute gradients for selected layers and loss functions
          Enhance clad to support it if necessary, and prepare performance benchmarks
          Enhance the LLM complexity to cover larger projects such as llama
          Repeat bugfixing and benchmarks
          Develop tests to ensure correctness, numerical stability, and efficiency
          Document the approach, implementation details, and performance gains
          Present progress and findings at relevant meetings and conferences
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Background in LLM is preferred but not required
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes


          ~~~~~~~~~~

          Enable Clad on ONNX-based models
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. Clad is useful in powering statistical analysis and uncertainty assessment applications. ONNX (Open Neural Network Exchange) provides a standardized format for machine learning models, widely used for interoperability between frameworks like PyTorch and TensorFlow

          This project aims to integrate Clad, an automatic differentiation (AD) plugin for Clang, with ONNX-based machine learning models. Clad can generate derivative computations for C++ functions, making it useful for sensitivity analysis, optimization, and uncertainty quantification. By extending Clad’s capabilities to ONNX models, this project will enable efficient differentiation of neural network operations within an ONNX execution environment.

          Expected Results
          Enumerate ONNX modules with increasing complexity and analyze their differentiation requirements.
          Develop a structured plan for differentiating the identified ONNX operations.
          Implement forward mode differentiation for selected ONNX operations.
          Extend support to reverse mode differentiation for more complex cases.
          Create comprehensive tests to validate correctness and efficiency.
          Write clear documentation to ensure ease of use and future maintenance.
          Present results at relevant meetings and conferences.
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Basic knowledge of Clang is preferred but not mandatory
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable automatic differentiation of OpenMP programs with Clad
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. Clad is useful in powering statistical analysis and uncertainty assessment applications. OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared-memory multiprocessing programming in C, C++, and other computing platforms.

          This project aims to develop infrastructure in Clad to support the differentiation of programs that contain OpenMP primitives.

          Expected Results
          Extend the pragma handling support
          List the most commonly used OpenMP concurrency primitives and prepare a plan for how they should be handled in both forward and reverse accumulation in Clad
          Add support for concurrency primitives in Clad’s forward and reverse mode automatic differentiation.
          Add proper tests and documentation.
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Parallel Programming
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Integrate Clad to PyTorch and compare the gradient execution times
          Description
          PyTorch is a popular machine learning framework that includes its own automatic differentiation engine, while Clad is a Clang plugin for automatic differentiation that performs source-to-source transformation to generate functions capable of computing derivatives at compile time.

          This project aims to integrate Clad-generated functions into PyTorch using its C++ API and expose them to a Python workflow. The goal is to compare the execution times of gradients computed by Clad with those computed by PyTorch’s native autograd system. Special attention will be given to CUDA-enabled gradient computations, as PyTorch also offers GPU acceleration capabilities.

          Expected Results
          Incorporate Clad’s API components (such as clad::array and clad::tape) into PyTorch using its C++ API
          Pass Clad-generated derivative functions to PyTorch and expose them to Python
          Perform benchmarks comparing the execution times and performance of Clad-derived gradients versus PyTorch’s autograd
          Automate the integration process
          Document thoroughly the integration process and the benchmark results and identify potential bottlenecks in Clad’s execution
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable automatic differentiation of C++ STL concurrency primitives in Clad
          Description
          Clad is an automatic differentiation (AD) clang plugin for C++. Given a C++ source code of a mathematical function, it can automatically generate C++ code for computing derivatives of the function. This project focuses on enabling automatic differentiation of codes that utilise C++ concurrency features such as std::thread, std::mutex, atomic operations and more. This will allow users to fully utilize their CPU resources.

          Expected Results
          Explore C++ concurrency primitives and prepare a report detailing the associated challenges involved and the features that can be feasibly supported within the given timeframe.
          Add concurrency primitives support in Clad’s forward-mode automatic differentiation.
          Add concurrency primitives support in Clad’s reverse-mode automatic differentiation.
          Add proper tests and documentation.
          Present the work at the relevant meetings and conferences.
          An example demonstrating the use of differentiation of codes utilizing parallelization primitives:

          #include <cmath>
          #include <iostream>
          #include <mutex>
          #include <numeric>
          #include <thread>
          #include <vector>
          #include "clad/Differentiator/Differentiator.h"

          using VectorD = std::vector<double>;
          using MatrixD = std::vector<VectorD>;

          std::mutex m;

          VectorD operator*(const VectorD &l, const VectorD &r) {
            VectorD v(l.size());
            for (std::size_t i = 0; i < l.size(); ++i)
              v[i] = l[i] * r[i];
            return v;
          }

          double dot(const VectorD &v1, const VectorD &v2) {
            VectorD v = v1 * v2;
            return std::accumulate(v.begin(), v.end(), 0.0);
          }

          double activation_fn(double z) { return 1 / (1 + std::exp(-z)); }

          double compute_loss(double y, double y_estimate) {
            return -(y * std::log(y_estimate) + (1 - y) * std::log(1 - y_estimate));
          }

          void compute_and_add_loss(VectorD x, double y, const VectorD &weights, double b,
                                    double &loss) {
            double z = dot(x, weights) + b;
            double y_estimate = activation_fn(z);
            std::lock_guard<std::mutex> guard(m);
            loss += compute_loss(y, y_estimate);
          }

          /// Compute total loss associated with a single neural neural-network.
          /// y_estimate = activation_fn(dot(X[i], weights) + b)
          /// Loss of a training data point = - (y_actual * std::log(y_estimate) + (1 - y_actual) * std::log(1 - y_estimate))
          /// total loss: summation of loss for all the data points
          double compute_total_loss(const MatrixD &X, const VectorD &Y,
                                    const VectorD &weights, double b) {
            double loss = 0;
            const std::size_t num_of_threads = std::thread::hardware_concurrency();
            std::vector<std::thread> threads(num_of_threads);
            int thread_id = 0;
            for (std::size_t i = 0; i < X.size(); ++i) {
              if (threads[thread_id].joinable())
                threads[thread_id].join();
              threads[thread_id] =
                  std::thread(compute_and_add_loss, std::cref(X[i]), Y[i],
                              std::cref(weights), b, std::ref(loss));
              thread_id = (thread_id + 1) % num_of_threads;
            }
            for (std::size_t i = 0; i < num_of_threads; ++i) {
              if (threads[i].joinable())
                threads[i].join();
            }

            return loss;
          }

          int main() {
            auto loss_grad = clad::gradient(compute_total_loss);
            // Fill the values as required!
            MatrixD X;
            VectorD Y;
            VectorD weights;
            double b;

            // derivatives
            // Zero the derivative variables and make them of the same dimension as the
            // corresponding primal values.
            MatrixD d_X;
            VectorD d_Y;
            VectorD d_weights;
            double d_b = 0;

            loss_grad.execute(X, Y, weights, b, &d_X, &d_Y, &d_weights, &d_b);

            std::cout << "dLossFn/dW[2]: " << d_weights[2] << "\n"; // Partial derivative of the loss function w.r.t weight[2]
            std::cout << "dLossFn/db: " << d_b << "\n"; // Partial derivative of the loss function w.r.t b
          }
          Requirements
          Automatic differentiation
          Parallel programming
          Reasonable expertise in C++ programming
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Support usage of Thrust API in Clad
          Description
          The rise of ML has shed light into the power of GPUs and researchers are looking for ways to incorporate them in their projects as a lightweight parallelization method. Consequently, General Purpose GPU programming is becoming a very popular way to speed up execution time.

          Clad is a clang plugin for automatic differentiation that performs source-to-source transformation and produces a function capable of computing the derivatives of a given function at compile time. This project aims to enhance Clad by adding support for Thrust, a parallel algorithms library designed for GPUs and other accelerators. By supporting Thrust, Clad will be able to differentiate algorithms that rely on Thrust’s parallel computing primitives, unlocking new possibilities for GPU-based machine learning, scientific computing, and numerical optimization.

          Expected Results
          Research and decide on the most valuable Thrust functions to support in Clad
          Create pushforward and pullback functions for these Thrust functions
          Write tests that cover the additions
          Include demos of using Clad on open source code examples that call Thrust functions
          Write documentation on which Thrust functions are supported in Clad
          Present the work at the relevant meetings and conferences.
          Requirements
          Automatic differentiation
          C++ programming
          Clang frontend
          Links
          Repo
          Mentors
          Vassil Vassilev
          David Lange
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Clad
          Participating Organizations
          CompRes

          ~~~~~~~~~~
          
          Extending the User Interface
          Description
          Constellation is a framework used for lab setups or small-scale experiments in HEP. One of its most important goals is that the framework should be easy to use for both scientists implementing new devices as well as experiment operators.

          Constellation features a Qt-based User Interfaces to control and monitor all devices in the experimental setup. The focus of this GSoC project is to add new user interfaces to Constellation and extend the current ones.

          Project Milestones
          Creating a new GUI to display monitoring data from devices using Qt Charts
          Modularization of UI elements into reusable Qt widgets
          Adding the monitoring widget to the existing GUI for device control
          Requirements
          Modern C++
          Knowledge of Qt is helpful but not required
          Practical experience with Unix and git
          Links
          Repository
          Documentation
          Mentors
          Stephan Lachnit - DESY
          Simon Spannagel - DESY
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          Constellation
          Participating Organizations
          DESY

          ~~~~~~~~~~

          Implement CppInterOp API exposing memory, ownership and thread safety information
          Description
          Incremental compilation pipelines process code chunk-by-chunk by building an ever-growing translation unit. Code is then lowered into the LLVM IR and subsequently run by the LLVM JIT. Such a pipeline allows creation of efficient interpreters. The interpreter enables interactive exploration and makes the C++ language more user friendly. The incremental compilation mode is used by the interactive C++ interpreter, Cling, initially developed to enable interactive high-energy physics analysis in a C++ environment.

          Clang and LLVM provide access to C++ from other programming languages, but currently only exposes the declared public interfaces of such C++ code even when it has parsed implementation details directly. Both the high-level and the low-level program representation has enough information to capture and expose more of such details to improve language interoperability. Examples include details of memory management, ownership transfer, thread safety, externalized side-effects, etc. For example, if memory is allocated and returned, the caller needs to take ownership; if a function is pure, it can be elided; if a call provides access to a data member, it can be reduced to an address lookup. The goal of this project is to develop API for CppInterOp which are capable of extracting and exposing such information AST or from JIT-ed code and use it in cppyy (Python-C++ language bindings) as an exemplar. If time permits, extend the work to persistify this information across translation units and use it on code compiled with Clang.

          Project Milestones
          Collect and categorize possible exposed interop information kinds
          Write one or more facilities to extract necessary implementation details
          Design a language-independent interface to expose this information
          Integrate the work in clang-repl and Cling
          Implement and demonstrate its use in cppyy as an exemplar
          Present the work at the relevant meetings and conferences.
          Requirements
          C++ programming
          Python programming
          Knowledge of Clang and LLVM
          Links
          Repo
          Mentors
          Aaron Jomy - CompRes
          Vassil Vassilev - CompRes
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          CppInterOp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Incorporate a Large Language Model to assist users
          Description
          The amount of data that is processed by individual scientists has grown hugely in the past decade. It is not unusual for a user to have data processed on tens of thousands of processors with these located at tens of different locations across the globe. The Ganga user interface was created to allow for the management of such large calculations. It helps the user to prepare the calculations, submitting the tasks to a resource broker, keeping track of which parts of the task that has been completed, and putting it all together in the end.

          As a scripting and command line interface, there will naturally be users that have problems with getting the syntax correct. To solve this, they will often spend time searching through mailing lists, FAQs and discussion fora or indeed just wait for another more advanced coder to debug their problem. The idea of this project is to integrate a Large Language Model (LLM) into the command prompt in Ganga. This should allow the user to describe in words what they would like to do and get an example that they can incorporate. It should also intercept exceptions thrown by the Ganga interface, help the user to understand them and propose solutions.

          We have an interface based on ollama that will build a RAG that contains extra information about Ganga that has not been available for the training of the underlying LLM.

          Task ideas
          Integrate the interaction with the LLM and RAG into Ganga.
          Integrate past input and output in the CLI to provide context for the CLI.
          Setup a server such that the LLM can run on a remote server requiring minimal installation by the user.
          Test which samples are most useful for adding to the RAG (mailing list discussions, manuals, instant messages)
          Develop continuous integration tests that ensures that LLM integration will keep working.
          Expected results
          For the scientific users of Ganga, this will speed up their development cycle as they will get a faster response to the usage queries that they have.

          As a student, you will gain experience with the challenges of large scale computing where some tasks of a large processing chain might take several days to process, have intermittent failures and have thousands of task processing in parallel. You will get experience with how LLMs can be integrated directly into projects to assist users in the use of the CLI and in understanding error messages.

          Evaluation Task
          Interested students please contact Ulrik (see contact below) to ask questions and for an evaluation task.

          Requirements
          Python programming (advanced), Linux command line experience (intermediate), use of git for code development and continuous integration testing (intermediate)

          Links
          Ganga
          Mentors
          Alex Richards - Imperial College
          Mark Smith - Imperial College
          Ulrik Egede - Monash University
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: May-November
          Corresponding Project
          Ganga
          Participating Organizations
          ImperialCollege
          MonashUniversity

          ~~~~~~~~~~

          Implement a deprecation system to keep code up to date
          Description
          The amount of data that is processed by individual scientists has grown hugely in the past decade. It is not unusual for a user to have data processed on tens of thousands of processors with these located at tens of different locations across the globe. The Ganga user interface was created to allow for the management of such large calculations. It helps the user to prepare the calculations, submitting the tasks to a resource broker, keeping track of which parts of the task that has been completed, and putting it all together in the end.

          As code that has developed over many years, there are part of the API that has become redundant. This means that for a period of time there will be both the old and now deprecated API as well as the new way of doing things. At the moment Ganga is missing a formal way of deprecating code. This means that warnings about using something deprecated are non-uniform and there is also very old code that has never been cleaned up.

          The idea in this project is to formalise the way that code can be declared deprecated and then use the continuous integration to ensure that the code eventually is deleted.

          Task ideas
          Have a well defined way of marking plugins, functions etc as deprecated with a warning about when they will be removed. Building on top of the python package deprecated might be an idea.
          Run tests in the testing framework that will alert developers to that certain parts of the code can now be removed.
          Apply in the testing framework a similar system that will identify when deprecated python features are used when moving to a new python version.
          Apply the deprecation system to parts of the code that is already deprecated.
          Expected results
          Obtain a cleaner code base where very old and since long deprecated code is no longer present. Provide the end user with consistent warnings about their use of deprecated code as well as when it will be removed.

          As a student, you will gain experience with the challenges of large scale computing where some tasks of a large processing chain might take several days to process, have intermittent failures and have thousands of task processing in parallel. You will get experience with working within a large code base that has gone through many developments.

          Evaluation Task
          Interested students please contact Ulrik (see contact below) to ask questions and for an evaluation task.

          Requirements
          Python programming (advanced), Linux command line experience (intermediate), use of git for code development and continuous integration testing (intermediate)

          Links
          Ganga
          Mentors
          Alex Richards - Imperial College
          Mark Smith - Imperial College
          Ulrik Egede - Monash University
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: May-November
          Corresponding Project
          Ganga
          Participating Organizations
          ImperialCollege
          MonashUniversity

          ~~~~~~~~~~

          Geant4-FastSim - Data Representation Optimisation for Generative Model-based Fast Calorimeter Shower Simulation
          Description
          High energy physics experiments such as those operated at the Large Hadron Collider (LHC) fundamentally rely on detailed and realistic simulations of particle interactions with the detector. The state-of-the-art Geant4 toolkit provides a means of conducting these simulations with Monte Carlo procedures. However, the simulation of particle showers in the calorimeter systems of collider detectors with such tools is a computationally intensive task. For this reason, alternative fast simulation approaches based on generative models have received significant attention, with these models now being deployed in production by current experiments at the LHC. In order to develop the next generation of fast simulation tools, approaches are being explored that would be able to handle larger data dimensionalities stemming from the higher granularity present in future detectors, while also being efficient enough to provide a sizable simulation speed-up for low energy showers.

          A shower representation which has the potential to meet these criteria is a point cloud, which can be constructed from the position, energy and time of hits in the calorimeter. Since Geant4 provides access to the (very numerous) individual physical interactions simulated in the calorimeter, it also provides a means to create a representation independent of the physical readout geometry of the detector. This project will explore different approaches to clustering these individual simulated hits into a point cloud, seeking to minimise the number of points while preserving key calorimetric observables.

          First Steps
          Gain a basic understanding of calorimeter shower simulation (G4FastSim)
          Try simulating some electromagnetic particle showers with the Key4hep framework (see test)
          Propose different approaches to clustering, with justification
          Project Milestones
          Survey different approaches to clustering
          Implement and experiment with the different methods
          Investigate the impact of varying the detector granularity on the performance of separate clustering algorithms
          If time allows, hadronic showers could also be investigated
          Expected Results
          A comparison of different approaches to clustering, with a performance evaluation in terms of the effect on calorimetric observables.
          An evaluation of the impact of varying the granularity of the detector readout on the performance of the clustering algorithm
          Requirements
          C++, Python
          Familiarity with PyTorch could be an advantage
          Evaluation Tasks and Timeline
          Find the test here. Please submit it by 9:00 CET 17th March 2025 along with a short proposal (2 pages max) describing how you would approach the problem. See submission instructions in the test doc. Please don’t forget to start the subject line with “GSoC’25 FastSim”.
          We will make the selections based on the test, short proposal and resume by 17:00 CET 24th March.
          Selected candidates will then write the full proposal and submit it according to the official GSoC timeline.
          Links
          G4FastSim
          CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation
          Mentors
          Peter McKeown - CERN
          Piyush Raikwar - CERN
          Anna Zaborowska - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Geant4
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Using ROOT in the field of genome sequencing
          Description
          The ROOT is a framework for data processing, born at CERN, at the heart of the research on high-energy physics. Every day, thousands of physicists use ROOT applications to analyze their data or to perform simulations. The ROOT software framework is foundational for the HEP ecosystem, providing capabilities such as IO, a C++ interpreter, GUI, and math libraries. It uses object-oriented concepts and build-time modules to layer between components. We believe additional layering formalisms will benefit ROOT and its users.

          ROOT has broader scientific uses than the field of high energy physics. Several studies have shown promising applications of the ROOT I/O system in the field of genome sequencing. This project is about extending the developed capability in GeneROOT and understanding better the requirements of the field.

          Expected results
          Reproduce the results based on previous comparisons against ROOT master
          Investigate and compare the latest compression strategies used by Samtools for conversions to BAM, with RAM(ROOT Alignment Maps).
          Explore ROOT’s RNTuple format to efficiently store RAM maps, in place of the previously used TTree.
          Investigate different ROOT file splitting techniques
          Produce a comparison report
          Requirements
          C++ and Python programming
          Familiarity with Git
          Knowledge of ROOT and/or the BAM file formats is a plus.
          Links
          Latest Presentation on GeneROOT
          ROOT
          GeneROOT
          Mentors
          Martin Vasilev - Uni Plovdiv
          Jonas Rembser - CERN
          Fons Rademakers - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-November
          Corresponding Project
          ROOT
          Participating Organizations
          CERN
          CompRes

          ~~~~~~~~~~

          Highly Granular Quantization for CICADA
          Description
          The CICADA (Calorimeter Image Convolutional Anomaly Detection Algorithm) project aims to provide an unbiased detection of new physics signatures in proton-proton collisions at the Large Hadron Collider’s Compact Muon Solenoid experiment (CMS). It detects anomalies in low-level trigger calorimeter information with a convolutional autoencoder, whose behaviour is transferred to a smaller model through knowledge distillation. Careful quantization of the deployed model allows it to meet the requirement of sub-500ns inference times on FPGAs. While CICADA currently employs Quantization Aware Training with different quantization schemes for each layer of the distilled model, a new gradient-based quantization optimization approach published in 2024 offers the possibility of optimizing quantization at the individual weight level. This project would explore implementing this highly granular quantization method to CICADA’s distilled model and evaluating its effects on both model performance and resource consumption on FPGAs. The work would involve implementing the new quantization approach, comparing it with the current implementation, and investigating the impact on both detection performance and hardware resource utilization while maintaining the strict timing requirements.

          Task ideas
          Transition CICADA’s quantization tool from QKeras to HGQ
          Optimize student model’s quantization with higher granularity
          Compare resulting model’s performance with legacy model
          Emulate deployment on FPGA w/ Vivado to evaluate resource consumption
          Expected results
          Extend existing training / quantization scripts to use HGQ in addition to QKeras
          A trained student model with highly granular quantization
          Estimates of that model’s performance and resource consumption on an FPGA
          Requirements
          Python, Tensorflow, Quantization

          Links
          CICADA (homepage)
          CICADA (code)
          HGQ (Paper)
          HGQ (code)
          Mentors
          Lino Gerlach - CERN
          Isobel Ojalvo - Princeton
          Jennifer Ngadiuba - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          CICADA
          Participating Organizations
          princeton

          ~~~~~~~~~~

          Intelligent Log Analysis for the HSF Conditions Database
          Description
          The nopayloaddb project works as an implementation of the Conditions Database reference for the HSF. It provides a RESTful API for managing payloads, global tags, payload types, and associated data.

          Our current system, composed of Nginx, Django, and database (link to helm chart), lacks a centralized logging solution making it difficult to effectively monitor and troubleshoot issues. This task will address this deficiency by implementing a centralized logging system aggregating logs from multiple components, and develop a machine learning model to perform intelligent log analysis. The model will identify unusual log entries indicative of software bugs, database bottlenecks, or other performance issues, allowing us to address problems before they escalate. Additionally, by analyzing system metrics, the model will provide insights for an optimal adjustment of parameters during periods of increased request rates.

          Steps
          Set up a centralized logging system
          Collect and structure logs from Nginx, Django, and the database
          Develop an ML model for log grouping and anomaly detection
          Implement Kubernetes-based database with replication
          Train an ML model to optimize Kubernetes parameters dynamically
          Expected Results
          A centralized logging system for improved monitoring and troubleshooting
          ML-powered anomaly detection
          ML-driven dynamic configuration for optimal performance
          Requirements
          Python and basic understanding of ML frameworks
          Kubernetes, basic understanding, k8s, Helm, Operators, OpenShift
          Django and Nginx, basic understanding of web frameworks and logging
          Database knowledge, PostgreSQL, database replication
          Links
          Django REST API: https://github.com/BNLNPPS/nopayloaddb
          Automized deployment with helm-chart: https://github.com/BNLNPPS/nopayloaddb-charts
          Mentors
          Ruslan Mashinistov - BNL
          John S. De Stefano Jr. - BNL
          Michel Hernandez Villanueva - BNL
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          HSFCondDB
          Participating Organizations
          BNL

          ~~~~~~~~~~

          Interactive Differential Debugging - Intelligent Auto-Stepping and Tab-Completion
          Description
          Differential debugging is a time-consuming task that is not well supported by existing tools. Existing state-of-the-art tools do not consider a baseline(working) version while debugging regressions in complex systems, often leading to manual efforts by developers to achieve an automatable task.

          The differential debugging technique analyzes a regressed system and identifies the cause of unexpected behaviors by comparing it to a previous version of the same system. The idd tool inspects two versions of the executable – a baseline and a regressed version. The interactive debugging session runs both executables side-by-side, allowing the users to inspect and compare various internal states.

          This project aims to implement intelligent stepping (debugging) and tab completions of commands. IDD should be able to execute until a stack frame or variable diverges between the two versions of the system, then drop to the debugger. This may be achieved by introducing new IDD-specific commands. IDD should be able to tab complete the underlying GDB/LLDB commands. The contributor is also expected to set up the necessary CI infrastructure to automate the testing process of IDD.

          Expected Results
          Enable stream capture
          Enable IDD-specific commands to execute until diverging stack or variable value.
          Enable tab completion of commands.
          Set up CI infrastructure to automate testing IDD.
          Present the work at the relevant meetings and conferences.
          Requirements
          Python & C/C++ programming
          Familiarity debugging with GDB/LLDB
          Links
          IDD Repository
          Mentors
          Vipul Cariappa - CompRes
          Martin Vasilev - University of Plovdiv
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          CppInterOp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Julia Interfaces to HepMC3
          Description
          In high-energy physics experiments at CERN it is necessary to simulate physics events in order to compare predicted observations with those that the LHC experiments actually observe. A key piece of the software chain used to do that is the HepMC3 event record library, which encodes the output from physics event generators in a standard way, so that they can be used by downstream detector simulation and analysis codes.

          There is now increasing interest in using Julia as a language for HEP software, as it combines the ease of programming in interactive languages, e.g., Python, with the speed of compiled language, such as C++. As part of building up the ecosystem of supporting packages for Julia in high-energy physics, developing interfaces to read, manipulated and write HepMC3 event records in Julia is the aim of this project.

          Task ideas
          This project would develop a wrapper library for HepMC3 allowing the HepMC3 data objects and methods, in C++, to be called from Julia.

          It would utilise the general underlying wrapper interfaces in CxxWrap and the automated wrapper code generator WrapIt! to allow for as easy and maintainable an interface as possible.

          A key outcome would be a set of unit tests and examples, based on the HepMC3 ones, demonstrating how to use the library and proving that the code is correct.

          Expected results and milestones
          Reading of HepMC3 event files
          Particularly the ASCII format will be targeted first
          Access to event data structures
          Access to particle properties
          Navigation of the event and the vertices between parent and child particles
          Access to run information
          Update of HepMC3 data structures
          Creation of new HepMC3 events
          Re-serialisation of these events to file
          Initially ASCII
          Documentation and examples on how to use the Julia interfaces
          HepMC3.jl package registered in the Julia general registry
          Extension of serialisation to ROOT format (stretch goal)
          Requirements
          Programming experience in C++
          Prior experience in Julia (very advantageous)
          A background understanding of high-energy physics (advantageous)
          Evaluation Exercise
          TBD

          Links
          Julia Programming Language
          JuliaHEP HSF Group
          HepMC3 Repository
          CxxWrap
          WrapIt!
          Mentors
          Graeme Stewart - CERN
          Mateusz Fila - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-July- August
          Corresponding Project
          JuliaHEP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          MCnet/MCviz - graph and 3D-viewer tools for simulated particle collisions
          Description
          Simulations are key to particle-physics research: many modern theoretical models have such complex consequences that we test theory not by comparing measurements of particle collisions to predicted functional forms, but by generating simulated collision-events from the theory and analyzing them identically to the real ones from the particle collider.

          This means that event generators are incredibly important to particle physics, as the most-used link between experiment and theory, and as a crucial data format for exchange of ideas. They are also an excellent way to introduce new researchers and the public to particle-physics concepts. However, the toolset for MC event manipulation and visualisation is less powerful and coherent than it should be, and this project seeks to improve that situation!

          Task ideas
          This project will pick up old ideas and code for MC-event visualisation – both of the interaction graph that illustrates the internal theory computation, and the external appearance of the resulting collision decay-products – and produce a new set of tools useful both to physicists and for public outreach.

          Expected results and milestones
          Extend the mcgraph tool to be usable with both the HepMC and LHE MC-event formats.
          Refactor mcgraph into a library capable of rendering to a web browser in a server app.
          Interface the Phoenix event-viewer library to display 3D events (with and without a dummy detector model) to a web browsers.
          Display interactive particle information and jet clustering in graph and 3D view interfaces.
          Requirements
          Command-line tools
          Python
          Web technologies
          Gitlab CI
          git
          Links
          Phoenix event view library
          Old MCview web-based MC event viewer
          MC event-graph viewer
          Old MCviz event-graph viewer
          HepMC3 event format
          LHE event format
          Mentors
          Andy Buckley - CERN
          Chris Gutschow - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          MCnet
          Participating Organizations
          UofGlasgow


          ~~~~~~~~~~

          MCnet/OpenData - tools and exercises for open-data exploration with MC simulations
          Description
          CERN’s experiments are committed to publishing their data in a form that is accessible to all, both for research purposes and for education. For example, the ATLAS experiment provides Jupyter notebook exercises based on live-analysing reduced forms of the real collider data.

          But particle-physics researchers also use simulations of data as a crucial tool for testing theories and for understanding the background processes that new physics effects have to be isolated from. For this we use Monte Carlo (MC) event-generator codes, which are statistical implementations of the fundamental physics theory that sample real-looking events from the predicted particle types and kinematics. These are not yet represented in open-data exercises.

          Task ideas
          In this project we will develop new tools and exercises for extending open-data analysis resources to include MC event simulations. It will both reduce the entry barriers to outreach with open data and enable more engaging exercises with hypothetical new-physics models.

          Expected results and milestones
          Develop a library of wrapper functions to make open-data analysis more approachable for non-experts.
          Create functions and datasets for loading and analysing MC event samples through Jupyter.
          Develop a new Jupyter+Binder worksheet for outreach-oriented open-data MC analysis.
          Requirements
          Python
          Jupyter
          Binder
          Gitlab CI
          git
          Links
          ATLAS open data
          Example open-data analysis notebook
          Jupyter
          Binder
          Mentors
          Andy Buckley - CERN
          Chris Gutschow - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 175 hours
          Mentor availability: June-October
          Corresponding Project
          MCnet
          Participating Organizations
          UofGlasgow


          ~~~~~~~~~~

          Integrating Support for Google XLS in HLS4ML
          Description
          Google XLS (Accelerated Hardware Synthesis) is an advanced open-source framework for high-level hardware design, offering flexible and efficient synthesis for FPGA and ASIC applications. By integrating XLS into HLS4ML, a framework for translating machine learning models into FPGA-friendly code, we can leverage XLS’s optimizing compiler and domain-specific language to improve resource efficiency, performance, and portability. This integration will enable seamless generation of highly optimized hardware implementations for ML models while maintaining the ease of use that HLS4ML provides.

          HLS4ML currently supports traditional HLS tools like Vivado HLS and Intel HLS, but adding XLS can bring further benefits such as better compilation times, improved hardware efficiency, and wider vendor compatibility. This project will focus on developing an interface between HLS4ML and XLS, allowing ML models to be translated into XLS IR and synthesized efficiently.

          Task Ideas
          Develop a backend in HLS4ML that translates neural network layers into XLS Intermediate Representation (IR).
          Implement the key ML operations (e.g., matrix multiplications, activations, and pooling) via XLS’s DSLX language and map them to HLS4ML operations.
          Benchmark and compare performance, resource utilization, and synthesis results against existing HLS4ML backends.
          Extend HLS4ML’s configuration options to allow selection of XLS as a backend, ensuring ease of integration.
          Expected Results
          A prototype of a backend in HLS4ML supporting XLS-based synthesis.
          Conversion scripts to map ML operations to XLS IR.
          Performance evaluation of XLS and existing HLS backends.
          Documentation and tutorials for using XLS with HLS4ML.
          Requirements
          Proficiency in Python and C++.
          Knowledge of hardware and compiler design.
          Basic familiarity with neural networks.
          Familiarity with version control systems like Git/GitHub.
          Links
          hls4ml documentation
          hls4ml Repository
          Google XLS documentation
          Google XLS repository
          Mentors
          Vladimir Loncar - CERN
          Dimitrios Danopoulos - CERN
          Additional Information
          Difficulty level (low / medium / high): medium/high
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          Optimizing Model Splitting in hls4ml for Efficient Multi-Graph Inference
          Description
          hls4ml is an open-source tool that enables the deployment of machine learning (ML) models on FPGAs using High-Level Synthesis (HLS). It automatically converts pre-trained models from popular deep learning frameworks (e.g., Keras, PyTorch, and ONNX) into optimized firmware for FPGA-based inference.

          Traditionally, the entire ML model is synthesized as a monolithic graph, which can lead to long synthesis times and complicated debugging, especially for large model topologies. Splitting the model graph at specified layers into independent subgraphs allows for parallel synthesis and step-wise optimization. However, finding the ‘optimal’ splitting points and optimizing FIFO buffers in between the subgraphs remains a challenge, especially when dealing with dynamic streaming architectures.

          This project aims to investigate optimal splitting strategies for complex ML models in hls4ml, focusing on efficient FIFO depth optimization across multi-graph designs. The goal is to develop methodologies that can be integrated into hls4ml to enable automated and optimal graph splitting for improved performance.

          Task ideas
          The contributor will start by familiarizing themselves with hls4ml and building ML models using multi-graph designs. They will implement profiling techniques (e.g., VCD logging) to measure FIFO occupancy and backpressure in order to develop a FIFO optimization strategy for multi-graph designs. They will also investigate multi-objective optimization algorithms to determine optimal splitting points based on subgraph resource usage or dataflow patterns. Finally, they will integrate these methodologies with hls4ml and run benchmarks to validate improvements in latency, resource utilization, etc.

          Expected results and milestones
          Familiarization with hls4ml: Understand the hls4ml workflow, including synthesis, and simulation.
          Research and Evaluation: Explore FIFO profiling and optimization strategies along with algorithms to partition the model graph given specific optimization objectives.
          Validation: Benchmark against monolithic implementations and compare differences in latency and resource utilization.
          Requirements
          Proficiency with computer architecture, FPGA design and simulation tools (e.g., Vivado)
          Experience with Python
          Understanding of ML concepts is beneficial.
          Familiarity with version control systems like Git/GitHub.
          Links
          hls4ml documentation
          hls4ml Repository
          Vivado Design Implementation
          Mentors
          Vladimir Loncar - CERN
          Dimitrios Danopoulos - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          RNTuple in JSROOT
          Description
          RNTuple is the next-generation data format for high-energy physics (HEP) data collected from the LHC. It is part of ROOT, a cornerstone software package for the storage, visualization and analysis of scientific data, widely used in the scientific community and particularly in HEP. ROOT is a C++ and Python framework, but it recently became available in the browsers as well through a Javascript implementation of some of its parts: JSROOT. Since RNTuple is still in the experimental phase, it currently lacks a JSROOT interface and its contents cannot be visualized in the browser, a common and desirable property of many ROOT objects. The goal of this project is filling this gap by making JSROOT able to read and display data stored inside an RNTuple.

          Task ideas
          In this project, the student will learn the internals of the RNTuple binary format and use this knowledge to implement a Javascript interface to expose RNTuple to JSROOT.

          Expected results and milestones
          Familiarize with the JSROOT framework, understanding how to integrate new components into it;
          read and implement (a subset of) the RNTuple binary format specifications, in Javascript; this will concretely mean implementing the deserialization code from a binary blob to a RNTuple object that may be used by JSROOT;
          enable the visualization of an RNTuple’s fields in the browser, leveraging the existing framework in JSROOT.
          Requirements
          Knowledge of Javascript / ES6
          Basic knowledge of “low-level” programming (primitive types binary layouts, bit-level manipulations, reinterpreting bytes as different types, …)
          Experience with git / github
          (Bonus): familiarity with any binary format
          Links
          ROOT Project homepage
          ROOT Project repository
          JSROOT homepage
          JSROOT repository
          Introduction to RNTuple
          RNTuple architecture overview
          RNTuple Binary Specification
          Mentors
          Serguei Linev - CERN
          Giacomo Parolini - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          ROOT
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Rucio WebUI Revamp
          Description
          Rucio is an open-source software framework that provides functionality to scientific collaborations to organize, manage, monitor, and access their distributed data and dataflows across heterogeneous infrastructures. Originally developed to meet the requirements of the high-energy physics experiment ATLAS, Rucio has been continuously enhanced to support diverse scientific communities. Since 2016, Rucio has orchestrated multiple exabytes of data access and data transfers globally.

          The Rucio WebUI is a Next.js application utilized by various users within collaborating communities to access, monitor, and manage their distributed data. Key features of the Rucio WebUI include:

          SDK for Streaming: Facilitates seamless data streaming from the Rucio server to page components, ensuring a responsive user interface.
          Typed in TypeScript with Generics: Strict typing ensures code integrity and enhances development efficiency.
          Accessibility and Responsiveness: Designed with accessibility and responsiveness in mind, ensuring usability across various devices.
          Testing and Stability: Extensive testing ensures robustness and reliability in all components.
          Feature Toggles: Dynamic feature toggles provide flexibility in enabling or disabling specific functionalities as needed.
          Component Library: Utilizes Storybook and TailwindCSS to enhance development speed and consistency.
          Tasks
          Upgrade to Next.js 15, React 19, TailwindCSS 4.x:
          Migrate the existing codebase to Next.js 15 to leverage the latest features and performance improvements.
          Utilize Server Side Rendering and React Query in Client Side Components to enhance data-fetching capabilities.
          Migrate tailwind.config.js to new CSS based configuration for TailwindCSS 4.x.
          Enhance User Experience for Site Administrators and Operators:
          Currently the WebUI focuses on List/Get views with the exception of allowing users to Create Rules. Add features to Create/Edit resources for site administrators and operational experts.
          Investigate legacy views in the previous Flask application and migrate them to the new WebUI.
          Redesign these views to be more user-friendly, incorporating feedback from site administrators and operators.
          Migrate Authentication to NextAuth (Auth.js):
          Transition existing x509 and user/password authentication mechanisms to NextAuth.
          Ensure compatibility with various authentication flows, including OAuth and OpenID Connect.
          Develop an RBAC system to ensure users have access only to functionalities relevant to their roles, enhancing security and usability.
          Transition to a Monorepo Structure:
          Migrate the Rucio WebUI to a monorepo structure to improve code organization and facilitate the sharing of common components across different projects.
          Requirements
          Mandatory:

          Proficiency in React.js and Next.js
          Experience with TailwindCSS
          Strong knowledge of JavaScript (ECMAScript 6) and TypeScript
          Familiarity with Python 3 and Flask
          Proficiency with Linux, Git, and Docker
          Good to Have:

          Understanding of NX Monorepos
          Experience with AGGrid Data Tables
          Experience with GitHub Actions
          Knowledge of HTTP REST APIs
          Familiarity with OpenID Connect and x509 protocols
          Expected Results
          By the end of GSoC 2025, we expect to have a revamped Rucio WebUI that:

          Is upgraded to Next.js 15 with integrated React Query.
          Utilizes both client and server-side components as per React 19’s stable features.
          Supports TailwindCSS 4.0 for a modern design system.
          Offers enhanced user experiences tailored for site administrators and operators.
          Employs NextAuth for streamlined authentication processes.
          Implements a robust RBAC system.
          Adopts a monorepo structure for improved code organization and component sharing.
          Links
          Rucio GitHub Repository
          Rucio UI Presentation
          Rucio Documentation
          Rucio System Overview Journal Article (Springer)
          Rucio Operational Experience Article (IEEE Computer Society)
          Mentors
          Mayank Sharma - University of Michigan, Ann Arbor
          Martin Barisits - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-November
          Corresponding Project
          Rucio
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Background Enrichment augmented Anomaly Detection (BEAD) for new physics searches at LHC
          

          Short description of the project
          A long-standing mystery of fundamental physics is the existence of dark matter (DM), a type of matter that has little interaction with ordinary matter but is supported by various astrophysical and cosmological observations and is six times more abundant than ordinary matter in the universe. Several Large Hadron Collider (LHC) experiments are conducting searches aimed at detecting dark matter. Unsupervised and semi-supervised learning outlier detection techniques are advantageous to these searches, for casting a wide net on a variety of possibilities for how dark matter manifests, as they impose minimal constraints from specific physics model details, but rather learn to separate characteristics of rare signals starting from the knowledge of the background they’ve been trained on. Developing innovative search techniques for probing dark matter signatures is crucial for broadening the DM search program at the LHC, and BEAD is a Python package that uses deep learning based methods for anomaly detection in HEP data for such new physics searches. BEAD has been designed with modularity in mind, to enable usage of various unsupervised latent variable models for any task.

          BEAD has five main running modes:

          Data handling: Deals with handling file types, conversions between them and pre-processing the data to feed as inputs to the DL models.

          Training: Trains a model to learn implicit representations of the background data that may come from multiple sources(/generators) to get a single, encriched latent representation of it.

          Inference: Using a model trained on an enriched background, the user can feed in samples where to detect anomalies in.

          Plotting: After running Inference, or Training, one can generate plots. These include performance plots as well as different visualizations of the learned data.

          Diagnostics: Enabling this mode allows running profilers that measure a host of metrics connected to the usage of the compute node to help optimization of the code (using CPU-GPU metrics).

          The package is under active development. The student in this project will work on the machine learning models available in BEAD, and implementing new models to perform anomaly detection, initially on simulated data.

          Task ideas
          Possible projects include:

          New auto-encoder models could be developed, better identifying correlations between data objects in a given particle physics dataset entry (containing event level and/or physics object level information). New models could also improve performance on live / unseen data. These could include transformer, GNN, probabilistic and other tiypes of networks.
          Existing models could be tested on different datasets, potentially identifying distinct latent spaces populated by the different LHC physics processes, that can enable improved anomaly detection.
          Ideas from the student working on this project are also welcome.

          Expected results
          An improved performance of selected models, with documentation and figures of merit that may include:

          Plots made in matplotlib that demonstrate the performance of the new models compared to the old
          Documentation of the design choices made for the improved models
          Documented evaluation of a physics analysis on data before and after compression
          Requirements
          Python
          Linux environment
          ML / unsupervised algorithms key concepts
          PyTorch

          Desired skills: transformers and/or graph neural networks, particle physics theory and experiments, particle physics simulations
          Links
          Paper on unsupervised ML algorithms using HEP datasets
          Review of LHC searches using unsupervised learning
          BEAD GitHub repository (WIP)
          ROOT
          Jupyter
          PyTorch
          Mentors
          Pratik Jawahar - CERN
          Sukanya Sinha - CERN
          Caterina Doglioni - Backup Mentor - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-August
          Corresponding Project
          BEAD
          Participating Organizations
          SMARTHEP
          UManchester

          ~~~~~~~~~~

          Estimating the energy cost of ML scientific software
          Description
          At a time where “energy crisis” is something that we hear daily, we can’t help but wonder whether our research software can be made more sustainable, and more efficient as a byproduct. In particular, this question arises for ML scientific software used in high-throughput scientific computing, where large datasets composed of many similar chunks are analysed with similar operations on each chunk of data. Moreover, CPU/GPU-efficient software algorithms are crucial for the real-time data selection (trigger) systems in LHC experiments, as the initial data analysis necessary to select interesting collision events is executed on a computing farm located at CERN that has finite CPU resources.

          The questions we want to start answering in this work are:

          what is the trade off between performance of a ML algorithm and its energetic efficiency?
          can small efficiency improvements in ML algorithms running on Large Hadron Collider data have a sizable energetic impact?
          how do these energy efficiency improvements vary when using different computing architectures (1) and/or job submission systems (2)?
          Task ideas
          The students in this project will use metrics from the Green Software Foundation and from other selected resources to estimate the energy efficiency of machine learning software from LHC experiments (namely, top tagging using ATLAS Open data) and from machine learning algorithms for data compression (there is another GSoC project developing this code, called Baler). This work will build on previous GSoC / Master’s thesis work, and will expand these results for GPU architectures. If time allows, the student will then have the chance to make small changes to the code to make it more efficient, and evaluate possible savings.

          Expected results and milestones
          Understand and summarise the metrics for software energy consumption, focusing on computing resources at CERN;
          Become familiar with running and debugging the selected software frameworks and algorithms;
          Set up tests and visualisation for applying metrics to the selected software
          Run tests and visualise results (preferably using a Jupyter notebook)
          Vary platforms and job submission systems
          Identify possible improvements, apply them, and run tests again
          Requirements
          Python
          git
          Jupyter notebooks
          PyTorch or equivalent ML toolkit
          Desirable: code profiling experience
          Links
          (1) Green Software Foundation course
          (2) Code by the previous GSoC student
          Mentors
          Caterina Doglioni - CERN
          Tobias Fitschen - Backup Mentor - CERN
          James Smith - Backup Mentor - University of Manchester
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October (with 2-3 weeks mentor vacation where student will work independently with minimal guidance)
          Corresponding Project
          SMARTHEP
          Participating Organizations
          UManchester
          CERN

          ~~~~~~~~~~

          Sustainable Quantum Computing algorithms for particle physics reconstruction
          Description
          Reconstructing the trajectories of charged particles as they traverse several detector layers is a key ingredient for event reconstruction at any LHC experiment. The limited bandwidth available, together with the high rate of tracks per second, makes this problem exceptionally challenging from the computational perspective. With this in mind, Quantum Computing is being explored as a new technology for future detectors, where larger datasets will further complicate this task. Furthermore, when choosing such alternative sustainability will play a crucial role and needs to be studied in detail. This project will consist in the implementation of both Quantum and Classical Machine Learning algorithms for track reconstruction, and using open-source, realistic event simulations to benchmark them from both a physics performance and an energy consumption perspective.

          First steps
          Basic understanding of track reconstruction at LHC using ACTS and/or Allen framework.
          Familiarizing her/himself with trackML simulation datasets https://www.kaggle.com/competitions/trackml-particle-identification/data?select=train_sample.zip.
          Learning how to use the quantum simulator for QML algorithms https://pennylane.ai/.
          Milestones
          Choosing a ML algorithm (or part of) in quantum computing and its classical counterpart for track reconstruction.
          Mapping of track reconstruction problem to Ising-like Hamiltonian.
          Prototype implementation of classical and quantum track reconstruction using trackML simulation inputs.
          Expected results
          Benchmarking physics output and energy consumption of the classical and quantum algorithm.
          Requirements
          CUDA, python, C++
          Evaluation Tasks and Timeline
          To be completed
          Corresponding Project
          QuantumForTracking
          Participating Organizations
          CERN

          ~~~~~~~~~~

          TMVA SOFIE - GPU Support for Machine Learning Inference
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          SOFIE currently supports various Machine Learning operators defined by the ONNX standards, as well as a Graph Neural Network (GNN) implementation. It supports the parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE continues to evolve, there’s a need to enable inference on GPUs. This project aims to explore different GPU stacks (such as CUDA, ROCm, ALPAKA) and implement GPU-based inference functionalities in SOFIE. There is already a SYCL implementation for SOFIE, developed in 2023, which can serve as a reference for future development.

          Task ideas
          In this project, the contributor will gain experience with GPU programming and its role in Machine Learning inference. They will start by understanding SOFIE and running inference on CPUs. After researching GPU stacks and methods of their integration with SOFIE, the contributor will implement GPU support for inference, ensuring the code is efficient and well-integrated with GPU technologies.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understanding the SOFIE architecture, working with its internals, and running inference on CPUs.
          Research and Evaluation: Analyzing various GPU stacks (CUDA, ROCm, ALPAKA, etc.) and determining their alignment with SOFIE.
          Implementation of GPU Inference: Developing functionalities for GPU-based inference in SOFIE.
          [Optional] Benchmarking: Evaluating the performance of the new GPU functionality by benchmarking memory usage, execution time, and comparing results with other frameworks (such as TensorFlow or PyTorch).
          Requirements
          Proficiency in C++ and Python.
          Knowledge of GPU programming (e.g., CUDA).
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          Implementation of SOFIE-SYCL
          Accelerating Machine Learning Inference on GPUs with SYCL
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN


          ~~~~~~~~~~

          TMVA SOFIE - HLS4ML Integration for Machine Learning Inference
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          Currently, SOFIE supports various machine learning operators defined by ONNX standards, as well as a Graph Neural Network implementation. It supports parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE evolves, there is a growing need for inference capabilities on models trained across a variety of frameworks. This project will focus on integrating hls4ml in SOFIE, thereby enabling generation of C++ inference functions on models parsed by hls4ml.

          Task ideas
          In this project, the contributor will gain experience with C++ and Python programming, hls4ml, and their role in machine learning inference. The contributor will start by familiarizing themselves with SOFIE and running inference on CPUs. After researching the possibilities for integration with hls4ml, they will implement functionalities that ensure efficient inference of ML models parsed by hls4ml, which were previously trained in external frameworks like TensorFlow and PyTorch.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understanding the SOFIE architecture, working with its internals, and running inference on CPUs.
          Research and Evaluation: Exploring hls4ml, its support for Keras and PyTorch, and possible integration with SOFIE.
          Integration with hls4ml: Developing functionalities for running inference on models parsed by hls4ml.
          Requirements
          Proficiency in C++ and Python.
          Knowledge of hls4ml
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          hls4ml documentation
          hls4ml Repository
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          TMVA SOFIE - Enhancing Keras Parser and JAX/FLAX Integration
          Description
          SOFIE (System for Optimized Fast Inference code Emit) is a Machine Learning Inference Engine within TMVA (Toolkit for Multivariate Data Analysis) in ROOT. SOFIE offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. Using the IR, SOFIE can produce C++ header files that can be seamlessly included and used in a ‘plug-and-go’ style.

          SOFIE currently supports various Machine Learning operators defined by the ONNX standards, as well as a Graph Neural Network (GNN) implementation. It supports the parsing and inference of Graph Neural Networks trained using DeepMind Graph Nets.

          As SOFIE continues to evolve, this project aims to:

          Enhance the Keras parser to support models trained in the latest TensorFlow v2.18.0, which introduces NumPy 2.0 compatibility.
          Integrate JAX/FLAX support, enabling SOFIE to generate C++ inference functions for models developed using JAX/FLAX.
          Task ideas
          In this project, the contributor will gain experience with C++ and Python programming, TensorFlow/Keras and its storage formats for trained machine learning models, and JAX/FLAX for accelerated machine learning. They will begin by familiarizing themselves with SOFIE and its Keras parser. After researching the changes required to support the latest TensorFlow version, they will implement functionalities to ensure the successful generation of inference code for the latest Keras models. In the next phase, they will explore the JAX/FLAX library and investigate its potential integration with SOFIE.

          Expected results and milestones
          Familiarization with TMVA SOFIE: Understand the SOFIE architecture, run inference using the existing Keras parser, and analyze the current parser’s capabilities.
          Researching latest TensorFlow/Keras: Investigate the latest TensorFlow/Keras developments and assess their alignment with SOFIE.
          Improving the Keras Parser: Implement parser enhancements to support the latest TensorFlow version and validate inference results.
          JAX/FLAX Integration: Design and develop a parsing mechanism for JAX/FLAX models, ensuring compatibility with SOFIE’s IR and further generation of inference code.
          Requirements
          Proficiency in C++ and Python.
          Knowledge of TensorFlow/Keras and JAX/FLAX.
          Familiarity with version control systems like Git/GitHub.
          Links
          ROOT Project homepage
          ROOT Project repository
          SOFIE Repository
          Keras: The high-level API for TensorFlow
          JAX Documentation
          FLAX Documentation
          Mentors
          Lorenzo Moneta - CERN
          Sanjiban Sengupta - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: Flexible
          Corresponding Project
          ML4EP
          Participating Organizations
          CERN

          ~~~~~~~~~~

          Implementing Debugging Support
          Description
          xeus-cpp is an interactive execution environment for C++ in Jupyter notebooks, built on the Clang-Repl C++ interpreter, provided by CppInterOp. While xeus-cpp enables a seamless workflow for running C++ code interactively, the lack of an integrated debugging experience remains a gap, especially when dealing with code that is dynamically compiled and executed through LLVM’s JIT(Just-In-Time) infrastructure.

          Jupyter’s debugging system follows the Debug Adapter Protocol (DAP), enabling seamless integration of debuggers into interactive kernels. Existing Jupyter kernels, such as the IPython & the xeus-python kernel, have successfully implemented debugging workflows that support breakpoints, variable inspection, and execution control, even in dynamically executed environments. These implementations address challenges such as symbol resolution and source mapping for dynamically generated code, ensuring that debugging within Jupyter remains intuitive and user-friendly.

          However, debugging C++ inside an interactive environment presents unique challenges, particularly due to Clang-Repl’s use of LLVM’s ORC JIT to compile and execute code dynamically. To integrate debugging into xeus-cpp, the project will explore existing solutions for DAP implementations like lldb_dap and debuggers like lldb that can interface with Jupyter while effectively supporting the execution model of Clang-Repl.

          Project Milestones
          Seamless debugging integration, establishing reliable interactions between xeus-cpp, a Debug Adapter Protocol (DAP) implementation, and a debugger.
          Implement a testing framework through xeus-zmq to thoroughly test the debugger. This can be inspired by an existing implementation in xeus-python.
          Present the work at the relevant meetings and conferences.
          Requirements
          C/C++
          Basic understanding of the Debug Adapter Protocol
          Basic understanding of the stack used by xeus-cpp: xeus, cppinterop, clang-repl
          Research on different DAP implementations like lldb_dap and debuggers like lldb/gdb that can be utilized for the project.
          Links
          Repo
          Debug Adaptor Protocol
          Debugging support through Jupyter:
          https://jupyterlab.readthedocs.io/en/stable/user/debugger.html
          https://jupyter-client.readthedocs.io/en/latest/messaging.html#debug-request
          Mentors
          Anutosh Bhat - QuantStack
          Johan Mabille - QuantStack
          Vipul Cariappa - CompRes
          Aaron Jomy - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Xeus-Cpp
          Participating Organizations
          CompRes

          ~~~~~~~~~~

          Enable GPU support and Python Interoperability via a Plugin System
          Description
          Xeus-Cpp integrates Clang-Repl with the xeus protocol via CppInterOp, providing a powerful platform for C++ development within Jupyter Notebooks.

          This project aims to introduce a plugin system for magic commands (cell, line, etc.), enabling a more modular and maintainable approach to extend Xeus-Cpp. Traditionally, magic commands introduce additional code and dependencies directly into the Xeus-Cpp kernel, increasing its complexity and maintenance burden. By offloading this functionality to a dedicated plugin library, we can keep the core kernel minimal while ensuring extensibility. This approach allows new magic commands to be developed, packaged, and deployed independently—eliminating the need to rebuild and release Xeus-Cpp for each new addition. Initial groundwork has already been laid with the Xplugin library, and this project will build upon that foundation. The goal is to clearly define magic command compatibility across different platforms while ensuring seamless integration. A key objective is to reimplement existing features, such as the LLM cell magic and the in-development Python magic, as plugins. This will not only improve modularity within Xeus-Cpp but also enable these features to be used in other Jupyter kernels.

          As an extended goal, we aim to develop a new plugin for GPU execution, leveraging CUDA or OpenMP to support high-performance computing workflows within Jupyter.

          Project Milestones
          Move the currently implemented magics and reframe using xplugin
          Complete the on-going work on the Python interoperability magic
          Implement a test suite for the plugins
          Extended: To be able to execute on GPU using CUDA or OpenMP
          Optional: Extend the magics for the wasm use case (xeus-cpp-lite)
          Present the work at the relevant meetings and conferences
          Requirements
          Python
          C/C++
          GPU programming; CUDA/OpenMP
          Links
          Repo
          Related Issues:
          https://github.com/compiler-research/xeus-cpp/issues/4
          https://github.com/compiler-research/xeus-cpp/issues/140
          Mentors
          Anutosh Bhat - QuantStack
          Johan Mabille - QuantStack
          Vipul Cariappa - CompRes
          Aaron Jomy - CERN
          Additional Information
          Difficulty level (low / medium / high): medium
          Duration: 350 hours
          Mentor availability: June-October
          Corresponding Project
          Xeus-Cpp
          Participating Organizations
          CompRes

          
    totalCharacters_of_ideas_content_parent: 
    totalwords_of_ideas_content_parent: 
    totalTokenCount_of_ideas_content_parent: 
    gsocorganization_dev_url: https://www.gsocorganizations.dev/organization/cern-hsf/
    idea_list_url: https://hepsoftwarefoundation.org/gsoc/2025/summary.html